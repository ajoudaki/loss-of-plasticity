--- Processing: models/mlp.py ---
"""
Customizable MLP model for continual learning experiments.
"""

import torch
import torch.nn as nn
from .layers import get_activation, get_normalization

class CustomizableMLP(nn.Module):
    def __init__(self, 
                 input_size=784, 
                 hidden_sizes=[512, 256, 128], 
                 output_size=10, 
                 activation='relu',
                 dropout_p=0.0,
                 normalization=None,
                 norm_after_activation=False,
                 bias=True,
                 record_activations=False):
        """
        Fully customizable MLP that supports various activations and normalizations.
        
        Parameters:
            input_size (int): Dimensionality of input features
            hidden_sizes (list): List of hidden layer dimensions
            output_size (int): Number of output classes
            activation (str): Activation function to use ('relu', 'tanh', 'sigmoid', etc.)
            dropout_p (float): Dropout probability (0 to disable)
            normalization (str): Normalization to use ('batch', 'layer', None)
            norm_after_activation (bool): If True, apply normalization after activation
            bias (bool): Whether to include bias terms in linear layers
            record_activations (bool): Whether to store activations for analysis
        """
        super(CustomizableMLP, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.record_activations = record_activations
        
        # Build network
        layers = []
        in_features = input_size
        
        # Store module references for easier access
        self.linear_layers = nn.ModuleList()
        self.activation_layers = nn.ModuleList()
        self.norm_layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        for hidden_size in hidden_sizes:
            # Linear layer
            linear = nn.Linear(in_features, hidden_size, bias=bias)
            layers.append(linear)
            self.linear_layers.append(linear)
            
            # Activation
            act_layer = get_activation(activation)
            self.activation_layers.append(act_layer)
            
            # Normalization and order
            norm_layer = get_normalization(normalization, hidden_size) if normalization else None
            self.norm_layers.append(norm_layer)
            
            if norm_after_activation:
                layers.append(act_layer)
                if norm_layer:
                    layers.append(norm_layer)
            else:
                if norm_layer:
                    layers.append(norm_layer)
                layers.append(act_layer)
            
            # Dropout
            if dropout_p > 0:
                dropout = nn.Dropout(dropout_p)
                layers.append(dropout)
                self.dropout_layers.append(dropout)
            else:
                self.dropout_layers.append(None)
            
            in_features = hidden_size
        
        # Output layer
        self.output_layer = nn.Linear(in_features, output_size, bias=bias)
        layers.append(self.output_layer)
        
        # Sequential container
        self.layers = nn.Sequential(*layers)
        
        # Activation storage
        self.stored_activations = {}
        
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data with shape [batch_size, input_size]
            store_activations (bool): Whether to store activations from this pass
        
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Flatten input if needed
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
        
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
            
        if should_store:
            activations['input'] = x
        
        # Apply all layers except the output layer
        h = x
        for idx, layer in enumerate(self.layers[:-1]):  # exclude output layer
            h = layer(h)
            # Store activations at meaningful points (after each block)
            if should_store and isinstance(layer, nn.Linear):
                activations[f'linear_{idx}'] = h.detach().clone()
                
            # After full block (activation & normalization)
            if should_store and (
                isinstance(layer, nn.ReLU) or 
                isinstance(layer, nn.Tanh) or 
                isinstance(layer, nn.Sigmoid)):
                activations[f'activation_{idx}'] = h.detach().clone()
        
        # Output layer
        output = self.layers[-1](h)
        
        if should_store:
            activations['output'] = output
            self.stored_activations = activations
            return output, activations
        
        return output
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/vit.py ---
"""
Vision Transformer (ViT) model for continual learning experiments.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .layers import get_activation, get_normalization, PatchEmbedding

class Attention(nn.Module):
    """Multi-head attention module."""
    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % n_heads == 0
        self.n_heads = n_heads
        head_dim = dim // n_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        # Store attention maps
        self.attention_maps = None

    def forward(self, x, store_attention=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        if store_attention:
            self.attention_maps = attn.detach().clone()
            
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
    
    def get_attention_maps(self):
        return self.attention_maps


class MLP(nn.Module):
    """MLP module with configurable activation."""
    def __init__(self, in_features, hidden_features, out_features, 
                 activation='gelu', drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = get_activation(activation)
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class TransformerBlock(nn.Module):
    """Transformer block with customizable components."""
    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., 
                 attn_drop=0., activation='gelu', normalization='layer'):
        super().__init__()
        self.norm1 = get_normalization(normalization, dim)
        self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, 
                              attn_drop=attn_drop, proj_drop=drop)
        self.norm2 = get_normalization(normalization, dim)
        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, 
                      activation=activation, drop=drop)
        
        # Activations storage
        self.stored_activations = {}

    def forward(self, x, store_activations=False):
        # Store input
        if store_activations:
            self.stored_activations['input'] = x.detach().clone()
        
        # Self-attention
        norm_x = self.norm1(x)
        if store_activations:
            self.stored_activations['norm1'] = norm_x.detach().clone()
            
        attn_out = self.attn(norm_x, store_attention=store_activations)
        if store_activations:
            self.stored_activations['attn_out'] = attn_out.detach().clone()
            self.stored_activations['attn_maps'] = self.attn.get_attention_maps()
            
        x = x + attn_out
        if store_activations:
            self.stored_activations['post_attn'] = x.detach().clone()
        
        # MLP
        norm_x = self.norm2(x)
        if store_activations:
            self.stored_activations['norm2'] = norm_x.detach().clone()
            
        mlp_out = self.mlp(norm_x)
        if store_activations:
            self.stored_activations['mlp_out'] = mlp_out.detach().clone()
            
        x = x + mlp_out
        if store_activations:
            self.stored_activations['output'] = x.detach().clone()
            
        return x
    
    def get_activations(self):
        return self.stored_activations


class VisionTransformer(nn.Module):
    """
    Vision Transformer (ViT) model with configurable architecture.
    """
    def __init__(self, 
                 img_size=32, 
                 patch_size=4, 
                 in_channels=3, 
                 num_classes=10, 
                 embed_dim=192,
                 depth=12, 
                 n_heads=8, 
                 mlp_ratio=4., 
                 qkv_bias=True, 
                 drop_rate=0.1,
                 attn_drop_rate=0.0,
                 activation='gelu',
                 normalization='layer',
                 record_activations=False):
        """
        Initialize Vision Transformer.
        
        Parameters:
            img_size (int): Input image size
            patch_size (int): Patch size for splitting image
            in_channels (int): Number of image channels
            num_classes (int): Number of output classes
            embed_dim (int): Embedding dimension
            depth (int): Number of transformer blocks
            n_heads (int): Number of attention heads
            mlp_ratio (float): Ratio for MLP hidden dimension
            qkv_bias (bool): Whether to use bias in QKV projection
            drop_rate (float): Dropout rate
            attn_drop_rate (float): Attention dropout rate
            activation (str): Activation function to use
            normalization (str): Normalization method to use
            record_activations (bool): Whether to store activations
        """
        super().__init__()
        self.record_activations = record_activations
        
        # Patch embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        n_patches = self.patch_embed.n_patches

        # Class token and position embeddings
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(drop_rate)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, n_heads, mlp_ratio, qkv_bias, 
                           drop_rate, attn_drop_rate, activation, normalization)
            for _ in range(depth)
        ])

        # Final normalization and classifier head
        self.norm = get_normalization(normalization, embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

        # Initialize weights
        self._init_weights()
        
        # Activation storage
        self.stored_activations = {}

    def _init_weights(self):
        # Initialize position embedding and class token
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        
        # Initialize other weights
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input images [batch_size, channels, height, width]
            store_activations (bool): Whether to store activations
            
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x.detach().clone()
        
        # Patch embedding
        x = self.patch_embed(x)
        if should_store:
            activations['patch_embed'] = x.detach().clone()
        
        # Add class token
        B = x.shape[0]
        cls_token = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        
        # Add position embeddings
        x = x + self.pos_embed
        if should_store:
            activations['pos_embed'] = x.detach().clone()
            
        x = self.pos_drop(x)

        # Apply transformer blocks
        for i, block in enumerate(self.blocks):
            x = block(x, store_activations=should_store)
            if should_store:
                activations[f'block_{i}_output'] = x.detach().clone()
                if store_activations:
                    block_acts = block.get_activations()
                    for k, v in block_acts.items():
                        activations[f'block_{i}_{k}'] = v
        
        # Final normalization
        x = self.norm(x)
        if should_store:
            activations['final_norm'] = x.detach().clone()
        
        # Extract class token and classify
        x = x[:, 0]  # Use only the cls token for classification
        if should_store:
            activations['cls_token'] = x.detach().clone()
            
        x = self.head(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
            
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/layers.py ---
"""
Custom layers and utilities for model construction.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def get_activation(activation_name):
    """
    Returns the activation function based on name
    
    Parameters:
        activation_name (str): Name of the activation function
        
    Returns:
        nn.Module: PyTorch activation module
    """
    activations = {
        'relu': nn.ReLU(),
        'leaky_relu': nn.LeakyReLU(0.1),
        'tanh': nn.Tanh(),
        'sigmoid': nn.Sigmoid(),
        'gelu': nn.GELU(),
        'elu': nn.ELU(),
        'selu': nn.SELU(),
        'none': nn.Identity()
    }
    
    if activation_name.lower() not in activations:
        raise ValueError(f"Activation {activation_name} not supported. "
                         f"Choose from: {list(activations.keys())}")
    
    return activations[activation_name.lower()]

def get_normalization(norm_name, num_features):
    """
    Returns the normalization layer based on name
    
    Parameters:
        norm_name (str): Name of the normalization ('batch', 'layer', etc.)
        num_features (int): Number of features for the normalization layer
        
    Returns:
        nn.Module: PyTorch normalization module or None
    """
    if norm_name is None:
        return None
        
    normalizations = {
        'batch': nn.BatchNorm1d(num_features),
        'layer': nn.LayerNorm(num_features),
        'instance': nn.InstanceNorm1d(num_features),
        'group': nn.GroupNorm(min(32, num_features), num_features),
        'none': None
    }
    
    norm_key = str(norm_name).lower()
    if norm_key not in normalizations:
        raise ValueError(f"Normalization {norm_name} not supported. "
                         f"Choose from: {list(normalizations.keys())}")
    
    return normalizations[norm_key]


class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization from the paper:
    "Root Mean Square Layer Normalization"
    https://arxiv.org/abs/1910.07467
    """
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(dim))
        self.eps = eps
        
    def forward(self, x):
        # Calculate RMS
        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)
        x_normalized = x / rms
        # Scale
        return self.scale * x_normalized


class PatchEmbedding(nn.Module):
    """
    Image to Patch Embedding for Vision Transformer
    """
    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        self.proj = nn.Conv2d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, x):
        x = self.proj(x)  # (B, E, H', W')
        # Rearrange to sequence of patches: [B, C, H, W] -> [B, N, C]
        B, C, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)  # (B, N, C)
        return x

================================================================================

--- Processing: models/cnn.py ---
"""
Customizable CNN model for continual learning experiments.
"""

import torch
import torch.nn as nn
from .layers import get_activation, get_normalization

class ConfigurableCNN(nn.Module):
    def __init__(self, 
                 in_channels=3,
                 conv_channels=[64, 128, 256], 
                 kernel_sizes=[3, 3, 3],
                 strides=[1, 1, 1],
                 paddings=[1, 1, 1],
                 fc_hidden_units=[512],
                 num_classes=10, 
                 input_size=32,
                 activation='relu',
                 dropout_p=0.0,
                 pool_type='max',
                 pool_size=2,
                 use_batchnorm=True,
                 norm_after_activation=False,
                 record_activations=False):
        """
        Customizable CNN with configurable layers, activations, and normalizations.
        
        Parameters:
            in_channels (int): Number of input channels (3 for RGB images)
            conv_channels (list): List of convolutional layer output channels
            kernel_sizes (list): List of kernel sizes for each conv layer
            strides (list): List of stride values for each conv layer
            paddings (list): List of padding values for each conv layer
            fc_hidden_units (list): List of hidden units for fully connected layers
            num_classes (int): Number of output classes
            input_size (int): Height/width of the input images (assumed square)
            activation (str): Activation function to use ('relu', 'tanh', etc.)
            dropout_p (float): Dropout probability (0 to disable)
            pool_type (str): Type of pooling ('max', 'avg', or None)
            pool_size (int): Size of the pooling window
            use_batchnorm (bool): Whether to use batch normalization
            norm_after_activation (bool): Apply normalization after activation
            record_activations (bool): Whether to store activations for analysis
        """
        super(ConfigurableCNN, self).__init__()
        
        self.record_activations = record_activations
        
        # Check if input lists are of the same length
        assert len(conv_channels) == len(kernel_sizes) == len(strides) == len(paddings), \
            "Convolutional parameters (channels, kernels, strides, paddings) must have the same length"
        
        # Conv layers
        self.conv_layers = nn.ModuleList()
        self.norm_layers = nn.ModuleList()
        self.activation_layers = nn.ModuleList()
        
        channels = in_channels
        
        for i, (out_channels, kernel_size, stride, padding) in enumerate(
                zip(conv_channels, kernel_sizes, strides, paddings)):
            # Conv layer
            conv = nn.Conv2d(channels, out_channels, kernel_size, stride, padding)
            self.conv_layers.append(conv)
            
            # Normalization
            if use_batchnorm:
                norm = nn.BatchNorm2d(out_channels)
                self.norm_layers.append(norm)
            else:
                self.norm_layers.append(None)
            
            # Activation
            act = get_activation(activation)
            self.activation_layers.append(act)
            
            channels = out_channels
        
        # Pooling
        if pool_type == 'max':
            self.pool = nn.MaxPool2d(pool_size, pool_size)
        elif pool_type == 'avg':
            self.pool = nn.AvgPool2d(pool_size, pool_size)
        else:
            self.pool = nn.Identity()
        
        # Calculate the size after all pooling operations
        num_pools = len(conv_channels) if pool_type in ['max', 'avg'] else 0
        final_size = input_size // (pool_size ** num_pools)
        self.flattened_size = conv_channels[-1] * final_size * final_size
        
        # Fully connected layers
        self.fc_layers = nn.ModuleList()
        self.fc_activations = nn.ModuleList()
        self.fc_dropouts = nn.ModuleList()
        
        fc_input_size = self.flattened_size
        for hidden_units in fc_hidden_units:
            self.fc_layers.append(nn.Linear(fc_input_size, hidden_units))
            self.fc_activations.append(get_activation(activation))
            self.fc_dropouts.append(nn.Dropout(dropout_p) if dropout_p > 0 else None)
            fc_input_size = hidden_units
        
        # Output layer
        self.fc_output = nn.Linear(fc_input_size, num_classes)
        
        # Activation storage
        self.stored_activations = {}
    
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data [batch_size, in_channels, height, width]
            store_activations (bool): Whether to store activations from this pass
        
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x
        
        # Conv layers
        for i, (conv, norm, act) in enumerate(zip(self.conv_layers, self.norm_layers, self.activation_layers)):
            x = conv(x)
            if should_store:
                activations[f'conv_{i}'] = x.detach().clone()
            
            if norm and not self.norm_after_activation:
                x = norm(x)
                
            x = act(x)
            if should_store:
                activations[f'conv_act_{i}'] = x.detach().clone()
                
            if norm and self.norm_after_activation:
                x = norm(x)
                
            x = self.pool(x)
            if should_store:
                activations[f'pool_{i}'] = x.detach().clone()
        
        # Flatten
        x = x.view(x.size(0), -1)
        if should_store:
            activations['flatten'] = x.detach().clone()
        
        # FC layers
        for i, (fc, act, dropout) in enumerate(zip(self.fc_layers, self.fc_activations, self.fc_dropouts)):
            x = fc(x)
            if should_store:
                activations[f'fc_{i}'] = x.detach().clone()
                
            x = act(x)
            if should_store:
                activations[f'fc_act_{i}'] = x.detach().clone()
                
            if dropout:
                x = dropout(x)
        
        # Output layer
        x = self.fc_output(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
        
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/__init__.py ---
"""
Model definitions for continual learning experiments.
"""

from .mlp import CustomizableMLP
from .cnn import ConfigurableCNN
from .resnet import ConfigurableResNet
from .vit import VisionTransformer

__all__ = ['CustomizableMLP', 'ConfigurableCNN', 'ConfigurableResNet', 'VisionTransformer']

================================================================================

--- Processing: models/resnet.py ---
"""
Customizable ResNet model for continual learning experiments.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .layers import get_activation, get_normalization

class BasicBlock(nn.Module):
    """Basic ResNet block with customizable activation and normalization."""
    expansion = 1
    
    def __init__(self, in_planes, planes, stride=1, activation='relu', 
                 use_batchnorm=True, norm_after_activation=False, downsample=None):
        super(BasicBlock, self).__init__()
        
        self.norm_after_activation = norm_after_activation
        
        # First convolution
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not use_batchnorm)
        
        # Normalization for first conv
        self.bn1 = nn.BatchNorm2d(planes) if use_batchnorm else None
        
        # Activation
        self.activation = get_activation(activation)
        
        # Second convolution
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not use_batchnorm)
        
        # Normalization for second conv
        self.bn2 = nn.BatchNorm2d(planes) if use_batchnorm else None
        
        # Downsample if needed (for shortcut connection)
        self.downsample = downsample
        
        # Activations storage
        self.stored_activations = {}
        
    def forward(self, x, store_activations=False):
        identity = x
        
        # Apply conv1
        out = self.conv1(x)
        
        # Apply norm1 if needed (before activation)
        if self.bn1 and not self.norm_after_activation:
            out = self.bn1(out)
            
        if store_activations:
            self.stored_activations['pre_activation1'] = out.detach().clone()
        
        # Apply activation
        out = self.activation(out)
        
        if store_activations:
            self.stored_activations['post_activation1'] = out.detach().clone()
        
        # Apply norm1 if needed (after activation)
        if self.bn1 and self.norm_after_activation:
            out = self.bn1(out)
            
        # Apply conv2
        out = self.conv2(out)
        
        # Apply norm2 if needed (before activation)
        if self.bn2 and not self.norm_after_activation:
            out = self.bn2(out)
            
        if store_activations:
            self.stored_activations['pre_activation2'] = out.detach().clone()
        
        # Handle shortcut connection
        if self.downsample is not None:
            identity = self.downsample(x)
            
        # Add identity
        out += identity
        
        # Final activation
        out = self.activation(out)
        
        if store_activations:
            self.stored_activations['post_activation2'] = out.detach().clone()
            
        # Apply norm2 if needed (after activation)
        if self.bn2 and self.norm_after_activation:
            out = self.bn2(out)
            
        return out
        
    def get_activations(self):
        return self.stored_activations


class ConfigurableResNet(nn.Module):
    """
    Configurable ResNet architecture for continual learning experiments.
    """
    def __init__(self, 
                 block=BasicBlock,
                 layers=[2, 2, 2, 2],  # ResNet18 by default
                 num_classes=10,
                 in_channels=3,
                 base_channels=64,
                 activation='relu',
                 dropout_p=0.0,
                 use_batchnorm=True,
                 norm_after_activation=False,
                 record_activations=False):
        """
        Initialize the ResNet.
        
        Parameters:
            block (nn.Module): The block type to use (BasicBlock)
            layers (list): Number of blocks in each layer
            num_classes (int): Number of output classes
            in_channels (int): Number of input channels (3 for RGB images)
            base_channels (int): Base number of channels (first layer)
            activation (str): Activation function to use
            dropout_p (float): Dropout probability before final layer
            use_batchnorm (bool): Whether to use batch normalization
            norm_after_activation (bool): Apply normalization after activation
            record_activations (bool): Whether to store activations for analysis
        """
        super(ConfigurableResNet, self).__init__()
        
        self.record_activations = record_activations
        self.in_planes = base_channels
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1, bias=not use_batchnorm)
        
        # Batch norm after first conv
        self.bn1 = nn.BatchNorm2d(base_channels) if use_batchnorm else None
        
        # Activation
        self.activation = get_activation(activation)
        
        # ResNet layers
        self.layer1 = self._make_layer(block, base_channels, layers[0], stride=1, 
                                       activation=activation, use_batchnorm=use_batchnorm, 
                                       norm_after_activation=norm_after_activation)
        self.layer2 = self._make_layer(block, base_channels*2, layers[1], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm, 
                                       norm_after_activation=norm_after_activation)
        self.layer3 = self._make_layer(block, base_channels*4, layers[2], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm,
                                       norm_after_activation=norm_after_activation)
        self.layer4 = self._make_layer(block, base_channels*8, layers[3], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm,
                                       norm_after_activation=norm_after_activation)
        
        # Global average pooling and final classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(dropout_p) if dropout_p > 0 else None
        self.fc = nn.Linear(base_channels*8*block.expansion, num_classes)
        
        # Initialize weights
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
        # Activation storage
        self.stored_activations = {}
                
    def _make_layer(self, block, planes, blocks, stride=1, activation='relu', 
                    use_batchnorm=True, norm_after_activation=False):
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            layers = [nn.Conv2d(self.in_planes, planes * block.expansion, 
                               kernel_size=1, stride=stride, bias=not use_batchnorm)]
            if use_batchnorm:
                layers.append(nn.BatchNorm2d(planes * block.expansion))
            downsample = nn.Sequential(*layers)
        
        layers = []
        layers.append(block(self.in_planes, planes, stride, activation, 
                          use_batchnorm, norm_after_activation, downsample))
        self.in_planes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_planes, planes, 1, activation, 
                               use_batchnorm, norm_after_activation))
            
        return nn.ModuleList(layers)
        
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data
            store_activations (bool): Whether to store activations
            
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x
        
        # Initial conv + norm + activation
        x = self.conv1(x)
        
        if self.bn1 and not self.norm_after_activation:
            x = self.bn1(x)
            
        if should_store:
            activations['conv1'] = x.detach().clone()
            
        x = self.activation(x)
        
        if should_store:
            activations['conv1_act'] = x.detach().clone()
            
        if self.bn1 and self.norm_after_activation:
            x = self.bn1(x)
        
        # ResNet blocks
        for layer_idx, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):
            for block_idx, block in enumerate(layer):
                x = block(x, store_activations=should_store)
                if should_store:
                    # Store output of each block
                    activations[f'layer{layer_idx+1}_block{block_idx+1}'] = x.detach().clone()
                    # Store internal activations from the block if store_activations is True
                    if store_activations:
                        block_acts = block.get_activations()
                        for k, v in block_acts.items():
                            activations[f'layer{layer_idx+1}_block{block_idx+1}_{k}'] = v
        
        # Global average pooling
        x = self.avgpool(x)
        if should_store:
            activations['avgpool'] = x.detach().clone()
            
        x = torch.flatten(x, 1)
        if should_store:
            activations['flatten'] = x.detach().clone()
            
        # Dropout if specified
        if self.dropout:
            x = self.dropout(x)
            
        # Final classifier
        x = self.fc(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
            
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/.ipynb_checkpoints/vit-checkpoint.py ---
"""
Vision Transformer (ViT) model for continual learning experiments.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .layers import get_activation, get_normalization, PatchEmbedding

class Attention(nn.Module):
    """Multi-head attention module."""
    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % n_heads == 0
        self.n_heads = n_heads
        head_dim = dim // n_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        
        # Store attention maps
        self.attention_maps = None

    def forward(self, x, store_attention=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        
        if store_attention:
            self.attention_maps = attn.detach().clone()
            
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
    
    def get_attention_maps(self):
        return self.attention_maps


class MLP(nn.Module):
    """MLP module with configurable activation."""
    def __init__(self, in_features, hidden_features, out_features, 
                 activation='gelu', drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = get_activation(activation)
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class TransformerBlock(nn.Module):
    """Transformer block with customizable components."""
    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., 
                 attn_drop=0., activation='gelu', normalization='layer'):
        super().__init__()
        self.norm1 = get_normalization(normalization, dim)
        self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, 
                              attn_drop=attn_drop, proj_drop=drop)
        self.norm2 = get_normalization(normalization, dim)
        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, 
                      activation=activation, drop=drop)
        
        # Activations storage
        self.stored_activations = {}

    def forward(self, x, store_activations=False):
        # Store input
        if store_activations:
            self.stored_activations['input'] = x.detach().clone()
        
        # Self-attention
        norm_x = self.norm1(x)
        if store_activations:
            self.stored_activations['norm1'] = norm_x.detach().clone()
            
        attn_out = self.attn(norm_x, store_attention=store_activations)
        if store_activations:
            self.stored_activations['attn_out'] = attn_out.detach().clone()
            self.stored_activations['attn_maps'] = self.attn.get_attention_maps()
            
        x = x + attn_out
        if store_activations:
            self.stored_activations['post_attn'] = x.detach().clone()
        
        # MLP
        norm_x = self.norm2(x)
        if store_activations:
            self.stored_activations['norm2'] = norm_x.detach().clone()
            
        mlp_out = self.mlp(norm_x)
        if store_activations:
            self.stored_activations['mlp_out'] = mlp_out.detach().clone()
            
        x = x + mlp_out
        if store_activations:
            self.stored_activations['output'] = x.detach().clone()
            
        return x
    
    def get_activations(self):
        return self.stored_activations


class VisionTransformer(nn.Module):
    """
    Vision Transformer (ViT) model with configurable architecture.
    """
    def __init__(self, 
                 img_size=32, 
                 patch_size=4, 
                 in_channels=3, 
                 num_classes=10, 
                 embed_dim=192,
                 depth=12, 
                 n_heads=8, 
                 mlp_ratio=4., 
                 qkv_bias=True, 
                 drop_rate=0.1,
                 attn_drop_rate=0.0,
                 activation='gelu',
                 normalization='layer',
                 record_activations=False):
        """
        Initialize Vision Transformer.
        
        Parameters:
            img_size (int): Input image size
            patch_size (int): Patch size for splitting image
            in_channels (int): Number of image channels
            num_classes (int): Number of output classes
            embed_dim (int): Embedding dimension
            depth (int): Number of transformer blocks
            n_heads (int): Number of attention heads
            mlp_ratio (float): Ratio for MLP hidden dimension
            qkv_bias (bool): Whether to use bias in QKV projection
            drop_rate (float): Dropout rate
            attn_drop_rate (float): Attention dropout rate
            activation (str): Activation function to use
            normalization (str): Normalization method to use
            record_activations (bool): Whether to store activations
        """
        super().__init__()
        self.record_activations = record_activations
        
        # Patch embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        n_patches = self.patch_embed.n_patches

        # Class token and position embeddings
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(drop_rate)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(embed_dim, n_heads, mlp_ratio, qkv_bias, 
                           drop_rate, attn_drop_rate, activation, normalization)
            for _ in range(depth)
        ])

        # Final normalization and classifier head
        self.norm = get_normalization(normalization, embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

        # Initialize weights
        self._init_weights()
        
        # Activation storage
        self.stored_activations = {}

    def _init_weights(self):
        # Initialize position embedding and class token
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        
        # Initialize other weights
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input images [batch_size, channels, height, width]
            store_activations (bool): Whether to store activations
            
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x.detach().clone()
        
        # Patch embedding
        x = self.patch_embed(x)
        if should_store:
            activations['patch_embed'] = x.detach().clone()
        
        # Add class token
        B = x.shape[0]
        cls_token = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        
        # Add position embeddings
        x = x + self.pos_embed
        if should_store:
            activations['pos_embed'] = x.detach().clone()
            
        x = self.pos_drop(x)

        # Apply transformer blocks
        for i, block in enumerate(self.blocks):
            x = block(x, store_activations=should_store)
            if should_store:
                activations[f'block_{i}_output'] = x.detach().clone()
                if store_activations:
                    block_acts = block.get_activations()
                    for k, v in block_acts.items():
                        activations[f'block_{i}_{k}'] = v
        
        # Final normalization
        x = self.norm(x)
        if should_store:
            activations['final_norm'] = x.detach().clone()
        
        # Extract class token and classify
        x = x[:, 0]  # Use only the cls token for classification
        if should_store:
            activations['cls_token'] = x.detach().clone()
            
        x = self.head(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
            
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/.ipynb_checkpoints/resnet-checkpoint.py ---
"""
Customizable ResNet model for continual learning experiments.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .layers import get_activation, get_normalization

class BasicBlock(nn.Module):
    """Basic ResNet block with customizable activation and normalization."""
    expansion = 1
    
    def __init__(self, in_planes, planes, stride=1, activation='relu', 
                 use_batchnorm=True, norm_after_activation=False, downsample=None):
        super(BasicBlock, self).__init__()
        
        self.norm_after_activation = norm_after_activation
        
        # First convolution
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=not use_batchnorm)
        
        # Normalization for first conv
        self.bn1 = nn.BatchNorm2d(planes) if use_batchnorm else None
        
        # Activation
        self.activation = get_activation(activation)
        
        # Second convolution
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=not use_batchnorm)
        
        # Normalization for second conv
        self.bn2 = nn.BatchNorm2d(planes) if use_batchnorm else None
        
        # Downsample if needed (for shortcut connection)
        self.downsample = downsample
        
        # Activations storage
        self.stored_activations = {}
        
    def forward(self, x, store_activations=False):
        identity = x
        
        # Apply conv1
        out = self.conv1(x)
        
        # Apply norm1 if needed (before activation)
        if self.bn1 and not self.norm_after_activation:
            out = self.bn1(out)
            
        if store_activations:
            self.stored_activations['pre_activation1'] = out.detach().clone()
        
        # Apply activation
        out = self.activation(out)
        
        if store_activations:
            self.stored_activations['post_activation1'] = out.detach().clone()
        
        # Apply norm1 if needed (after activation)
        if self.bn1 and self.norm_after_activation:
            out = self.bn1(out)
            
        # Apply conv2
        out = self.conv2(out)
        
        # Apply norm2 if needed (before activation)
        if self.bn2 and not self.norm_after_activation:
            out = self.bn2(out)
            
        if store_activations:
            self.stored_activations['pre_activation2'] = out.detach().clone()
        
        # Handle shortcut connection
        if self.downsample is not None:
            identity = self.downsample(x)
            
        # Add identity
        out += identity
        
        # Final activation
        out = self.activation(out)
        
        if store_activations:
            self.stored_activations['post_activation2'] = out.detach().clone()
            
        # Apply norm2 if needed (after activation)
        if self.bn2 and self.norm_after_activation:
            out = self.bn2(out)
            
        return out
        
    def get_activations(self):
        return self.stored_activations


class ConfigurableResNet(nn.Module):
    """
    Configurable ResNet architecture for continual learning experiments.
    """
    def __init__(self, 
                 block=BasicBlock,
                 layers=[2, 2, 2, 2],  # ResNet18 by default
                 num_classes=10,
                 in_channels=3,
                 base_channels=64,
                 activation='relu',
                 dropout_p=0.0,
                 use_batchnorm=True,
                 norm_after_activation=False,
                 record_activations=False):
        """
        Initialize the ResNet.
        
        Parameters:
            block (nn.Module): The block type to use (BasicBlock)
            layers (list): Number of blocks in each layer
            num_classes (int): Number of output classes
            in_channels (int): Number of input channels (3 for RGB images)
            base_channels (int): Base number of channels (first layer)
            activation (str): Activation function to use
            dropout_p (float): Dropout probability before final layer
            use_batchnorm (bool): Whether to use batch normalization
            norm_after_activation (bool): Apply normalization after activation
            record_activations (bool): Whether to store activations for analysis
        """
        super(ConfigurableResNet, self).__init__()
        
        self.record_activations = record_activations
        self.in_planes = base_channels
        
        # Initial convolutional layer
        self.conv1 = nn.Conv2d(in_channels, base_channels, kernel_size=3, stride=1, padding=1, bias=not use_batchnorm)
        
        # Batch norm after first conv
        self.bn1 = nn.BatchNorm2d(base_channels) if use_batchnorm else None
        
        # Activation
        self.activation = get_activation(activation)
        
        # ResNet layers
        self.layer1 = self._make_layer(block, base_channels, layers[0], stride=1, 
                                       activation=activation, use_batchnorm=use_batchnorm, 
                                       norm_after_activation=norm_after_activation)
        self.layer2 = self._make_layer(block, base_channels*2, layers[1], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm, 
                                       norm_after_activation=norm_after_activation)
        self.layer3 = self._make_layer(block, base_channels*4, layers[2], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm,
                                       norm_after_activation=norm_after_activation)
        self.layer4 = self._make_layer(block, base_channels*8, layers[3], stride=2, 
                                       activation=activation, use_batchnorm=use_batchnorm,
                                       norm_after_activation=norm_after_activation)
        
        # Global average pooling and final classifier
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(dropout_p) if dropout_p > 0 else None
        self.fc = nn.Linear(base_channels*8*block.expansion, num_classes)
        
        # Initialize weights
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
                
        # Activation storage
        self.stored_activations = {}
                
    def _make_layer(self, block, planes, blocks, stride=1, activation='relu', 
                    use_batchnorm=True, norm_after_activation=False):
        downsample = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            layers = [nn.Conv2d(self.in_planes, planes * block.expansion, 
                               kernel_size=1, stride=stride, bias=not use_batchnorm)]
            if use_batchnorm:
                layers.append(nn.BatchNorm2d(planes * block.expansion))
            downsample = nn.Sequential(*layers)
        
        layers = []
        layers.append(block(self.in_planes, planes, stride, activation, 
                          use_batchnorm, norm_after_activation, downsample))
        self.in_planes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_planes, planes, 1, activation, 
                               use_batchnorm, norm_after_activation))
            
        return nn.ModuleList(layers)
        
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data
            store_activations (bool): Whether to store activations
            
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x
        
        # Initial conv + norm + activation
        x = self.conv1(x)
        
        if self.bn1 and not self.norm_after_activation:
            x = self.bn1(x)
            
        if should_store:
            activations['conv1'] = x.detach().clone()
            
        x = self.activation(x)
        
        if should_store:
            activations['conv1_act'] = x.detach().clone()
            
        if self.bn1 and self.norm_after_activation:
            x = self.bn1(x)
        
        # ResNet blocks
        for layer_idx, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):
            for block_idx, block in enumerate(layer):
                x = block(x, store_activations=should_store)
                if should_store:
                    # Store output of each block
                    activations[f'layer{layer_idx+1}_block{block_idx+1}'] = x.detach().clone()
                    # Store internal activations from the block if store_activations is True
                    if store_activations:
                        block_acts = block.get_activations()
                        for k, v in block_acts.items():
                            activations[f'layer{layer_idx+1}_block{block_idx+1}_{k}'] = v
        
        # Global average pooling
        x = self.avgpool(x)
        if should_store:
            activations['avgpool'] = x.detach().clone()
            
        x = torch.flatten(x, 1)
        if should_store:
            activations['flatten'] = x.detach().clone()
            
        # Dropout if specified
        if self.dropout:
            x = self.dropout(x)
            
        # Final classifier
        x = self.fc(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
            
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/.ipynb_checkpoints/cnn-checkpoint.py ---
"""
Customizable CNN model for continual learning experiments.
"""

import torch
import torch.nn as nn
from .layers import get_activation, get_normalization

class ConfigurableCNN(nn.Module):
    def __init__(self, 
                 in_channels=3,
                 conv_channels=[64, 128, 256], 
                 kernel_sizes=[3, 3, 3],
                 strides=[1, 1, 1],
                 paddings=[1, 1, 1],
                 fc_hidden_units=[512],
                 num_classes=10, 
                 input_size=32,
                 activation='relu',
                 dropout_p=0.0,
                 pool_type='max',
                 pool_size=2,
                 use_batchnorm=True,
                 norm_after_activation=False,
                 record_activations=False):
        """
        Customizable CNN with configurable layers, activations, and normalizations.
        
        Parameters:
            in_channels (int): Number of input channels (3 for RGB images)
            conv_channels (list): List of convolutional layer output channels
            kernel_sizes (list): List of kernel sizes for each conv layer
            strides (list): List of stride values for each conv layer
            paddings (list): List of padding values for each conv layer
            fc_hidden_units (list): List of hidden units for fully connected layers
            num_classes (int): Number of output classes
            input_size (int): Height/width of the input images (assumed square)
            activation (str): Activation function to use ('relu', 'tanh', etc.)
            dropout_p (float): Dropout probability (0 to disable)
            pool_type (str): Type of pooling ('max', 'avg', or None)
            pool_size (int): Size of the pooling window
            use_batchnorm (bool): Whether to use batch normalization
            norm_after_activation (bool): Apply normalization after activation
            record_activations (bool): Whether to store activations for analysis
        """
        super(ConfigurableCNN, self).__init__()
        
        self.record_activations = record_activations
        
        # Check if input lists are of the same length
        assert len(conv_channels) == len(kernel_sizes) == len(strides) == len(paddings), \
            "Convolutional parameters (channels, kernels, strides, paddings) must have the same length"
        
        # Conv layers
        self.conv_layers = nn.ModuleList()
        self.norm_layers = nn.ModuleList()
        self.activation_layers = nn.ModuleList()
        
        channels = in_channels
        
        for i, (out_channels, kernel_size, stride, padding) in enumerate(
                zip(conv_channels, kernel_sizes, strides, paddings)):
            # Conv layer
            conv = nn.Conv2d(channels, out_channels, kernel_size, stride, padding)
            self.conv_layers.append(conv)
            
            # Normalization
            if use_batchnorm:
                norm = nn.BatchNorm2d(out_channels)
                self.norm_layers.append(norm)
            else:
                self.norm_layers.append(None)
            
            # Activation
            act = get_activation(activation)
            self.activation_layers.append(act)
            
            channels = out_channels
        
        # Pooling
        if pool_type == 'max':
            self.pool = nn.MaxPool2d(pool_size, pool_size)
        elif pool_type == 'avg':
            self.pool = nn.AvgPool2d(pool_size, pool_size)
        else:
            self.pool = nn.Identity()
        
        # Calculate the size after all pooling operations
        num_pools = len(conv_channels) if pool_type in ['max', 'avg'] else 0
        final_size = input_size // (pool_size ** num_pools)
        self.flattened_size = conv_channels[-1] * final_size * final_size
        
        # Fully connected layers
        self.fc_layers = nn.ModuleList()
        self.fc_activations = nn.ModuleList()
        self.fc_dropouts = nn.ModuleList()
        
        fc_input_size = self.flattened_size
        for hidden_units in fc_hidden_units:
            self.fc_layers.append(nn.Linear(fc_input_size, hidden_units))
            self.fc_activations.append(get_activation(activation))
            self.fc_dropouts.append(nn.Dropout(dropout_p) if dropout_p > 0 else None)
            fc_input_size = hidden_units
        
        # Output layer
        self.fc_output = nn.Linear(fc_input_size, num_classes)
        
        # Activation storage
        self.stored_activations = {}
    
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data [batch_size, in_channels, height, width]
            store_activations (bool): Whether to store activations from this pass
        
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
        
        if should_store:
            activations['input'] = x
        
        # Conv layers
        for i, (conv, norm, act) in enumerate(zip(self.conv_layers, self.norm_layers, self.activation_layers)):
            x = conv(x)
            if should_store:
                activations[f'conv_{i}'] = x.detach().clone()
            
            if norm and not self.norm_after_activation:
                x = norm(x)
                
            x = act(x)
            if should_store:
                activations[f'conv_act_{i}'] = x.detach().clone()
                
            if norm and self.norm_after_activation:
                x = norm(x)
                
            x = self.pool(x)
            if should_store:
                activations[f'pool_{i}'] = x.detach().clone()
        
        # Flatten
        x = x.view(x.size(0), -1)
        if should_store:
            activations['flatten'] = x.detach().clone()
        
        # FC layers
        for i, (fc, act, dropout) in enumerate(zip(self.fc_layers, self.fc_activations, self.fc_dropouts)):
            x = fc(x)
            if should_store:
                activations[f'fc_{i}'] = x.detach().clone()
                
            x = act(x)
            if should_store:
                activations[f'fc_act_{i}'] = x.detach().clone()
                
            if dropout:
                x = dropout(x)
        
        # Output layer
        x = self.fc_output(x)
        if should_store:
            activations['output'] = x.detach().clone()
            self.stored_activations = activations
            return x, activations
        
        return x
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/.ipynb_checkpoints/__init__-checkpoint.py ---
"""
Model definitions for continual learning experiments.
"""

from .mlp import CustomizableMLP
from .cnn import ConfigurableCNN
from .resnet import ConfigurableResNet
from .vit import VisionTransformer

__all__ = ['CustomizableMLP', 'ConfigurableCNN', 'ConfigurableResNet', 'VisionTransformer']

================================================================================

--- Processing: models/.ipynb_checkpoints/mlp-checkpoint.py ---
"""
Customizable MLP model for continual learning experiments.
"""

import torch
import torch.nn as nn
from .layers import get_activation, get_normalization

class CustomizableMLP(nn.Module):
    def __init__(self, 
                 input_size=784, 
                 hidden_sizes=[512, 256, 128], 
                 output_size=10, 
                 activation='relu',
                 dropout_p=0.0,
                 normalization=None,
                 norm_after_activation=False,
                 bias=True,
                 record_activations=False):
        """
        Fully customizable MLP that supports various activations and normalizations.
        
        Parameters:
            input_size (int): Dimensionality of input features
            hidden_sizes (list): List of hidden layer dimensions
            output_size (int): Number of output classes
            activation (str): Activation function to use ('relu', 'tanh', 'sigmoid', etc.)
            dropout_p (float): Dropout probability (0 to disable)
            normalization (str): Normalization to use ('batch', 'layer', None)
            norm_after_activation (bool): If True, apply normalization after activation
            bias (bool): Whether to include bias terms in linear layers
            record_activations (bool): Whether to store activations for analysis
        """
        super(CustomizableMLP, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.record_activations = record_activations
        
        # Build network
        layers = []
        in_features = input_size
        
        # Store module references for easier access
        self.linear_layers = nn.ModuleList()
        self.activation_layers = nn.ModuleList()
        self.norm_layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        for hidden_size in hidden_sizes:
            # Linear layer
            linear = nn.Linear(in_features, hidden_size, bias=bias)
            layers.append(linear)
            self.linear_layers.append(linear)
            
            # Activation
            act_layer = get_activation(activation)
            self.activation_layers.append(act_layer)
            
            # Normalization and order
            norm_layer = get_normalization(normalization, hidden_size) if normalization else None
            self.norm_layers.append(norm_layer)
            
            if norm_after_activation:
                layers.append(act_layer)
                if norm_layer:
                    layers.append(norm_layer)
            else:
                if norm_layer:
                    layers.append(norm_layer)
                layers.append(act_layer)
            
            # Dropout
            if dropout_p > 0:
                dropout = nn.Dropout(dropout_p)
                layers.append(dropout)
                self.dropout_layers.append(dropout)
            else:
                self.dropout_layers.append(None)
            
            in_features = hidden_size
        
        # Output layer
        self.output_layer = nn.Linear(in_features, output_size, bias=bias)
        layers.append(self.output_layer)
        
        # Sequential container
        self.layers = nn.Sequential(*layers)
        
        # Activation storage
        self.stored_activations = {}
        
    def forward(self, x, store_activations=False):
        """
        Forward pass with optional activation storage.
        
        Parameters:
            x (torch.Tensor): Input data with shape [batch_size, input_size]
            store_activations (bool): Whether to store activations from this pass
        
        Returns:
            torch.Tensor: Output logits
            dict (optional): Hidden activations if record_activations=True
        """
        # Flatten input if needed
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
        
        # Should we store activations for this pass?
        should_store = store_activations or self.record_activations
        activations = {} if should_store else None
            
        if should_store:
            activations['input'] = x
        
        # Apply all layers except the output layer
        h = x
        for idx, layer in enumerate(self.layers[:-1]):  # exclude output layer
            h = layer(h)
            # Store activations at meaningful points (after each block)
            if should_store and isinstance(layer, nn.Linear):
                activations[f'linear_{idx}'] = h.detach().clone()
                
            # After full block (activation & normalization)
            if should_store and (
                isinstance(layer, nn.ReLU) or 
                isinstance(layer, nn.Tanh) or 
                isinstance(layer, nn.Sigmoid)):
                activations[f'activation_{idx}'] = h.detach().clone()
        
        # Output layer
        output = self.layers[-1](h)
        
        if should_store:
            activations['output'] = output
            self.stored_activations = activations
            return output, activations
        
        return output
    
    def get_activations(self):
        """Returns stored activations from the last forward pass"""
        return self.stored_activations

================================================================================

--- Processing: models/.ipynb_checkpoints/layers-checkpoint.py ---
"""
Custom layers and utilities for model construction.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def get_activation(activation_name):
    """
    Returns the activation function based on name
    
    Parameters:
        activation_name (str): Name of the activation function
        
    Returns:
        nn.Module: PyTorch activation module
    """
    activations = {
        'relu': nn.ReLU(),
        'leaky_relu': nn.LeakyReLU(0.1),
        'tanh': nn.Tanh(),
        'sigmoid': nn.Sigmoid(),
        'gelu': nn.GELU(),
        'elu': nn.ELU(),
        'selu': nn.SELU(),
        'none': nn.Identity()
    }
    
    if activation_name.lower() not in activations:
        raise ValueError(f"Activation {activation_name} not supported. "
                         f"Choose from: {list(activations.keys())}")
    
    return activations[activation_name.lower()]

def get_normalization(norm_name, num_features):
    """
    Returns the normalization layer based on name
    
    Parameters:
        norm_name (str): Name of the normalization ('batch', 'layer', etc.)
        num_features (int): Number of features for the normalization layer
        
    Returns:
        nn.Module: PyTorch normalization module or None
    """
    if norm_name is None:
        return None
        
    normalizations = {
        'batch': nn.BatchNorm1d(num_features),
        'layer': nn.LayerNorm(num_features),
        'instance': nn.InstanceNorm1d(num_features),
        'group': nn.GroupNorm(min(32, num_features), num_features),
        'none': None
    }
    
    norm_key = str(norm_name).lower()
    if norm_key not in normalizations:
        raise ValueError(f"Normalization {norm_name} not supported. "
                         f"Choose from: {list(normalizations.keys())}")
    
    return normalizations[norm_key]


class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization from the paper:
    "Root Mean Square Layer Normalization"
    https://arxiv.org/abs/1910.07467
    """
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(dim))
        self.eps = eps
        
    def forward(self, x):
        # Calculate RMS
        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)
        x_normalized = x / rms
        # Scale
        return self.scale * x_normalized


class PatchEmbedding(nn.Module):
    """
    Image to Patch Embedding for Vision Transformer
    """
    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        self.proj = nn.Conv2d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, x):
        x = self.proj(x)  # (B, E, H', W')
        # Rearrange to sequence of patches: [B, C, H, W] -> [B, N, C]
        B, C, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)  # (B, N, C)
        return x

================================================================================

