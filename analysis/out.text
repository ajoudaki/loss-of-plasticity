--- Processing: continaul_learning.ipynb ---

Cell 1 (markdown):
# 
----------------------------------------

Cell 2 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import math

# -------------------------------
# 1. Data Preparation
# -------------------------------

# Transform: convert to tensor and normalize.
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Download CIFAR-10 training and test datasets.
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                             download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                            download=True, transform=transform)

# Helper function: Given a dataset and a list of target classes, return a Subset.
def get_subset_by_classes(dataset, class_list):
    indices = [i for i, (img, label) in enumerate(dataset) if label in class_list]
    return Subset(dataset, indices)

# Define our 5 tasks: each task is a pair of classes.
tasks = [
    [0, 1],  # airplane, automobile
    [2, 3],  # bird, cat
    [4, 5],  # deer, dog
    [6, 7],  # frog, horse
    [8, 9]   # ship, truck
]

# -------------------------------
# 2. Define the MLP and Linear Model
# -------------------------------

# MLP that returns both output and hidden layer activations
class MLP(nn.Module):
    def __init__(self, input_size=3*32*32, hidden_size=512, num_classes=10):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # Flatten the input: [batch, 3,32,32] -> [batch, 3072]
        x = x.view(x.size(0), -1)
        h1 = self.relu(self.fc1(x))
        h2 = self.relu(self.fc2(h1))
        out = self.fc3(h2)
        return out, h1, h2  # returning activations for layers fc1 and fc2

# A simple linear model (single linear layer)
class LinearModel(nn.Module):
    def __init__(self, input_size=3*32*32, num_classes=10):
        super(LinearModel, self).__init__()
        self.fc = nn.Linear(input_size, num_classes)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.fc(x)

# -------------------------------
# 3. Utility Functions for Continual Metrics
# -------------------------------

# Compute effective rank of a representation (assumes activations is 2D: [N, D])
def compute_effective_rank(activations):
    # activations: [N, D]
    # Use torch.svd (or torch.linalg.svd in recent versions)
    try:
        u, s, vh = torch.svd(activations)
    except RuntimeError:
        # Fall back to torch.linalg.svd if necessary
        s = torch.linalg.svdvals(activations)
    s_sum = torch.sum(s) + 1e-8
    p = s / s_sum
    erank = torch.exp(-torch.sum(p * torch.log(p + 1e-8)))
    return erank.item()

# Compute contribution utility for a layer.
# activations: [N, D] for the given layer.
# next_weight: weight matrix of the next layer, shape [out_dim, D].
def compute_contrib_utility(activations, next_weight):
    mean_abs = torch.mean(torch.abs(activations), dim=0)  # [D]
    sum_out = torch.sum(torch.abs(next_weight), dim=0)      # [D]
    return mean_abs * sum_out  # [D]

# Compute adaptation utility for a layer given its incoming weight matrix.
# weight_in: shape [D, in_features]
def compute_adaptation_utility(weight_in):
    sum_in = torch.sum(torch.abs(weight_in), dim=1)  # [D]
    return 1.0 / (sum_in + 1e-8)

# Overall utility: here defined as product of contribution and adaptation utilities.
def compute_overall_utility(contrib, adaptation):
    return contrib * adaptation

# Exponential moving average update.
def update_running_avg(old, current, eta):
    if old is None:
        return current
    else:
        return (1 - eta) * current + eta * old

# Bias-correct the running average given the age (number of updates).
def bias_corrected(running_avg, age, eta):
    # age is assumed to be an integer or a torch scalar.
    denom = 1 - eta ** (age + 1)
    return running_avg / (denom + 1e-8)

# -------------------------------
# 4. Metric Measurement Routine
# -------------------------------

def measure_metrics(model, dataloader, device, eta, layer_names, running_avgs, ages):
    """
    For each layer in layer_names (e.g., ['fc1','fc2']), this function:
     - Collects activations from the model (by running through dataloader)
     - Computes effective rank of the activations.
     - Computes contribution utility using the next layer's weights.
     - Computes adaptation utility from the layer's incoming weights.
     - Computes overall utility = contrib * adaptation.
     - Updates a running average for contribution and overall utilities using exponential smoothing.
     - Increments the "age" (number of epochs seen).
     - Computes bias-corrected running average utilities.
     
    running_avgs is a dict with keys: 'contrib' and 'overall', each mapping layer name -> tensor of shape [D] (or None initially).
    ages is a dict mapping layer name -> age (integer).
    
    Returns a dict of metrics and updated running_avgs and ages.
    """
    # We will aggregate activations for each layer over the dataloader.
    activations = {ln: [] for ln in layer_names}
    model.eval()
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            out, h1, h2 = model(images)
            # Save activations for each layer:
            activations['fc1'].append(h1.cpu())
            activations['fc2'].append(h2.cpu())
    # Concatenate activations along the batch dimension.
    for ln in layer_names:
        activations[ln] = torch.cat(activations[ln], dim=0)  # shape: [N, D]
    
    metrics = {}
    # Effective rank of each layer's representation.
    for ln in layer_names:
        er = compute_effective_rank(activations[ln])
        metrics[f'erank_{ln}'] = er

    # For contribution utility we need the weight matrices of the next layer.
    # For fc1, next layer is fc2; for fc2, next layer is fc3.
    with torch.no_grad():
        # Assume model has attributes fc1, fc2, fc3.
        # Contribution utility:
        contrib_fc1 = compute_contrib_utility(activations['fc1'], model.fc2.weight.data.cpu())
        contrib_fc2 = compute_contrib_utility(activations['fc2'], model.fc3.weight.data.cpu())
        # Adaptation utility:
        # For fc1, incoming weights: model.fc1.weight, shape [hidden, input]
        adapt_fc1 = compute_adaptation_utility(model.fc1.weight.data.cpu())
        # For fc2, incoming weights: model.fc2.weight, shape [hidden, hidden]
        adapt_fc2 = compute_adaptation_utility(model.fc2.weight.data.cpu())
        # Overall utility:
        overall_fc1 = compute_overall_utility(contrib_fc1, adapt_fc1)
        overall_fc2 = compute_overall_utility(contrib_fc2, adapt_fc2)
        
    # Update running averages and ages for each layer.
    for ln, current_contrib in zip(layer_names, [contrib_fc1, contrib_fc2]):
        # running average for contribution:
        if running_avgs['contrib'].get(ln) is None:
            running_avgs['contrib'][ln] = current_contrib.clone()
        else:
            running_avgs['contrib'][ln] = update_running_avg(running_avgs['contrib'][ln], current_contrib, eta)
        # running average for overall:
        current_overall = overall_fc1 if ln=='fc1' else overall_fc2
        if running_avgs['overall'].get(ln) is None:
            running_avgs['overall'][ln] = current_overall.clone()
        else:
            running_avgs['overall'][ln] = update_running_avg(running_avgs['overall'][ln], current_overall, eta)
        # Increase age
        ages[ln] = ages.get(ln, 0) + 1
        # Bias-corrected running averages:
        bc_contrib = bias_corrected(running_avgs['contrib'][ln], ages[ln], eta)
        bc_overall = bias_corrected(running_avgs['overall'][ln], ages[ln], eta)
        metrics[f'avg_contrib_{ln}'] = torch.mean(running_avgs['contrib'][ln]).item()
        metrics[f'avg_overall_{ln}'] = torch.mean(running_avgs['overall'][ln]).item()
        metrics[f'bc_avg_contrib_{ln}'] = torch.mean(bc_contrib).item()
        metrics[f'bc_avg_overall_{ln}'] = torch.mean(bc_overall).item()
    return metrics, running_avgs, ages

# -------------------------------
# 5. Training and Evaluation Functions (Same as before)
# -------------------------------

def train_epoch(model, optimizer, criterion, dataloader, device):
    model.train()
    running_loss = 0.0
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs, _, _ = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    epoch_loss = running_loss / len(dataloader.dataset)
    return epoch_loss

def evaluate(model, dataloader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs, _, _ = model(images)
            _, predicted = torch.max(outputs, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

# -------------------------------
# 6. Continual Learning Training with Metrics Logging
# -------------------------------

def continual_training(model, tasks, train_dataset, eval_loader, epochs_per_task, batch_size, device, eta=0.99):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    # We will track metrics for layers 'fc1' and 'fc2'
    layer_names = ['fc1', 'fc2']
    # Dictionaries to store running averages and ages per layer.
    running_avgs = {'contrib': {}, 'overall': {}}
    ages = {}
    
    # Before training, measure initial metrics.
    print("=== Before Training: Metrics ===")
    metrics, running_avgs, ages = measure_metrics(model, eval_loader, device, eta, layer_names, running_avgs, ages)
    for k, v in metrics.items():
        print(f"{k}: {v:.4f}")
        
    # Now loop over tasks sequentially.
    for task in tasks:
        print(f"\n=== Training on Task (classes {task}) ===")
        task_subset = get_subset_by_classes(train_dataset, task)
        task_loader = DataLoader(task_subset, batch_size=batch_size, shuffle=True, num_workers=2)
        for epoch in range(epochs_per_task):
            loss = train_epoch(model, optimizer, criterion, task_loader, device)
            print(f"Task {task} - Epoch {epoch+1}/{epochs_per_task}: Loss = {loss:.4f}")
            # At the end of each epoch, measure the metrics on the eval_loader.
            metrics, running_avgs, ages = measure_metrics(model, eval_loader, device, eta, layer_names, running_avgs, ages)
            print("Metrics after epoch:")
            for k, v in metrics.items():
                print(f"  {k}: {v:.4f}")
    return model

# -------------------------------
# 7. Joint Training (Train on Entire CIFAR10) -- without continual metrics
# -------------------------------

def joint_training(model, train_dataset, epochs, batch_size, device):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    for epoch in range(epochs):
        loss = train_epoch(model, optimizer, criterion, dataloader, device)
        print(f"Joint Training - Epoch {epoch+1}/{epochs}: Loss = {loss:.4f}")
    return model

# -------------------------------
# 8. Main Experiment
# -------------------------------

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    batch_size = 128
    epochs_per_task = 5  # epochs per paired-task
    num_tasks = len(tasks)
    total_epochs_joint = epochs_per_task * num_tasks  # same total epochs for joint training
    
    # We also prepare an evaluation DataLoader (for metrics) using the test set.
    eval_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    print("=== Continual Learning Experiment (Sequential Training on Task Pairs) ===")
    continual_model = MLP().to(device)
    continual_model = continual_training(continual_model, tasks, train_dataset, eval_loader,
                                          epochs_per_task, batch_size, device, eta=0.99)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    acc_continual = evaluate(continual_model, test_loader, device)
    print(f"\nTest Accuracy of Continual Learning MLP: {acc_continual*100:.2f}%")
    
    print("\n=== Joint Training Experiment (Train on entire CIFAR10) ===")
    joint_model = MLP().to(device)
    joint_model = joint_training(joint_model, train_dataset, total_epochs_joint, batch_size, device)
    acc_joint = evaluate(joint_model, test_loader, device)
    print(f"\nTest Accuracy of Joint Training MLP: {acc_joint*100:.2f}%")
    
    print("\n=== Linear Model Experiment (Train on entire CIFAR10) ===")
    linear_model = LinearModel().to(device)
    linear_optimizer = optim.SGD(linear_model.parameters(), lr=0.01, momentum=0.9)
    criterion = nn.CrossEntropyLoss()
    joint_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    for epoch in range(total_epochs_joint):
        loss = train_epoch(linear_model, linear_optimizer, criterion, joint_loader, device)
        print(f"Linear Model - Epoch {epoch+1}/{total_epochs_joint}: Loss = {loss:.4f}")
    acc_linear = evaluate(linear_model, test_loader, device)
    print(f"\nTest Accuracy of Linear Model: {acc_linear*100:.2f}%")

----------------------------------------

Cell 3 (code):

----------------------------------------

Cell 4 (code):

----------------------------------------

Cell 5 (code):
model
----------------------------------------

Cell 6 (code):

----------------------------------------

Cell 7 (code):

----------------------------------------

Cell 8 (code):
model
----------------------------------------

Cell 9 (code):

----------------------------------------

================================================================================

--- Processing: ResNet.ipynb ---

Cell 1 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# -------------------------------
# Utility Functions
# -------------------------------
def num_connected_components(A, tol=1e-8, thresh=0.98):
    A = A - A.mean(dim=1,keepdim=True)
    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)
    Corr = A_norm @ A_norm.T
    Corr.fill_diagonal_(0)
    Corr = Corr.abs()
    Adj = (Corr > thresh).float()
    degrees = torch.sum(Adj, dim=1)
    D = torch.diag(degrees)
    L = D - Adj
    eigenvalues = torch.linalg.eigvalsh(L)
    num_components = torch.sum(eigenvalues < tol).item()
    return num_components

def compute_effective_rank(activation_matrix, eps=1e-12):
    act = activation_matrix.double()
    U, S, V = torch.linalg.svd(act, full_matrices=False)
    S_sum = S.sum() + eps
    p = S / S_sum
    p_clamped = p.clamp(min=eps)
    entropy = -(p * torch.log(p_clamped)).sum()
    eff_rank = torch.exp(entropy)
    return eff_rank.item()

# -------------------------------
# Define a ResNet with Hooks to Record Activations
# -------------------------------
# We use torchvision.models.resnet18 as our candidate ResNet.
# We insert forward hooks into chosen layers (e.g., after layer1, layer2, layer3, and layer4).

from torchvision.models import resnet18

class ResNetWithHooks(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetWithHooks, self).__init__()
        # Load a pre-defined resnet18
        self.resnet = resnet18(pretrained=False, num_classes=num_classes)
        # Dictionary to store activations
        self.activations = {}
        # Register hooks on chosen layers: here we choose layer1, layer2, layer3, and layer4.
        self.resnet.layer1.register_forward_hook(self._get_activation_hook('layer1'))
        self.resnet.layer2.register_forward_hook(self._get_activation_hook('layer2'))
        self.resnet.layer3.register_forward_hook(self._get_activation_hook('layer3'))
        self.resnet.layer4.register_forward_hook(self._get_activation_hook('layer4'))

    def _get_activation_hook(self, name):
        def hook(module, input, output):
            # Save output activation
            self.activations[name] = output.detach()
        return hook

    def forward(self, x):
        return self.resnet(x)

# -------------------------------
# Data Preparation
# -------------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
val_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_set, batch_size=1024, shuffle=False, num_workers=2, drop_last=True)

# -------------------------------
# Initialize Model, Loss, Optimizer
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ResNetWithHooks(num_classes=10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1,
                     momentum=0.9, weight_decay=5e-4)
th = 0.90  # threshold for counting connected components

# -------------------------------
# Training Loop
# -------------------------------
num_epochs = 40
for epoch in range(num_epochs):
    if epoch == 0:
        avg_train_loss = 0.0
    # Training phase (skip reporting metrics at epoch 0)
    if epoch > 0:
        model.train()
        train_loss_total = 0.0
        num_train_batches = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss_total += loss.item()
            num_train_batches += 1
        avg_train_loss = train_loss_total / num_train_batches

    # Validation phase: compute average loss and record activations from one batch
    model.eval()
    val_loss_total = 0.0
    num_val_batches = 0
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss_total += loss.item()
            num_val_batches += 1
            # Capture activations from the first validation batch
            if num_val_batches == 1:
                val_batch_activations = {k: v for k, v in model.activations.items()}
    avg_val_loss = val_loss_total / num_val_batches

    print(f'\nEpoch {epoch} Connected Components stats (threshold = {th:.3f}):')
    # For each hooked layer, compute number of connected components.
    for name, A in model.activations.items():
        # Flatten spatial dimensions if necessary: A shape is [B, C, H, W]
        if A.dim() > 2:
            A_flat = A.transpose(0,1).flatten(1).transpose(0,1)  # shape [BH*W, C]
            # print(A.shape, A_flat.shape)
            # assert False
            # We average over spatial locations
            # A_flat = A_flat.mean(dim=2)  # shape [B, C]
        else:
            A_flat = A
        # Transpose so each row corresponds to a feature.
        num_cc = num_connected_components(A_flat.T, thresh=th)
        print(f"Layer {name} feature dim = {A_flat.shape[1]}  # connected components: {num_cc}")

    # Compute effective rank for each recorded layer from the first validation batch
    erank_dict = {}
    for name, act in val_batch_activations.items():
        if act.dim() > 2:
            act_flat = act.flatten(2).mean(dim=2)
        else:
            act_flat = act
        erank = compute_effective_rank(act_flat)
        erank_dict[name] = erank
    erank_str = ", ".join([f"{name}: {val:.2f}" for name, val in erank_dict.items()])
    print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | Effective Rank per layer: {erank_str}")

----------------------------------------

Cell 2 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import time
import copy

from collections import defaultdict

def collect_activations(model, dataloader, device, num_batches=None):
    """
    Collect activations for multiple batches of data
    
    Args:
        model: ResNetActivations model
        dataloader: DataLoader instance
        device: torch device
        num_batches: Number of batches to process (None for all)
    
    Returns:
        list of dictionaries containing activations for each batch
    """
    model.eval()
    all_batch_activations = []
    
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(dataloader):
            if num_batches is not None and batch_idx >= num_batches:
                break
                
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # Forward pass
            outputs = model(inputs)
            
            # Collect activations
            batch_activations = model.get_all_activations()
            batch_activations['labels'] = labels.cpu()
            all_batch_activations.append(batch_activations)
    
    return all_batch_activations

def num_connected_components(A, tol=1e-8, thresh=0.98):
    A = A - A.mean(dim=1,keepdim=True)
    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)
    Corr = A_norm @ A_norm.T
    # print(f'Corr dims = {Corr.shape}')
    Corr.fill_diagonal_(0)
    Corr = Corr.abs()
    # print(f'Corr dims = {Corr.shape} Corr max (off-diag) median per unit: {Corr.max(dim=1).values.median():.3f}')
    Adj = (Corr > thresh).float()
    degrees = torch.sum(Adj, dim=1)
    D = torch.diag(degrees)
    L = D - Adj
    eigenvalues = torch.linalg.eigvalsh(L)
    num_components = torch.sum(eigenvalues < tol).item()
    return num_components

def report_CC_stats(model, loader, thresh, num_batches = 10):
    activations = collect_activations(model, loader, device, num_batches=num_batches)
    
    for k in activations[0]['main'].keys():
        
        A = torch.concat([act['main'][k].detach().cpu() for act in activations])
        A_flat = A.flatten(1)
        # A = A.transpose(0,1).flatten(1)#.transpose(0,1)
        # print('A shape = ', A.shape, ' A flat shape = ', A_flat.shape)
        try:
            CC = num_connected_components(A_flat,thresh=thresh)
            print('key = ', k, ' CC = ', CC, ' rank(A) = ', min(A_flat.shape))
        except Exception:
            pass


# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Data augmentation and normalization for training
# Just normalization for validation
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_val = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# Load CIFAR10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                      download=True, transform=transform_train)
trainloader = DataLoader(trainset, batch_size=128,
                        shuffle=True, num_workers=2)

valset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                     download=True, transform=transform_val)
valloader = DataLoader(valset, batch_size=128,
                      shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck')

# Define ResNet model
def conv3x3(in_planes, out_planes, stride=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu1 = nn.ReLU(inplace=False)  # Changed to not use inplace
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu2 = nn.ReLU(inplace=False)  # Changed to not use inplace
        self.downsample = downsample
        self.stride = stride
        
        # Store activations
        self.activations = {}

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu1(out)
        self.activations['relu1'] = out.detach()

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu2(out)
        self.activations['relu2'] = out.detach()

        return out

class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes=10):
        super(ResNet, self).__init__()
        self.inplanes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,
                              bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=False)  # Changed to not use inplace
        
        self.layer1 = self._make_layer(block, 512, layers[0])
        self.layer2 = self._make_layer(block, 512, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 512, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
        # Store all activations
        self.activations = {}

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        self.activations['initial_relu'] = x.detach()

        x = self.layer1(x)
        self.activations['layer1'] = x.detach()
        
        x = self.layer2(x)
        self.activations['layer2'] = x.detach()
        
        x = self.layer3(x)
        self.activations['layer3'] = x.detach()
        
        x = self.layer4(x)
        self.activations['layer4'] = x.detach()

        x = self.avgpool(x)
        self.activations['avgpool'] = x.detach()
        
        x = torch.flatten(x, 1)
        logits = self.fc(x)
        self.activations['logits'] = logits.detach()

        return logits

    def get_all_activations(self):
        """Collect all activations from the model, including those from BasicBlocks"""
        all_activations = defaultdict(dict)
        
        # Get main activations
        for name, activation in self.activations.items():
            all_activations['main'][name] = activation
        
        # Get activations from each BasicBlock
        for layer_idx, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):
            for block_idx, block in enumerate(layer):
                for act_name, activation in block.activations.items():
                    all_activations[f'layer{layer_idx+1}_block{block_idx+1}'][act_name] = activation
        
        return all_activations

def train_model(model, criterion, optimizer, num_epochs=25):
    since = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
                dataloader = trainloader
            else:
                model.eval()
                dataloader = valloader

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_corrects.double() / len(dataloader.dataset)

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase=='val':
                print(f'report CC stats for phase {phase}')
                report_CC_stats(model, dataloader, thresh=0.9,num_batches=10)

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best val Acc: {best_acc:4f}')

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

# Initialize model, criterion, and optimizer
model = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)  # ResNet18
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1,
                     momentum=0.9, weight_decay=5e-4)

# Train and evaluate
model = train_model(model, criterion, optimizer, num_epochs=25)
----------------------------------------

================================================================================

--- Processing: CNN.ipynb ---

Cell 1 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import tqdm

import numpy as np
from scipy.sparse.csgraph import connected_components

# -------------------------
# Configurable CNN with Batch Normalization and Hidden Activations Collection
# -------------------------
class ConfigurableCNN(nn.Module):
    def __init__(self, conv_channels, fc_hidden_units=512, dropout_p=0.25,
                 num_classes=10, input_size=32, input_channels=3, use_batchnorm=True):
        """
        Args:
            conv_channels (list of int): List of output channels for each convolutional layer.
            fc_hidden_units (int): Number of neurons in the hidden fully connected layer.
            dropout_p (float): Dropout probability.
            num_classes (int): Number of output classes.
            input_size (int): Height/width of the input images (assumed square).
            input_channels (int): Number of channels in the input images.
            use_batchnorm (bool): Whether to use batch normalization after each convolution.
        """
        super(ConfigurableCNN, self).__init__()
        self.use_batchnorm = use_batchnorm
        self.conv_layers = nn.ModuleList()
        if self.use_batchnorm:
            self.bn_layers = nn.ModuleList()
        
        in_channels = input_channels  # For colored images, this is 3.
        self.num_pool = len(conv_channels)  # One pooling per conv layer
        
        # Create convolutional layers along with optional batch normalization.
        for out_channels in conv_channels:
            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
            if self.use_batchnorm:
                self.bn_layers.append(nn.BatchNorm2d(out_channels))
            in_channels = out_channels
        
        # Define a max pooling layer (2x2) applied after each conv block.
        self.pool = nn.MaxPool2d(2, 2)
        
        # Compute the spatial size after all pooling operations.
        final_size = input_size // (2 ** self.num_pool)
        self.flattened_size = conv_channels[-1] * final_size * final_size
        
        # Fully connected layers.
        self.fc1 = nn.Linear(self.flattened_size, fc_hidden_units)
        self.fc2 = nn.Linear(fc_hidden_units, num_classes)
        
        # Dropout layer for regularization.
        self.dropout = nn.Dropout(dropout_p)
        self.act = F.tanh  # You can change this activation if desired

    def forward(self, x, return_hidden=False):
        hidden_activations = []  # List to collect hidden activations

        # Pass through each convolutional layer
        for idx, conv in enumerate(self.conv_layers):
            x = conv(x)
            if self.use_batchnorm:
                x = self.bn_layers[idx](x)
            if return_hidden:
                hidden_activations.append(x.detach().cpu())
            x = self.act(x)
            x = self.pool(x)
        
        x = self.dropout(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor

        # First fully connected layer with activation
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)

        # Final fully connected layer (logits)
        x = self.fc2(x)
        
        if return_hidden:
            return x, hidden_activations
        return x


def eval_features(model, testloader, thresh=0.9, tol=1e-10, rank_atol=1e-2, dead_tol=0.05):
    model.eval()
    sample_inputs, _ = next(iter(testloader))
    sample_inputs = sample_inputs.to(device)
    with torch.no_grad():
        _, hidden_activations = model(sample_inputs, return_hidden=True)
        
    for act in hidden_activations:
        # Reshape: (batch, channels, H, W) --> (channels, batch * H * W)
        A = act.transpose(0, 1).flatten(1)
        # Normalize each row (avoid division by zero with a small epsilon)
        # A = A - A.mean(dim=1,keepdim=True)
        A = A / (A.norm(dim=1, keepdim=True) + tol)
        stds = A.std(dim=1) / A.abs().mean(dim=1)
        # print(stds.shape, stds)
        dead_features = (stds<dead_tol).sum()
        # Compute cosine similarity matrix
        C = A @ A.t()
        rank = torch.linalg.matrix_rank(C, atol=rank_atol)
        soft_rank = torch.trace(C)**2 / torch.trace(C @ C)
        # Remove self-similarity by zeroing the diagonal and take absolute value.
        C.fill_diagonal_(0)
        C = C.abs()
        # Create an adjacency matrix by thresholding.
        Adj = (C > thresh).float()
        
        # Convert to numpy array (scipy works with numpy arrays)
        Adj_np = Adj.numpy()
        # Compute the number of connected components using SciPy's stable routine.
        n_components, labels = connected_components(csgraph=Adj_np, directed=False)
        R = Adj_np.shape[0]
        print(f'# CC  = {n_components:4}, e-rank = {rank:4}, soft rank = {soft_rank:4.3f}, dead features = {dead_features:4} / {R}')

# -------------------------
# Data Preparation (Tiny ImageNet with Selected Classes)
# -------------------------
# Specify which classes to use.
selected_classes = range(50)  # Set to None to use all available classes

# Define the image size for resizing
input_size = 128

transform = transforms.Compose([
    transforms.Resize((input_size, input_size)),  # Resize images to input_size x input_size
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load the Tiny ImageNet datasets using ImageFolder.
# Adjust the root paths to where you have Tiny ImageNet stored.
trainset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/train', transform=transform)
testset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/val', transform=transform)

# If selected_classes is specified, filter the dataset to include only those classes.
if selected_classes is not None:
    train_indices = [i for i, (_, label) in enumerate(trainset.samples) if label in selected_classes]
    trainset = torch.utils.data.Subset(trainset, train_indices)
    test_indices = [i for i, (_, label) in enumerate(testset.samples) if label in selected_classes]
    testset = torch.utils.data.Subset(testset, test_indices)
    num_used_classes = len(selected_classes)
else:
    num_used_classes = len(trainset.classes)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,
                                          shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=128,
                                         shuffle=False, num_workers=2)

# -------------------------
# Model Configuration and Instantiation
# -------------------------
conv_channels = [256] * 7  # Example configuration
fc_hidden_units = conv_channels[0]
dropout_p = 0.0
use_batchnorm = True

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net = ConfigurableCNN(conv_channels, fc_hidden_units, dropout_p,
                      num_classes=num_used_classes, input_size=input_size, input_channels=3,
                      use_batchnorm=use_batchnorm).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

# -------------------------
# Training Loop
# -------------------------
num_epochs = 100  # Adjust the number of epochs as needed
for epoch in range(num_epochs):
    net.train()
    running_loss = 0.0
    for i, data in tqdm.tqdm(enumerate(trainloader, 0), total=len(trainloader)):
        inputs, labels = data[0].to(device), data[1].to(device)
        
        optimizer.zero_grad()  # Zero the parameter gradients
        outputs = net(inputs)   # Forward pass
        loss = criterion(outputs, labels)  # Compute loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Update parameters
        
        running_loss += loss.item()
    print(f'Epoch {epoch + 1}, Train Loss: {running_loss / len(trainloader):.5f}')
    running_loss = 0.0

    # Optionally, evaluate feature connectivity
    eval_features(net, testloader, thresh=0.95)
    
    # -------------------------
    # Validation after each epoch
    # -------------------------
    net.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f'Accuracy on test set after epoch {epoch + 1}: {accuracy:.2f}%')

print("Training complete!")
x
# -------------------------
# Example: Obtaining Hidden Activations
# -------------------------
net.eval()
sample_inputs, _ = next(iter(testloader))
sample_inputs = sample_inputs.to(device)
with torch.no_grad():
    output, hidden_activations = net(sample_inputs, return_hidden=True)
print("Collected {} hidden activations.".format(len(hidden_activations)))

----------------------------------------

Cell 2 (code):

----------------------------------------

Cell 3 (code):

----------------------------------------

Cell 4 (code):

----------------------------------------

Cell 5 (code):

----------------------------------------

Cell 6 (code):

----------------------------------------

================================================================================

--- Processing: MLP.ipynb ---

Cell 1 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

def num_connected_components(A, tol=1e-8, thresh = 0.98):
    # eps = 1e-8
    A = A - A.mean(dim=1,keepdim=True)
    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)
    Corr = A_norm @ A_norm.T
    # print(Corr[:2,:2], Corr.shape)
    Corr.fill_diagonal_(0)
    Corr = Corr.abs()
    Adj = (Corr>0.98
          ).float()
    # Compute the degree vector and degree matrix D
    degrees = torch.sum(Adj, dim=1)
    D = torch.diag(degrees)
    # Compute the Laplacian L = D - Adj
    L = D - Adj
    # Compute eigenvalues of L. Since L is symmetric, use eigvalsh.
    eigenvalues = torch.linalg.eigvalsh(L)
    # Count the number of eigenvalues that are close to zero.
    num_components = torch.sum(eigenvalues < tol).item()
    return num_components


def compute_effective_rank(activation_matrix, eps=1e-12):
    """
    Compute the effective rank of an activation matrix in a numerically stable manner.
    
    activation_matrix: Tensor of shape (batch_size, feature_dim)
    eps: Small constant to prevent division by zero or log(0)
    
    Returns: effective rank (float)
    """
    # Use double precision for stability
    act = activation_matrix.double()
    # Compute SVD
    U, S, V = torch.linalg.svd(act, full_matrices=False)
    S_sum = S.sum() + eps  # Avoid division by zero
    p = S / S_sum         # Normalized singular values
    # Clamp probabilities to avoid log(0)
    p_clamped = p.clamp(min=eps)
    # Compute entropy and effective rank
    entropy = -(p * torch.log(p_clamped)).sum()
    eff_rank = torch.exp(entropy)
    return eff_rank.item()

# ---------------------
# Define a Wide, Deep MLP with Hooks to Record Activations
# ---------------------
class WideDeepMLP(nn.Module):
    def __init__(self, input_dim=3*32*32, hidden_dim=1000, num_layers=10, num_classes=10):
        super(WideDeepMLP, self).__init__()
        self.layers = nn.ModuleList()
        # First Linear Layer + ReLU
        self.layers.append(nn.Linear(input_dim, hidden_dim, bias=False))
        self.layers.append(nn.Tanh())
        # Additional hidden layers: each has a Linear layer followed by ReLU
        for _ in range(num_layers - 1):
            self.layers.append(nn.Linear(hidden_dim, hidden_dim, bias=False))
            self.layers.append(nn.Tanh())
        # Final classifier layer (not included in activation collection)
        self.layers.append ( nn.Linear(hidden_dim, num_classes, bias=False) ) 
        # Dictionary to store activations (from each hidden Linear layer output)
        self.activations = {}
        # Register hooks on each Linear layer in self.layers (skip ReLU modules)
        layer_idx = 0
        for module in self.layers:
            # store pre-activations (after applying linear )
            if not isinstance(module, nn.Linear):
                module.register_forward_hook(self._get_activation_hook(layer_idx))
                layer_idx += 1

        for layer in self.layers:
            if isinstance(layer, nn.Linear):
                nn.init.normal_(layer.weight, mean=0, std=(1.0/0.39)**0.5/layer.weight.shape[0]**0.5)

    def _get_activation_hook(self, idx):
        def hook(module, input, output):
            self.activations[f"layer_{idx}"] = output.detach()
        return hook

    def forward(self, x):
        # Flatten the input
        x = x.view(x.size(0), -1)
        for layer in self.layers:
            x = layer(x)
        return x

# ---------------------
# Data Preparation
# ---------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
val_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_set, batch_size=6000, shuffle=False, num_workers=2, drop_last=True)

# ---------------------
# Initialize Model, Loss, Optimizer
# ---------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = WideDeepMLP(num_layers=7).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
th = 0.999 # threshold for counting the connected components 

# ---------------------
# Training Loop`
# ---------------------
num_epochs = 1
for epoch in range(num_epochs):
    # Training phase, skip epoch 0, only report validation metrics 
    if epoch==0:
        avg_train_loss = 0
    if epoch > 0:
        model.train()
        train_loss_total = 0.0
        num_train_batches = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss_total += loss.item()
            num_train_batches += 1
        avg_train_loss = train_loss_total / num_train_batches

    # Validation phase: compute average loss on validation set
    model.eval()
    val_loss_total = 0.0
    num_val_batches = 0
    # Also, capture activations from one batch for effective rank
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss_total += loss.item()
            num_val_batches += 1
            # For effective rank metrics, only use the first batch
            if num_val_batches == 1:
                val_batch_activations = {k: v for k, v in model.activations.items()}
    avg_val_loss = val_loss_total / num_val_batches
    print(f'epoch {epoch} CC stats using threshold = {th:.3f}')
    for k,A in model.activations.items():
        print(f"layer {k} feature dim = {A.shape[1]} # of connected components: {num_connected_components(A.T,thresh=th)}")

    # Compute effective rank for each recorded hidden layer on the first validation batch
    erank_dict = {}
    for layer_name, act in val_batch_activations.items():
        act_matrix = act.view(act.size(0), -1)
        erank_dict[layer_name] = compute_effective_rank(act_matrix)

    # Report training loss, validation loss, and effective rank per layer for this epoch
    erank_str = ", ".join([f"{k}: {v:.2f}" for k, v in erank_dict.items()])
    print(f"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | Effective Rank per layer: {erank_str}")

----------------------------------------

Cell 2 (code):
def compute_per_sample_gradients(model, data_loader, criterion, device, max_samples=1000):
    """
    Compute gradients with memory optimization.
    
    Args:
        model (nn.Module): The neural network model
        data_loader (DataLoader): Data loader
        criterion (callable): Loss function
        device (torch.device): Device for computation
        max_samples (int): Maximum number of samples to process
    """
    model.eval()
    per_sample_grads = []
    samples_processed = 0
    
    for images, labels in data_loader:
        batch_size = images.size(0)
        
        # Process smaller mini-batches
        mini_batch_size = 10  # Adjust this based on your GPU memory
        for i in range(0, batch_size, mini_batch_size):
            if samples_processed >= max_samples:
                break
                
            end_idx = min(i + mini_batch_size, batch_size)
            mini_images = images[i:end_idx].to(device)
            mini_labels = labels[i:end_idx].to(device)
            
            for j in range(mini_images.size(0)):
                # Process single sample
                sample_image = mini_images[j:j+1]
                sample_label = mini_labels[j:j+1]
                
                outputs = model(sample_image)
                loss = criterion(outputs, sample_label)
                
                # Compute and store gradients
                grads = torch.autograd.grad(loss, model.parameters(), create_graph=False)
                
                sample_grad = {}
                for (name, _), grad in zip(model.named_parameters(), grads):
                    # Store gradients as CPU tensors to save GPU memory
                    sample_grad[name] = grad.detach().cpu()
                
                per_sample_grads.append(sample_grad)
                samples_processed += 1
                
                # Clear some memory
                del grads, outputs, loss
                torch.cuda.empty_cache()
            
            # Clear mini-batch tensors
            del mini_images, mini_labels
            torch.cuda.empty_cache()
            
            if samples_processed >= max_samples:
                break
                
        if samples_processed >= max_samples:
            break
    
    return per_sample_grads

# Usage example:
max_samples = 100  # Adjust based on your needs
sample_gradients = compute_per_sample_gradients(model, train_loader, criterion, device, max_samples=max_samples)

# To analyze gradients:
print(f"Processed {len(sample_gradients)} samples")
first_sample_grads = sample_gradients[0]
for param_name, grad in first_sample_grads.items():
    print(f"{param_name}: {grad.shape}")
----------------------------------------

Cell 3 (code):
keys = sample_gradients[0].keys()
N = len(sample_gradients)
ntk = {k:torch.zeros((N,N)) for k in keys}
----------------------------------------

Cell 4 (code):
torch.zeros((2,2))
----------------------------------------

Cell 5 (code):

----------------------------------------

================================================================================

--- Processing: neural_collapse.ipynb ---

Cell 1 (code):
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as T
import torchvision.datasets as datasets
import tqdm 
from torch.utils.data import Dataset

class SubsampledDataset(Dataset):
    def __init__(self, dataset, select_classes):
        """
        Wrap an existing dataset to only include samples whose labels are in select_classes.
        Labels are remapped to 0, 1, ..., len(select_classes)-1.
        """
        self.dataset = dataset
        self.select_classes = set(select_classes)
        # Create mapping: original label -> new label (0-indexed)
        self.class_map = {orig_label: new_label 
                          for new_label, orig_label in enumerate(sorted(select_classes))}
        # Filter indices for samples that belong to the desired classes
        self.indices = [i for i, (_, label) in enumerate(dataset) if label in self.select_classes]
        
    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        actual_idx = self.indices[idx]
        x, y = self.dataset[actual_idx]
        return x, self.class_map[y]


# --- Hyperparameters ---
batch_size = 1024
lr = 1e-2
epochs = 40
weight_decay_list = [0.0, 1e-4]  # as an example


# Specify which classes to include; for example, only classes 0, 1, and 2.
select_classes = [0, 1, ]

transform = T.Compose([T.ToTensor()])
# Load the full MNIST dataset
full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
full_test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Wrap the dataset to only include the selected classes
train_dataset = SubsampledDataset(full_train_dataset, select_classes)
test_dataset  = SubsampledDataset(full_test_dataset, select_classes)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Number of classes
num_classes = 10
# Penultimate dimension, bigger than num_classes
hidden_dim = 512

class Noramalization(nn.Module):
    def __init__(self,):
        super().__init__()
        


# ---- Model Definition with Configurable Activation and Norm ----
class SimpleNet(nn.Module):
    def __init__(self, activation='relu', norm='none', weight_decay=0.0):
        super().__init__()
        
        # Just a simple MLP for demonstration; you can add CNN layers for CIFAR
        self.activation_choice = activation
        if activation == 'relu':
            self.act = nn.ReLU()
        elif activation == 'tanh':
            self.act = nn.Tanh()
        elif activation == 'sigmoid':
            self.act = nn.Sigmoid()
        else:
            raise ValueError("Unsupported activation")

        self.norm_choice = norm
        if norm == 'batchnorm':
            norm_layer = nn.BatchNorm1d
        elif norm == 'layernorm':
            norm_layer = nn.LayerNorm
        elif norm == 'none':
            norm_layer = nn.Identity
        else:
            raise ValueError("Unsupported normalization")

        # A small MLP: Flatten -> Dense -> Activation -> Normalization -> Dense -> ...
        self.fc1 = nn.Linear(28*28, hidden_dim)
        self.norm1 = norm_layer(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.norm2 = norm_layer(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)

    def forward(self, x, return_penultimate=False):
        x = x.view(x.size(0), -1)
        z = self.fc1(x)
        z = self.norm1(z)
        z = self.act(z)
        z = self.fc2(z)
        z = self.norm2(z)
        z = self.act(z)
        out = self.fc3(z)
        
        if return_penultimate:
            # Return both the final logits and the penultimate features
            return out, z
        return out

# ---- Training loop utility ----
def train_one_variant(activation, norm, weight_decay):
    # Instantiate model
    model = SimpleNet(activation=activation, norm=norm)
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    # Train
    model.train()
    for ep in tqdm.trange(epochs):
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            preds = model(images)
            loss = criterion(preds, labels)
            loss.backward()
            optimizer.step()

    # Evaluate classification performance on test set
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    acc = 100.0 * correct / total
    return model, acc

# ---- Extract penultimate layer features and compute rank ----
def analyze_penultimate_features(model):
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    model.eval()
    all_feats = []
    all_labels = []
    with torch.no_grad():
        for images, labels in train_loader:
            images = images.to(device)
            outputs, feats = model(images, return_penultimate=True)
            all_feats.append(feats.cpu())
            all_labels.append(labels)

    all_feats = torch.cat(all_feats, dim=0)
    all_labels = torch.cat(all_labels, dim=0)

    # Compute numerical rank of (N x hidden_dim) features
    # We'll do SVD or rank via torch.linalg.svdvals
    svs = torch.linalg.svdvals(all_feats)
    # For numerical rank, define a threshold e.g. 1e-5 * largest singular value
    threshold = svs.mean().item() * 1e-2
    rank_est = int((svs > threshold).sum().item())

    # Also you can check rank per class, or measure within-class scatter
    # Example: rank across entire dataset
    print("Estimated rank of penultimate features:", rank_est)

    # Optionally compute within-class covariance or scatter
    # for c in range(num_classes):
    #     class_feats = all_feats[all_labels == c]
    #     # Compute variance or some measure for these...
    #     ...

    return rank_est

# ------------- MAIN EXPERIMENT -------------
if __name__ == "__main__":
    for activation in ['relu', 'tanh', 'sigmoid']:
        for norm in ['none', 'batchnorm', 'layernorm']:
            for wd in weight_decay_list:
                model, acc = train_one_variant(activation, norm, wd)
                print(f"Activation={activation}, Norm={norm}, WD={wd}, Test Acc={acc:.2f}")
                rank_est = analyze_penultimate_features(model)
                print(f"Penultimate layer rank={rank_est}")
                print("----------------------------------------------------")

----------------------------------------

Cell 2 (code):
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import math
import matplotlib.pyplot as plt

# ------------------------
# Define an optional RMSNorm
# ------------------------
class RMSNorm(nn.Module):
    def __init__(self, d, eps=1e-8):
        super().__init__()
        self.d = d
        self.eps = eps
        # Optional learnable scale (set to 1 by default)
        self.scale = nn.Parameter(torch.ones(d))
    
    def forward(self, x):
        # Compute the root-mean-square along the last dimension
        rms = x.pow(2).mean(dim=-1, keepdim=True).sqrt() + self.eps
        return self.scale * x / rms

# ------------------------
# Define the two-layer model
# ------------------------
class TwoLayerModel(nn.Module):
    def __init__(self, d, C, activation='relu', norm_type=None):
        """
        d: hidden dimension (and input dimension)
        C: number of classes (output dimension)
        activation: 'relu', 'tanh', or 'sigmoid'
        norm_type: None, 'layer_norm', 'batch_norm', or 'rms_norm'
        """
        super().__init__()
        self.d = d
        self.C = C
        self.activation = activation
        self.norm_type = norm_type
        
        # First layer: dxd Gaussian matrix with entries ~ N(0, 1/d)
        self.W1 = nn.Parameter(torch.randn(d, d) / math.sqrt(d))
        
        # Second layer: d x C Gaussian matrix with entries ~ N(0, 1/d^2)
        self.W2 = nn.Parameter(torch.randn(d, C) / d)
        
        # Optional normalization before activation
        if norm_type == 'LN':
            self.norm = nn.LayerNorm(d)
        elif norm_type == 'BN':
            self.norm = nn.BatchNorm1d(d)
        elif norm_type == 'RMS':
            self.norm = RMSNorm(d)
        else:
            self.norm = None

    def forward(self, X):
        # X: (N, d)
        pre_act = X @ self.W1  # (N, d)
        
        # Optionally apply normalization to the pre-activations
        if self.norm is not None:
            pre_act = self.norm(pre_act)
        
        # Apply activation
        if self.activation == 'relu':
            act = F.relu(pre_act)
        elif self.activation == 'tanh':
            act = torch.tanh(pre_act)
        elif self.activation == 'sigmoid':
            act = torch.sigmoid(pre_act)
        else:
            raise ValueError("Unsupported activation")
        
        # Compute logits via the second weight matrix
        logits = act @ self.W2  # (N, C)
        return logits, act, pre_act

# ------------------------
# Experiment Setup
# ------------------------

# Set experiment parameters
N = 5000       # total number of samples
d = 1000       # dimension of each sample (and hidden layer)
C = 10        # number of classes
num_epochs = 400

# Generate N random vectors (each of dimension d)
X = torch.randn(N, d)

# Assign labels: i-th sample has label (i mod C)
labels = torch.tensor([i%C for i in range(N)], dtype=torch.long)

# Choose activation and normalization options
activation = 'relu'         # options: 'relu', 'tanh', 'sigmoid'
norm_type = '‌‌BN'            # options: None, 'layer_norm', 'batch_norm', 'rms_norm'

# Instantiate the model
model = TwoLayerModel(d, C, activation=activation, norm_type=norm_type)

# Set up the optimizer and loss function
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

def test_model(model, C):
    logits, act, pre_act = model(X)
    M = act
    # M = M - M.mean(dim=1,keepdim=True)
    M = M / (M**2).mean(dim=1,keepdim=True)**0.5
    G = M.T @ M  / N
    svals = torch.linalg.svdvals(G,).detach()
    # plt.plot(svals,marker='.');
    print('top C eigs / total eigs =  ' , (svals[:C].sum()/svals.sum()).item())


# ------------------------
# Training Loop
# ------------------------
for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # Forward pass: compute logits
    logits, _, _ = model(X)
    
    # Compute cross-entropy loss
    loss = criterion(logits, labels)
    
    # Backward pass and update
    loss.backward()
    optimizer.step()
    if (epoch)%10==0:
        print(loss.item())
        test_model(model, C)
----------------------------------------

Cell 3 (code):
logits, act, pre_act = model(X)
M = pre_act
# M = M - M.mean(dim=1,keepdim=True)
M = M / (M**2).mean(dim=1,keepdim=True)**0.5
G = M.T @ M  / N
svals = torch.linalg.svdvals(G,).detach()
plt.plot(svals,marker='.');
svals[:10].sum()/svals.sum()


G[:5,:5]
# plt.imshow(G[:5,:5].detach())
# plt.colorbar()
----------------------------------------

Cell 4 (code):
G_flat = torch.triu(G.detach(), diagonal=1).flatten()
G_flat = G_flat[G_flat!=0]
plt.hist(G_flat,bins=30);
----------------------------------------

Cell 5 (code):
import matplotlib.pyplot as plt
import numpy as np
f = 1
plt.hist(act[:,f].detach(),label=' act',alpha=0.5,bins=30)
plt.hist(pre_act[:,f].detach(),label='pre act',alpha=0.5,bins=30)
plt.legend();
----------------------------------------

Cell 6 (code):
act.shape
----------------------------------------

Cell 7 (code):
torch.linalg.svdvals(G,).detach()
----------------------------------------

Cell 8 (code):

----------------------------------------

================================================================================

--- Processing: CL_RS.md ---

# Rich Sutton's Research Contributions to Continual Learning

## 1. Foundational Contributions
Rich Sutton has played a pivotal role in shaping reinforcement learning (RL), laying groundwork that also underpins continual learning. Many of his seminal contributions introduced algorithms and frameworks for agents to learn incrementally from ongoing experience – a core aspect of continual learning. Key foundational contributions include:

- **Temporal-Difference (TD) Learning**: Sutton’s 1988 work on TD learning introduced a method for an agent to update predictions by bootstrapping from newer estimates rather than waiting for final outcomes ([Reinforcement Learning: Temporal Difference (TD) Learning](https://www.lancaster.ac.uk/stor-i-student-sites/jordan-j-hood/2021/04/12/reinforcement-learning-temporal-difference-td-learning/#:~:text=Learning%20www,as%20the%20name%20suggests%2C)). TD learning merged strengths of dynamic programming and Monte Carlo methods, enabling effective incremental learning of value functions. This concept is *“likely the most core concept in Reinforcement Learning”* ([Reinforcement Learning: Temporal Difference (TD) Learning](https://www.lancaster.ac.uk/stor-i-student-sites/jordan-j-hood/2021/04/12/reinforcement-learning-temporal-difference-td-learning/#:~:text=Learning%20www,as%20the%20name%20suggests%2C)) and allowed agents to learn from a continuous stream of data, a prerequisite for continual adaptation.

- **Dyna Architecture**: In 1991, Sutton proposed the Dyna architecture, an integrated approach combining learning, planning, and reacting. In Dyna, an agent learns a world model online and uses it for simulated experience (planning) alongside real experience ([[PDF] Integrated Modeling and Control Based on Reinforcement Learning](https://papers.nips.cc/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf#:~:text=,Dyna%20is%20based%20on)). *“Dyna architectures are those that learn a world model online while using approximations to [dynamic programming] to learn and plan optimal behavior”* ([[PDF] Integrated Modeling and Control Based on Reinforcement Learning](https://papers.nips.cc/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf#:~:text=,Dyna%20is%20based%20on)). This framework was foundational for continual learning, as it showed how an agent could keep learning and improving by reusing past knowledge (via the learned model) in an ongoing way.

- **Options Framework (Hierarchical RL)**: Sutton (with Precup and Singh, 1999) introduced the options framework, which defines *“temporally extended ways of behaving”* (options) in RL ([[PDF] Temporal Abstraction in Temporal-difference Networks](http://papers.neurips.cc/paper/2826-temporal-abstraction-in-temporal-difference-networks.pdf#:~:text=%5BPDF%5D%20Temporal%20Abstraction%20in%20Temporal,and%20about%20predictions%20of)). Options are higher-level actions or skills that consist of lower-level primitives, with policies and termination conditions. This framework allows an agent to learn and reuse skills across tasks, effectively providing a form of knowledge transfer and memory over time. *“Generalization of one-step actions to option models… enables an agent to predict and reason at multiple time scales”* ([[PDF] Temporally Abstract Partial Models - OpenReview](https://openreview.net/pdf?id=LGvlCcMgWqb#:~:text=,reason%20at%20multiple%20time%20scales)), which is crucial for continual learning scenarios where the agent must build on prior skills.

- **General Value Functions and the Horde Architecture**: In more recent work, Sutton and colleagues developed the concept of General Value Functions (GVFs) and the Horde architecture (2011) for learning many predictions in parallel. Horde is a framework with a “democracy” of prediction-learning processes (termed “demons”) each learning a GVF about the agent’s sensorimotor stream. Sutton’s team demonstrated that an agent can scale to *“learn multiple pre-defined objectives in parallel”* and accumulate predictive knowledge continuously ([[1206.6262] Scaling Life-long Off-policy Learning - arXiv](https://arxiv.org/abs/1206.6262#:~:text=We%20build%20on%20our%20prior,to%20represent%20a%20wide)). In their words, *“GVFs have been shown able to represent a wide [range of predictions]”* in a lifelong learning setting ([[1206.6262] Scaling Life-long Off-policy Learning - arXiv](https://arxiv.org/abs/1206.6262#:~:text=We%20build%20on%20our%20prior,to%20represent%20a%20wide)). This idea of learning many predictions simultaneously without forgetting earlier ones directly informs continual learning research.

Sutton’s foundational work, including the widely used RL textbook (Sutton & Barto, 1998), established core algorithms and principles (e.g. incremental updates, bootstrapping, and exploration strategies) that enable an agent to learn continually. These contributions introduced formalisms and tools – such as TD error, experience replay (used later in deep RL and continual learning), and function approximation techniques – that remain central in modern continual learning research.

## 2. Relation to Reinforcement Learning
Continual learning and reinforcement learning are deeply intertwined, and Sutton’s work bridges them both conceptually and methodologically. Reinforcement learning deals with agents learning from an ongoing stream of interactions with an environment, which naturally aligns with the idea of *continual* learning (learning that never truly stops). Sutton himself has emphasized the importance of agents that keep learning over time. For example, continual reinforcement learning has been defined as the setting in which an agent *“never stop[s] learning”* ([A Definition of Continual Reinforcement Learning - arXiv](https://arxiv.org/html/2307.11046v2#:~:text=In%20contrast%2C%20continual%20reinforcement%20learning,the%20importance%20of%20continual)), highlighting that the best agents are those that can learn indefinitely. This ethos is a direct reflection of Sutton’s lifelong advocacy for incremental, online learning in RL.

Several RL principles introduced or popularized by Sutton have influenced continual learning algorithms:
- **Online Incremental Updates**: Methods like TD learning and gradient-descent updates allow learning to happen incrementally with each new observation, rather than in large batches. This is essential for continual learning, where data arrives sequentially. Sutton’s algorithms (e.g. TD(λ), SARSA, Q-learning refinements) showed how an agent can update knowledge on the fly and revisit old predictions efficiently, which is also how continual learning systems update without retraining from scratch.
- **Experience Replay and Off-Policy Learning**: While not invented solely by Sutton, the idea of reusing past experiences (experience replay) in RL (pioneered by Lin and later used in DQN) connects to rehearsal strategies in continual learning. Off-policy learning algorithms (such as Q-learning or off-policy TD) studied by Sutton enable learning from older data or from hypothetical trajectories (as in Dyna) ([[PDF] Integrated Modeling and Control Based on Reinforcement Learning](https://papers.nips.cc/paper/1990/file/d9fc5b73a8d78fad3d6dffe419384e70-Paper.pdf#:~:text=,Dyna%20is%20based%20on)), analogous to how rehearsal or memory replay methods mitigate forgetting in continual learning.
- **Exploration and Non-Stationarity**: RL deals with non-stationary data distributions when an agent’s policy changes or the environment changes. Sutton’s work on exploration strategies and non-stationary value functions (e.g. in continuing tasks) provides insight into continual learning, where the data distribution can shift over time (new tasks or contexts). Techniques ensuring stability in RL (like eligibility traces and stable function approximation) help inspire mechanisms to balance stability and plasticity in continual learning.

Importantly, Sutton has argued that the traditional ML focus on training static models (what he calls “non-continual learning”) is limiting. He suggests that solving AI requires agents that learn and adapt continually in the long run. In a recent interview, he *“argues the focus on non-continual learning over the past 40 years is now holding AI back”* ([Rich Sutton's new path for AI - Audacy](https://www.audacy.com/podcast/approximately-correct-an-ai-podcast-from-amii-d6257/episodes/rich-suttons-new-path-for-ai-4a1fa#:~:text=Rich%20Sutton%27s%20new%20path%20for,is%20now%20holding%20AI%20back)). In other words, many successes in ML (e.g. deep learning on fixed datasets) may plateau unless we embrace continual learning principles inherent in the RL paradigm. This perspective has encouraged researchers to apply RL-based thinking (like continual exploration, reward-driven adaptation, and lifelong skill acquisition) to broader continual learning problems.

The influence of Sutton’s RL work is also evident in how continual learning researchers design their algorithms. For example, the formal definition of continual reinforcement learning in recent literature ([A Definition of Continual Reinforcement Learning - arXiv](https://arxiv.org/html/2307.11046v2#:~:text=In%20contrast%2C%20continual%20reinforcement%20learning,the%20importance%20of%20continual)) echoes Sutton’s vision of an *always-learning* agent. Overall, Sutton’s reinforcement learning contributions provide both the theoretical foundation and practical algorithms that continual learning research builds upon, underscoring that an agent’s knowledge should *accumulate and adapt over its entire lifetime* rather than being learned once and for all.

## 3. Recent Advances
Continual learning has seen rapid progress in recent years, spurred in part by the deep learning revolution and by the principles established by Sutton and others. Researchers have proposed various strategies to enable neural networks to learn sequentially without forgetting past knowledge. Many of these advances can be seen as elaborations of ideas present in RL or directly influenced by Sutton’s insights (such as using regularization to protect learned knowledge or replaying experiences). Notable recent developments include:

- **Regularization-Based Methods**: These methods add constraints to the learning process to prevent catastrophic forgetting. A prime example is **Elastic Weight Consolidation (EWC)** by Kirkpatrick et al. (2017), which introduces a penalty term to slow down changes to weights important for old tasks. *“EWC allows knowledge of previous tasks to be protected during new learning, thereby avoiding catastrophic forgetting of old abilities”* ([Overcoming catastrophic forgetting in neural networks - ar5iv - arXiv](https://ar5iv.labs.arxiv.org/html/1612.00796#:~:text=arXiv%20ar5iv,It%20does%20so%20by)). This idea of selectively preserving important parameters connects to Sutton’s notion of valuing previously learned predictions – effectively treating certain learned weights as valuable predictions that shouldn’t be overwritten without penalty.

- **Replay and Rehearsal Methods**: Inspired by the replay buffers in RL (which themselves echo Sutton’s Dyna idea of learning from stored experiences), replay-based continual learning stores samples (or generative models of past data) to intermix old and new experiences. For instance, experience replay and **generative replay** (Shin et al., 2017) train the model on both new data and pseudo-data from previous tasks to refresh its memory. These methods operationalize the idea that reusing past experience (as in off-policy RL) can mitigate forgetting.

- **Dynamic Architectures and Expansion**: Some approaches dynamically grow or adjust the model’s architecture to accommodate new tasks, rather than forcing a single static network to handle everything. **Progressive Neural Networks** (Rusu et al., 2016) grow new columns for new tasks and leverage lateral connections to old knowledge, while other methods add neurons or modules on demand. The concept of **transferable features** and **soft gating** in these models resonates with hierarchical RL (options) – retaining modules (skills) learned before and choosing when to use or adapt them. Although Sutton’s work did not explicitly add neurons over time, his options framework and skill reuse ideas provide conceptual support for building systems that accumulate modules of knowledge.

- **Meta-Learning and Few-Shot Adaptation**: Another trend is applying meta-learning so that models can *learn how to learn* continually. Techniques like continual meta-learning adjust a model’s initialization or learning rules such that it can adapt quickly to new tasks without forgetting old ones. These approaches often draw on optimization-based meta-learning, which can be traced back to ideas in RL about tuning learning processes (for example, Sutton’s work on meta-gradient RL for adjusting parameters). The integration of meta-learning with continual learning reflects a convergence of ideas: using past experience to improve future learning efficiency – a principle that is central in reinforcement learning as well.

In addition to these methods, **recent work by Sutton’s own group has directly tackled continual learning challenges in deep networks**. Notably, Hernandez-Garcia, Sutton, and colleagues (2023) identified the “loss of plasticity” phenomenon: deep networks can become resistant to learning new information after prolonged training. They demonstrated this effect on image recognition and RL tasks and underscored its importance. The abstract of their work states that a learning system *“must continue to learn indefinitely. Unfortunately, our most advanced deep-learning networks gradually lose their ability to learn”* ([Maintaining Plasticity in Deep Continual Learning - Rich Sutton](https://www.youtube.com/watch?v=p_zknyfV9fY#:~:text=Abstract%3A%20Any%20learning%20system%20worthy,learning)). By highlighting this issue, they have spurred research into methods to maintain plasticity, such as resetting certain optimizer states, using regularizers to reinvigorate learning, or architectural tweaks (e.g. LayerNorm) to prevent saturation. This is a cutting-edge area building explicitly on Sutton’s legacy – ensuring agents remain adaptable over time.

The state-of-the-art in continual learning is a vibrant mix of these strategies. No single method has completely solved continual learning, but the community has made strides by combining ideas (for example, using both replay and regularization, or meta-learning with dynamic architectures). Researchers like James Kirkpatrick, David Lopez-Paz, Sylvain Lescouz, Joelle Pineau, and many others (often in collaboration with deep learning pioneers like Geoffrey Hinton or Yoshua Bengio) are actively contributing to the field. Ongoing research trends include applying continual learning to large-scale models (e.g., keeping large language models up-to-date), exploring unsupervised continual learning, and improving benchmarks and evaluation protocols for more realistic scenarios. The influence of Sutton’s foundational work is evident throughout these advances – from the incremental learning algorithms at their core to the broader vision of agents that accumulate knowledge over a lifetime.

## 4. Theoretical and Practical Challenges
Despite significant progress, continual learning still faces major theoretical and practical challenges. A foremost issue is **catastrophic forgetting**, the tendency of neural networks to forget previously learned tasks when trained on new ones. This problem was recognized in the 1980s and *“remains a core challenge in continual learning (CL), where models struggle to retain previous knowledge”* ([Mitigating Catastrophic Forgetting in Online Continual Learning by...](https://openreview.net/forum?id=olbTrkWo1D&referrer=%5Bthe%20profile%20of%20Peilin%20Zhao%5D(%2Fprofile%3Fid%3D~Peilin_Zhao2)#:~:text=Mitigating%20Catastrophic%20Forgetting%20in%20Online,to%20retain%20previous%20knowledge)). In other words, even with methods like EWC or replay, completely eliminating forgetting is an open problem ([A Study on Catastrophic Forgetting in Deep LSTM Networks](https://www.researchgate.net/publication/335698970_A_Study_on_Catastrophic_Forgetting_in_Deep_LSTM_Networks#:~:text=Networks%20www,forgetting%20remains%20an%20open%20problem)). Each class of solution so far comes with trade-offs – for example, regularization methods can constrain learning of new tasks, while replay methods require storage or generative models. *“Despite these advances, the problem of catastrophic forgetting remains unresolved. Each proposed solution comes with trade-offs”* ([Catastrophic Forgetting // is FT isn't the answer/solution? - sbagency](https://sbagency.medium.com/catastrophic-forgetting-is-ft-isnt-the-answer-84d251edd726#:~:text=sbagency%20sbagency,offs)).

One underlying difficulty is the **stability–plasticity dilemma**: a learning system must remain stable enough to preserve old knowledge (stability) yet plastic enough to integrate new knowledge (plasticity). Balancing this trade-off is non-trivial ([[PDF] New Insights for the Stability-Plasticity Dilemma in Online Continual ...](https://iclr.cc/media/iclr-2023/Slides/11634.pdf#:~:text=%E2%80%A2%20Stability,%E2%80%A2%20The)). Too much stability and the model becomes rigid (unable to learn new tasks); too much plasticity and it quickly overwrites old knowledge. Sutton’s observation of deep networks losing plasticity ([Maintaining Plasticity in Deep Continual Learning - Rich Sutton](https://www.youtube.com/watch?v=p_zknyfV9fY#:~:text=Abstract%3A%20Any%20learning%20system%20worthy,learning)) is one side of this coin – methods are needed to restore plasticity without causing forgetting. From a theoretical standpoint, there is not yet a unifying framework that explains how to optimally navigate this stability-plasticity balance in continually learning systems.

Another challenge is the **lack of formal theoretical guarantees** in continual learning. Unlike classical machine learning, which has well-developed theories for convergence and generalization (e.g., PAC learning or online learning regret bounds), continual learning scenarios (especially with non-i.i.d. data streams and task switching) are less understood. Researchers are beginning to address this by precisely defining the continual learning problem and its objectives. For instance, recent work has attempted to *“carefully defin[e] the continual reinforcement learning problem”* and formalize agents that learn indefinitely ([A Definition of Continual Reinforcement Learning - arXiv](https://arxiv.org/html/2307.11046v2#:~:text=In%20contrast%2C%20continual%20reinforcement%20learning,the%20importance%20of%20continual)). Such definitions are a first step toward theoretical analysis, but much remains to be done to derive performance guarantees or convergence proofs for continual learning algorithms.

On the practical side, **scalability and real-world deployment** pose challenges. Many continual learning methods are evaluated on relatively small-scale benchmarks or simplified tasks. There is concern about whether these methods will scale to more complex, real-world situations (e.g. robotics, continual learning in autonomous driving, or lifelong learning in interactive agents). A recent study noted a *“misalignment between the actual challenges of continual learning and the evaluation protocols in use”* ([Is Continual Learning Ready for Real-world Challenges? - arXiv](https://arxiv.org/abs/2402.10130#:~:text=This%20paper%20contends%20that%20this,the%20evaluation%20protocols%20in%20use)) – meaning that current benchmarks might not capture real-world complexity (such as continuous task blending, ambiguous task boundaries, or need for open-world learning where new classes emerge). Bridging this gap is essential for practical impact.

Additional practical challenges include:
- **Memory and Compute Constraints**: Some algorithms require storing data from all past tasks or training separate models for each task, which is impractical as tasks accumulate. Continual learners in the wild might be embedded in edge devices with limited resources, so efficiency is key.
- **Task Recognition and Transfer**: In many settings, the boundaries between tasks are not clearly given. The agent must detect distribution shifts or new tasks on its own (the **task-agnostic continual learning** scenario). The agent should also leverage commonalities between tasks (positive transfer) without interference. Achieving strong forward transfer (learning new tasks faster because of prior knowledge) while avoiding negative backward transfer (forgetting or degrading old task performance) is an open research frontier.
- **Theoretical Understanding of Neural Mechanisms**: Catastrophic forgetting is closely linked to how connectionist models distribute knowledge. A deeper theoretical understanding of why neural networks forget (e.g., weight interference, representational overlap) would inform better solutions. Similarly, understanding the “loss of plasticity” in optimization terms (such as plateaus in the loss landscape or saturation of activations) is an ongoing theoretical quest.

Looking forward, researchers identify several **future directions** to address these challenges. Developing a *unified theory of continual learning* is one such direction – possibly extending frameworks like Markov Decision Processes (MDPs) or online learning theory to encompass multiple tasks and non-stationary data. There is also interest in biologically inspired solutions: for example, taking inspiration from how humans and animals consolidate memories during sleep or through complementary learning systems (hippocampus and cortex). Such mechanisms could inform algorithms like experience rehearsal, generative replay, or dynamic reorganization of networks to protect important memories.

In summary, continual learning must overcome enduring challenges of forgetting and stability-plasticity, scale up to realistic problems, and gain stronger theoretical underpinnings. These challenges define an exciting research agenda: each limitation of current approaches points to an opportunity for innovation, where insights from reinforcement learning, neuroscience, and other fields can converge to advance our understanding and capabilities of lifelong learning systems.

## 5. Pathways for Contribution
For a researcher new to the field, there are rich opportunities to contribute to continual learning, especially on the theoretical side. Given the nascent state of a unifying theory, one promising pathway is to work on **formal frameworks and definitions** for continual learning. Clear definitions (such as recent attempts to formally define continual RL ([A Definition of Continual Reinforcement Learning - arXiv](https://arxiv.org/html/2307.11046v2#:~:text=In%20contrast%2C%20continual%20reinforcement%20learning,the%20importance%20of%20continual))) help in deriving analysis and comparing algorithms fairly. A newcomer could contribute by refining these definitions or proposing new metrics to evaluate continual learning (e.g., better measures of forgetting and knowledge transfer, or establishing theoretical bounds on performance degradation). Aligning evaluation protocols with real-world requirements is another impact area – for instance, defining benchmarks or challenge environments that capture the complexities of continual learning (as suggested by the misalignment noted in evaluations ([Is Continual Learning Ready for Real-world Challenges? - arXiv](https://arxiv.org/abs/2402.10130#:~:text=This%20paper%20contends%20that%20this,the%20evaluation%20protocols%20in%20use))).

On the theoretical front, one could delve into **analysis of learning dynamics** in neural networks under continual learning. This might involve studying the mathematical properties of loss landscapes when tasks change, or analyzing simplified models to understand catastrophic forgetting. For example, researching why certain regularization methods succeed or fail could lead to more principled algorithms. There is also room for developing **new algorithms with provable guarantees** – perhaps borrowing techniques from online convex optimization, game theory, or control theory to ensure stability. Bridging reinforcement learning theory (which deals with non-i.i.d. data and long-term credit assignment) with continual learning is fertile ground; ideas like regret minimization in non-stationary bandits or meta-learning guarantees could inspire continual learning theory.

Interdisciplinary intersections are especially promising. A new researcher might explore **neuroscience-inspired mechanisms** in a mathematically rigorous way. For instance, mechanisms of memory consolidation, neurogenesis (growing new neurons), or synaptic gating in the brain could translate into novel neural network architectures that dynamically grow or compartmentalize knowledge. Investigating such biologically motivated approaches could address the stability-plasticity dilemma in new ways (e.g., by creating separate fast and slow learning components, analogous to hippocampus and cortex). Collaboration with cognitive scientists or neuroscientists can provide insights into how natural systems achieve lifelong learning, which in turn can spark theoretical models for artificial systems.

Another pathway is to connect continual learning with **other areas of AI** that are currently booming. For example, continual learning for **large-scale models and lifelong knowledge** is a timely topic – how can we update large language models or vision models with new information continuously, without retraining from scratch or forgetting? This intersects with transfer learning and domain adaptation. A researcher could contribute by devising methods that allow pretrained models to absorb new data over time (important for keeping AI systems up-to-date in dynamic environments). There is also an intersection with **meta-learning and automated curriculum learning**: one can study how an agent might automatically generate or select experiences to maximally retain old knowledge while learning new things (essentially, self-curation of its training data stream).

From an applications standpoint, identifying real-world problems that benefit from continual learning and demonstrating solutions there can be highly impactful. Robotics is a clear example – an autonomous robot should learn from each experience throughout its life. A newcomer might work on a specific application (say, a household robot that learns new chores incrementally, or a recommendation system that adapts to user preference shifts) and contribute algorithms tailored to that context. Such applied work often reveals new theoretical challenges too, closing the loop between practice and theory.

In terms of community and resources, the continual learning field is very open and collaborative. Engaging with workshops and conferences dedicated to lifelong learning is a great way to contribute and get feedback. Notably, the **Conference on Lifelong Learning Agents (CoLLAs)** was launched in 2022 to bring together researchers focusing on continual learning agents ([Conference on Lifelong Learning Agents (CoLLAs)](https://lifelong-ml.cc/#:~:text=The%20Conference%20on%20Lifelong%20Learning,ideas%20on%20advancing%20machine%20learning)). Top machine learning venues (NeurIPS, ICML, ICLR) regularly feature continual learning papers, and journals like *IEEE TPAMI* and *JMLR* have published surveys and special issues on the topic ([A Comprehensive Survey of Continual Learning: Theory, Method ...](https://ieeexplore.ieee.org/document/10444954/#:~:text=A%20Comprehensive%20Survey%20of%20Continual,representative%20methods%2C%20and%20practical)). For a new researcher, contributing could mean publishing innovative findings at these venues, or even simply collaborating on open-source projects (the **ContinualAI** community, for instance, curates repositories and benchmarks for continual learning).

To summarize, a newcomer can contribute to continual learning by:
- **Developing Theory**: Work on formal definitions, stability-plasticity analysis, and deriving guarantees for algorithms.
- **Innovating Algorithms**: Propose new methods (regularization techniques, memory systems, meta-learning strategies) that address current limitations.
- **Cross-Pollination**: Bring ideas from other domains (neuroscience, RL, meta-learning, even evolutionary algorithms or federated learning) to continual learning.
- **Applications and Benchmarks**: Demonstrate continual learning in new applications or create more realistic benchmarks, guiding the field toward practical relevance.
- **Community Engagement**: Participate in continual learning workshops, share findings, and build upon the work of Sutton and others by keeping the conversation between theory and practice active.

Continual learning remains a frontier with many open questions. Rich Sutton’s contributions provide a strong foundation and inspiration – emphasizing that truly intelligent systems must learn *continually*. By building on this foundation and exploring the open problems, new researchers have the opportunity to make significant theoretical and practical advances in the quest for lifelong learning AI systems. 


Below is a concise summary of the key ideas that emerge from the talks and the two papers:

1. **Standard Deep Learning and Continual Learning:**
   - **One‐Time vs. Continual Learning:** Traditional deep‐learning methods (using backpropagation with gradient descent or variants such as Adam) are designed for “one‐time” training on a fixed dataset. In many real‐world applications—such as robotics, streaming data, or online reinforcement learning—the data distribution changes over time, requiring the network to continually learn.
   - **Loss of Plasticity:** Over time, as standard training continues in a nonstationary (continual) learning setting, deep networks lose their “plasticity” (i.e. the ability to quickly adapt to new data). This loss is manifested in several ways:
     - The weights tend to grow larger.
     - A growing fraction of neurons become “dead” (or saturated), meaning that they rarely change their output.
     - The internal representations (the “feature diversity”) become less rich, as measured by a decrease in the effective rank of the hidden layers.
   - This degradation means that—even if early performance on new tasks is good—the network eventually learns no better than a shallow (or even a linear) system when faced with many successive tasks.

2. **Empirical Demonstrations:**
   - Extensive experiments were conducted on supervised tasks (e.g., variations of ImageNet, class-incremental CIFAR‑100, Online Permuted MNIST, and a “Slowly Changing Regression” problem) and reinforcement learning tasks (such as controlling an “Ant” robot with changing friction).
   - In all these settings, standard backpropagation methods initially learn well but then gradually “forget how to learn” (i.e. they lose plasticity) over hundreds or thousands of tasks.

3. **Maintaining Plasticity by Injecting Randomness:**
   - The initial random weight initialization provides many advantages (diverse features, small weights, non-saturation) that enable rapid learning early on. However, because standard backprop only applies this “randomness” at the start, these beneficial properties fade with continued training.
   - The key idea is that **continual learning requires a sustained injection of randomness or variability** to maintain plasticity.

4. **Continual Backpropagation (CBP):**
   - To counteract the decay of plasticity, the authors propose an algorithm called **Continual Backpropagation**. CBP is almost identical to standard backpropagation except that, on every update, it selectively reinitializes a very small fraction of the network’s units.
   - **Selective Reinitialization:** Using a “utility measure” that assesses how useful a neuron (or feature) is for the current task (based on factors such as its activation, its outgoing weight magnitudes, and how much it is changing), the algorithm identifies neurons that are “underused” or “dead.” These neurons are then reinitialized (with the initial small random values), thereby reintroducing diversity and the benefits of a fresh start.
   - This process—sometimes called a “generate-and-test” mechanism—allows the network to continually inject new random features without having to completely reset or lose past learning.

5. **Comparison with Other Methods:**
   - Other techniques such as L2 regularization, Shrink and Perturb (which combines weight shrinkage with noise injection), dropout, and normalization were examined.
   - Although L2 regularization and Shrink and Perturb help slow the growth of weights and partially mitigate the loss of plasticity, they are generally less robust than CBP. In some experiments (both in supervised and reinforcement learning settings), popular methods like Adam (with standard parameters), dropout, and even batch normalization actually worsened the loss of plasticity over time.

6. **Implications for Continual and Reinforcement Learning:**
   - The findings imply that if deep neural networks are to be deployed in environments where continual adaptation is necessary, the training algorithms must be modified to continuously “refresh” the network’s ability to learn.
   - In reinforcement learning, where both the environment and the agent’s behavior can change over time, the loss of plasticity is especially problematic. The continual backpropagation approach (sometimes combined with a small amount of L2 regularization) was shown to significantly improve performance in nonstationary RL tasks (for example, in controlling an ant robot in environments with changing friction).

7. **Broader Perspective:**
   - The work challenges the assumption that gradient descent alone is sufficient for deep learning in dynamic, nonstationary settings.
   - It suggests that “sustained deep learning” (learning that continues to adapt over time) may require algorithms that combine traditional gradient-based methods with mechanisms for continual variability—in effect, a built-in “refresh” mechanism similar to how biological systems continually reorganize and adapt their neural circuitry.

In summary, the key idea is that standard deep learning methods gradually lose their ability to adapt (loss of plasticity) when faced with a continual stream of new tasks. The proposed solution is to modify backpropagation so that it continuously injects new random features (through selective reinitialization of low-utility units), thereby maintaining the network’s plasticity and enabling it to learn indefinitely in nonstationary environments.


Below is a mathematical‐level explanation of the key ideas behind loss of plasticity in continual learning and the “Continual Backpropagation” (CBP) solution.

---

### 1. Standard Backpropagation and Its Limitations

A deep neural network is parameterized by weights
$$
\mathbf{w} = \{w_{l,i,k}\}
$$
where
- $l$ indexes layers,
- $i$ indexes neurons (or “features”) in layer $l$,
- $k$ indexes neurons in layer $l+1$.

**Initialization:**  
Weights are initially drawn from a “small‐random” distribution, e.g.,
$$
w_{l,i,k}(0) \sim d \quad \text{with} \quad d = \mathcal{U}(-b,b),
$$
where $b$ is chosen (e.g., via Kaiming initialization) so that the activations do not saturate.

**Gradient Descent Update:**  
For each training example (or mini‐batch), the standard update is
$$
w_{l,i,k}(t+1) = w_{l,i,k}(t) - \alpha\, \nabla_{w_{l,i,k}} L(t),
$$
where
- $\alpha$ is the learning rate,
- $L(t)$ is the loss at time $t$.

**Loss of Plasticity:**  
When training continually on a nonstationary stream of data (or a long sequence of tasks), several phenomena occur:
- **Weight Growth:** The weights tend to grow larger over time.
- **Feature Saturation / “Dead” Units:** For activations like ReLU, if a neuron’s output $h_{l,i}(x)$ is zero (or nearly so) for almost all inputs, then
  $$
  \mathbb{P}\bigl[h_{l,i}(x)=0\bigr] \approx 1,
  $$
  the neuron is “dead” and its gradient becomes zero.
- **Representation Collapse (Low Effective Rank):**  
  For a given hidden layer, let $\Phi$ be the matrix of activations across examples. The *effective rank* of $\Phi$ is defined as
  $$
  \operatorname{erank}(\Phi) = \exp\left(-\sum_{k=1}^{q} p_k \log p_k\right),\quad p_k = \frac{\sigma_k}{\sum_{j=1}^{q}\sigma_j},
  $$
  where $\sigma_1,\dots,\sigma_q$ are the singular values of $\Phi$ (with $q = \max\{n, m\}$). A decrease in $\operatorname{erank}(\Phi)$ indicates that the network’s internal representation has lost diversity.

In continual learning, it is observed that after many tasks the network’s performance (say, measured by the error $E(t)$) deteriorates—often approaching or even falling below the performance of a shallow or linear model. In symbols, one finds for standard backpropagation that
$$
\lim_{T\to\infty} E_{\text{BP}}(T) \gtrsim E_{\text{linear}},
$$
indicating a loss of the “plasticity” needed to learn new tasks.

---

### 2. A Utility Measure for Neurons

The intuition is that the “good” properties of the network—diverse, non‐saturated features with small weights—arise from the initial random distribution. To maintain these advantages over time, one can track the “utility” of each neuron and selectively refresh those that are under‐used.

**Contribution Utility:**  
For neuron $i$ in layer $l$ at time $t$, define an instantaneous measure of its contribution as:
$$
c_{l,i}(t) = \; |h_{l,i}(t)| \; \sum_{k=1}^{n_{l+1}} |w_{l,i,k}(t)|,
$$
where
- $h_{l,i}(t)$ is the neuron's output,
- $\sum_{k}|w_{l,i,k}(t)|$ measures the total “influence” of neuron $i$ on the next layer.

To smooth this over time, one can maintain a running average:
$$
c_{l,i,t} = (1-\eta)\, c_{l,i}(t) + \eta\, c_{l,i,t-1},
$$
with decay rate $\eta \in (0,1)$.

**Adaptation Utility:**  
Because the speed at which a neuron can change is also important, one may consider an “adaptation utility” inversely related to the magnitude of its incoming weights:
$$
a_{l,i}(t) = \frac{1}{\sum_{j=1}^{n_{l-1}} |w_{l-1,j,i}(t)|},
$$
or a running average thereof.

**Overall Utility:**  
A combined measure might then be given by (after bias‐correction)
$$
y_{l,i}(t) = \frac{|h_{l,i}(t) - \hat{f}_{l,i}(t)|\;\sum_{k=1}^{n_{l+1}} |w_{l,i,k}(t)|}{\sum_{j=1}^{n_{l-1}} |w_{l-1,j,i}(t)|},
$$
and its running average
$$
u_{l,i,t} = (1-\eta)\, y_{l,i}(t) + \eta\, u_{l,i,t-1}.
$$
Finally, a bias-corrected utility can be computed as
$$
\hat{u}_{l,i,t} = \frac{u_{l,i,t}}{1-\eta^{a_{l,i}}},
$$
where $a_{l,i}$ may also serve as the “age” of the neuron (i.e. the number of updates since its last reinitialization).

Low values of $\hat{u}_{l,i,t}$ indicate that the neuron is “underperforming” or has become “stale.”

---

### 3. Continual Backpropagation (CBP) Algorithm

The CBP algorithm augments standard gradient descent by periodically “refreshing” low-utility neurons. Mathematically, the CBP procedure for each layer $l$ is as follows:

1. **Standard Update:**  
   For each weight, perform the gradient update:
   $$
   w_{l,i,k}(t+1) = w_{l,i,k}(t) - \alpha\, \nabla_{w_{l,i,k}} L(t).
   $$

2. **Age Update:**  
   For each neuron $i$ in layer $l$, update its age:
   $$
   a_{l,i} \leftarrow a_{l,i} + 1.
   $$

3. **Utility Update:**  
   Update the running utility $u_{l,i,t}$ as described above.

4. **Selective Reinitialization:**  
   For each layer $l$, define a replacement fraction $\rho$ (a small number, e.g. such that on average one neuron is reinitialized every few hundred updates). Then for neurons $i$ that satisfy:
   - $a_{l,i} \ge m$ (i.e. they are “mature” enough), and
   - $\hat{u}_{l,i,t}$ is among the lowest $\rho n_l$ values,
   
   perform the following:
   - **Reset Incoming Weights:**  
     $$
     w_{l-1}[:, i] \sim d,
     $$
     i.e. re-sample the incoming weights from the original distribution.
   - **Reset Outgoing Weights:**  
     $$
     w_{l}[i, :] = 0,
     $$
     so that the new neuron does not perturb the current function.
   - **Reset Utility and Age:**  
     $$
     u_{l,i,t} \leftarrow 0,\quad a_{l,i} \leftarrow 0.
     $$

This additional “generate-and-test” step keeps a small fraction of neurons “fresh” so that the benefits of the initial randomness (small weights, diverse activations) persist indefinitely.

---

### 4. Mathematical Effects and Comparisons

**Under Standard Backpropagation:**  
- The weight magnitudes $W(t) = \frac{1}{N}\sum_{l,i,k} |w_{l,i,k}(t)|$ tend to increase with time.
- The effective rank $\operatorname{erank}(\Phi(t))$ of hidden layer activations decreases.
- The fraction of dead neurons (those for which $h_{l,i}(x)=0$ for almost all $x$) increases.

As a consequence, if $E_{\text{BP}}(T)$ denotes the error after $T$ tasks, then
$$
\lim_{T\to\infty} E_{\text{BP}}(T) \approx E_{\text{linear}},
$$
meaning the network’s performance degrades to that of a shallow model.

**Under Continual Backpropagation (CBP):**  
The periodic reinitialization maintains:
- **Bounded Weight Magnitudes:** $W(t)$ remains low.
- **High Effective Rank:** $\operatorname{erank}(\Phi(t))$ stays high, indicating diverse representations.
- **Low Fraction of Dead Units:** Most neurons remain active.
  
Thus, the error $E_{\text{CBP}}(T)$ remains low (and often improves) over many tasks:
$$
\lim_{T\to\infty} E_{\text{CBP}}(T) \ll E_{\text{BP}}(T).
$$

In reinforcement learning, where both the environment and the agent’s actions continually change the data distribution, similar mathematical effects are observed. For example, the agent’s cumulative reward $R(t)$ under standard methods may plateau or even decrease, whereas with CBP (often combined with a modest amount of L2 regularization), the reward remains high.

---

### 5. Summary

Mathematically, the key innovations are:
- Recognizing that the properties of the initial random weight distribution—small magnitude, diversity, non-saturation—are crucial for rapid adaptation.
- Defining a utility measure $u_{l,i,t}$ for each neuron that combines its contribution (via activations and outgoing weights) and its capacity to adapt (via the inverse of the incoming weights).
- Implementing a selective reinitialization rule that, when $a_{l,i} \ge m$ and $u_{l,i,t}$ is low (specifically, among the lowest $\rho$ fraction), resets the neuron's weights to reintroduce the beneficial properties of the initial state.
- This procedure mathematically maintains a low overall weight magnitude, high effective rank, and low incidence of dead neurons, thereby preserving the network’s plasticity and ensuring continued learning in nonstationary or continual learning settings.

This approach—combining standard gradient descent with a continual, selective “refresh” of low‐utility neurons—provides a mathematically grounded mechanism to overcome the loss of plasticity that plagues standard deep learning when faced with a long sequence of tasks.


================================================================================

--- Processing: nn_interpret.md ---
Thanks for the detailed clarifications! I will conduct research on existing academic and open-source efforts related to analyzing neural network activations, weights, and interpretability, particularly with an emphasis on:
- Tracking activations and weight changes both during and after training.
- Extensible architectures (starting from MLP but scalable to CNNs, Transformers, etc.).
- Methods for displaying raw activations and weight matrices while considering cases where the data is too large.
- Interactive vs. static approaches for understanding neural network behavior.
- Potential for an "agentic" framework, where the tool actively interacts with the model, making decisions on what to analyze next based on intermediate results.

I'll return with findings on relevant existing projects, techniques, and potential inspiration for refining the conceptual design.

# Analyzing Neural Network Activations, Weights, and Interpretability

## Tools for Tracking Activations and Weights During Training
- **TensorBoard & Logging Libraries:** Basic tools like TensorBoard can log metrics, weights, and activations at each training step, providing simple visualizations (e.g. histograms of weight magnitudes) over time. However, these often focus on summary statistics and may miss instance-specific behaviors ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=What%20Parts%20of%20the%20Network,these%20correlate%20with%20other%20inputs)). More advanced logging frameworks (Weights & Biases, Visdom, etc.) offer custom dashboards for tracking arbitrary values during training ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Visdom,for%20different%20types%20of%20data)), but still require deciding **a priori** what to record.  
- **Comgra (2024):** An open-source library designed to flexibly inspect network parameters and intermediate tensors throughout training ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Comgra.%20Comgra%20,details%20for%20a%20specific%20input)). Comgra addresses the “combinatorial explosion” of possible things to monitor by letting users interactively choose which activations or weights to explore after training, avoiding the need to rerun experiments ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=The%20Need%20for%20Flexibility,At)). It records selected tensors during training and provides a GUI to navigate them, balancing aggregate statistics with per-example details in one interface ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=layer,a%20noticeable%20loss%20in%20performance)). This ensures one can examine both *overall trends* and *specific cases* without a huge logging overhead. Usage is analogous to TensorBoard: instrument the training loop to save data, then open a browser UI for interactive exploration ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Usage,as%20well%20as%20code%20examples)).  
- **TorchLens (2023):** A Python package for extracting and visualizing hidden-layer activations in PyTorch models ([TorchLens: A Python package for extracting and visualizing hidden ...](https://www.biorxiv.org/content/10.1101/2023.03.16.532916v1#:~:text=,layer%20activations%20in%20PyTorch%20models)). It makes it easy to tap into any layer of an MLP, CNN, or Transformer to record activations and even the computational graph. TorchLens provides detailed visualizations of model architectures (like an improved computation graph over TensorBoard’s basic graph) ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Visualizing%20the%20Computation%20Graph,a%20toolkit%20for%20visualizing%20models)), and lets researchers **probe activations** without writing boilerplate code. This facilitates comparisons of activation patterns across layers or between two training epochs.  
- **Tracking Weight Dynamics:** Beyond activations, some tools focus on how **weight matrices** evolve. For example, WeightWatcher is a research-inspired tool that analyzes trained weight matrices using heavy-tailed spectrum metrics ([WeightWatcher: Data-Free Diagnostics for Deep Learning](https://weightwatcher.ai/#:~:text=WeightWatcher%20%28w%7Cw%29%20is%20an%20open,JMLR%2C%20Nature%20Communications%2C%20and%20NeurIPS2023)) ([WeightWatcher: Data-Free Diagnostics for Deep Learning](https://weightwatcher.ai/#:~:text=%23%20Weightwatcher%20computes%20unique%20layer,quality%20metrics)). It can flag layers that are *under-trained* or *over-regularized* via an alpha metric (ideal range ~2–6 for well-trained layers) ([WeightWatcher: Data-Free Diagnostics for Deep Learning](https://weightwatcher.ai/#:~:text=weightwatcher%20,be%20between%202%20and%206)). Such analysis can be done at checkpoints during training to see if certain layers have converged or not. Academic studies of weight dynamics (e.g. how initialization or optimization affects weight trajectories) provide theoretical insight ([New insights into training dynamics of deep classifiers | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2023/training-dynamics-deep-classifiers-0308#:~:text=The%20paper%2C%20%E2%80%9CDynamics%20in%20Deep,of%20the%20layers%20are%20intertwined)) ([New insights into training dynamics of deep classifiers | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2023/training-dynamics-deep-classifiers-0308#:~:text=fit%20a%20training%20dataset%20will,to%20accurately%20classify%20new%20examples)), but practical tools for live weight tracking tend to reduce information to summaries (means, variances, spectral norms, etc.). A well-designed framework might log weight distribution histograms over time or compute metrics like WeightWatcher’s *alpha* at each epoch to observe training progress per layer.

## Interpretability Techniques from MLPs to CNNs to Transformers
- **Attribution Libraries:** For trained models, general interpretability frameworks like **Captum** (PyTorch’s interpretability library) provide a suite of attribution methods (saliency maps, integrated gradients, etc.) to explain predictions ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Attribution%20and%20Interpretability,to%20interpret%20your%20model%E2%80%99s%20behavior)). Such tools apply to MLPs, CNNs, or Transformers in a model-agnostic way, treating the network as a black box to attribute importance to inputs or neurons. This helps answer “why did this input yield that output?” by tracking influence through the network.  
- **Mechanistic Interpretability Toolkits:** A growing set of libraries focus on opening the black box of **internal mechanisms**, often starting with simple models and scaling up. For example, **TransformerLens** (Nanda & Bloom 2022) allows loading pretrained transformers and inspecting or even modifying their internals (attention patterns, layer outputs) easily ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Mechanistic%20Interpretability,toolkit%20for%20sequence%20generation%20models)). Similarly, **Pyrene** and **NNSight** provide interfaces to intervene on activations or weights during runs ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Mechanistic%20Interpretability,toolkit%20for%20sequence%20generation%20models)). These emerged from research in **mechanistic interpretability**, where investigators often begin with small MLPs or toy models to understand learned algorithms, then extend methods to larger CNNs and Transformers. Techniques like **network surgery** (ablating or re-weighting neurons) and **counterfactual inputs** (designed to target specific neurons) are supported in such frameworks to test hypotheses about model behavior.  
- **Architecture-General vs. Specialized Approaches:** Simpler multi-layer perceptrons (MLPs) can be analyzed with generic approaches (e.g. recording activation values, visualizing weights as heatmaps) that carry over to deeper architectures. CNNs introduce structured weights (filters) that can be visualized as images, and activations that can be seen as feature maps; accordingly, tools like **ActiVis** and **Deep Visualization Toolbox** were created to explore CNN internals in real time by showing each layer’s feature maps for a given input ([Understanding Neural Networks Through Deep Visualization](https://anhnguyen.me/project/understanding-neural-networks-through-deep-visualization/#:~:text=Understanding%20Neural%20Networks%20Through%20Deep,files%20or%20read%20video)) ([Deep Visualization Toolbox Open-source software...](https://prostheticknowledge.tumblr.com/post/123726938701/deep-visualization-toolbox-open-source-software#:~:text=software,the%20reaction%20of%20every%20neuron)). Transformers have specialized components (self-attention matrices, multi-head attention, etc.), spurring custom visualization tools like **BertViz** (for attention patterns) and libraries like **Inseq** for sequence-to-sequence models ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Mechanistic%20Interpretability,toolkit%20for%20sequence%20generation%20models)). The key trend is that methods are increasingly *extensible*: an insight gained from a simple MLP (say, tracking neuron activation distributions) can be scaled to thousands of neurons in a CNN, or millions in a Transformer, with the aid of automation and visualization techniques.

## Visualizing and Summarizing Large Activations and Weights
Interpreting a network often means dealing with **high-dimensional data** – e.g. millions of activations across a dataset, or weight matrices with thousands of parameters. Researchers have developed strategies to summarize and visualize this information:

- **Activation Atlases:** Google Brain’s *Activation Atlas* technique (2019) is a prime example of summarizing large activation spaces. By applying dimensionality reduction and feature visualization to millions of intermediate activations, they create an *“explorable activation atlas”* that maps out the prominent features a network has learned ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=By%20using%20feature%20inversion%20to,network%20typically%20represents%20some%20concepts)). Instead of examining one input at a time, an atlas provides a global view of concepts (for instance, clusters of neurons responding to “electronics” or “animal faces” appear as regions in the map) ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=Above%2C%20an%20activation%20atlas%20of,screen%20version)). This kind of visualization helps show *which concepts are represented and where*, giving a big-picture understanding of a CNN’s feature space. It directly addresses the limitation that inspecting single inputs “doesn’t give us a big picture view… when what we want is a map of an entire forest, inspecting one tree at a time will not suffice” ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=Unfortunately%2C%20visualizing%20activations%20has%20a,a%20time%20will%20not%20suffice)).
- **Network Dissection:** *Network Dissection* (Bau et al. 2017) offers a way to compress a complex CNN’s behavior into human-readable summaries by automatically labeling neurons with semantic concepts ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=,our%20method%20to%20compare%20the)). It uses a broad set of visual concepts (like textures, objects, parts) and checks which neurons strongly respond to those concepts in a dataset. The result is a dictionary of neurons and their likely semantic roles (e.g. “neuron 123 = detects cats”). Importantly, this framework quantifies interpretability (what fraction of neurons have clear meanings) and can be applied at different training stages ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=latent%20representations%20of%20various%20networks,interpretability%20of%20deep%20visual%20representations)). For instance, one can see neurons gradually specialize as training proceeds ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=latent%20representations%20of%20various%20networks,interpretability%20of%20deep%20visual%20representations)), helping track *how representational structure emerges*. This is a form of summarization: instead of showing all weights, it highlights a few salient ones with descriptions.
- **Weight Visualizations & Metrics:** For weight matrices, straightforward visualizations include heatmaps of weight values or their distributions. For CNNs, visualizing the first-layer filters gives an intuition of learned edges or color detectors; for deeper layers, tools like **Netron** and **Penzai** focus on visualizing model structures (shapes of weight tensors, connectivity) to manage complexity ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Visualizing%20the%20Computation%20Graph,a%20toolkit%20for%20visualizing%20models)). Summarization metrics can distill large weight sets into numbers: e.g., the **WeightWatcher** tool computes metrics like the *alpha* exponent of layer weight spectra, condensing each layer’s quality into a single number ([WeightWatcher: Data-Free Diagnostics for Deep Learning](https://weightwatcher.ai/#:~:text=weightwatcher%20,be%20between%202%20and%206)). These numbers can be tracked across layers or over training time to find outliers (e.g. a layer whose weights are degenerate or poorly trained). Such quantitative summaries are easier to visualize (as bar charts or trend lines) than the raw weight matrices themselves.
- **Dimensionality Reduction and Embeddings:** Another approach is to embed high-dimensional activations or neurons into lower dimensions for visualization. Techniques like t-SNE or UMAP can project activation vectors (for many data points) into 2D, revealing clustering of neurons or data by similarity. For example, plotting the activations of a certain layer for thousands of inputs might show distinct clusters corresponding to classes. Similarly, one can treat each neuron as a point in a high-dimensional space (defined by its responses across many inputs) and use embedding to find groups of neurons that behave similarly. These visualizations, while not as directly interpretable as Activation Atlases, help **spot structure** in otherwise unwieldy tensors – an important step before feeding results to an interpretability assistant like ChatGPT for summarization.

## Interactive vs. Static Approaches
Interpretability tools vary in how users engage with them:

- **Interactive Tools and Frameworks:** Interactive systems allow users to pose new queries, adjust inputs, and immediately see results, which is invaluable for exploratory analysis. **ActiVis** (Facebook, 2017) is an early example: an interactive visualization system for large-scale models that integrates multiple coordinated views, such as an architecture graph and a neuron activation heatmap, enabling pattern discovery at both instance-level and subset-level ([[1704.01942] ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models](https://arxiv.org/abs/1704.01942#:~:text=participatory%20design%20sessions%20with%20over,of%20how%20ActiVis%20may%20work)). Users could select subsets of data (say, all images of “cats”) and see which neurons fire strongly, or click on a neuron to see what inputs activate it – all in a fluid UI. Similarly, Google’s open-source **Language Interpretability Tool (LIT)** provides an interactive dashboard for NLP models ([The Language Interpretability Tool (LIT): Interactive Exploration and Analysis o](https://research.google/blog/the-language-interpretability-tool-lit-interactive-exploration-and-analysis-of-nlp-models/#:~:text=With%20these%20challenges%20in%20mind%2C,extensible%20visualizations%20and%20model%20analysis)). It supports *local explanations* (e.g. salience maps on a specific sentence) and *aggregate analysis* (e.g. embedding projections of an entire dataset) side by side ([The Language Interpretability Tool (LIT): Interactive Exploration and Analysis o](https://research.google/blog/the-language-interpretability-tool-lit-interactive-exploration-and-analysis-of-nlp-models/#:~:text=LIT%20supports%20local%20explanations%2C%20including,in%20our%20system%20demonstration%20paper)). Users can generate counterfactual inputs on the fly and see how the model’s predictions and internal activations change, facilitating a tight human-in-the-loop investigation cycle ([The Language Interpretability Tool (LIT): Interactive Exploration and Analysis o](https://research.google/blog/the-language-interpretability-tool-lit-interactive-exploration-and-analysis-of-nlp-models/#:~:text=LIT%20supports%20local%20explanations%2C%20including,in%20our%20system%20demonstration%20paper)). Interactive tools typically emphasize **flexibility and drill-down**: one can start with a broad overview and then zoom into particular cases, or vice versa, to test hypotheses about model behavior in real time.  
- **Static Analysis and Reports:** In contrast, many interpretability techniques yield static outputs – think of a research paper figure showing a set of maximally activating images for several neurons, or a plot of weight distributions at epoch end. Static approaches include saliency maps or Grad-CAM heatmaps produced for a fixed set of inputs, or feature visualizations of neurons (e.g. the synthesized images that strongly activate a neuron). These are often insightful but are inherently limited to the scenarios the researcher anticipated. They don’t easily allow asking new questions of the model without going back to code. For example, a static *feature visualization* shows what one neuron likes, but if you suddenly wonder how that neuron behaves for a specific real input, you’d need to run an experiment outside of the static report. Static results are great for communication (e.g. illustrating a learned feature in a publication) and for documenting known behaviors, but they lack the ability to **adapt** to the analyst’s curiosity in the moment. Modern tools aim to bridge this gap: even Distill.pub articles often embed interactive widgets so that what starts as a “static” article becomes a playground for the reader. This trend recognizes that interpretability is often an iterative process of discovery, benefitting from interactive exploration rather than one-shot analysis.

## Agentic and Automated Analysis Approaches
A recent and exciting development is the idea of an **“agentic” interpretability tool** – one that actively decides what to inspect next, rather than just passively visualizing predetermined data. Instead of a human manually probing the network step by step, an *AI agent* can leverage intermediate findings to guide further analysis. Two notable examples:

- **MIT’s Multimodal Automated Interpretability Agent (MAIA, 2024):** This system uses a pretrained language model equipped with a suite of tools to conduct interpretability research on neural nets ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=This%20paper%20describes%20maia%2C%20a,of%20maia%20to%20computer%20vision)) ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=trained%20vision,Across%20several%20trained%20models%20and)). In essence, MAIA behaves like a research scientist: it can synthesize new inputs (for example, generate images or edit text) to test what causes a particular neuron or sub-network to activate, find real dataset examples that maximally activate a component, and then *summarize its observations in natural language*. The agent iteratively forms hypotheses (“Neuron X seems to respond to *cars*”), designs experiments to verify them (e.g. feed images of cars, planes, boats to see if it fires only on cars), and refines its understanding based on results ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=For%20example%2C%20FIND%20contains%20synthetic,The)) ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Sarah%20Schwettmann%20PhD%20%2721%2C%20co,%E2%80%9D)). Notably, MAIA was able to produce neuron descriptions for vision models that were comparable to human experts’ descriptions ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=experimentation%20on%20subcomponents%20of%20other,truth)) ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=Interpretability%20experiments%20proposed%20by%20maia,classified.%E2%80%A1)). It also tackled higher-level tasks like identifying biases or spurious features by actively searching for inputs that trigger those behaviors. This agentic approach demonstrates that language models (with the right tooling) can go beyond static summarization – they can *actively explore* a neural network, which is a promising direction for complex models that are too large for exhaustive manual probing ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Central%20to%20this%20strategy%20is,other%20systems%20in%20real%20time)) ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=examples%2C%20the%20AIA%20actively%20participates,other%20systems%20in%20real%20time)).
- **OpenAI’s Automated Neuron Explanations (2023):** OpenAI researchers recently explored using GPT-4 to explain neurons in GPT-2 ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=A%20recent%20paper%20by%20a,step%20towards%20automating%20DNN%20interpretability)). Their method is a three-step loop: (1) **Explain** – prompt GPT-4 with examples of a neuron’s top activations and ask it to hypothesize in plain English what the neuron looks for; (2) **Simulate** – have GPT-4 (or another model) predict the neuron’s activation on a wide range of inputs based on that hypothesis; (3) **Score** – compare the simulated activations against the actual neuron activations to see how well the explanation holds up ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=A%20recent%20paper%20by%20a,step%20towards%20automating%20DNN%20interpretability)) ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=In%20the%20first%20step%2C%20a,find%20phrases%20related%20to%20community)). For example, GPT-4 might guess *“Neuron 245 activates for phrases about community or gatherings”* ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=In%20the%20first%20step%2C%20a,find%20phrases%20related%20to%20community)). The system then checks this by seeing if Neuron 245 indeed fires on words like *“team, group,”* etc., and not on unrelated words. If the match is good, the explanation is validated; if not, the process can iterate with a refined prompt. This approach effectively uses an LLM as an *analyst* that both proposes and evaluates interpretability hypotheses. It’s “agentic” in the sense that the AI is taking on tasks a human analyst would do – generating candidate explanations and testing them – all in an automated loop. While currently focused on individual neurons in language models, the method could extend to analyzing entire circuits or interactions between neurons. It highlights how a ChatGPT-like model can be harnessed as a powerful interpretability assistant, leveraging its world knowledge to articulate what a pattern in activations might represent, and its generation capabilities to design experiments.

These agent-driven methods are at the frontier of interpretability research. They marry the strengths of deep learning (pattern recognition and generation) with the investigative process of science. Crucially, they can scale analysis in ways humans alone might struggle with, by quickly sifting through thousands of neurons and zeroing in on the interesting ones with proposed meanings ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=For%20example%2C%20FIND%20contains%20synthetic,and%20not%20air%20or%20sea)) ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Sarah%20Schwettmann%20PhD%20%2721%2C%20co,%E2%80%9D)). The “FIND” benchmark introduced alongside the AIA work ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Complementing%20the%20AIA%20method%20is,g)) even provides ground-truth functions to systematically evaluate how well such agents explain known computations – a sign that this approach is maturing.

## Inspirations for a ChatGPT-Based Neural Network Analysis Tool
Bringing these findings together, we can envision a new neural network analysis tool powered by ChatGPT (or similar LLMs) that leverages the best of both worlds: human-friendly dialogue and powerful automated analysis. Key design inspirations include:

- **Logging and UI from Training to Inference:** Like Comgra and TorchLens, the tool should allow tracking of any activation or weight of interest during training and afterward. The **flexibility** emphasized by Comgra – to choose between individual activations vs. summary stats, different time points, and different inputs on the fly ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=What%20Parts%20of%20the%20Network,these%20correlate%20with%20other%20inputs)) ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=The%20Need%20for%20Flexibility,At)) – suggests our ChatGPT-based tool must be able to fetch *both* granular data (e.g. “What were the layer-2 weights at epoch 5 vs epoch 50?”) and high-level summaries (“How did the weight distribution change?”) on demand. A possible implementation is a back-end logging system that records extensive data (perhaps guided by heuristics to keep it manageable), which ChatGPT can query via an API. The ChatGPT interface can then present insights in natural language, supplemented by small charts or tables, much like a conversational TensorBoard. This marries the interactive exploration of training dynamics with an LLM’s ability to summarize and explain those dynamics in plain English.
- **Model-Agnostic Analysis Modules:** To handle MLPs, CNNs, and Transformers uniformly, the tool can draw on ideas from Captum and mechanistic interpretability libraries. For instance, it could have a **“saliency probe”** that ChatGPT can invoke to compute attributions for a given input and model, or a **“activation extractor”** for any layer. By wrapping these techniques in an API, ChatGPT could say, *“I will compute which features most influenced this output”*, call an attribution method, and then explain the results. The tool’s architecture might thus be an *agent* (ChatGPT) orchestrating various modules (for attribution, activation visualization, etc.), similar to how MAIA uses a library of interpretability tools ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=rather%20than%20labeling%20features%20in,sweeps%20over%20entire%20networks%2C%20or)) ([A Multimodal Automated Interpretability Agent](https://arxiv.org/html/2404.14394v1#:~:text=modular%20design%20of%20the%20tool,we%20use%20the%20following%20set)). Starting with simple tests on an MLP (e.g. *“Does neuron 4 fire for positive numbers?”*), the same agent could seamlessly scale up to a ResNet or Transformer, because it can query appropriate modules (e.g. attention pattern analyzer for Transformers, filter visualizer for CNNs). The **extensible design** of LIT – where new components can be added for new model types ([The Language Interpretability Tool (LIT): Interactive Exploration and Analysis o](https://research.google/blog/the-language-interpretability-tool-lit-interactive-exploration-and-analysis-of-nlp-models/#:~:text=Customizability)) – is a good blueprint for keeping the tool relevant as architectures evolve.
- **Handling Large Data via Summarization:** The challenge of large activations and weight matrices can be tackled by combining visualization techniques with ChatGPT’s summarization capabilities. For example, the tool might internally generate an Activation Atlas for a particular layer  ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=By%20using%20feature%20inversion%20to,network%20typically%20represents%20some%20concepts)), then have ChatGPT *interpret the atlas*: *“Layer 5 appears to have clusters for ‘building structures’ and ‘foliage textures’, indicating specialized feature detectors.”* By automating techniques like dimensionality reduction or concept labeling (à la Network Dissection), the tool can feed ChatGPT higher-level descriptors instead of raw numbers. ChatGPT’s strength in language means it could take a set of neuron labels or a graph of neuron clusters and produce a coherent narrative: *“Early layers differentiate mainly simple edges, while later layers in the CNN have neurons grouped into semantically rich concepts (faces, text, etc.) ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=Above%2C%20an%20activation%20atlas%20of,screen%20version)). Many neurons are devoted to texture in layer 3, which might explain why the model is texture-biased.”* Similarly, for weights, ChatGPT could report: *“Layer 10’s weight matrix has a heavy-tailed distribution (alpha ~7), which WeightWatcher suggests is a sign of under-training ([WeightWatcher: Data-Free Diagnostics for Deep Learning](https://weightwatcher.ai/#:~:text=weightwatcher%20,be%20between%202%20and%206)). This might be a weak link in the network’s performance.”* These kinds of interpretations directly draw from research insights and make them accessible.
- **Interactive Conversational Interface:** Inspired by interactive tools like ActiVis and LIT, the new tool’s interface is conversational but could also include rich media. A user might ask, *“Show me how the activations of layer 2 changed during training”*, and ChatGPT could present a small trend plot of activation means or a description: *“Layer 2’s activation variance increased and then stabilized after epoch 10, suggesting it learned a diversified set of features early on.”* The user could then ask, *“Which neurons in layer 2 are most active for class ‘cat’ images?”*, and the agent would fetch that info (perhaps by scanning the dataset) and reply with an answer and possibly an embedded image of those neurons’ feature visualizations. This *interactive Q&A* style makes analysis accessible – you don’t need to write code or dig through logs; you can simply ask questions about the network’s internals. It’s essentially ChatGPT acting as a knowledgeable guide through the model, powered by real data.
- **Automated “Agentic” Investigations:** Taking a cue from MAIA and OpenAI’s neuron explainer, the tool could have an *autonomous mode* where it performs a series of analyses by itself and reports findings. For example, upon loading a new model, the ChatGPT agent could systematically: scan for neurons with high variance, generate hypotheses about their roles (perhaps by retrieving the top activating inputs and asking itself what they have in common), and then present the user with a summary: *“I noticed neuron 87 in layer 5 consistently activates for images with text – it might be an ‘OCR/text detector’ unit. Neuron 21 in layer 7 seems to pick up on dog faces. There are 10 neurons in layer 9 that together respond to different colors, suggesting color-sensitive features.”* Each of these findings would be backed by evidence the agent gathered (which the user could ask to see, e.g. “show me examples”). The agent can also identify problematic behaviors: *“I tested the model on counterfactual inputs and found that flipping gendered words changes the output significantly, indicating a bias.”* By proactively looking for such patterns, the tool can surface critical insights without the user needing to know exactly what to ask. This *exploratory analysis mode* is directly inspired by the success of automated interpretability agents ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Central%20to%20this%20strategy%20is,other%20systems%20in%20real%20time)) ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Sarah%20Schwettmann%20PhD%20%2721%2C%20co,%E2%80%9D)) and would be a standout feature of a ChatGPT-powered framework.

In summary, recent academic research and open-source projects chart a clear path toward a more intelligent neural network analysis tool. From **tracking training dynamics** (Comgra, TorchLens) to **visualizing internals** (ActiVis, LIT) and **automating interpretability** (MAIA, OpenAI’s GPT-4 explainer), each provides pieces of the puzzle. A new tool based on ChatGPT APIs can combine these pieces: it would log and visualize like existing frameworks, but crucially, also **converse and reason** about the network’s behavior. By doing so, it directly leverages the strengths identified in the literature – flexibility in data inspection ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=The%20Need%20for%20Flexibility,At)), multi-scale visualization ([[1704.01942] ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models](https://arxiv.org/abs/1704.01942#:~:text=participatory%20design%20sessions%20with%20over,of%20how%20ActiVis%20may%20work)), conceptual summarization ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=,our%20method%20to%20compare%20the)), and autonomous hypothesis generation ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=examples%2C%20the%20AIA%20actively%20participates,other%20systems%20in%20real%20time)) – to help users deeply understand their neural networks in an intuitive, human-centered way. The result would be an AI-powered “copilot” for neural network interpretability, turning state-of-the-art research ideas into practical tooling for model developers. 

**Sources:**

1. Dietz, F. *et al.* (2023). *Comgra: A Tool for Analyzing and Debugging Neural Networks* ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=The%20Need%20for%20Flexibility,At)) ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Comgra.%20Comgra%20,details%20for%20a%20specific%20input))  
2. Taylor, J.M. & Kriegeskorte, N. (2023). *TorchLens: A Python package for extracting and visualizing hidden activations* ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Visualizing%20the%20Computation%20Graph,a%20toolkit%20for%20visualizing%20models))  
3. Kokhlikyan, N. *et al.* (2020). *Captum: A unified model interpretability library for PyTorch* ([Comgra: A Tool for Analyzing and Debugging Neural Networks](https://arxiv.org/html/2407.21656v1#:~:text=Attribution%20and%20Interpretability,to%20interpret%20your%20model%E2%80%99s%20behavior))  
4. Carter, S. *et al.* (2019). *Activation Atlas (Distill)* ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=By%20using%20feature%20inversion%20to,network%20typically%20represents%20some%20concepts)) ([Activation Atlas](https://distill.pub/2019/activation-atlas#:~:text=Unfortunately%2C%20visualizing%20activations%20has%20a,a%20time%20will%20not%20suffice))  
5. Bau, D. *et al.* (2017). *Network Dissection: Quantifying Interpretability of Deep Visual Representations* ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=,our%20method%20to%20compare%20the)) ([[1704.05796] Network Dissection: Quantifying Interpretability of Deep Visual Representations](https://arxiv.org/abs/1704.05796#:~:text=latent%20representations%20of%20various%20networks,interpretability%20of%20deep%20visual%20representations))  
6. Kahng, M. *et al.* (2018). *ActiVis: Visual Exploration of Industry-Scale DNNs* ([[1704.01942] ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models](https://arxiv.org/abs/1704.01942#:~:text=participatory%20design%20sessions%20with%20over,of%20how%20ActiVis%20may%20work))  
7. Tenney, I. *et al.* (2020). *Language Interpretability Tool (LIT)* ([The Language Interpretability Tool (LIT): Interactive Exploration and Analysis o](https://research.google/blog/the-language-interpretability-tool-lit-interactive-exploration-and-analysis-of-nlp-models/#:~:text=LIT%20supports%20local%20explanations%2C%20including,in%20our%20system%20demonstration%20paper))  
8. Rott Shaham, T. *et al.* (2024). *MAIA: A Multimodal Automated Interpretability Agent* ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=Central%20to%20this%20strategy%20is,other%20systems%20in%20real%20time)) ([AI agents help explain other AI systems | MIT News | Massachusetts Institute of Technology](https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103#:~:text=For%20example%2C%20FIND%20contains%20synthetic,and%20not%20air%20or%20sea))  
9. OpenAI (2023). *Language Models can Explain Neurons in Language Models* ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=A%20recent%20paper%20by%20a,step%20towards%20automating%20DNN%20interpretability)) ([OpenAI is Using GPT-4 to Explain Neurons' Behavior in GPT-2 - InfoQ](https://www.infoq.com/news/2023/05/openai-gpt4-explains-gpt2/#:~:text=In%20the%20first%20step%2C%20a,find%20phrases%20related%20to%20community))  
10. Martin, C. *et al.* (2023). *WeightWatcher: Data-Free Diagnostics for Deep Neural Networks*

================================================================================

--- Processing: forward_backward_alignment.ipynb ---

Cell 1 (markdown):
# Recap and background
Here we recap the findings we have and review some background on  continual learning from the point of view of Ricahrd Sutton. 

*Outline* We proved in the draft that if we duplicate a hyperclone a model in a super-symmetric way (more accurately, forward and backward symmetry hold), then the forward and backward vectors of the network are cloned. More concretely, the forward and backward of the model are essentially the cloned (duplicated) versions of a smaller model from which they are cloned. This situation has a very dramatic consequence that, we can perfectly predict the training dynamics of the larger model with a smaller model, with the only caveat that the learning rate for different layers are set in a layer and module-dependent manner. This suggests that this particular way of cloning may catasrophically limit the model's ability to learn, because it is at best as good as the smaller model. This brings this result closer to the notion of *loss of plasticity* in continual learning literature, which we will very briefly review here. 

*Richard Sutton's view on Continual Learning (CL)*: Through a series of works on Richard Sutton, he proposes that one of the most fundumental p
----------------------------------------

Cell 2 (code):
import torch
import torch.nn as nn
import torch.func as functorch

# Define a customizable MLP with a Sequential container
class CustomizableMLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, activation=nn.ReLU):
        """
        Parameters:
            input_size (int): Number of input features.
            hidden_sizes (list of int): Sizes of the hidden layers.
            output_size (int): Number of output features.
            activation (nn.Module): Activation function (e.g., nn.ReLU, nn.Tanh).
        """
        super(CustomizableMLP, self).__init__()
        layers = []
        in_features = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(in_features, hidden_size))
            layers.append(activation())
            in_features = hidden_size
        # Output layer (no activation here, but you can add one if desired)
        layers.append(nn.Linear(in_features, output_size))
        self.layers = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.layers(x)

# Instantiate the model
input_size = 784
hidden_sizes = [256, 128, 64]
output_size = 10
model = CustomizableMLP(input_size, hidden_sizes, output_size, activation=nn.ReLU)

# --- Slicing the model ---
# Let's say we want the Jacobian from the input up to the output of the first hidden block.
# Since our Sequential container is a list of modules, you can slice it.
# For example, if we want to use the first two modules (a Linear layer and its activation):
# layer_slice = model.layers[0:2]  # This represents: Linear(input_size, 256) followed by ReLU
slice1 = model.layers[0:1]
slice2 = model.layers[1:]

# For a single input vector (shape: [input_size]), the Jacobian will have shape [output_dim, input_size]
x_single = torch.randn(input_size)
h_single = slice1(x_single) 
jacobian_single = functorch.jacrev(slice2)(h_single)
print("Jacobian for single input shape:", jacobian_single.shape)
# For example, if the Linear layer outputs 256 features, then jacobian_single.shape will be [256, 784]

# For a batch of inputs, we can use vmap to compute a Jacobian per input.
# Assume a batch of 5 inputs (shape: [5, input_size])
x_batch = torch.randn(5, input_size)
h_batch = slice1(x_batch)
jacobian_batch = functorch.vmap(functorch.jacrev(slice2))(h_batch)
print("Jacobian for batch input shape:", jacobian_batch.shape)
# Expected shape: [batch, output_dim, input_size], e.g., [5, 256, 784]

----------------------------------------

Cell 3 (code):

----------------------------------------

================================================================================

--- Processing: rank_deficiency.md ---
Thanks for the clarification! I will look into theoretical and semi-theoretical results on conditions that lead to stable rank deficiencies in neural networks, particularly in the middle or penultimate layers. The focus will be on:

- Rank collapse or stable low-rank representations under training.
- Loss functions like cross-entropy and MSE.
- The role of nonlinear activations (especially ReLU, sigmoid).
- The effect of standard stochastic gradient descent (SGD).
- Scenarios relevant to overparameterized MLPs.

I will get back to you with findings on theoretical guarantees or conditions that enforce such stable rank deficiencies.

# Stable Rank Deficiencies in Hidden Layers of Neural Networks

**Overview:** Deep neural networks often learn **low-rank representations** in their hidden layers, especially in later layers. Theory and empirical evidence suggest that as training progresses (and with sufficient overparameterization), the **effective rank** of hidden-layer activations or weight matrices can remain low or even *collapse*, rather than utilizing the full potential rank. Below, we outline key factors and findings that explain **when and why hidden layers maintain a stable, low rank** during training, focusing on theoretical conditions, loss functions, activation types, SGD dynamics, and overparameterization. 

## Monotonic Rank Reduction in Deep Networks (Theoretical Conditions)

**Composition Limits Rank:** A fundamental theoretical insight is that the **rank of a composed function (multiple layers)** cannot increase with depth – it tends to *monotonically decrease* or stay the same ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)). In other words, as inputs propagate through layers, the dimension of the learned feature manifold is non-increasing. Any layer’s output is constrained by the information in previous layers, so once a representation loses rank (becomes low-dimensional), subsequent layers cannot recover the lost dimensions. Feng *et al.* (2022) formally prove a **“universal monotonic decreasing property”** of network rank based on differential and algebraic composition rules ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)). This means **if a hidden layer becomes rank-deficient, that deficiency tends to persist or worsen in deeper layers**. For example, common operations like pooling, downsampling, or even standard fully-connected layers can significantly **drop the rank** of their outputs ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)). This structural constraint helps explain why certain middle or penultimate layers might **lock in a low rank** — once features collapse onto a low-dimensional subspace, later layers can only use those dimensions (barring injection of new information).

**Intrinsic Data Constraints:** Theoretical conditions for low rank also relate to the data and function being learned. If the target function or data manifold is inherently low-dimensional, a network might **find a low-rank representation that suffices**. In fact, the rank of a layer’s output (viewed as a function) measures the “volume of independent information” it carries ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=In%20mathematics%2C%20the%20rank%20of,learning%20that%20underlies%20many%20tasks)). Under mild assumptions, an optimal network will not inflate rank beyond what’s needed to represent the underlying data structure. This connects to ideas like the **Information Bottleneck**, where networks compress intermediate representations. Patel & Shwartz-Ziv (2024) define a “local rank” measure of feature-manifold dimensionality and show that hidden-layer rank tends to **decrease in later training, forming an emergent bottleneck** ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)) ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=particularly%20multilayer%20perceptrons%20,2023a)). In summary, **deep networks naturally compress information**, and theory indicates that **once a layer’s rank becomes low (e.g. due to a bottleneck or saturating behavior), it remains stably low or further collapses as training continues** ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)).

## Implicit Low-Rank Bias in Overparameterized Models

**Gradient Descent Favors Low Rank:** When neural networks are *overparameterized* (more parameters than data or than strictly needed), there are infinitely many solutions that fit the training data. Theory shows that standard training algorithms have an **implicit bias toward “simpler” (lower-complexity) solutions**, often reflected in low-rank structure. In particular, for deep *linear* networks (a simplified case with no nonlinear activations), it’s proven that gradient descent (or flow) on the squared loss converges to solutions with minimal **effective rank**. Gunasekar *et al.* (2017) demonstrated that a depth-2 linear network trained on a matrix factorization task converges to the minimum nuclear-norm solution ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)) – essentially the **lowest-rank weight matrix** that fits the data. More generally, later works showed that gradient descent tends to act as a *greedy rank minimizer* in linear matrix factorization problems ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)). In deep linear networks, this means if the training data can be fit by a low-rank mapping, gradient-based optimization will converge to that low-rank solution, leaving the weight matrices **rank-deficient** (many singular values driven to zero). This implicit bias arises *without* any explicit rank regularization – it’s a property of the dynamics of overparameterized models.

**Depth Amplifies Rank Bias:** Depth itself contributes to the bias. Adding more hidden layers (even linear ones) can strengthen the tendency toward low-rank solutions. Arora *et al.* (2019) found that in deep linear networks, **singular values of the effective mapping decay faster with increased depth**, indicating that deeper architectures induce a stronger preference for concentrating information in a few directions ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=solutions,rank%20solutions.%20Pennington)). In practice, **deeper networks often end up using only a subset of their neurons or directions effectively**, yielding low-rank feature matrices in intermediate and penultimate layers. Empirically, researchers observed that simply increasing depth (without changing the training objective) biases the network toward learning embeddings with lower rank ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=work%20on%20over%02parameterization%20and%20highlights,matrix%20has%20a%20low%201)). In fact, random initialized deep models already tend to map data into a low-rank feature space (as measured by the Gram matrix of features) and this bias remains after training ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=work%20on%20over%02parameterization%20and%20highlights,matrix%20has%20a%20low%201)). This “**low-rank simplicity bias**” means that among the many possible solutions in an overparameterized setting, deep networks prefer ones where features live in a smaller subspace ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=hypothesis%20that%20deeper%20networks%20are,wide%20variety%20of%20commonly%20used)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). Notably, this bias appears *robust*: it occurs across different initializations, hyperparameters, and even different optimizers ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)). In short, **overparameterization (especially increased depth) combined with gradient-based training often leads to persistent rank deficiencies** in hidden layers – the network finds a solution that uses far fewer independent dimensions than the layer width, and stays there.

**Permanent Rank Collapse:** Once a network converges to such a low-rank solution, those rank deficiencies tend to be “permanent” in the sense that continued training doesn’t reintroduce dropped dimensions. Instead, extended training can further reinforce the collapsed structure. For example, in overparameterized classifiers, one often observes **Neural Collapse** at the penultimate layer: features for each class collapse to their class mean, and those means become maximally spaced in a $C$-dimensional simplex (for $C$ classes). This implies the penultimate layer’s output has rank at most $C$ (much lower than its potential dimension). Importantly, this collapsed configuration emerges *in the late stage of training* and then remains stable ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Any extra degrees of freedom (e.g. additional neurons) simply align redundantly with this low-dimensional structure rather than expanding it. Thus, in highly overparameterized Multi-Layer Perceptrons (MLPs), it’s common to see entire directions in weight space or neuron activations effectively unused – **the network has more capacity than it needs, and gradient descent naturally finds a solution that leaves a “rank gap”**. The theoretical and empirical works above provide conditions for this: **if a low-rank solution exists (e.g. data lies on a low-dimensional manifold, or fewer than $N$ independent features are needed to classify $N$ classes), an overparameterized network will often converge to that solution, making the hidden-layer rank deficit permanent** ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). In summary, **overparameterization doesn’t increase the utilized rank – paradoxically, it often ensures some neurons or directions are redundant, locking in a low-rank representation** for middle and penultimate layers.

## Loss Functions (Cross-Entropy vs. MSE) and Rank Stability

**Cross-Entropy and Neural Collapse:** The choice of loss function can influence training dynamics and feature geometry, but common losses in classification (cross-entropy) and regression (mean squared error) ultimately can lead to similar low-rank feature outcomes. Cross-entropy (with softmax output) is known to drive networks into the **“neural collapse”** regime during the terminal phase of training, especially in classification tasks. Under cross-entropy loss, as training error approaches zero, the network continues to sharpen the separations between classes: penultimate layer features for each class become nearly identical (collapsed to their mean), and different class means maximize their mutual distances in feature space. This is a highly symmetric, low-rank configuration (features span roughly a $C-1$ dimensional subspace for $C$ classes). Papyan *et al.* (2020) observed this phenomenon empirically, and it has since been analyzed theoretically. In particular, for sufficiently large networks trained to convergence on cross-entropy, **the only global minimizers are those exhibiting neural collapse** ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). That means the **optimal solution inherently has low-rank feature structure** (the rank of the class-feature matrix equals the number of classes, not the number of features). Any features beyond that subspace are essentially unused.

**MSE Loss and Other Losses:** One might suspect that mean squared error (MSE) loss, which doesn’t push outputs to extremes the way cross-entropy does, might behave differently. However, recent theoretical work shows that **MSE loss can also lead to neural collapse at optimality** for overparameterized models. Zhou *et al.* (2022) compare cross-entropy vs. MSE and find that **both losses (and even variants like label smoothing or focal loss) yield the same neural collapse structure in the learned features**, given a large enough network trained to minimal loss ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=global%20solution%20and%20landscape%20analyses,FL%20loss%20near%20the%20optimal)). In other words, the global optimum features under MSE classification loss still have all samples of a class coincident at the class mean, and class means maximally separated (simplex configuration). This result implies that **the low-rank collapse of penultimate-layer features is not specific to cross-entropy** – it is a property of the *classification problem* itself at the optimum, rather than the particular loss formula ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Empirically, networks trained with either cross-entropy or MSE (on the same classification task) tend to end up with very similar penultimate-layer geometry and test performance, as long as they are sufficiently overparameterized and trained long enough ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=model%20assumption%2C%20we%20provide%20either,large%20and%20trained%20until%20convergence)).

**Dynamics Differences:** While the *final outcomes* under cross-entropy and MSE can be similar (both can collapse features to a low-rank configuration), the **training dynamics** may differ. Cross-entropy is an **exponential-type loss** that continues to penalize even tiny classification errors, which often drives weights to grow in norm and features to become extremely pure (one-hot like probabilities). This can encourage faster or more pronounced collapse of features during the later stages of training. In contrast, MSE (for classification) treats the problem more like regression to one-hot targets; once the network fits the training points, there’s no additional push to exaggerate the features. As a result, some studies noted that **neural collapse emerges more clearly or earlier with cross-entropy** (which implicitly maximizes class separation margin), whereas with MSE, collapse may still occur but perhaps requires more epochs or stronger overparameterization to mirror the same effect ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Nonetheless, in either case the penultimate layer ends up **low-rank (approximately rank = number of classes)** at convergence. For regression tasks, MSE can also lead to low-rank internal representations if the target function is low-dimensional. Overall, common loss functions like cross-entropy and MSE *do not prevent* rank deficiency in hidden layers; at optimum they often **demand it**, by driving the network toward a simplified, structured solution (neural collapse being a prime example) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).

## Activation Functions (ReLU vs. Sigmoid) and Rank Collapse

**ReLU Networks:** The choice of activation affects how information flows and whether it’s preserved or squashed, which in turn influences rank. **ReLU (Rectified Linear Unit)** activations are piecewise-linear and can introduce **dead neurons or linear dependencies** that reduce rank. For example, if a ReLU neuron’s input is negative for all training samples, that neuron outputs all zeros – effectively removing one dimension from that layer’s output (a rank drop). Even when active, ReLUs output either a linear scaling of the input or zero, so large groups of neurons can end up encoding redundant directions (especially if their weight vectors are correlated). From a theoretical perspective, understanding rank in nonlinear networks is harder than in linear ones. Recent work by Timor *et al.* (2023) shows that in contrast to linear networks, **gradient flow on ReLU networks doesn’t always minimize rank** – in fact, they construct scenarios where a shallow ReLU network does *not* find the lowest-rank solution ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=what%20extent%20this%20gen%02eralizes%20to,solutions%20in%20several%20reasonable%20settings)). This is a “negative result” indicating that ReLU’s piecewise linearity can sometimes preserve or create just enough variation to avoid trivial rank collapse in small cases. **However, on the positive side, deeper ReLU networks **are** biased toward low-rank solutions in many settings ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=datasets%20of%20size%202%29,solutions%20in%20several%20reasonable%20settings)).** In other words, depth appears to restore the implicit rank minimization tendency even with ReLUs. Sufficiently deep ReLU MLPs have been proven to favor low-rank function mappings under certain assumptions ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=datasets%20of%20size%202%29,solutions%20in%20several%20reasonable%20settings)), aligning with the empirical findings discussed earlier (depth-driven low-rank bias). Moreover, experiments show that the phenomenon of rank reduction with depth holds **across a variety of activation functions** – ReLU included. Rosenfeld *et al.* (2021) observed that increasing the number of layers **decreases the effective rank of the feature matrix for ReLU networks**, just as it does for smooth activations ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). In summary, ReLU’s on-off behavior can cause **rank collapse by deactivating neurons or duplicating features**, and while a shallow ReLU net might not automatically minimize rank, a deep ReLU net trained with SGD still tends to learn a compressed (low-rank) representation in later layers.

**Sigmoid and Saturating Activations:** **Sigmoid or tanh activations** (smooth, squashing non-linearities) have their own influence on rank. These functions saturate at extreme values (output approaching 0 or 1 for sigmoid, -1 or 1 for tanh), which can effectively **flatten variations** in input. During training, it is often observed that later in training (or in early layers of very deep networks), many sigmoid/tanh neurons enter saturation for a wide range of inputs. A neuron stuck near 0 or 1 for all inputs contributes almost no meaningful variability – it’s nearly a constant output, reducing the rank of that layer’s activation matrix (similar to a dead or saturated unit). This behavior ties in with the **Information Bottleneck (IB) theory**: Tishby and colleagues (2017) reported that networks with saturating activations show phases of training where **mutual information between the layer and the input drops**, implying the layer is discarding information and compressing its representation. This compression often corresponds to many neurons saturating, hence fewer effective degrees of freedom (a lower-dimensional manifold of activations). In practice, sigmoid networks were found to undergo an initial fitting phase followed by a **compression phase**, where the hidden-layer information (and empirically, the variance or entropy of activations) collapses significantly. This suggests that **sigmoid/tanh networks may exhibit an even stronger rank-collapse tendency in later layers** compared to ReLU, since saturation can make large portions of the layer’s output almost constant. Indeed, Patel & Shwartz-Ziv (2024) note that networks compress their feature manifolds in later training, and this was originally observed in saturating networks consistent with IB predictions ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)).

**Activation-Type Comparisons:** Empirical studies directly comparing activation functions find that **the trend of low-rank representations is quite general**. For instance, one study computed the “effective rank” of the feature Gram matrix for networks with ReLU, leaky ReLU, tanh, GELU, and even sinusoidal activations, across various depths. The result was universal: **adding more layers consistently lowered the effective rank of the penultimate-layer features for all activation types tested** ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). While the absolute level of rank may differ (e.g. some activations might retain slightly more information in shallow layers), the *qualitative behavior* – a bias toward low-rank, structured representations – appears across the board ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=depth%20of%20the%20model,By%20hierarchically)). Therefore, common nonlinearities like ReLU and sigmoid both permit rank-deficient solutions. ReLU may allow networks to retain more piecewise-linear information in some cases (not *always* minimizing rank), but in deep and overparameterized regimes it still converges to low-rank feature mappings. Sigmoidal activations naturally encourage compression via saturation, often leading to **“rank collapse” as training goes on** (neurons saturate and outputs cluster). In both cases, once neurons either saturate or become redundant, those dimensions effectively drop out of the model’s representation. No matter the activation, a deep network that has more capacity than needed will tend to **only use a few dominant directions** in each layer’s output – yielding a stable, low-rank representation by the penultimate layer.

## Role of SGD and Training Dynamics in Rank Deficiency

**SGD as an Implicit Regularizer:** Interestingly, the stochastic nature of training (stochastic gradient descent, SGD) itself plays a role in enforcing low-rank structures. Recent theoretical analyses point out that **mini-batch SGD injects noise that biases the solution toward low-complexity (low-rank) weights**. Galanti & Poggio (2022) proved that when training deep networks with small-batch SGD (especially with common tweaks like weight decay), the noise in the gradients creates an *implicit constraint* that favors low-rank solutions ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). In fact, they show the very source of SGD noise can be viewed as a form of *rank regularization*: all else equal, SGD tends to drive the weight matrices to smaller effective rank over training ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). Their theory connects batch size, learning rate, and weight decay to the rank of the learned weight matrices ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Specifically, **smaller batch sizes and the use of weight decay act as strong low-rank regularizers** ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Intuitively, gradient noise continually perturbs the solution within the space of zero training error solutions, and it preferentially guides the weights toward configurations that are “simpler” (analogous to how noise in linear regression can favor solutions with smaller norm). In deep networks, that simplicity manifests as weight matrices with many singular values effectively zero – i.e. **SGD implicitly pushes toward rank deficiency** in each layer’s weights ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). This complements the implicit bias of gradient descent itself; even without noise, gradient descent finds a low-rank solution in many cases, and **with noise (SGD) this bias is even more pronounced**.

**Empirical Evidence in SGD Dynamics:** Empirically, one finds that SGD-trained networks often learn features in a stage-wise fashion, capturing the most significant structure first. Early in training, networks latch onto the largest variance or easiest features in the data (sometimes called *spectral bias* or *dominant feature first* learning). As Pezeshki *et al.* (2020) observed, **SGD tends to learn statistically dominant features first, which leads to learning low-rank solutions** for the data ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)). In practical terms, this means the network might initially increase the rank of representations to fit the data variation, but once it has fit the major patterns, additional training **compresses the representation**, aligning with those dominant patterns and ignoring minor ones. This is consistent with a reduction in rank as training continues. Some works also note that different optimization algorithms (SGD vs. adaptive methods like Adam) yield similar low-rank phenomena in deep models ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)), suggesting the effect is not solely due to SGD’s noise but also due to the parameterization. Nevertheless, **SGD’s stochastic noise reinforces symmetry-breaking and flat minima selection** that often coincides with low-rank weight configurations. The *theorem* by Galanti & Poggio even implies that as long as there is some SGD noise (e.g. mini-batches) and weight decay, the training will *never exactly converge* but instead keep hovering around a solution, effectively preventing the network from utilizing extra degrees of freedom that aren’t needed ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=only%20assumed%20to%20be%20differentiable,rank)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Sec,4)). In other words, the noise keeps the model near a minimal-rank solution. 

**Breaking vs. Enforcing Rank Deficiency:** One might wonder if SGD noise could ever *break* a rank deficiency (for example, jostle the network out of a bad symmetric solution where two neurons are identical, thereby increasing rank). In practice, SGD **does break exact symmetric degeneracies** (it’s rare for two neurons to remain perfectly identical during SGD training because tiny gradient differences will separate them). However, the net effect of SGD’s randomness is not to maximize rank, but rather to explore solutions of similar performance and favor the ones with smoother or simpler structure. If a certain low-rank configuration suffices to fit the data, SGD is unlikely to kick the network into a higher-rank regime without a clear benefit. In fact, small-batch SGD will add noise that, on average, drives the weights toward the flat region of the loss landscape that often corresponds to compressive solutions ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Thus, **SGD more often enforces rank deficiency than alleviates it**. The overall training dynamic typically sees the rank of hidden-layer activations **initially high (when learning diverse features), then stabilizing or decreasing as training converges**, especially under SGD. This aligns with the “compression phase” idea and has been measured directly: for example, Patel (2024) found that the *local rank* of features in each layer dropped during the final phase of training, indicating SGD was fine-tuning the model by compressing representations further ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)) ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=dimensionality%20and%20demonstrate%2C%20both%20theoretically,information%20bottlenecks%20and%20representation%20learning)). In summary, **SGD (with typical settings) implicitly regularizes the network toward low-rank solutions**, making rank-deficient hidden layers a persistent outcome of the training process ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)).

## Overparameterized MLPs and Conditions for Rank Collapse

**Excess Capacity Leads to Redundancy:** In multi-layer perceptrons with more neurons or layers than necessary, certain conditions practically guarantee rank deficiencies. If an MLP has layers much wider than the intrinsic dimension of the problem, it has the freedom to realize a solution where many neurons are simply not needed. **Gradient descent will often utilize only a subset of neurons (or a subset of independent directions in those neurons)** to solve the task, leaving the rest effectively redundant. For instance, if an MLP could solve a task with an internal representation of dimension $d$, giving it $d \times 2$ neurons in that layer doesn’t force it to use all $2d$ directions – it may well use only $d$ of them (making the layer’s output rank $d$). The other neurons might end up as linear combinations of the first $d$ or stuck at zero weights. **This is a common scenario for “permanent” rank deficiency**: the network finds a low-rank configuration early (or by midpoint of training) and never needs to activate the extra capacity. Once those extra neurons settle into redundancy (e.g., duplicating another neuron’s behavior or outputting near-constant), the rank of that layer stays low. Any slight perturbation (like SGD noise) doesn’t overcome the bias to keep them redundant, because deviating would not improve the loss.

**Bottleneck Layers and Architecture:** Some networks explicitly include bottleneck layers (fewer neurons) in the middle by design; those obviously enforce low rank at that point. But even in **uniformly wide networks, an “effective bottleneck” can emerge**. Rangamani *et al.* (2023) empirically found that as one goes deeper into a trained classifier, the **within-class variability of features shrinks and class means become the dominant components** ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)). In effect, by the penultimate layer, most of the variation in features is between classes rather than within – which means the representation has roughly one degree of freedom per class (a very low rank structure) ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)). This happens even if every hidden layer had the same width; the *network learned* to create an information bottleneck near the end. Such **self-induced bottlenecks** are a hallmark of overparameterized models: they have enough layers/neurons to first separate the classes and then compress each class cluster tightly. The penultimate layer in these cases is severely rank-deficient (often close to rank $C$ for $C$ classes). Notably, this low-rank state is maintained – it doesn’t revert or expand – even if training continues longer (the clusters just tighten further). Conditions that encourage this include having **much more model capacity than the minimum required**, and optimizing to near-zero training loss (so the network can afford to project data onto a structured low-dimensional subspace that cleanly separates classes).

**Explicit Regularization and Rank:** It’s worth mentioning that some explicit regularization techniques can also encourage low-rank solutions (e.g. weight decay, which is commonly used, biases toward smaller weights that often imply fewer independent components). However, what we’ve discussed are *implicit* phenomena – even in the absence of explicit rank penalties, overparameterized MLPs tend to end up with stable rank deficiencies. In fact, adding too strong an explicit rank penalty is often unnecessary or even harmful; simply relying on the network’s inductive biases and SGD tends to find a good low-rank solution on its own ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=maps%20to%20low,parameters%2C%20and)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=shows%20that%20linear%20models%20with,demonstrate%20the%20practical%20applications%20of)). Researchers have demonstrated that **deliberately overparameterizing a model (especially in depth) can improve generalization by leveraging this implicit low-rank bias**, rather than explicitly constraining rank ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=optimizers,parameterizing%20standard%20models%20at%20training)). For example, inserting extra linear layers (which add depth but no new nonlinearity) during training was shown to yield lower-rank features and better generalization, even though the model’s theoretical capacity didn’t change ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=optimizers,parameterizing%20standard%20models%20at%20training)). This underscores that **overparameterization itself (when coupled with SGD) is a form of regularization** that often realizes as low-rank internal representations.

**Summary of Conditions:** In an overparameterized MLP, you are likely to see **permanent rank collapse** in hidden layers when: (1) the model has significantly more parameters than needed to fit the data, (2) the training is run to near convergence (zero or negligible training error), and (3) standard losses (like cross-entropy or MSE) and optimizers (SGD or similar) are used. Under these conditions, theoretical and empirical studies indicate the network will converge to a solution where hidden layers (especially the penultimate layer) have **stable, low rank** – often determined by the problem’s inherent dimensionality (such as number of classes or principal components of the data). The **rank remains low throughout the end of training** because neither the architecture nor the training dynamics provide an incentive to reinflate it. On the contrary, deep architectures and SGD training both *favor* collapsing dimensions and finding efficient, low-dimensional encodings ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). This is why we observe phenomena like neural collapse and rank-deficient features in practice. In conclusion, hidden-layer rank deficiency in neural networks is backed by several semi-theoretical and theoretical insights: **deep compositions naturally restrict rank, gradient descent implicitly seeks low-rank solutions (especially in overparameterized setups), common loss functions do not oppose (and in fact often drive) rank collapse, nonlinear activations (ReLU or sigmoid) still end up compressing information, and SGD’s stochastic nature further encourages simplicity**. All these factors together explain **when/why a network’s middle or penultimate layers might maintain a low, stable rank despite ongoing training** ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)).

**References:**

- R. Feng *et al.*, *“Rank Diminishing in Deep Neural Networks,”* NeurIPS 2022 – establishes that network rank **monotonically decreases** with depth and analyzes rank deficiencies per layer ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)).  
- S. Gunasekar *et al.*, *“Implicit Regularization in Matrix Factorization,”* NeurIPS 2017 – shows gradient descent on overparametrized linear models converges to **minimum-nuclear-norm (low-rank)** solutions ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)).  
- J. Zhou *et al.*, *“Are All Losses Created Equal? A Neural Collapse Perspective,”* NeurIPS 2022 – proves that both cross-entropy and MSE losses (and others) yield **Neural Collapse** at global optima, implying low-rank penultimate features for sufficiently large networks ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).  
- N. Timor *et al.*, *“Implicit Regularization Towards Rank Minimization in ReLU Networks,”* ALT 2023 – finds that while shallow ReLU nets may not always minimize rank, **deeper ReLU nets are biased towards low-rank solutions** under gradient flow ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=what%20extent%20this%20gen%02eralizes%20to,solutions%20in%20several%20reasonable%20settings)).  
- T. Galanti & T. Poggio, *“SGD Noise and Implicit Low-Rank Bias in Deep Neural Networks,”* CBMM Memo 2022 – theoretically shows mini-batch **SGD + weight decay imposes a low-rank constraint** on weight matrices; smaller batches and higher weight decay strengthen this effect ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)).  
- A. Rangamani *et al.*, *“Feature Learning in Deep Classifiers through Intermediate Neural Collapse,”* ICML 2023 – empirically demonstrates that **intermediate layers progressively collapse** class-wise: deeper layers have much lower within-class variance (effectively lower rank) relative to between-class variance ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)).  
- M. Rosenfeld *et al.* (OpenReview preprint 2021), *“The Low-Rank Simplicity Bias in Deep Networks,”* – provides empirical evidence that **increasing depth consistently reduces the effective rank** of learned features across various architectures and activations, and that this bias is robust to different optimizers ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)).  
- Additional references: S. Arora *et al.* 2019; B. Pezeshki *et al.* 2020; H. Papyan *et al.* 2020; and others as cited above, which further support these points on rank collapse and implicit biases in deep learning ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=solutions,rank%20solutions.%20Pennington)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).

================================================================================

--- Processing: script.py ---
import os
import nbformat
import argparse

def extract_notebook_content(nb_path):
    """
    Extracts code and markdown content from a Jupyter Notebook.
    Returns a list of dictionaries with keys "type" and "source".
    """
    nb = nbformat.read(nb_path, as_version=4)
    cells = []
    for cell in nb.cells:
        if cell.cell_type in ["code", "markdown"]:
            cells.append({
                "type": cell.cell_type,
                "source": cell.source
            })
    return cells

def process_file(file_path, ext):
    """
    Processes a single file based on its extension.
    For Jupyter notebooks (.ipynb), it extracts and prints code & markdown cells.
    For other files, it outputs their content directly.
    """
    print(f"--- Processing: {os.path.basename(file_path)} ---")
    
    if ext == ".ipynb":
        cells = extract_notebook_content(file_path)
        for idx, cell in enumerate(cells, start=1):
            print(f"\nCell {idx} ({cell['type']}):")
            print(cell['source'])
            print("-" * 40)
    else:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                print(content)
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
    print("\n" + "=" * 80 + "\n")

def process_directory(directory, allowed_extensions):
    """
    Processes all files in the specified directory that match the allowed extensions.
    """
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        # Skip directories
        if os.path.isdir(file_path):
            continue
        
        _, ext = os.path.splitext(filename)
        ext = ext.lower()
        if ext in allowed_extensions:
            process_file(file_path, ext)

def main():
    parser = argparse.ArgumentParser(
        description="Extract contents from Jupyter notebooks and other files with specified extensions."
    )
    parser.add_argument(
        "directory",
        nargs="?",
        default=".",
        help="Directory to scan for files (default: current directory)."
    )
    parser.add_argument(
        "--extensions",
        type=str,
        default=".ipynb",
        help=("Comma-separated list of file extensions to process. " 
              "For example: .ipynb,.py,.txt (default: .ipynb)")
    )
    
    args = parser.parse_args()
    # Create a set of allowed extensions, ensuring they are in lowercase and start with a dot.
    allowed_extensions = {ext.strip().lower() for ext in args.extensions.split(",") if ext.strip()}
    
    process_directory(args.directory, allowed_extensions)

if __name__ == "__main__":
    main()


================================================================================

--- Processing: download_tiny_imagenet.py ---
import os
import urllib.request
import zipfile

def download_and_extract_tiny_imagenet(url='http://cs231n.stanford.edu/tiny-imagenet-200.zip', dest_path='.'):
    """
    Downloads and extracts the Tiny ImageNet dataset.
    
    Args:
        url (str): URL of the Tiny ImageNet zip file.
        dest_path (str): Directory to save the downloaded file and extract its contents.
    
    Returns:
        str: Path to the extracted Tiny ImageNet directory.
    """
    zip_filename = os.path.join(dest_path, 'tiny-imagenet-200.zip')
    extract_path = os.path.join(dest_path, 'tiny-imagenet-200')
    
    if os.path.exists(extract_path):
        print("Tiny ImageNet is already downloaded and extracted at:", extract_path)
        return extract_path
    
    # Download the dataset if the zip file doesn't exist
    if not os.path.exists(zip_filename):
        print("Downloading Tiny ImageNet...")
        urllib.request.urlretrieve(url, zip_filename)
        print("Download complete.")
    else:
        print("Zip file already exists:", zip_filename)
    
    # Extract the downloaded zip file
    print("Extracting Tiny ImageNet...")
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(dest_path)
    print("Extraction complete.")
    
    # Optionally, remove the zip file after extraction to save space
    os.remove(zip_filename)
    print("Removed zip file:", zip_filename)
    
    return extract_path

if __name__ == "__main__":
    dataset_path = download_and_extract_tiny_imagenet()
    print("Tiny ImageNet is ready at:", dataset_path)


================================================================================

--- Processing: ViT.ipynb ---

Cell 1 (code):
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from einops import rearrange
import numpy as np

# Vision Transformer Model
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.n_patches = (img_size // patch_size) ** 2
        
        self.proj = nn.Conv2d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, x):
        x = self.proj(x)  # (B, E, H', W')
        x = rearrange(x, 'b e h w -> b (h w) e')  # (B, N, E)
        return x

class Attention(nn.Module):
    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % n_heads == 0
        self.n_heads = n_heads
        head_dim = dim // n_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class MLP(nn.Module):
    def __init__(self, in_features, hidden_features, out_features, drop=0.):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class TransformerBlock(nn.Module):
    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, drop)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=32, patch_size=4, in_channels=3, n_classes=10, embed_dim=192,
                 depth=12, n_heads=8, mlp_ratio=4., qkv_bias=True, drop_rate=0.1):
        super().__init__()
        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        n_patches = self.patch_embed.n_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(drop_rate)

        self.blocks = nn.Sequential(*[
            TransformerBlock(embed_dim, n_heads, mlp_ratio, qkv_bias, drop_rate, drop_rate)
            for _ in range(depth)
        ])

        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, n_classes)

        # Initialize weights
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        nn.init.trunc_normal_(self.cls_token, std=0.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)

        cls_token = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_token, x), dim=1)
        x = x + self.pos_embed
        x = self.pos_drop(x)

        x = self.blocks(x)
        x = self.norm(x)
        x = self.head(x[:, 0])  # Use only the cls token for classification

        return x

# Training function
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    return total_loss / len(dataloader), 100. * correct / total

# Validation function
def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    return total_loss / len(dataloader), 100. * correct / total

# Main training loop
if __name__ == '__main__':
    # Hyperparameters
    batch_size = 128
    num_epochs = 100
    learning_rate = 1e-3
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Data augmentation and normalization
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    # Load CIFAR-10 dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                          download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=batch_size,
                           shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                         download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=batch_size,
                          shuffle=False, num_workers=2)

    # Initialize model, criterion, and optimizer
    model = VisionTransformer().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    # Training loop
    best_acc = 0
    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)
        val_loss, val_acc = validate(model, testloader, criterion, device)
        scheduler.step()
        
        print(f'Epoch: {epoch+1}/{num_epochs}')
        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')
        
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'vit_cifar10_best.pth')

    print(f'Best validation accuracy: {best_acc:.2f}%')

----------------------------------------

Cell 2 (code):
model
----------------------------------------

Cell 3 (code):
with torch.no_grad():
    for images, labels in testloader:
        images, labels = images.to(device), labels.to(device)
        break
----------------------------------------

Cell 4 (code):

----------------------------------------

================================================================================

