{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3810978-1dc2-49d2-b1bf-4129f212f25a",
   "metadata": {},
   "source": [
    "# Mathematical Note on Backward Kernels and Training Dynamics\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This document outlines the mathematical framework necessary for simulating and analyzing the training dynamics of neural networks using backward kernels and gradient alignment. The definitions, propositions, and statements provided are detailed to enable the implementation of numerical simulations and machine learning training scripts.\n",
    "\n",
    "---\n",
    "\n",
    "## Notations and Definitions\n",
    "\n",
    "### Neural Network Parameters\n",
    "\n",
    "- **Parameters Vector**: Let $ \\theta \\in \\mathbb{R}^m $ denote the vector of all trainable parameters (weights and biases) in the neural network, where $ m $ is the total number of parameters.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Training Data**: $ \\{ (x_i, y_i) \\}_{i=1}^n $, where:\n",
    "  - $ x_i \\in \\mathbb{R}^d $: Input features.\n",
    "  - $ y_i \\in \\mathbb{R}^k $: Target outputs.\n",
    "  - $ n $: Number of data samples.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "- **Loss for a Single Data Point**: The loss function $ L_i(\\theta) $ measures the discrepancy between the predicted output $ f(x_i; \\theta) $ and the target $ y_i $:\n",
    "  $$\n",
    "  L_i(\\theta) = \\ell\\left( f(x_i; \\theta), y_i \\right)\n",
    "  $$\n",
    "  - $ \\ell $: The loss function (e.g., mean squared error, cross-entropy).\n",
    "\n",
    "### Gradients\n",
    "\n",
    "- **Gradient Vector**: The gradient of the loss with respect to the parameters for data point $ i $:\n",
    "  $$\n",
    "  g_i = \\nabla_\\theta L_i(\\theta) \\in \\mathbb{R}^m\n",
    "  $$\n",
    "- **Layer-Wise Gradient**: For layer $ l $:\n",
    "  $$\n",
    "  g_i^{(l)} = \\nabla_{\\theta^{(l)}} L_i(\\theta) \\in \\mathbb{R}^{m_l}\n",
    "  $$\n",
    "  - $ \\theta^{(l)} $: Parameters of layer $ l $.\n",
    "  - $ m_l $: Number of parameters in layer $ l $.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Matrices\n",
    "\n",
    "### Full Gradient Matrix\n",
    "\n",
    "- **Definition**:\n",
    "  $$\n",
    "  G = \\begin{bmatrix}\n",
    "  g_1^\\top \\\\\n",
    "  g_2^\\top \\\\\n",
    "  \\vdots \\\\\n",
    "  g_n^\\top\n",
    "  \\end{bmatrix} \\in \\mathbb{R}^{n \\times m}\n",
    "  $$\n",
    "\n",
    "### Layer-Wise Gradient Matrix\n",
    "\n",
    "- **Definition**:\n",
    "  $$\n",
    "  G^{(l)} = \\begin{bmatrix}\n",
    "  \\left( g_1^{(l)} \\right)^\\top \\\\\n",
    "  \\left( g_2^{(l)} \\right)^\\top \\\\\n",
    "  \\vdots \\\\\n",
    "  \\left( g_n^{(l)} \\right)^\\top\n",
    "  \\end{bmatrix} \\in \\mathbb{R}^{n \\times m_l}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Kernel\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Total Backward Kernel**:\n",
    "  $$\n",
    "  K = G G^\\top \\in \\mathbb{R}^{n \\times n}\n",
    "  $$\n",
    "  - $ K_{ij} = \\left\\langle g_i, g_j \\right\\rangle $\n",
    "\n",
    "### Layer-Wise Backward Kernel\n",
    "\n",
    "- **Definition**:\n",
    "  $$\n",
    "  K^{(l)} = G^{(l)} \\left( G^{(l)} \\right)^\\top \\in \\mathbb{R}^{n \\times n}\n",
    "  $$\n",
    "  - $ K_{ij}^{(l)} = \\left\\langle g_i^{(l)}, g_j^{(l)} \\right\\rangle $\n",
    "\n",
    "- **Total Kernel as Sum of Layer-Wise Kernels**:\n",
    "  $$\n",
    "  K = \\sum_{l=1}^{L} K^{(l)}\n",
    "  $$\n",
    "  - $ L $: Total number of layers.\n",
    "\n",
    "---\n",
    "\n",
    "## Hessian Matrix Approximation\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Hessian Approximation**:\n",
    "  $$\n",
    "  H \\approx G^\\top G \\in \\mathbb{R}^{m \\times m}\n",
    "  $$\n",
    "  - This approximates the Hessian by the sum of outer products of gradients:\n",
    "    $$\n",
    "    H \\approx \\sum_{i=1}^n g_i g_i^\\top\n",
    "    $$\n",
    "\n",
    "### Relationship to Backward Kernel\n",
    "\n",
    "- **Eigenspectra Connection**:\n",
    "  - The non-zero eigenvalues of $ K $ and $ H $ are the same (up to multiplicities).\n",
    "  - **Singular Value Decomposition (SVD)**:\n",
    "    - If $ G $ has singular values $ \\sigma_i $, then:\n",
    "      - Eigenvalues of $ K = G G^\\top $ are $ \\lambda_i = \\sigma_i^2 $.\n",
    "      - Eigenvalues of $ H = G^\\top G $ are the same $ \\lambda_i = \\sigma_i^2 $.\n",
    "  - **Implication**:\n",
    "    - Analyzing the eigenspectrum of $ K $ provides insights into the Hessian's eigenspectrum and, consequently, the curvature of the loss landscape.\n",
    "\n",
    "---\n",
    "\n",
    "## Eigenvalues, Moments, and Hessian Conditioning\n",
    "\n",
    "### Eigenvalues and Moments of the Hessian\n",
    "\n",
    "- **First Eigenmoment $ M_1 $**:\n",
    "  $$\n",
    "  M_1 = \\frac{1}{m} \\operatorname{tr}(H) = \\frac{1}{m} \\sum_{i=1}^m \\lambda_i = \\frac{1}{m} \\mathbb{E}[ \\| g \\|^2 ]\n",
    "  $$\n",
    "\n",
    "- **Second Eigenmoment $ M_2 $**:\n",
    "  $$\n",
    "  M_2 = \\frac{1}{m} \\operatorname{tr}(H^2) = \\frac{1}{m} \\sum_{i=1}^m \\lambda_i^2 = \\frac{1}{m} \\mathbb{E}_{g, g'} \\left[ \\left( \\langle g, g' \\rangle \\right)^2 \\right]\n",
    "  $$\n",
    "\n",
    "### Inequality Between Moments\n",
    "\n",
    "- **Condition Number Indicator**:\n",
    "  $$\n",
    "  1 \\leq \\frac{M_2}{M_1^2} \\leq m\n",
    "  $$\n",
    "  - **Interpretation**:\n",
    "    - $ \\frac{M_2}{M_1^2} = 1 $: All eigenvalues are equal; the Hessian is well-conditioned.\n",
    "    - Higher values indicate greater disparity among eigenvalues, leading to an ill-conditioned Hessian.\n",
    "\n",
    "### Link to Backward Kernel\n",
    "\n",
    "- **First Moment Using Backward Kernel**:\n",
    "  $$\n",
    "  M_1 = \\frac{1}{m} \\mathbb{E}_{x} [ K_{ii} ] = \\frac{1}{m} \\mathbb{E}_{x} [ \\| g(x) \\|^2 ]\n",
    "  $$\n",
    "\n",
    "- **Second Moment Using Backward Kernel**:\n",
    "  $$\n",
    "  M_2 = \\frac{1}{m} \\mathbb{E}_{x, x'} [ ( K_{ij} )^2 ] = \\frac{1}{m} \\mathbb{E}_{x, x'} [ \\left( \\langle g(x), g(x') \\rangle \\right)^2 ]\n",
    "  $$\n",
    "\n",
    "- **Implication**:\n",
    "  - By computing these moments using the backward kernel, we can assess the Hessian's condition number and the smoothness of the loss landscape.\n",
    "\n",
    "---\n",
    "\n",
    "## Propositions and Statements\n",
    "\n",
    "### Gradient Alignment and Training Dynamics\n",
    "\n",
    "- **Proposition 1**: **Positive Gradient Alignment**\n",
    "  - If $ K_{ij} > 0 $, gradients $ g_i $ and $ g_j $ are positively aligned.\n",
    "  - **Implication**: Updates from data points $ i $ and $ j $ reinforce each other, potentially accelerating convergence.\n",
    "\n",
    "- **Proposition 2**: **Negative Gradient Alignment**\n",
    "  - If $ K_{ij} < 0 $, gradients $ g_i $ and $ g_j $ are negatively aligned.\n",
    "  - **Implication**: Updates conflict, potentially slowing down training or causing oscillations.\n",
    "\n",
    "### Loss Landscape Curvature\n",
    "\n",
    "- **Proposition 3**: **Curvature Representation**\n",
    "  - The curvature of the loss landscape is captured by the Hessian $ H $.\n",
    "  - Large eigenvalues of $ H $ correspond to directions of high curvature.\n",
    "\n",
    "- **Proposition 4**: **Backward Kernel and Curvature**\n",
    "  - Since $ K $ and $ H $ share the same non-zero eigenvalues, analyzing $ K $ provides insights into the curvature without computing $ H $ directly.\n",
    "\n",
    "### Hessian Conditioning and Gradient Orthogonality\n",
    "\n",
    "- **Proposition 5**: **Gradient Orthogonality and Hessian Conditioning**\n",
    "  - Orthogonal gradients lead to a Hessian with more uniform eigenvalues, improving its condition number.\n",
    "  - **Implication**: A well-conditioned Hessian facilitates stable optimization but may slow down convergence due to lack of gradient reinforcement.\n",
    "\n",
    "- **Proposition 6**: **Gradient Alignment and Hessian Ill-Conditioning**\n",
    "  - Highly aligned gradients can result in a Hessian with large disparities among eigenvalues, leading to ill-conditioning.\n",
    "  - **Implication**: While convergence may be faster due to reinforcing gradients, optimization may become unstable or sensitive to learning rates.\n",
    "\n",
    "### Layer-Wise Dynamics\n",
    "\n",
    "- **Proposition 7**: **Gradient Flow in Layers**\n",
    "  - The magnitude and alignment of $ K^{(l)} $ affect how effectively gradients are propagated through layer $ l $.\n",
    "\n",
    "- **Proposition 8**: **Vanishing and Exploding Gradients**\n",
    "  - **Vanishing Gradients**: If $ \\| K^{(l)} \\| $ decreases significantly with depth, early layers may learn slowly.\n",
    "  - **Exploding Gradients**: If $ \\| K^{(l)} \\| $ increases excessively, it may cause numerical instability.\n",
    "\n",
    "- **Proposition 9**: **Balanced Gradient Alignment**\n",
    "  - Maintaining consistent $ K^{(l)} $ across layers promotes stable and efficient training.\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-Off Between Gradient Alignment and Hessian Conditioning\n",
    "\n",
    "### Convergence Speed vs. Optimization Stability\n",
    "\n",
    "- **Positive Gradient Alignment**:\n",
    "  - **Pros**:\n",
    "    - Accelerated convergence due to reinforcing updates.\n",
    "  - **Cons**:\n",
    "    - May lead to an ill-conditioned Hessian with large eigenvalue disparities.\n",
    "    - Optimization can become unstable and sensitive to hyperparameters.\n",
    "\n",
    "- **Gradient Orthogonality**:\n",
    "  - **Pros**:\n",
    "    - Results in a well-conditioned Hessian with uniform eigenvalues.\n",
    "    - Facilitates stable optimization and potentially better generalization.\n",
    "  - **Cons**:\n",
    "    - Slower convergence due to lack of reinforcement among gradient updates.\n",
    "\n",
    "### Balancing the Trade-Off\n",
    "\n",
    "- **Optimal Training Dynamics**:\n",
    "  - Aim for a moderate level of gradient alignment to benefit from faster convergence while maintaining a reasonably conditioned Hessian.\n",
    "- **Strategies**:\n",
    "  - **Regularization**: Use weight decay or dropout to prevent excessive alignment.\n",
    "  - **Normalization**: Apply batch normalization to stabilize gradients and promote healthy gradient flow.\n",
    "  - **Adaptive Optimizers**: Utilize optimizers like Adam to handle varying gradient magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Steps for Simulations\n",
    "\n",
    "### Computing Gradients\n",
    "\n",
    "- **For Each Data Point $ i $**:\n",
    "  1. Perform a forward pass to compute $ L_i(\\theta) $.\n",
    "  2. Perform a backward pass to compute:\n",
    "     - Full gradient $ g_i = \\nabla_\\theta L_i(\\theta) $.\n",
    "     - Layer-wise gradients $ g_i^{(l)} = \\nabla_{\\theta^{(l)}} L_i(\\theta) $.\n",
    "\n",
    "### Constructing Gradient Matrices\n",
    "\n",
    "- **Assemble $ G $ and $ G^{(l)} $** using the computed gradients.\n",
    "\n",
    "### Computing Backward Kernels\n",
    "\n",
    "- **Total Kernel**:\n",
    "  $$\n",
    "  K = G G^\\top\n",
    "  $$\n",
    "\n",
    "- **Layer-Wise Kernels**:\n",
    "  $$\n",
    "  K^{(l)} = G^{(l)} \\left( G^{(l)} \\right)^\\top\n",
    "  $$\n",
    "\n",
    "### Analyzing Eigenspectra\n",
    "\n",
    "- **Eigenvalue Decomposition**:\n",
    "  - Compute eigenvalues $ \\lambda_i $ and eigenvectors $ v_i $ of $ K $ and $ K^{(l)} $.\n",
    "\n",
    "- **Moments Calculation**:\n",
    "  - **First Moment**:\n",
    "    $$\n",
    "    M_1 = \\frac{1}{m} \\operatorname{tr}(H) = \\frac{1}{m} \\sum_{i=1}^m \\lambda_i\n",
    "    $$\n",
    "  - **Second Moment**:\n",
    "    $$\n",
    "    M_2 = \\frac{1}{m} \\operatorname{tr}(H^2) = \\frac{1}{m} \\sum_{i=1}^m \\lambda_i^2\n",
    "    $$\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **Condition Number Indicator**:\n",
    "    $$\n",
    "    1 \\leq \\frac{M_2}{M_1^2} \\leq m\n",
    "    $$\n",
    "    - Lower values indicate a well-conditioned Hessian.\n",
    "\n",
    "### Monitoring Gradient Alignment\n",
    "\n",
    "- **Inner Product Analysis**:\n",
    "  - Examine $ K_{ij} $ values to assess gradient alignment.\n",
    "  - Identify pairs of data points with high conflicting gradients.\n",
    "\n",
    "### Layer-Wise Analysis\n",
    "\n",
    "- **Gradient Magnitude**:\n",
    "  - Compute the norm $ \\| g_i^{(l)} \\| $ for each layer.\n",
    "  - Monitor how gradient norms change with depth.\n",
    "\n",
    "- **Kernel Norms**:\n",
    "  - Compute $ \\| K^{(l)} \\| $ (e.g., Frobenius norm) to assess overall alignment in each layer.\n",
    "\n",
    "### Adjusting Training Strategies\n",
    "\n",
    "- **Activation Functions**:\n",
    "  - Use activations that preserve gradient flow (e.g., ReLU, Leaky ReLU).\n",
    "\n",
    "- **Normalization Techniques**:\n",
    "  - Apply Batch Normalization or Layer Normalization to stabilize gradients.\n",
    "\n",
    "- **Skip Connections**:\n",
    "  - Implement residual connections to alleviate vanishing gradients.\n",
    "\n",
    "- **Optimizer Choices**:\n",
    "  - Use adaptive optimizers (e.g., Adam, RMSprop) to handle varying gradient magnitudes.\n",
    "\n",
    "- **Learning Rate Scheduling**:\n",
    "  - Adjust learning rates based on observations from $ K $ and $ K^{(l)} $.\n",
    "\n",
    "- **Regularization**:\n",
    "  - Apply techniques like weight decay or dropout to prevent overfitting and manage gradient norms.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Steps for Simulations\n",
    "\n",
    "1. **Initialize the Network**:\n",
    "   - Define the architecture, including layers and activation functions.\n",
    "\n",
    "2. **Prepare the Dataset**:\n",
    "   - Load and preprocess the data.\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - For each epoch:\n",
    "     - For each batch:\n",
    "       - Compute forward pass.\n",
    "       - Compute loss.\n",
    "       - Compute gradients $ g_i $ and $ g_i^{(l)} $.\n",
    "       - Update parameters using an optimizer.\n",
    "\n",
    "4. **Compute Backward Kernels Periodically**:\n",
    "   - At specified intervals, compute $ K $ and $ K^{(l)} $ for the current batch.\n",
    "\n",
    "5. **Analyze and Log Metrics**:\n",
    "   - Track loss, accuracy, gradient norms, kernel eigenvalues, and Hessian moments $ M_1 $ and $ M_2 $.\n",
    "   - Visualize how these metrics evolve over time.\n",
    "\n",
    "6. **Adjust Training Based on Analysis**:\n",
    "   - Modify hyperparameters or architectures in response to observed issues (e.g., vanishing gradients, ill-conditioned Hessian).\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-Off Between Gradient Alignment and Hessian Conditioning\n",
    "\n",
    "### Convergence Speed vs. Optimization Stability\n",
    "\n",
    "- **Positive Gradient Alignment**:\n",
    "  - **Pros**:\n",
    "    - Accelerated convergence due to reinforcing updates.\n",
    "  - **Cons**:\n",
    "    - May lead to an ill-conditioned Hessian with large eigenvalue disparities.\n",
    "    - Optimization can become unstable and sensitive to hyperparameters.\n",
    "\n",
    "- **Gradient Orthogonality**:\n",
    "  - **Pros**:\n",
    "    - Results in a well-conditioned Hessian with uniform eigenvalues.\n",
    "    - Facilitates stable optimization and potentially better generalization.\n",
    "  - **Cons**:\n",
    "    - Slower convergence due to lack of reinforcement among gradient updates.\n",
    "\n",
    "### Balancing the Trade-Off\n",
    "\n",
    "- **Optimal Training Dynamics**:\n",
    "  - Aim for a moderate level of gradient alignment to benefit from faster convergence while maintaining a reasonably conditioned Hessian.\n",
    "- **Strategies**:\n",
    "  - **Regularization**: Use weight decay or dropout to prevent excessive alignment.\n",
    "  - **Normalization**: Apply batch normalization to stabilize gradients and promote healthy gradient flow.\n",
    "  - **Adaptive Optimizers**: Utilize optimizers like Adam to handle varying gradient magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Examples of Hessian Conditioning and Training Instability\n",
    "\n",
    "### Poor Hessian Conditioning Leading to Instability\n",
    "\n",
    "#### **Example Scenario**\n",
    "\n",
    "Consider a simple quadratic loss function in two dimensions:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} \\theta^\\top A \\theta\n",
    "$$\n",
    "\n",
    "where $ \\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix} $ and $ A $ is a symmetric positive definite matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\alpha & 0 \\\\\n",
    "0 & \\beta\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $ \\alpha \\gg \\beta > 0 $. The Hessian $ H $ of this loss is $ A $ itself.\n",
    "\n",
    "#### **Condition Number**\n",
    "\n",
    "- **Condition Number $ \\kappa $**:\n",
    "  $$\n",
    "  \\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\alpha}{\\beta}\n",
    "  $$\n",
    "  - A large $ \\kappa $ indicates poor conditioning.\n",
    "\n",
    "#### **Gradient Descent Update**\n",
    "\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta H \\theta_t = \\theta_t - \\eta A \\theta_t\n",
    "  $$\n",
    "\n",
    "- **Component-Wise Updates**:\n",
    "  $$\n",
    "  \\theta_{1, t+1} = \\theta_{1, t} - \\eta \\alpha \\theta_{1, t} \\\\\n",
    "  \\theta_{2, t+1} = \\theta_{2, t} - \\eta \\beta \\theta_{2, t}\n",
    "  $$\n",
    "\n",
    "#### **Instability Due to Poor Conditioning**\n",
    "\n",
    "- **Large Learning Rate $ \\eta $**:\n",
    "  - If $ \\eta $ is chosen based on the largest eigenvalue $ \\alpha $, it might be too large for the smaller eigenvalue $ \\beta $.\n",
    "  - Specifically, to ensure convergence, $ \\eta $ must satisfy:\n",
    "    $$\n",
    "    0 < \\eta < \\frac{2}{\\alpha + \\beta}\n",
    "    $$\n",
    "    - However, as $ \\alpha $ increases (poor conditioning), the upper bound on $ \\eta $ decreases.\n",
    "\n",
    "- **Oscillations and Divergence**:\n",
    "  - Choosing $ \\eta $ too close to $ \\frac{2}{\\alpha + \\beta} $ can cause oscillations in $ \\theta_1 $, leading to divergence.\n",
    "  - The parameter $ \\theta_1 $ may oscillate with increasing amplitude if $ \\eta \\alpha > 1 $.\n",
    "\n",
    "#### **Illustration**\n",
    "\n",
    "- **Iteration Behavior**:\n",
    "  - For $ \\alpha = 100 $, $ \\beta = 1 $, and $ \\eta = 0.02 $:\n",
    "    - $ \\eta \\alpha = 2 $ (boundary case).\n",
    "    - Updates:\n",
    "      $$\n",
    "      \\theta_{1, t+1} = \\theta_{1, t} - 2 \\theta_{1, t} = -\\theta_{1, t} \\\\\n",
    "      \\theta_{2, t+1} = \\theta_{2, t} - 0.02 \\times 1 \\times \\theta_{2, t} = 0.98 \\theta_{2, t}\n",
    "      $$\n",
    "    - $ \\theta_1 $ alternates in sign without converging to zero.\n",
    "    - If $ \\eta > 0.02 $, $ \\theta_1 $ diverges.\n",
    "\n",
    "---\n",
    "\n",
    "### SGD with Mini-Batches and Poor Hessian Conditioning\n",
    "\n",
    "#### **Stochastic Gradient Descent (SGD) Overview**\n",
    "\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_{\\text{batch}}(\\theta_t)\n",
    "  $$\n",
    "  - $ \\nabla_\\theta L_{\\text{batch}}(\\theta_t) $ is the gradient computed on a mini-batch.\n",
    "\n",
    "#### **Impact of Poor Conditioning**\n",
    "\n",
    "- **Variance in Gradient Estimates**:\n",
    "  - Mini-batch gradients introduce noise due to sampling variance.\n",
    "  - Poor Hessian conditioning ($ \\kappa $ large) exacerbates sensitivity to this noise.\n",
    "\n",
    "#### **Instability Mechanism**\n",
    "\n",
    "1. **Directional Sensitivity**:\n",
    "   - Directions corresponding to large eigenvalues ($ \\lambda_{\\max} $) receive aggressive updates.\n",
    "   - Directions with small eigenvalues ($ \\lambda_{\\min} $) receive negligible updates.\n",
    "\n",
    "2. **Noise Amplification**:\n",
    "   - In directions with large curvature, the noise in gradient estimates can cause significant deviations.\n",
    "   - This leads to erratic updates, oscillations, and potential divergence.\n",
    "\n",
    "3. **Learning Rate Constraints**:\n",
    "   - To maintain stability, the learning rate $ \\eta $ must be small enough to accommodate the largest eigenvalue.\n",
    "   - However, small $ \\eta $ slows down training and can make SGD more susceptible to being trapped in flat regions.\n",
    "\n",
    "#### **Concrete Example**\n",
    "\n",
    "Consider the same quadratic loss function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} \\theta^\\top A \\theta\n",
    "$$\n",
    "\n",
    "with $ A = \\begin{bmatrix} \\alpha & 0 \\\\ 0 & \\beta \\end{bmatrix} $, $ \\alpha \\gg \\beta > 0 $.\n",
    "\n",
    "- **Mini-Batch Gradient Estimate**:\n",
    "  - Assume each mini-batch consists of a single data point, leading to:\n",
    "    $$\n",
    "    \\nabla_\\theta L_{\\text{batch}}(\\theta_t) = A \\theta_t + \\epsilon_t\n",
    "    $$\n",
    "    - $ \\epsilon_t $: Noise due to stochasticity.\n",
    "\n",
    "- **Update Rule with Noise**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta (A \\theta_t + \\epsilon_t)\n",
    "  $$\n",
    "\n",
    "- **Impact of Noise**:\n",
    "  - In the $ \\theta_1 $ direction:\n",
    "    $$\n",
    "    \\theta_{1, t+1} = \\theta_{1, t} - \\eta \\alpha \\theta_{1, t} - \\eta \\epsilon_{1, t}\n",
    "    $$\n",
    "    - If $ \\eta \\alpha \\approx 2 $, even small $ \\epsilon_{1, t} $ can cause $ \\theta_1 $ to oscillate or diverge.\n",
    "\n",
    "  - In the $ \\theta_2 $ direction:\n",
    "    $$\n",
    "    \\theta_{2, t+1} = \\theta_{2, t} - \\eta \\beta \\theta_{2, t} - \\eta \\epsilon_{2, t}\n",
    "    $$\n",
    "    - Updates are more stable due to smaller $ \\beta $, but overall instability in $ \\theta_1 $ affects the entire parameter vector.\n",
    "\n",
    "#### **Consequences**\n",
    "\n",
    "- **Divergence**:\n",
    "  - The parameter $ \\theta_1 $ may not converge to zero but instead oscillate or grow without bound due to poor conditioning and stochastic noise.\n",
    "\n",
    "- **Optimization Failure**:\n",
    "  - The model fails to minimize the loss effectively, resulting in poor training performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example: Poor Hessian Conditioning Leading to Divergent Behavior\n",
    "\n",
    "### **Scenario**\n",
    "\n",
    "Consider a neural network trained on a simple quadratic loss function with two parameters $ \\theta = [\\theta_1, \\theta_2]^\\top $:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} \\theta^\\top A \\theta\n",
    "$$\n",
    "\n",
    "where $ A $ is a symmetric positive definite matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "100 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $ \\alpha = 100 $ and $ \\beta = 1 $, resulting in a condition number $ \\kappa = 100 $, indicating poor conditioning.\n",
    "\n",
    "### **Gradient Descent Dynamics**\n",
    "\n",
    "- **Gradient**:\n",
    "  $$\n",
    "  \\nabla_\\theta L(\\theta) = A \\theta = \\begin{bmatrix}\n",
    "  100 \\theta_1 \\\\\n",
    "  \\theta_2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t) = \\begin{bmatrix}\n",
    "  \\theta_{1,t} - 100 \\eta \\theta_{1,t} \\\\\n",
    "  \\theta_{2,t} - \\eta \\theta_{2,t}\n",
    "  \\end{bmatrix} = \\begin{bmatrix}\n",
    "  (1 - 100 \\eta) \\theta_{1,t} \\\\\n",
    "  (1 - \\eta) \\theta_{2,t}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Stability Condition**:\n",
    "  - For convergence in $ \\theta_1 $:\n",
    "    $$\n",
    "    |1 - 100 \\eta| < 1 \\quad \\Rightarrow \\quad 0 < \\eta < \\frac{2}{100} = 0.02\n",
    "    $$\n",
    "  - For convergence in $ \\theta_2 $:\n",
    "    $$\n",
    "    |1 - \\eta| < 1 \\quad \\Rightarrow \\quad 0 < \\eta < 2\n",
    "    $$\n",
    "\n",
    "- **Choosing $ \\eta = 0.02 $**:\n",
    "  - **Update in $ \\theta_1 $**:\n",
    "    $$\n",
    "    \\theta_{1,t+1} = (1 - 100 \\times 0.02) \\theta_{1,t} = (1 - 2) \\theta_{1,t} = -\\theta_{1,t}\n",
    "    $$\n",
    "    - **Behavior**: $ \\theta_1 $ oscillates between positive and negative values without converging to zero.\n",
    "\n",
    "  - **Update in $ \\theta_2 $**:\n",
    "    $$\n",
    "    \\theta_{2,t+1} = (1 - 0.02) \\theta_{2,t} = 0.98 \\theta_{2,t}\n",
    "    $$\n",
    "    - **Behavior**: $ \\theta_2 $ converges to zero.\n",
    "\n",
    "- **Result**:\n",
    "  - $ \\theta_1 $ does not converge and continues to oscillate indefinitely.\n",
    "  - This illustrates how a poorly conditioned Hessian can lead to divergent behavior in certain parameter directions.\n",
    "\n",
    "---\n",
    "\n",
    "## Impact of Poor Hessian Conditioning on Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### **Stochastic Gradient Descent (SGD) Overview**\n",
    "\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L_{\\text{batch}}(\\theta_t)\n",
    "  $$\n",
    "  - $ \\nabla_\\theta L_{\\text{batch}}(\\theta_t) $ is the gradient computed on a mini-batch.\n",
    "\n",
    "### **Challenges with Poor Hessian Conditioning**\n",
    "\n",
    "#### **1. Increased Sensitivity to Noise**\n",
    "\n",
    "- **Mini-Batch Gradients**:\n",
    "  - Mini-batch gradients are noisy estimates of the true gradient.\n",
    "  - High condition numbers amplify the effect of this noise.\n",
    "\n",
    "- **Impact**:\n",
    "  - In directions with large curvature (high eigenvalues), even small noise can cause significant parameter updates, leading to instability.\n",
    "  - In directions with small curvature (low eigenvalues), updates are minimal, slowing down convergence.\n",
    "\n",
    "#### **2. Amplified Oscillations**\n",
    "\n",
    "- **Directional Updates**:\n",
    "  - In poorly conditioned systems, updates in high-curvature directions can cause oscillations.\n",
    "  - The noise in SGD exacerbates these oscillations, preventing the optimizer from settling into minima.\n",
    "\n",
    "#### **3. Divergence Risk**\n",
    "\n",
    "- **Learning Rate Constraints**:\n",
    "  - To maintain stability, the learning rate $ \\eta $ must be small enough to accommodate the largest eigenvalue.\n",
    "  - However, small $ \\eta $ reduces the effectiveness of SGD, requiring more iterations to converge.\n",
    "\n",
    "- **Example**:\n",
    "  - Using the earlier quadratic loss with $ A = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix} $, if $ \\eta = 0.02 $:\n",
    "    - $ \\theta_1 $ oscillates due to the high curvature, and the stochastic noise can cause $ \\theta_1 $ to diverge over time.\n",
    "    - Even if $ \\eta < 0.02 $, the high curvature direction remains challenging for SGD.\n",
    "\n",
    "### **Illustrative Example**\n",
    "\n",
    "Consider training with SGD on the quadratic loss:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} \\theta^\\top A \\theta\n",
    "$$\n",
    "\n",
    "where $ A = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix} $.\n",
    "\n",
    "- **Mini-Batch Gradient with Noise**:\n",
    "  $$\n",
    "  \\nabla_\\theta L_{\\text{batch}}(\\theta_t) = A \\theta_t + \\epsilon_t\n",
    "  $$\n",
    "  - $ \\epsilon_t $: Stochastic noise due to mini-batch sampling.\n",
    "\n",
    "- **Update Rule**:\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta (A \\theta_t + \\epsilon_t) = \\begin{bmatrix}\n",
    "  \\theta_{1,t} - \\eta \\times 100 \\theta_{1,t} - \\eta \\epsilon_{1,t} \\\\\n",
    "  \\theta_{2,t} - \\eta \\times 1 \\theta_{2,t} - \\eta \\epsilon_{2,t}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Effects**:\n",
    "  - **In $ \\theta_1 $ Direction**:\n",
    "    - High curvature ($ \\alpha = 100 $) means that even small $ \\epsilon_{1,t} $ can cause large deviations.\n",
    "    - The update $ \\theta_{1,t+1} = (1 - 100 \\eta) \\theta_{1,t} - \\eta \\epsilon_{1,t} $ can lead to significant oscillations or divergence if $ \\eta $ is not adequately small.\n",
    "  \n",
    "  - **In $ \\theta_2 $ Direction**:\n",
    "    - Low curvature ($ \\beta = 1 $) allows for more stable updates, but overall instability in $ \\theta_1 $ can dominate the parameter dynamics.\n",
    "\n",
    "- **Outcome**:\n",
    "  - The parameter $ \\theta_1 $ experiences large, noisy updates, leading to erratic behavior and potential divergence.\n",
    "  - The parameter $ \\theta_2 $ may converge smoothly, but the overall optimization process is compromised by the instability in $ \\theta_1 $.\n",
    "\n",
    "### **Consequences for Training**\n",
    "\n",
    "- **Optimization Failure**:\n",
    "  - The optimizer fails to minimize the loss effectively, resulting in poor training performance.\n",
    "  \n",
    "- **Inconsistent Learning**:\n",
    "  - Different parameters learn at vastly different rates, causing imbalance in the network's feature representations.\n",
    "\n",
    "- **Increased Training Time**:\n",
    "  - Even with small learning rates, the presence of noisy and oscillating updates can require significantly more iterations to approach minima.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Recommendations to Mitigate Poor Hessian Conditioning\n",
    "\n",
    "### **1. Use of Adaptive Learning Rates**\n",
    "\n",
    "- **Adam, RMSprop, and AdaGrad**:\n",
    "  - These optimizers adjust learning rates based on the historical gradients, helping to stabilize updates in high-curvature directions.\n",
    "  - They can mitigate the effects of poor conditioning by scaling updates appropriately.\n",
    "\n",
    "### **2. Gradient Clipping**\n",
    "\n",
    "- **Purpose**:\n",
    "  - Prevents gradients from becoming too large, which can cause instability in high-curvature directions.\n",
    "\n",
    "- **Implementation**:\n",
    "  - Clip gradients based on their norm or per-parameter basis before applying updates.\n",
    "\n",
    "### **3. Second-Order Optimization Methods**\n",
    "\n",
    "- **Natural Gradient Descent**:\n",
    "  - Incorporates curvature information to make more informed updates.\n",
    "  \n",
    "- **Quasi-Newton Methods (e.g., L-BFGS)**:\n",
    "  - Approximate the Hessian to adjust updates, improving convergence in poorly conditioned scenarios.\n",
    "\n",
    "### **4. Regularization Techniques**\n",
    "\n",
    "- **Weight Decay**:\n",
    "  - Adds a penalty for large weights, reducing the variance in gradient magnitudes.\n",
    "\n",
    "- **Dropout**:\n",
    "  - Introduces randomness, promoting diverse gradient directions and preventing over-reliance on specific parameters.\n",
    "\n",
    "### **5. Network Architecture Adjustments**\n",
    "\n",
    "- **Residual Connections**:\n",
    "  - Facilitate gradient flow, reducing issues related to vanishing or exploding gradients.\n",
    "\n",
    "- **Batch Normalization**:\n",
    "  - Stabilizes activations and gradients, promoting healthier gradient alignments.\n",
    "\n",
    "### **6. Learning Rate Scheduling**\n",
    "\n",
    "- **Dynamic Adjustment**:\n",
    "  - Reduce learning rates during training to accommodate challenging curvature landscapes.\n",
    "\n",
    "### **7. Initialization Strategies**\n",
    "\n",
    "- **Proper Weight Initialization**:\n",
    "  - Techniques like He or Xavier initialization can help maintain gradient magnitudes across layers, improving Hessian conditioning.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By defining and analyzing the backward kernel and its layer-wise counterparts, we gain valuable insights into the training dynamics of neural networks. The connection between the Hessian approximation's eigenspectrum and the backward kernel allows us to assess the loss landscape's curvature and the Hessian's conditioning without direct computation of the Hessian.\n",
    "\n",
    "Understanding the trade-off between gradient alignment and Hessian conditioning is crucial:\n",
    "\n",
    "- **Positive Gradient Alignment** accelerates convergence but may lead to an ill-conditioned Hessian.\n",
    "- **Gradient Orthogonality** promotes a well-conditioned Hessian but may slow down convergence due to lack of reinforcing updates.\n",
    "\n",
    "Balancing these factors enables informed adjustments to the training process, enhancing both convergence speed and optimization stability, ultimately improving generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **Gradient Descent Methods**: Understanding how gradients influence parameter updates.\n",
    "- **Neural Network Optimization**: Strategies for mitigating vanishing and exploding gradients.\n",
    "- **Loss Landscape Analysis**: Studying the curvature and its impact on training.\n",
    "- **Numerical Linear Algebra**: Concepts related to eigenvalues, eigenvectors, and matrix conditioning.\n",
    "- **Optimization Theory**: Insights into convergence behavior and stability of optimization algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "*This mathematical note provides the foundational concepts and practical guidelines necessary for implementing simulations and training scripts focused on backward kernels and neural network training dynamics. By leveraging these insights, practitioners can optimize neural network training for both efficiency and generalization.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a0e798-92d8-41b3-81b3-6057c16f2b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/1], Loss: 1.8364\n",
      "Finished Training\n",
      "Accuracy on test set: 42.45%\n",
      "Jacobian between Layer 0 and Layer 1: torch.Size([128, 1, 3072])\n",
      "Jacobian between Layer 1 and Layer 2: torch.Size([128, 1, 128])\n",
      "Jacobian between Layer 2 and Layer 3: torch.Size([128, 1, 128])\n",
      "Jacobian between Layer 3 and Layer 4: torch.Size([128, 1, 128])\n",
      "Jacobian between Layer 4 and Layer 5: torch.Size([128, 1, 128])\n",
      "Jacobian between Layer 5 and Layer 6: torch.Size([10, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.functional import jacobian\n",
    "import numpy as np\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation_fn):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.view(x.size(0), -1)  # Flatten input\n",
    "        self.activations = [h]     # Store activations for Jacobian computation\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.activation_fn(self.layers[i](h))\n",
    "            self.activations.append(h)\n",
    "\n",
    "        output = self.layers[-1](h)\n",
    "        self.activations.append(output)\n",
    "        return output\n",
    "\n",
    "def load_data(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                               download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                              download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}')\n",
    "    print('Finished Training')\n",
    "\n",
    "def validate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "def compute_layer_jacobians(model, input_tensor):\n",
    "    \"\"\"\n",
    "    Compute Jacobian matrices between consecutive layers.\n",
    "    \n",
    "    Args:\n",
    "        model (CustomMLP): The neural network model\n",
    "        input_tensor (torch.Tensor): Input tensor to compute Jacobians for\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Jacobian matrices between consecutive layers\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    _ = model(input_tensor)  # Forward pass to store activations\n",
    "    jacobians = []\n",
    "    \n",
    "    for i in range(len(model.activations) - 1):\n",
    "        def layer_output(x):\n",
    "            if i < len(model.layers) - 1:\n",
    "                return model.activation_fn(model.layers[i](x))\n",
    "            else:\n",
    "                return model.layers[i](x)\n",
    "        \n",
    "        jac = jacobian(layer_output, model.activations[i])\n",
    "        jacobians.append(jac.squeeze(0))\n",
    "    \n",
    "    return jacobians\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define network parameters\n",
    "    layer_sizes = [3*32*32] + [128]*5+[10]  # Input size for CIFAR-10 images\n",
    "    activation_fn = torch.relu\n",
    "\n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = CustomMLP(layer_sizes, activation_fn)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Load data\n",
    "    trainloader, testloader = load_data(batch_size=512)\n",
    "\n",
    "    # Train model\n",
    "    train_model(model, trainloader, criterion, optimizer, num_epochs=1)\n",
    "\n",
    "    # Validate model\n",
    "    validate_model(model, testloader)\n",
    "\n",
    "    # Select a single input from test set\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "    input_image = images[0].unsqueeze(0)\n",
    "\n",
    "    # Compute Jacobians\n",
    "    jacobians = compute_layer_jacobians(model, input_image)\n",
    "\n",
    "    # Print shapes of Jacobians\n",
    "    for idx, jac in enumerate(jacobians):\n",
    "        print(f'Jacobian between Layer {idx} and Layer {idx+1}: {jac.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead48f9e-5f27-4b3d-9f09-6ac6f519b53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
