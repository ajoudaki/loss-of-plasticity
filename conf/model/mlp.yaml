# @package _global_.model
name: mlp

# MLP configuration
mlp:
  _target_: src.models.MLP
  hidden_sizes: [128, 128, 128, 128, 128]
  activation: relu
  dropout_p: 0.2
  normalization: batch
  norm_after_activation: false
  bias: true
  normalization_affine: true