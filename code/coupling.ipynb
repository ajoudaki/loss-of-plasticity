{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6b4e25-d707-4026-9cc6-489c207da0c6",
   "metadata": {},
   "source": [
    "## Theory Overview: Stability of Neuronal Coupling via the Restricted Hessian\n",
    "\n",
    "## Manifold Stability and the Symmetric‐Units Example\n",
    "\n",
    "### 1. Manifold Stability (General Concept)\n",
    "\n",
    "In a **gradient flow** system, parameters $\\theta$ evolve over time according to \n",
    "$$\n",
    "\\frac{d\\theta}{dt} \\;=\\; -\\,\\nabla f(\\theta),\n",
    "$$\n",
    "where $f(\\theta)$ is the loss function. A **manifold** $M\\subset \\mathbb{R}^n$ is said to be *invariant* under this flow if, whenever $\\theta(0)$ lies on $M$, the trajectory $\\theta(t)$ **remains** on $M$ for all $t>0$. In that case, the gradient $\\nabla f(\\theta)$ is always *tangent* to $M$ for any $\\theta\\in M$.  \n",
    "\n",
    "An **additional** property we often want is *stability*: if we start *near* the manifold $M$, will the flow bring us *closer* to $M$? If so, we say the manifold is **locally stable** or **attracting**. Formally, if the parameters deviate slightly off $M$, a stable manifold implies the dynamics push them back toward $M$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example: Symmetric Units in an MLP\n",
    "\n",
    "Consider a multi‐layer perceptron (MLP) with many neurons. Each neuron $i$ has:\n",
    "- An *incoming weight vector* $W_i$, i.e. its row in the layer’s weight matrix (plus possibly a bias $b_i$).\n",
    "- An *outgoing weight vector* in the **next** layer’s weight matrix (i.e. a column if you view that matrix from the next layer’s perspective).\n",
    "\n",
    "Now suppose two neurons, $i$ and $j$, have **identical (or nearly identical)** incoming weights ($W_i \\approx W_j$) and identical biases ($b_i \\approx b_j$) as well as *identical outgoing weights*. Then the set of all parameters $\\theta$ that enforce “$W_i = W_j$, $b_i = b_j$, and outgoing weights also equal” is a **manifold** in parameter space.  \n",
    "\n",
    "1. **Invariance:**  \n",
    "   - If your parameter $\\theta$ lies exactly on this “symmetry manifold,” the gradient $\\nabla f(\\theta)$ typically satisfies $\\nabla f(\\theta)\\big|_{W_i} = \\nabla f(\\theta)\\big|_{W_j}$.  \n",
    "   - This implies that under gradient descent, the updates for units $i$ and $j$ remain identical, so the parameters for those two neurons move *together*, keeping $W_i$ and $W_j$ (and $b_i$, $b_j$) equal at all times.  \n",
    "\n",
    "2. **Stable Symmetry:**  \n",
    "   - We further ask if small deviations off that manifold (i.e. $W_i \\neq W_j$ by a small amount) get pulled **back** to it by the gradient flow.  \n",
    "   - If so, we say the manifold of “coupled units” is **locally stable**: that means if at some point in training they are nearly identical, the gradient flow *reinforces* that similarity rather than letting them diverge.\n",
    "\n",
    "Hence, for two symmetric neurons, the **subspace** of parameters satisfying $W_i = W_j$ and $b_i = b_j$ is an **invariant manifold**. Demonstrating **local stability** amounts to showing that, near that point, the gradient flow *reduces* the difference $W_i - W_j$, $b_i - b_j$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Local Stability Criterion (Sketch)\n",
    "\n",
    "A common way to test local stability is:\n",
    "1. Identify the **manifold** $M$: in this case, $M = \\{\\theta : W_i = W_j,\\ b_i = b_j,\\ \\dots\\}$.  \n",
    "2. Check the **gradient** is tangent at $M$: ensures *invariance*.  \n",
    "3. Study the **Hessian** $\\nabla^2 f(\\theta_0)$ at a point $\\theta_0 \\in M$. In particular, look at directions **normal** to $M$—i.e., directions that *break* the symmetry (differences $W_i - W_j$).  \n",
    "   - If the Hessian is *positively curved* in those normal directions (for gradient *descent*), small deviations get pushed back to $M$.  \n",
    "   - Concretely, that means the restricted Hessian on the difference directions is **positive definite**, which implies a restoring force toward the manifold.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Conclusion\n",
    "\n",
    "- **Manifold Stability:** A manifold $M$ of parameters is stable under gradient descent if starting near $M$ keeps you (or pushes you) closer to $M$.  \n",
    "- **Symmetric Neurons:** Having two neurons with identical weights/biases is a special case of such a manifold. If the gradient flow is tangent to that manifold (no immediate push off it) *and* any small deviations are corrected (positive curvature in the difference directions), then *neuron coupling* is stable in the local neighborhood.  \n",
    "\n",
    "This concept underscores why, in practice, if two neurons become nearly identical, they may remain so (or become even more similar) throughout training—reflecting a **stable** symmetry in the parameter space.\n",
    "\n",
    "In a multi-layer neural network (MLP), each “unit” (or neuron) has associated trainable parameters—most notably the incoming weights and the bias for that neuron. When we **couple** two neurons, we deliberately constrain or make their parameters very similar so that they effectively share “behavior.” \n",
    "\n",
    "### 1. Manifold of Coupled Neurons\n",
    "\n",
    "If we denote two neurons in the same layer by indices $i$ and $j$, then coupling them amounts to imposing a relationship such as\n",
    "$$\n",
    "    W_i \\;\\approx\\; W_j, \n",
    "    \\quad\n",
    "    b_i \\;\\approx\\; b_j,\n",
    "$$\n",
    "where $W_i, W_j$ are the *rows* (or *columns*, depending on context) in the weight matrix corresponding to neurons $i$ and $j$, and $b_i, b_j$ are their biases.\n",
    "\n",
    "In a strict sense, **perfect** coupling would set $W_i = W_j$ and $b_i = b_j$.  Geometrically, this means the parameters lie on a *subspace* (or *manifold*) defined by those equality constraints.\n",
    "\n",
    "### 2. Gradient Flow and Stability\n",
    "\n",
    "We typically train neural networks via **gradient descent** on a loss function $f(\\theta)$, where $\\theta$ collects all network parameters.  For local stability analysis, we consider a trajectory $\\dot{\\theta}(t) = -\\,\\nabla f(\\theta(t))$.  \n",
    "\n",
    "- If we start exactly on the “coupled manifold,” meaning $W_i = W_j$ and $b_i = b_j$, and the flow **keeps** us there (i.e., no forces pulling us off), that subspace is *invariant*.  \n",
    "- More importantly, we want to see if *small deviations* off this manifold get pulled back (stable) or pushed away (unstable).\n",
    "\n",
    "### 3. Hessian Restriction and Normal Directions\n",
    "\n",
    "To test local stability near a point $\\theta_0$ on the coupled manifold, we look at the **Hessian** $\\nabla^2 f(\\theta_0)$.  The directions “normal” to the manifold are those that *break* the coupling constraints—for instance, a small difference $\\delta = (W_i - W_j)$.\n",
    "\n",
    "1. **Invariance Condition:**  \n",
    "   If $\\theta_0$ lies on the coupled manifold, for the manifold to be invariant under gradient descent, the gradient $\\nabla f(\\theta_0)$ must *respect* the coupling constraints (i.e., no immediate push off the manifold).\n",
    "\n",
    "2. **Stability Condition (Positive Curvature):**  \n",
    "   In gradient descent, small deviations in a normal direction $\\delta$ are pushed back to the manifold if the Hessian $\\nabla^2 f(\\theta_0)$ is **positive** in that direction.  Formally, we want\n",
    "   $$\n",
    "     \\delta^\\top \\,\\nabla^2 f(\\theta_0)\\,\\delta \\;>\\; 0\n",
    "   $$\n",
    "   for any $\\delta$ that lies in the normal space.  Concretely, if $\\delta$ is the difference of parameters between neurons $i$ and $j$, then the 2×2 (or small block) of the Hessian capturing partial derivatives w.r.t. these neurons’ parameters should be **positive definite**.\n",
    "\n",
    "### 4. The Experiment\n",
    "\n",
    "1. **Coupling Operation:**  \n",
    "   - We select neurons $i$ and $j$ in layer $\\ell$ and make their parameters similar by copying $W_i \\approx W_j$ and $b_i \\approx b_j$.  \n",
    "   - We optionally add a small random perturbation so they are not *exactly* identical.\n",
    "\n",
    "2. **Hessian Computation:**  \n",
    "   - We compute the Hessian of the loss function w.r.t. the relevant parameters (in practice, w.r.t. the entire layer or just the biases).  \n",
    "   - We extract the submatrix of that Hessian that corresponds specifically to $W_i, W_j$ (or $b_i, b_j$), which can be visualized as a 2×2 block if we look at just the pair $\\{i, j\\}$.  \n",
    "\n",
    "3. **Testing Positive Definiteness:**  \n",
    "   - If the 2×2 Hessian block \n",
    "     $$\n",
    "       \\begin{pmatrix}\n",
    "         H_{ii} & H_{ij}\\\\\n",
    "         H_{ij} & H_{jj}\n",
    "       \\end{pmatrix}\n",
    "     $$\n",
    "     is **positive definite**, it means any infinitesimal difference between neurons $i$ and $j$ leads to an *increase* in loss—and thus gradient descent tends to “push” the parameters back together.  Mathematically, that requires $H_{ii}>0$ and $\\det>0$.  \n",
    "   - In simpler terms, the difference direction $\\delta = W_i - W_j$ (or $b_i - b_j$) is a direction of *positive* curvature.  \n",
    "\n",
    "### 5. Conclusion\n",
    "\n",
    "By examining the **restricted Hessian** in these “difference directions,” we can test **whether coupling two neurons is locally stable** under gradient descent. Positive curvature in that sub-block indicates that if they begin coupled (equal parameters) and deviate slightly, the training dynamics will bring them back together. If the curvature is negative or indefinite, then small deviations grow larger, implying the coupling is *unstable*.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Coupled Manifold:** Parameters satisfying $W_i = W_j, b_i = b_j$.  \n",
    "- **Local Stability Check:** Restrict the Hessian to the subspace normal to the manifold (the difference directions).  \n",
    "- **Positive Definiteness:** Ensures stable coupling; negative or indefinite curvature indicates potential instability.\n",
    "\n",
    "This theoretical framework underlies the experiment: **we couple two neurons, measure the Hessian’s curvature in the difference directions, and then draw conclusions about the stability of that coupling under gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ec666-9e6f-490c-9761-a5211677ce9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caaa865d-8584-4291-9203-2c7715084f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "================= longer trainig =================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 15, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0535, Euclidean distance: 0.8162\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/15 - Loss: 1.7773\n",
      "  Acc (Task 1): 44.75%, Acc (Task 2): 0.80%\n",
      "  Unit similarity - Cosine: 0.0451, Euclidean: 1.4688\n",
      "Task 1 Epoch 2/15 - Loss: 1.2036\n",
      "  Acc (Task 1): 50.62%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.3002, Euclidean: 1.4888\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 2, step 100, cos_sim = 0.226 \n",
      "bias = 0\n",
      "win = tensor([-1.0057e-02, -5.1312e-03, -2.1936e-03, -1.3560e-03, -8.5231e-04,\n",
      "        -6.2902e-04, -3.8814e-04, -3.0525e-04, -2.3610e-04, -1.7160e-04,\n",
      "        -1.3853e-04, -1.1702e-04, -8.7452e-05, -6.8232e-05, -6.2275e-05,\n",
      "        -5.4817e-05, -4.9105e-05, -4.4498e-05, -4.0845e-05, -3.4880e-05,\n",
      "        -3.1727e-05, -2.9022e-05, -2.7472e-05, -2.4894e-05, -2.3911e-05,\n",
      "        -2.3127e-05, -1.9648e-05, -1.9302e-05, -1.6461e-05, -1.5482e-05,\n",
      "        -1.3889e-05, -1.3249e-05, -1.1376e-05, -1.0827e-05, -1.0226e-05,\n",
      "        -9.6130e-06, -9.1119e-06, -7.9667e-06, -7.7880e-06, -6.7689e-06,\n",
      "        -6.1904e-06, -5.7642e-06, -5.3821e-06, -5.1030e-06, -4.6747e-06,\n",
      "        -4.5487e-06, -4.0443e-06, -3.9421e-06, -3.6772e-06, -3.3436e-06,\n",
      "        -2.9826e-06, -2.5600e-06, -2.4867e-06, -2.2529e-06, -2.2487e-06,\n",
      "        -2.1313e-06, -1.9475e-06, -1.8693e-06, -1.7487e-06, -1.6174e-06,\n",
      "        -1.5541e-06, -1.3737e-06, -1.3085e-06, -1.2977e-06, -1.1998e-06,\n",
      "        -1.0187e-06, -9.8918e-07, -8.8717e-07, -7.9938e-07, -7.7126e-07,\n",
      "        -6.7950e-07, -6.0994e-07, -6.0029e-07, -5.6114e-07, -5.2655e-07,\n",
      "        -4.7183e-07, -4.2981e-07, -3.9606e-07, -3.5178e-07, -3.1202e-07,\n",
      "        -3.0348e-07, -2.6678e-07, -2.5550e-07, -2.4668e-07, -2.0309e-07,\n",
      "        -1.9182e-07, -1.6957e-07, -1.5407e-07, -1.2975e-07, -1.1366e-07,\n",
      "        -1.0295e-07, -8.5751e-08, -7.1628e-08, -6.6532e-08, -4.6866e-08,\n",
      "        -4.1010e-08, -2.6711e-08, -1.7890e-08, -1.3557e-08, -8.8876e-09,\n",
      "         2.0638e-09,  8.3925e-09,  1.0655e-08,  1.8861e-08,  2.4660e-08,\n",
      "         3.2114e-08,  3.5867e-08,  4.7037e-08,  5.6790e-08,  6.2467e-08,\n",
      "         7.7749e-08,  8.3655e-08,  8.9018e-08,  1.0117e-07,  1.0948e-07,\n",
      "         1.2022e-07,  1.4213e-07,  1.4547e-07,  1.5629e-07,  1.8649e-07,\n",
      "         2.0425e-07,  2.2747e-07,  2.6712e-07,  2.8202e-07,  3.1227e-07,\n",
      "         3.5448e-07,  4.0429e-07,  4.2277e-07,  4.9357e-07,  5.2650e-07,\n",
      "         5.3872e-07,  5.9698e-07,  6.2708e-07,  6.7523e-07,  7.1746e-07,\n",
      "         8.2204e-07,  9.0339e-07,  9.6861e-07,  1.1276e-06,  1.2028e-06,\n",
      "         1.4386e-06,  1.4928e-06,  1.4983e-06,  1.6612e-06,  1.9446e-06,\n",
      "         1.9932e-06,  2.1099e-06,  2.2221e-06,  2.4097e-06,  2.7161e-06,\n",
      "         3.1099e-06,  3.3205e-06,  3.6180e-06,  3.6573e-06,  4.0603e-06,\n",
      "         4.2753e-06,  4.6583e-06,  5.1534e-06,  5.6185e-06,  6.0334e-06,\n",
      "         6.5340e-06,  6.5718e-06,  6.9418e-06,  8.2012e-06,  8.5788e-06,\n",
      "         9.4842e-06,  9.7684e-06,  1.0356e-05,  1.1737e-05,  1.3090e-05,\n",
      "         1.4761e-05,  1.5105e-05,  1.6200e-05,  1.7876e-05,  1.9788e-05,\n",
      "         2.1155e-05,  2.4244e-05,  2.5977e-05,  2.9080e-05,  3.2113e-05,\n",
      "         3.5684e-05,  3.9754e-05,  4.3807e-05,  5.0937e-05,  6.9808e-05,\n",
      "         7.6612e-05,  7.7409e-05,  1.0836e-04,  1.2000e-04,  1.6210e-04,\n",
      "         1.7023e-04,  2.3360e-04,  3.0635e-04,  3.6824e-04,  6.0974e-04,\n",
      "         7.8660e-04,  9.5023e-04,  2.5522e-03,  2.9793e-03,  7.8267e-03])\n",
      "wout = tensor([-4.1889e-03, -2.5049e-03, -2.1425e-03, -2.0004e-03, -1.6610e-03,\n",
      "        -1.6325e-03, -1.6167e-03, -1.4190e-03, -1.3886e-03, -1.3453e-03,\n",
      "        -1.2898e-03, -1.2558e-03, -1.2167e-03, -1.1874e-03, -1.0712e-03,\n",
      "        -1.0055e-03, -9.7643e-04, -9.6065e-04, -9.3378e-04, -9.0063e-04,\n",
      "        -8.8003e-04, -8.7469e-04, -8.1025e-04, -7.9755e-04, -7.8159e-04,\n",
      "        -7.7395e-04, -7.5348e-04, -7.4634e-04, -7.2069e-04, -7.1176e-04,\n",
      "        -7.0247e-04, -6.9839e-04, -6.8567e-04, -6.7485e-04, -6.5642e-04,\n",
      "        -6.4734e-04, -6.3878e-04, -6.3576e-04, -6.1525e-04, -6.1081e-04,\n",
      "        -6.0668e-04, -6.0371e-04, -6.0078e-04, -5.9264e-04, -5.7707e-04,\n",
      "        -5.6232e-04, -5.5597e-04, -5.4456e-04, -5.3344e-04, -5.3011e-04,\n",
      "        -5.2433e-04, -5.0846e-04, -4.9927e-04, -4.9472e-04, -4.8515e-04,\n",
      "        -4.8009e-04, -4.7099e-04, -4.6927e-04, -4.5216e-04, -4.4543e-04,\n",
      "        -4.3297e-04, -4.2649e-04, -4.2296e-04, -4.0706e-04, -4.0270e-04,\n",
      "        -3.7099e-04, -3.6719e-04, -3.5518e-04, -3.5035e-04, -3.3659e-04,\n",
      "        -3.2008e-04, -3.0475e-04, -2.8417e-04, -2.7307e-04, -2.5020e-04,\n",
      "        -2.4544e-04, -2.3856e-04, -2.3250e-04, -2.2834e-04, -2.1722e-04,\n",
      "        -2.0873e-04, -2.0846e-04, -1.9828e-04, -1.8271e-04, -1.7508e-04,\n",
      "        -1.7286e-04, -1.6561e-04, -1.6459e-04, -1.5883e-04, -1.5155e-04,\n",
      "        -1.4902e-04, -1.4057e-04, -1.3379e-04, -1.3104e-04, -1.2211e-04,\n",
      "        -1.1974e-04, -1.1124e-04, -1.1055e-04, -1.0594e-04, -9.8060e-05,\n",
      "        -9.3520e-05, -8.4067e-05, -7.9154e-05, -7.7646e-05, -6.9114e-05,\n",
      "        -6.6556e-05, -6.4758e-05, -5.6819e-05, -5.2154e-05, -3.5197e-05,\n",
      "        -3.3839e-05, -3.1581e-05, -2.7997e-05, -2.3989e-05, -2.1616e-05,\n",
      "        -1.1675e-05, -3.9317e-06,  5.0112e-06,  2.1778e-05,  2.5254e-05,\n",
      "         3.2547e-05,  3.7640e-05,  4.1841e-05,  4.2757e-05,  4.7739e-05,\n",
      "         4.9339e-05,  5.0470e-05,  5.6751e-05,  6.3962e-05,  6.7687e-05,\n",
      "         7.2942e-05,  7.4932e-05,  8.5498e-05,  1.0269e-04,  1.1082e-04,\n",
      "         1.1650e-04,  1.2216e-04,  1.4793e-04,  1.5467e-04,  1.7067e-04,\n",
      "         1.7335e-04,  1.8267e-04,  1.9510e-04,  2.0723e-04,  2.0891e-04,\n",
      "         2.2655e-04,  2.3242e-04,  2.3746e-04,  2.4995e-04,  2.7686e-04,\n",
      "         2.9441e-04,  2.9592e-04,  3.0133e-04,  3.1722e-04,  3.2028e-04,\n",
      "         3.2613e-04,  3.7316e-04,  3.7860e-04,  3.8742e-04,  4.1536e-04,\n",
      "         4.2770e-04,  4.4159e-04,  4.5381e-04,  4.6361e-04,  4.8310e-04,\n",
      "         4.9197e-04,  5.1468e-04,  5.4758e-04,  5.5926e-04,  5.6802e-04,\n",
      "         6.0765e-04,  6.2335e-04,  6.6058e-04,  7.2131e-04,  7.4027e-04,\n",
      "         7.7613e-04,  8.0230e-04,  8.4708e-04,  8.6924e-04,  9.1016e-04,\n",
      "         9.3606e-04,  1.0097e-03,  1.0459e-03,  1.0758e-03,  1.1230e-03,\n",
      "         1.1473e-03,  1.1708e-03,  1.3123e-03,  1.3327e-03,  1.3651e-03,\n",
      "         1.4308e-03,  1.4919e-03,  1.5450e-03,  1.6404e-03,  1.8393e-03,\n",
      "         2.1804e-03,  2.2305e-03,  2.3874e-03,  2.6052e-03,  3.0225e-03])\n",
      "Task 1 Epoch 3/15 - Loss: 1.1619\n",
      "  Acc (Task 1): 53.73%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.2668, Euclidean: 1.6859\n",
      "Task 1 Epoch 4/15 - Loss: 1.1104\n",
      "  Acc (Task 1): 46.58%, Acc (Task 2): 0.02%\n",
      "  Unit similarity - Cosine: 0.3695, Euclidean: 1.6502\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 4, step 200, cos_sim = 0.327 \n",
      "bias = 0\n",
      "win = tensor([-8.9000e-03, -6.1350e-03, -1.4802e-03, -3.3769e-04, -1.7355e-04,\n",
      "        -1.5304e-04, -1.9809e-05, -8.1923e-06, -1.4996e-06, -7.3851e-07,\n",
      "        -3.6681e-07, -9.1473e-08, -6.3117e-08, -3.2057e-08, -1.8657e-08,\n",
      "        -7.7962e-09, -4.0226e-09, -3.8120e-09, -1.3610e-09, -1.1447e-09,\n",
      "        -9.9895e-10, -9.6391e-10, -9.4408e-10, -9.2264e-10, -8.8079e-10,\n",
      "        -8.5963e-10, -8.3422e-10, -7.9557e-10, -7.6440e-10, -7.4061e-10,\n",
      "        -7.3604e-10, -7.2263e-10, -7.1302e-10, -6.8719e-10, -6.7176e-10,\n",
      "        -6.4372e-10, -6.3070e-10, -6.0264e-10, -5.9452e-10, -5.8793e-10,\n",
      "        -5.7769e-10, -5.6144e-10, -5.3143e-10, -5.2666e-10, -5.1480e-10,\n",
      "        -5.0700e-10, -5.0015e-10, -4.7927e-10, -4.7756e-10, -4.6789e-10,\n",
      "        -4.5455e-10, -4.3608e-10, -4.2756e-10, -4.1992e-10, -4.1676e-10,\n",
      "        -4.0352e-10, -3.7924e-10, -3.6740e-10, -3.6480e-10, -3.5847e-10,\n",
      "        -3.5229e-10, -3.4408e-10, -3.3967e-10, -3.2416e-10, -3.2121e-10,\n",
      "        -3.1499e-10, -3.0552e-10, -2.9848e-10, -2.8305e-10, -2.7255e-10,\n",
      "        -2.5137e-10, -2.4402e-10, -2.2917e-10, -2.2714e-10, -2.1784e-10,\n",
      "        -2.1349e-10, -1.9728e-10, -1.8625e-10, -1.8266e-10, -1.7242e-10,\n",
      "        -1.6201e-10, -1.5393e-10, -1.4680e-10, -1.4249e-10, -1.3339e-10,\n",
      "        -1.2331e-10, -1.0853e-10, -1.0519e-10, -9.9320e-11, -9.3694e-11,\n",
      "        -9.1571e-11, -8.3249e-11, -6.8460e-11, -6.0927e-11, -5.0324e-11,\n",
      "        -4.4371e-11, -4.0897e-11, -3.2875e-11, -2.6813e-11, -1.4521e-11,\n",
      "        -7.9215e-12,  4.5404e-12,  8.8290e-12,  1.6009e-11,  1.8839e-11,\n",
      "         3.1333e-11,  3.7386e-11,  4.3977e-11,  5.2361e-11,  5.9477e-11,\n",
      "         6.1465e-11,  6.8137e-11,  8.2165e-11,  9.5005e-11,  1.0297e-10,\n",
      "         1.0806e-10,  1.2283e-10,  1.3250e-10,  1.3476e-10,  1.4605e-10,\n",
      "         1.5254e-10,  1.6020e-10,  1.6536e-10,  1.7652e-10,  1.8054e-10,\n",
      "         1.9177e-10,  2.0705e-10,  2.1762e-10,  2.1926e-10,  2.3162e-10,\n",
      "         2.3544e-10,  2.5067e-10,  2.5554e-10,  2.6870e-10,  2.7505e-10,\n",
      "         2.8417e-10,  2.9667e-10,  3.0472e-10,  3.1360e-10,  3.1958e-10,\n",
      "         3.2771e-10,  3.4063e-10,  3.4948e-10,  3.5631e-10,  3.6227e-10,\n",
      "         3.8058e-10,  4.0154e-10,  4.0340e-10,  4.1683e-10,  4.2093e-10,\n",
      "         4.2986e-10,  4.4249e-10,  4.5603e-10,  4.6965e-10,  4.8221e-10,\n",
      "         4.8983e-10,  4.9722e-10,  5.1449e-10,  5.4361e-10,  5.5054e-10,\n",
      "         5.5471e-10,  5.6805e-10,  5.8228e-10,  5.9044e-10,  6.1755e-10,\n",
      "         6.2000e-10,  6.4445e-10,  6.4937e-10,  6.6795e-10,  6.7669e-10,\n",
      "         7.0153e-10,  7.2149e-10,  7.4226e-10,  7.5847e-10,  7.6953e-10,\n",
      "         7.9569e-10,  8.1785e-10,  8.3789e-10,  8.6583e-10,  9.2511e-10,\n",
      "         9.3856e-10,  1.0115e-09,  1.0256e-09,  1.1040e-09,  1.1936e-09,\n",
      "         1.2349e-09,  1.4737e-09,  2.3563e-09,  2.9965e-09,  1.7342e-08,\n",
      "         1.9323e-08,  3.0786e-08,  4.7470e-08,  1.8604e-07,  1.2005e-04,\n",
      "         2.6638e-04,  5.4098e-04,  1.1015e-02,  2.9481e-02,  7.2220e-02])\n",
      "wout = tensor([-7.5057e-03, -6.1410e-03, -3.3826e-03, -3.1069e-03, -2.7272e-03,\n",
      "        -1.8961e-03, -1.6798e-03, -1.6467e-03, -1.6102e-03, -1.5157e-03,\n",
      "        -1.3859e-03, -1.2399e-03, -1.2123e-03, -1.1632e-03, -1.0894e-03,\n",
      "        -1.0815e-03, -9.5126e-04, -9.0304e-04, -8.7442e-04, -8.4899e-04,\n",
      "        -8.0990e-04, -7.6722e-04, -7.6333e-04, -7.2734e-04, -6.9520e-04,\n",
      "        -6.8845e-04, -6.5486e-04, -6.3532e-04, -6.3159e-04, -6.2137e-04,\n",
      "        -5.9365e-04, -5.6293e-04, -5.5006e-04, -5.4293e-04, -5.1428e-04,\n",
      "        -5.0982e-04, -4.7563e-04, -4.6560e-04, -4.4701e-04, -4.4210e-04,\n",
      "        -4.3463e-04, -4.2670e-04, -4.2118e-04, -3.9565e-04, -3.7029e-04,\n",
      "        -3.5563e-04, -3.3361e-04, -3.3173e-04, -3.2949e-04, -3.2326e-04,\n",
      "        -3.1569e-04, -3.0592e-04, -2.9718e-04, -2.9121e-04, -2.8260e-04,\n",
      "        -2.7211e-04, -2.6500e-04, -2.5396e-04, -2.4872e-04, -2.3733e-04,\n",
      "        -2.2700e-04, -2.1850e-04, -2.0201e-04, -1.7210e-04, -1.5245e-04,\n",
      "        -1.5032e-04, -1.4189e-04, -1.3206e-04, -1.2113e-04, -1.1539e-04,\n",
      "        -1.0834e-04, -1.0042e-04, -9.4400e-05, -8.2591e-05, -6.6391e-05,\n",
      "        -5.7608e-05, -4.4981e-05, -3.2408e-05, -2.4649e-05, -2.3585e-05,\n",
      "        -2.2775e-05, -1.7735e-05, -8.0449e-06, -2.9221e-06, -2.1421e-06,\n",
      "        -1.6526e-06, -7.3620e-07, -4.4987e-07, -7.9348e-08, -5.0698e-08,\n",
      "         7.7545e-08,  1.3376e-07,  4.3845e-06,  5.9053e-06,  9.8915e-06,\n",
      "         1.8562e-05,  2.5369e-05,  3.2373e-05,  3.5421e-05,  3.8796e-05,\n",
      "         4.0209e-05,  4.5745e-05,  4.6916e-05,  5.3550e-05,  5.5267e-05,\n",
      "         6.9788e-05,  7.9994e-05,  8.7181e-05,  9.0248e-05,  9.5082e-05,\n",
      "         9.6791e-05,  1.0730e-04,  1.1053e-04,  1.1243e-04,  1.1891e-04,\n",
      "         1.2771e-04,  1.2981e-04,  1.3369e-04,  1.4487e-04,  1.5127e-04,\n",
      "         1.6823e-04,  1.7823e-04,  1.9171e-04,  1.9644e-04,  2.0911e-04,\n",
      "         2.1292e-04,  2.2753e-04,  2.3482e-04,  2.3626e-04,  2.4657e-04,\n",
      "         2.4894e-04,  2.5894e-04,  2.6720e-04,  2.7257e-04,  2.8495e-04,\n",
      "         2.9869e-04,  3.0204e-04,  3.3645e-04,  3.4643e-04,  3.4717e-04,\n",
      "         3.5896e-04,  3.6194e-04,  3.6950e-04,  3.8076e-04,  4.1543e-04,\n",
      "         4.1810e-04,  4.2318e-04,  4.4246e-04,  4.5489e-04,  4.6430e-04,\n",
      "         4.8421e-04,  4.9129e-04,  5.1252e-04,  5.2570e-04,  5.3942e-04,\n",
      "         5.5260e-04,  5.5731e-04,  5.7919e-04,  5.8643e-04,  6.0352e-04,\n",
      "         6.1191e-04,  6.3267e-04,  6.5438e-04,  6.7957e-04,  6.8421e-04,\n",
      "         6.9109e-04,  7.1974e-04,  7.2789e-04,  7.2966e-04,  7.5011e-04,\n",
      "         8.0202e-04,  8.1712e-04,  8.2682e-04,  8.3163e-04,  8.3954e-04,\n",
      "         8.4665e-04,  8.6304e-04,  8.7375e-04,  8.9561e-04,  9.1531e-04,\n",
      "         9.2832e-04,  9.3869e-04,  9.4698e-04,  1.0710e-03,  1.1351e-03,\n",
      "         1.1650e-03,  1.1735e-03,  1.1833e-03,  1.2426e-03,  1.2693e-03,\n",
      "         1.3016e-03,  1.3597e-03,  1.3947e-03,  1.5986e-03,  1.6474e-03,\n",
      "         2.0840e-03,  2.2314e-03,  2.8614e-03,  3.9225e-03,  4.5059e-03])\n",
      "Task 1 Epoch 5/15 - Loss: 1.0858\n",
      "  Acc (Task 1): 53.67%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.3272, Euclidean: 1.8249\n",
      "Task 1 Epoch 6/15 - Loss: 1.0876\n",
      "  Acc (Task 1): 52.60%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.2491, Euclidean: 2.0198\n",
      "Task 1 Epoch 7/15 - Loss: 1.0623\n",
      "  Acc (Task 1): 52.00%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.1142, Euclidean: 2.3138\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 7, step 300, cos_sim = 0.119 \n",
      "bias = 0\n",
      "win = tensor([-1.8042e-02, -9.5536e-03, -4.1741e-03, -2.0024e-03, -1.2566e-03,\n",
      "        -1.0749e-03, -8.4785e-04, -7.1986e-04, -6.1573e-04, -4.6832e-04,\n",
      "        -3.4895e-04, -3.1335e-04, -2.4156e-04, -2.1831e-04, -2.0647e-04,\n",
      "        -1.8915e-04, -1.5636e-04, -1.4857e-04, -1.2014e-04, -1.1218e-04,\n",
      "        -1.0513e-04, -9.7014e-05, -8.4301e-05, -7.4918e-05, -7.3294e-05,\n",
      "        -6.7750e-05, -6.2316e-05, -5.5473e-05, -4.9862e-05, -4.8802e-05,\n",
      "        -4.0420e-05, -3.7591e-05, -3.3643e-05, -3.1666e-05, -2.8097e-05,\n",
      "        -2.5183e-05, -2.4948e-05, -2.3310e-05, -2.1658e-05, -2.0268e-05,\n",
      "        -1.8099e-05, -1.7234e-05, -1.6417e-05, -1.4803e-05, -1.3386e-05,\n",
      "        -1.2886e-05, -1.1686e-05, -1.1293e-05, -1.0011e-05, -9.0095e-06,\n",
      "        -8.1966e-06, -7.6470e-06, -7.3347e-06, -6.6333e-06, -5.7170e-06,\n",
      "        -5.4347e-06, -5.0900e-06, -4.9726e-06, -4.2500e-06, -3.9522e-06,\n",
      "        -3.6468e-06, -3.3201e-06, -2.9069e-06, -2.6633e-06, -2.5314e-06,\n",
      "        -2.3144e-06, -2.2522e-06, -2.0377e-06, -1.9057e-06, -1.6515e-06,\n",
      "        -1.5781e-06, -1.4962e-06, -1.4071e-06, -1.3628e-06, -1.1243e-06,\n",
      "        -1.0372e-06, -9.7804e-07, -9.4461e-07, -8.1601e-07, -7.3222e-07,\n",
      "        -7.0363e-07, -6.8617e-07, -6.0812e-07, -5.1862e-07, -4.3145e-07,\n",
      "        -4.2246e-07, -3.9814e-07, -3.4394e-07, -3.1492e-07, -2.5583e-07,\n",
      "        -2.4218e-07, -1.9087e-07, -1.6748e-07, -1.4894e-07, -1.3007e-07,\n",
      "        -1.1106e-07, -9.3624e-08, -6.1446e-08, -5.4203e-08, -4.3809e-08,\n",
      "        -3.3264e-08, -4.0390e-09,  2.0852e-09,  2.9193e-08,  3.5647e-08,\n",
      "         6.5747e-08,  9.4229e-08,  1.1625e-07,  1.2983e-07,  1.7360e-07,\n",
      "         2.0811e-07,  2.5217e-07,  2.8058e-07,  3.4201e-07,  3.7712e-07,\n",
      "         4.4304e-07,  4.9233e-07,  5.3850e-07,  6.4243e-07,  6.7187e-07,\n",
      "         6.8648e-07,  7.9801e-07,  8.4946e-07,  8.5629e-07,  9.8863e-07,\n",
      "         1.1247e-06,  1.1569e-06,  1.3864e-06,  1.5866e-06,  1.6437e-06,\n",
      "         1.7438e-06,  1.7909e-06,  2.1101e-06,  2.3381e-06,  2.4919e-06,\n",
      "         2.7911e-06,  2.9737e-06,  3.1212e-06,  3.3549e-06,  3.4769e-06,\n",
      "         3.9592e-06,  4.5043e-06,  4.7905e-06,  5.0005e-06,  5.2629e-06,\n",
      "         5.6521e-06,  5.6888e-06,  6.5544e-06,  7.1558e-06,  7.6192e-06,\n",
      "         8.2178e-06,  9.3140e-06,  9.9092e-06,  1.0865e-05,  1.1727e-05,\n",
      "         1.2444e-05,  1.4668e-05,  1.4952e-05,  1.6460e-05,  1.7527e-05,\n",
      "         1.8343e-05,  2.0723e-05,  2.3233e-05,  2.5054e-05,  2.7493e-05,\n",
      "         2.8807e-05,  3.5025e-05,  3.6487e-05,  4.0958e-05,  4.6807e-05,\n",
      "         4.9479e-05,  5.2310e-05,  5.7770e-05,  6.0746e-05,  6.3647e-05,\n",
      "         7.7044e-05,  8.4300e-05,  9.0960e-05,  9.2988e-05,  1.0775e-04,\n",
      "         1.3288e-04,  1.3984e-04,  1.4869e-04,  1.6205e-04,  1.8118e-04,\n",
      "         2.0192e-04,  2.0938e-04,  2.2979e-04,  2.4388e-04,  2.8643e-04,\n",
      "         3.4257e-04,  3.6179e-04,  4.9924e-04,  5.1772e-04,  9.4573e-04,\n",
      "         1.0913e-03,  1.3975e-03,  3.2392e-03,  5.3611e-03,  2.9932e-02])\n",
      "wout = tensor([-1.8215e-03, -1.6852e-03, -1.5170e-03, -1.3533e-03, -1.3284e-03,\n",
      "        -1.1855e-03, -1.1170e-03, -1.1047e-03, -9.8364e-04, -9.1152e-04,\n",
      "        -9.0688e-04, -8.3305e-04, -8.1528e-04, -7.9728e-04, -6.9743e-04,\n",
      "        -6.2960e-04, -5.9585e-04, -5.9095e-04, -5.7694e-04, -5.6971e-04,\n",
      "        -5.5799e-04, -5.4796e-04, -5.4150e-04, -5.1522e-04, -5.0820e-04,\n",
      "        -5.0402e-04, -4.9851e-04, -4.9016e-04, -4.7110e-04, -4.6327e-04,\n",
      "        -4.6315e-04, -4.5422e-04, -4.2008e-04, -4.1131e-04, -3.9422e-04,\n",
      "        -3.8455e-04, -3.7406e-04, -3.7258e-04, -3.6150e-04, -3.5784e-04,\n",
      "        -3.4980e-04, -3.2495e-04, -3.1988e-04, -3.0809e-04, -2.9262e-04,\n",
      "        -2.8817e-04, -2.8095e-04, -2.6605e-04, -2.4920e-04, -2.3459e-04,\n",
      "        -2.3225e-04, -2.2314e-04, -2.2021e-04, -2.1716e-04, -2.1151e-04,\n",
      "        -2.0908e-04, -2.0700e-04, -2.0029e-04, -1.9688e-04, -1.9226e-04,\n",
      "        -1.9092e-04, -1.8576e-04, -1.8321e-04, -1.8004e-04, -1.7640e-04,\n",
      "        -1.7225e-04, -1.6894e-04, -1.6587e-04, -1.4677e-04, -1.4001e-04,\n",
      "        -1.3833e-04, -1.3478e-04, -1.3057e-04, -1.2718e-04, -1.2087e-04,\n",
      "        -1.1203e-04, -1.0978e-04, -1.0217e-04, -9.6816e-05, -9.3411e-05,\n",
      "        -9.3285e-05, -9.0957e-05, -8.8620e-05, -8.1967e-05, -8.0074e-05,\n",
      "        -7.6076e-05, -6.8798e-05, -5.9203e-05, -5.3542e-05, -4.9117e-05,\n",
      "        -4.6225e-05, -3.7007e-05, -3.2003e-05, -2.9246e-05, -2.6892e-05,\n",
      "        -1.7816e-05, -1.3533e-05, -1.1022e-05, -9.0116e-06, -4.4877e-06,\n",
      "        -3.2732e-06,  1.8971e-07,  4.6670e-06,  9.4552e-06,  1.6610e-05,\n",
      "         2.0684e-05,  2.4652e-05,  2.5929e-05,  3.3542e-05,  3.9474e-05,\n",
      "         4.3617e-05,  4.7980e-05,  4.9988e-05,  5.7990e-05,  6.7829e-05,\n",
      "         7.1116e-05,  7.7014e-05,  8.8324e-05,  9.2102e-05,  1.0118e-04,\n",
      "         1.1387e-04,  1.2546e-04,  1.3979e-04,  1.4705e-04,  1.6508e-04,\n",
      "         1.6665e-04,  1.8015e-04,  1.8175e-04,  1.8838e-04,  1.9368e-04,\n",
      "         1.9626e-04,  2.0724e-04,  2.1468e-04,  2.3353e-04,  2.6383e-04,\n",
      "         2.8426e-04,  2.9132e-04,  2.9947e-04,  3.0700e-04,  3.1433e-04,\n",
      "         3.2463e-04,  3.3978e-04,  3.6591e-04,  4.0523e-04,  4.1567e-04,\n",
      "         4.3675e-04,  4.5460e-04,  4.6844e-04,  4.9450e-04,  5.0653e-04,\n",
      "         5.0853e-04,  5.3924e-04,  5.4790e-04,  5.4936e-04,  5.7244e-04,\n",
      "         5.8575e-04,  6.0085e-04,  6.4054e-04,  6.7307e-04,  7.2752e-04,\n",
      "         7.4007e-04,  7.5426e-04,  7.6691e-04,  8.1877e-04,  8.2654e-04,\n",
      "         8.3988e-04,  9.0541e-04,  9.3504e-04,  9.6264e-04,  9.8072e-04,\n",
      "         1.0165e-03,  1.0219e-03,  1.0574e-03,  1.0731e-03,  1.1053e-03,\n",
      "         1.1224e-03,  1.1496e-03,  1.1931e-03,  1.2050e-03,  1.2620e-03,\n",
      "         1.2756e-03,  1.3524e-03,  1.3924e-03,  1.4607e-03,  1.5093e-03,\n",
      "         1.5516e-03,  1.6469e-03,  1.6746e-03,  1.7116e-03,  1.7385e-03,\n",
      "         1.8582e-03,  1.8996e-03,  2.2571e-03,  2.3324e-03,  3.1493e-03,\n",
      "         3.2048e-03,  3.5121e-03,  3.5896e-03,  4.1729e-03,  1.1883e-02])\n",
      "Task 1 Epoch 8/15 - Loss: 1.0520\n",
      "  Acc (Task 1): 56.58%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.1574, Euclidean: 2.3417\n",
      "Task 1 Epoch 9/15 - Loss: 1.0291\n",
      "  Acc (Task 1): 56.00%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.0475, Euclidean: 2.6647\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 9, step 400, cos_sim = -0.136 \n",
      "bias = 0\n",
      "win = tensor([-3.9390e-02, -1.1708e-02, -4.0231e-03, -6.9799e-04, -3.8119e-04,\n",
      "        -1.8504e-04, -9.7692e-05, -1.8135e-05, -1.6625e-06, -3.2811e-07,\n",
      "        -2.2887e-08, -1.3083e-08, -4.3868e-09, -2.9613e-09, -2.4019e-09,\n",
      "        -1.7794e-09, -1.4830e-09, -9.8643e-10, -7.4006e-10, -6.7222e-10,\n",
      "        -6.2651e-10, -6.1616e-10, -5.7735e-10, -5.5703e-10, -5.4347e-10,\n",
      "        -5.4092e-10, -5.1647e-10, -5.0665e-10, -4.8184e-10, -4.6964e-10,\n",
      "        -4.6116e-10, -4.4039e-10, -4.3397e-10, -4.2559e-10, -4.2113e-10,\n",
      "        -4.1021e-10, -4.0374e-10, -3.9360e-10, -3.9270e-10, -3.7480e-10,\n",
      "        -3.6974e-10, -3.5834e-10, -3.5438e-10, -3.4108e-10, -3.3363e-10,\n",
      "        -3.1568e-10, -3.0913e-10, -2.9991e-10, -2.9773e-10, -2.8940e-10,\n",
      "        -2.8501e-10, -2.7554e-10, -2.7203e-10, -2.6454e-10, -2.5974e-10,\n",
      "        -2.4802e-10, -2.4032e-10, -2.3409e-10, -2.2359e-10, -2.1920e-10,\n",
      "        -2.1736e-10, -2.1086e-10, -2.0362e-10, -1.9806e-10, -1.9353e-10,\n",
      "        -1.9019e-10, -1.8291e-10, -1.7127e-10, -1.6575e-10, -1.6107e-10,\n",
      "        -1.4668e-10, -1.3950e-10, -1.3154e-10, -1.2735e-10, -1.2152e-10,\n",
      "        -1.2076e-10, -1.1268e-10, -1.0596e-10, -1.0333e-10, -9.5163e-11,\n",
      "        -9.2519e-11, -8.6174e-11, -8.3731e-11, -7.5832e-11, -7.1628e-11,\n",
      "        -6.6721e-11, -6.2733e-11, -5.4562e-11, -5.3631e-11, -4.4794e-11,\n",
      "        -3.9383e-11, -3.5394e-11, -2.8710e-11, -2.3997e-11, -1.6467e-11,\n",
      "        -1.5580e-11, -1.0693e-11, -9.3997e-12, -3.9555e-12,  1.4710e-12,\n",
      "         6.0918e-12,  1.2862e-11,  1.7582e-11,  2.3461e-11,  2.6655e-11,\n",
      "         3.5048e-11,  4.0182e-11,  4.6581e-11,  5.2347e-11,  5.2748e-11,\n",
      "         5.6433e-11,  6.4035e-11,  6.6502e-11,  7.6059e-11,  8.3017e-11,\n",
      "         9.3141e-11,  9.4773e-11,  1.0027e-10,  1.0458e-10,  1.1144e-10,\n",
      "         1.1405e-10,  1.1684e-10,  1.2683e-10,  1.2915e-10,  1.3159e-10,\n",
      "         1.3711e-10,  1.4290e-10,  1.5074e-10,  1.5708e-10,  1.6507e-10,\n",
      "         1.7231e-10,  1.8002e-10,  1.8222e-10,  1.8977e-10,  1.9707e-10,\n",
      "         1.9816e-10,  2.0243e-10,  2.0701e-10,  2.2116e-10,  2.2451e-10,\n",
      "         2.3264e-10,  2.4094e-10,  2.4498e-10,  2.5243e-10,  2.5706e-10,\n",
      "         2.5919e-10,  2.6935e-10,  2.8395e-10,  2.8665e-10,  3.0003e-10,\n",
      "         3.0980e-10,  3.1372e-10,  3.2097e-10,  3.2583e-10,  3.3637e-10,\n",
      "         3.4043e-10,  3.4676e-10,  3.5662e-10,  3.6616e-10,  3.7952e-10,\n",
      "         3.8610e-10,  3.9715e-10,  3.9997e-10,  4.0809e-10,  4.1396e-10,\n",
      "         4.2096e-10,  4.4352e-10,  4.5408e-10,  4.6490e-10,  4.7930e-10,\n",
      "         4.8782e-10,  5.0176e-10,  5.1423e-10,  5.3429e-10,  5.4389e-10,\n",
      "         5.7189e-10,  5.8503e-10,  6.1713e-10,  7.0501e-10,  7.1124e-10,\n",
      "         7.4293e-10,  7.4883e-10,  9.5147e-10,  1.2215e-09,  1.3033e-09,\n",
      "         1.5463e-09,  4.4765e-09,  3.8020e-08,  1.2014e-07,  5.2965e-06,\n",
      "         6.7622e-06,  7.5540e-06,  1.5507e-05,  7.6261e-05,  9.0224e-05,\n",
      "         9.6088e-05,  2.1109e-03,  5.9523e-03,  1.0700e-02,  1.4225e-02])\n",
      "wout = tensor([-7.8380e-03, -5.9265e-03, -3.8480e-03, -3.5815e-03, -2.6769e-03,\n",
      "        -2.5276e-03, -2.4527e-03, -2.1334e-03, -1.9281e-03, -1.9196e-03,\n",
      "        -1.9105e-03, -1.8001e-03, -1.7184e-03, -1.6626e-03, -1.5548e-03,\n",
      "        -1.4551e-03, -1.3934e-03, -1.3149e-03, -1.1747e-03, -1.1609e-03,\n",
      "        -1.1321e-03, -1.1102e-03, -1.0901e-03, -1.0496e-03, -1.0418e-03,\n",
      "        -9.8770e-04, -9.5787e-04, -9.3346e-04, -8.5309e-04, -8.4770e-04,\n",
      "        -7.9191e-04, -7.8664e-04, -7.7423e-04, -7.6653e-04, -7.3835e-04,\n",
      "        -7.2765e-04, -7.0917e-04, -6.9588e-04, -6.8257e-04, -6.7966e-04,\n",
      "        -6.4138e-04, -6.2887e-04, -6.0640e-04, -5.9813e-04, -5.8958e-04,\n",
      "        -5.8788e-04, -5.5837e-04, -5.2656e-04, -5.2216e-04, -4.7395e-04,\n",
      "        -4.3175e-04, -4.1313e-04, -3.9811e-04, -3.9376e-04, -3.4875e-04,\n",
      "        -3.3181e-04, -3.1614e-04, -3.1285e-04, -2.9988e-04, -2.8045e-04,\n",
      "        -2.4452e-04, -2.2399e-04, -2.1234e-04, -2.0762e-04, -2.0718e-04,\n",
      "        -1.7495e-04, -1.3683e-04, -1.1802e-04, -1.1381e-04, -9.4607e-05,\n",
      "        -7.3958e-05, -6.3390e-05, -6.0946e-05, -3.7057e-05, -2.5153e-05,\n",
      "        -1.8572e-05,  4.2476e-06,  5.3908e-06,  8.3070e-06,  2.2766e-05,\n",
      "         2.8106e-05,  5.5593e-05,  6.0783e-05,  8.2796e-05,  9.0947e-05,\n",
      "         9.8746e-05,  1.0422e-04,  1.0638e-04,  1.0724e-04,  1.3269e-04,\n",
      "         1.3589e-04,  1.4262e-04,  1.5322e-04,  1.5848e-04,  1.6718e-04,\n",
      "         1.8157e-04,  1.8913e-04,  1.9704e-04,  2.0139e-04,  2.0476e-04,\n",
      "         2.2494e-04,  2.3513e-04,  2.4841e-04,  2.5441e-04,  2.5924e-04,\n",
      "         2.6802e-04,  2.7993e-04,  3.0338e-04,  3.1402e-04,  3.2139e-04,\n",
      "         3.4172e-04,  3.5029e-04,  3.5490e-04,  3.6083e-04,  3.7033e-04,\n",
      "         3.7452e-04,  3.7628e-04,  3.7988e-04,  3.9074e-04,  4.0368e-04,\n",
      "         4.2100e-04,  4.3438e-04,  4.5127e-04,  4.6064e-04,  4.7274e-04,\n",
      "         4.9263e-04,  5.0528e-04,  5.1356e-04,  5.3904e-04,  5.7095e-04,\n",
      "         5.9819e-04,  6.0826e-04,  6.3247e-04,  6.4183e-04,  6.5110e-04,\n",
      "         6.5553e-04,  6.6854e-04,  6.7885e-04,  6.9146e-04,  7.0356e-04,\n",
      "         7.0930e-04,  7.4835e-04,  7.7391e-04,  8.0936e-04,  8.2993e-04,\n",
      "         8.6441e-04,  8.7500e-04,  9.0860e-04,  9.2729e-04,  9.4742e-04,\n",
      "         9.4995e-04,  9.6426e-04,  9.8207e-04,  9.9672e-04,  1.0052e-03,\n",
      "         1.0239e-03,  1.0703e-03,  1.1038e-03,  1.1083e-03,  1.1346e-03,\n",
      "         1.1768e-03,  1.2094e-03,  1.2242e-03,  1.2406e-03,  1.2552e-03,\n",
      "         1.2664e-03,  1.2863e-03,  1.3361e-03,  1.3549e-03,  1.3742e-03,\n",
      "         1.4332e-03,  1.4562e-03,  1.5040e-03,  1.5535e-03,  1.5774e-03,\n",
      "         1.5973e-03,  1.6402e-03,  1.7174e-03,  1.7461e-03,  1.7733e-03,\n",
      "         1.8302e-03,  1.8544e-03,  1.9405e-03,  1.9873e-03,  2.0287e-03,\n",
      "         2.0736e-03,  2.1173e-03,  2.2504e-03,  2.2908e-03,  2.3509e-03,\n",
      "         2.6269e-03,  2.8263e-03,  2.9941e-03,  3.5333e-03,  3.8689e-03,\n",
      "         4.5064e-03,  5.2537e-03,  1.1972e-02,  1.4385e-02,  1.6054e-02])\n",
      "Task 1 Epoch 10/15 - Loss: 1.0138\n",
      "  Acc (Task 1): 56.55%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.1364, Euclidean: 2.8206\n",
      "Task 1 Epoch 11/15 - Loss: 1.0216\n",
      "  Acc (Task 1): 54.95%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.0062, Euclidean: 2.7361\n",
      "Task 1 Epoch 12/15 - Loss: 1.0397\n",
      "  Acc (Task 1): 56.42%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.1951, Euclidean: 2.5849\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 12, step 500, cos_sim = 0.248 \n",
      "bias = 0\n",
      "win = tensor([-1.8294e-02, -8.7189e-03, -4.1951e-03, -3.1640e-03, -2.7652e-03,\n",
      "        -1.8977e-03, -1.4684e-03, -1.2841e-03, -9.4647e-04, -8.8019e-04,\n",
      "        -7.7831e-04, -6.1625e-04, -5.1603e-04, -5.0746e-04, -4.4290e-04,\n",
      "        -4.0296e-04, -3.4177e-04, -3.0977e-04, -3.0005e-04, -2.7491e-04,\n",
      "        -2.7001e-04, -2.6035e-04, -2.4148e-04, -2.2299e-04, -2.1325e-04,\n",
      "        -1.8816e-04, -1.7337e-04, -1.6551e-04, -1.5781e-04, -1.4616e-04,\n",
      "        -1.3924e-04, -1.3270e-04, -1.2393e-04, -1.1221e-04, -1.0884e-04,\n",
      "        -1.0280e-04, -9.4374e-05, -8.8374e-05, -8.3856e-05, -7.1803e-05,\n",
      "        -6.9116e-05, -6.7128e-05, -6.0043e-05, -5.9469e-05, -5.4558e-05,\n",
      "        -5.3193e-05, -5.2735e-05, -4.5273e-05, -4.3431e-05, -4.2471e-05,\n",
      "        -4.0634e-05, -3.8749e-05, -3.8008e-05, -3.7233e-05, -3.5205e-05,\n",
      "        -3.2023e-05, -3.1429e-05, -2.8205e-05, -2.7087e-05, -2.5747e-05,\n",
      "        -2.3673e-05, -2.2694e-05, -2.1353e-05, -2.0325e-05, -1.9778e-05,\n",
      "        -1.8503e-05, -1.7384e-05, -1.5906e-05, -1.5116e-05, -1.3912e-05,\n",
      "        -1.3663e-05, -1.2873e-05, -1.1962e-05, -1.1586e-05, -9.9578e-06,\n",
      "        -9.6919e-06, -9.2680e-06, -8.2914e-06, -7.6315e-06, -7.4404e-06,\n",
      "        -6.9592e-06, -6.7573e-06, -6.0463e-06, -5.5877e-06, -5.3239e-06,\n",
      "        -5.2135e-06, -4.6444e-06, -4.4377e-06, -4.1472e-06, -3.5071e-06,\n",
      "        -3.2770e-06, -3.1225e-06, -2.9975e-06, -2.8036e-06, -2.6752e-06,\n",
      "        -2.5671e-06, -2.2112e-06, -2.0678e-06, -1.7682e-06, -1.6483e-06,\n",
      "        -1.3821e-06, -1.0957e-06, -8.9137e-07, -8.6445e-07, -7.9893e-07,\n",
      "        -6.8985e-07, -4.8215e-07, -4.3792e-07, -3.7059e-07, -1.4612e-07,\n",
      "        -1.3114e-07,  8.4487e-09,  4.7348e-08,  3.1300e-07,  3.6219e-07,\n",
      "         4.1755e-07,  5.0378e-07,  5.8474e-07,  7.3831e-07,  8.5054e-07,\n",
      "         9.1399e-07,  1.0991e-06,  1.2605e-06,  1.3658e-06,  1.5205e-06,\n",
      "         1.6185e-06,  1.8492e-06,  2.1667e-06,  2.3208e-06,  2.4776e-06,\n",
      "         2.8681e-06,  3.0075e-06,  3.4322e-06,  3.7213e-06,  3.8196e-06,\n",
      "         4.4638e-06,  4.9303e-06,  5.0866e-06,  5.2132e-06,  5.3096e-06,\n",
      "         6.0559e-06,  7.0995e-06,  7.5346e-06,  8.6836e-06,  8.8771e-06,\n",
      "         9.1924e-06,  9.6075e-06,  1.0866e-05,  1.1688e-05,  1.2233e-05,\n",
      "         1.2969e-05,  1.5052e-05,  1.5727e-05,  1.6851e-05,  1.8029e-05,\n",
      "         2.0841e-05,  2.3240e-05,  2.3977e-05,  2.4510e-05,  2.6125e-05,\n",
      "         2.8983e-05,  3.1635e-05,  3.4984e-05,  3.6154e-05,  3.9229e-05,\n",
      "         4.1865e-05,  4.6052e-05,  5.4280e-05,  5.5659e-05,  6.0062e-05,\n",
      "         6.9194e-05,  7.2376e-05,  7.6745e-05,  8.5880e-05,  9.6602e-05,\n",
      "         1.0437e-04,  1.0987e-04,  1.2292e-04,  1.3211e-04,  1.5077e-04,\n",
      "         1.6255e-04,  1.6677e-04,  1.9530e-04,  2.2595e-04,  2.7157e-04,\n",
      "         3.1628e-04,  3.3010e-04,  3.7971e-04,  4.5057e-04,  4.9722e-04,\n",
      "         5.6860e-04,  6.3539e-04,  7.5047e-04,  8.8965e-04,  1.0276e-03,\n",
      "         1.0989e-03,  1.3178e-03,  1.8918e-03,  4.0409e-03,  2.4616e-02])\n",
      "wout = tensor([-2.0915e-03, -1.1944e-03, -1.1786e-03, -1.0594e-03, -9.4262e-04,\n",
      "        -8.3789e-04, -7.9178e-04, -7.1907e-04, -6.8671e-04, -5.8699e-04,\n",
      "        -5.7486e-04, -5.6819e-04, -5.4411e-04, -5.1754e-04, -4.6185e-04,\n",
      "        -4.5574e-04, -4.4029e-04, -4.3088e-04, -4.2383e-04, -4.1581e-04,\n",
      "        -3.9985e-04, -3.7982e-04, -3.6182e-04, -3.4977e-04, -3.4520e-04,\n",
      "        -3.4152e-04, -3.3593e-04, -3.3450e-04, -3.3073e-04, -3.2805e-04,\n",
      "        -3.2061e-04, -3.1341e-04, -3.0426e-04, -3.0171e-04, -2.8617e-04,\n",
      "        -2.8298e-04, -2.7579e-04, -2.7372e-04, -2.6678e-04, -2.5816e-04,\n",
      "        -2.5237e-04, -2.4800e-04, -2.4554e-04, -2.3723e-04, -2.3197e-04,\n",
      "        -2.1669e-04, -2.0873e-04, -2.0451e-04, -2.0175e-04, -1.9067e-04,\n",
      "        -1.8885e-04, -1.8785e-04, -1.7866e-04, -1.7386e-04, -1.6733e-04,\n",
      "        -1.6259e-04, -1.6050e-04, -1.5499e-04, -1.5093e-04, -1.4711e-04,\n",
      "        -1.4357e-04, -1.4144e-04, -1.3686e-04, -1.3548e-04, -1.2703e-04,\n",
      "        -1.2163e-04, -1.1928e-04, -1.1652e-04, -1.1092e-04, -1.0676e-04,\n",
      "        -1.0046e-04, -9.9410e-05, -9.5942e-05, -8.9684e-05, -8.5895e-05,\n",
      "        -8.1290e-05, -7.9059e-05, -7.6917e-05, -7.5765e-05, -7.2352e-05,\n",
      "        -6.6695e-05, -6.2335e-05, -5.8362e-05, -5.1172e-05, -5.0029e-05,\n",
      "        -4.8215e-05, -4.6824e-05, -4.5708e-05, -4.2294e-05, -4.1269e-05,\n",
      "        -3.6693e-05, -3.4703e-05, -3.2358e-05, -2.6999e-05, -2.3558e-05,\n",
      "        -2.1945e-05, -1.2349e-05, -1.1733e-05, -7.0836e-06, -1.9973e-06,\n",
      "        -1.0242e-06,  3.2390e-06,  4.9392e-06,  6.6318e-06,  9.1717e-06,\n",
      "         1.0644e-05,  1.3143e-05,  1.4756e-05,  1.9960e-05,  2.2681e-05,\n",
      "         2.5882e-05,  3.4219e-05,  3.8892e-05,  4.5451e-05,  4.8583e-05,\n",
      "         5.0470e-05,  5.4462e-05,  6.3968e-05,  6.5275e-05,  7.0083e-05,\n",
      "         7.2602e-05,  8.1028e-05,  8.1346e-05,  8.9445e-05,  9.3412e-05,\n",
      "         1.0300e-04,  1.0913e-04,  1.1436e-04,  1.1842e-04,  1.2579e-04,\n",
      "         1.2856e-04,  1.3088e-04,  1.3647e-04,  1.3835e-04,  1.4272e-04,\n",
      "         1.4668e-04,  1.5483e-04,  1.5736e-04,  1.6469e-04,  1.6604e-04,\n",
      "         1.6955e-04,  1.8394e-04,  1.9077e-04,  1.9589e-04,  2.0853e-04,\n",
      "         2.1000e-04,  2.1285e-04,  2.2054e-04,  2.2194e-04,  2.2806e-04,\n",
      "         2.4128e-04,  2.4544e-04,  2.5387e-04,  2.5610e-04,  2.6147e-04,\n",
      "         2.7619e-04,  2.8093e-04,  2.8454e-04,  2.8730e-04,  2.9557e-04,\n",
      "         3.1291e-04,  3.1613e-04,  3.2380e-04,  3.3645e-04,  3.4092e-04,\n",
      "         3.4667e-04,  3.5655e-04,  3.6331e-04,  3.7334e-04,  3.7862e-04,\n",
      "         3.9163e-04,  3.9476e-04,  4.1835e-04,  4.3821e-04,  4.5006e-04,\n",
      "         5.1508e-04,  5.4775e-04,  5.7201e-04,  5.8983e-04,  5.9607e-04,\n",
      "         6.1084e-04,  6.4642e-04,  6.8950e-04,  7.2214e-04,  7.2618e-04,\n",
      "         7.5107e-04,  8.3102e-04,  8.4609e-04,  9.1653e-04,  9.3766e-04,\n",
      "         9.8555e-04,  1.0791e-03,  1.1424e-03,  1.1840e-03,  1.1984e-03,\n",
      "         1.4166e-03,  1.5404e-03,  1.6504e-03,  2.6169e-03,  3.0524e-03])\n",
      "Task 1 Epoch 13/15 - Loss: 0.9917\n",
      "  Acc (Task 1): 57.17%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.2166, Euclidean: 2.6132\n",
      "Task 1 Epoch 14/15 - Loss: 0.9991\n",
      "  Acc (Task 1): 57.02%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.2834, Euclidean: 2.5871\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 14, step 600, cos_sim = 0.191 \n",
      "bias = 0\n",
      "win = tensor([-6.7798e-02, -1.0681e-02, -3.1075e-03, -2.4606e-03, -1.4652e-03,\n",
      "        -6.2563e-04, -4.1152e-04, -2.6972e-04, -1.7655e-04, -1.2897e-04,\n",
      "        -1.1462e-04, -6.5108e-05, -3.9539e-05, -9.7528e-06, -5.9538e-06,\n",
      "        -3.8466e-06, -2.7542e-06, -2.5190e-06, -2.0637e-06, -1.3066e-06,\n",
      "        -7.0763e-07, -6.6507e-09, -1.8380e-09, -1.7391e-09, -1.6497e-09,\n",
      "        -1.5531e-09, -1.4213e-09, -1.4009e-09, -1.3970e-09, -1.3115e-09,\n",
      "        -1.2387e-09, -1.2250e-09, -1.2175e-09, -1.2091e-09, -1.1579e-09,\n",
      "        -1.1397e-09, -1.1159e-09, -1.0429e-09, -1.0218e-09, -1.0007e-09,\n",
      "        -9.8587e-10, -9.7751e-10, -9.2644e-10, -9.2145e-10, -8.9320e-10,\n",
      "        -8.8981e-10, -8.5070e-10, -8.3827e-10, -8.2179e-10, -8.0056e-10,\n",
      "        -7.8261e-10, -7.6538e-10, -7.5835e-10, -7.3455e-10, -7.0645e-10,\n",
      "        -6.9379e-10, -6.8224e-10, -6.7455e-10, -6.6653e-10, -6.4855e-10,\n",
      "        -6.1829e-10, -6.0815e-10, -5.8958e-10, -5.7433e-10, -5.5905e-10,\n",
      "        -5.4392e-10, -5.1252e-10, -5.0521e-10, -4.9633e-10, -4.6961e-10,\n",
      "        -4.6461e-10, -4.3228e-10, -4.1328e-10, -3.9984e-10, -3.9438e-10,\n",
      "        -3.8398e-10, -3.7484e-10, -3.6288e-10, -3.5189e-10, -3.4022e-10,\n",
      "        -3.1779e-10, -2.8474e-10, -2.8016e-10, -2.7560e-10, -2.6321e-10,\n",
      "        -2.5070e-10, -2.4295e-10, -2.2069e-10, -2.0546e-10, -1.9611e-10,\n",
      "        -1.7702e-10, -1.7623e-10, -1.6628e-10, -1.5399e-10, -1.4744e-10,\n",
      "        -1.2877e-10, -1.0494e-10, -9.9695e-11, -8.9686e-11, -7.9216e-11,\n",
      "        -6.6504e-11, -5.7082e-11, -2.2826e-11, -7.3233e-12, -1.3810e-12,\n",
      "         1.3353e-11,  2.5495e-11,  5.6247e-11,  6.6014e-11,  8.0121e-11,\n",
      "         9.1732e-11,  9.7424e-11,  1.1091e-10,  1.1364e-10,  1.2604e-10,\n",
      "         1.4101e-10,  1.5916e-10,  1.6818e-10,  1.7232e-10,  1.8718e-10,\n",
      "         1.9225e-10,  2.0540e-10,  2.3314e-10,  2.3861e-10,  2.5423e-10,\n",
      "         2.6744e-10,  2.8293e-10,  3.0368e-10,  3.1548e-10,  3.1725e-10,\n",
      "         3.3047e-10,  3.4967e-10,  3.7133e-10,  3.8047e-10,  3.9961e-10,\n",
      "         4.0844e-10,  4.2445e-10,  4.3769e-10,  4.4556e-10,  4.6619e-10,\n",
      "         4.6906e-10,  4.7646e-10,  5.0894e-10,  5.2406e-10,  5.3114e-10,\n",
      "         5.5639e-10,  5.6873e-10,  5.7309e-10,  5.9123e-10,  6.0405e-10,\n",
      "         6.2671e-10,  6.3799e-10,  6.4796e-10,  6.8618e-10,  6.9396e-10,\n",
      "         7.2476e-10,  7.3916e-10,  7.6274e-10,  7.7717e-10,  8.0054e-10,\n",
      "         8.0562e-10,  8.2477e-10,  8.5014e-10,  8.5868e-10,  8.9170e-10,\n",
      "         9.0257e-10,  9.2590e-10,  9.4282e-10,  9.6120e-10,  9.7896e-10,\n",
      "         9.9610e-10,  1.0307e-09,  1.0807e-09,  1.0911e-09,  1.1285e-09,\n",
      "         1.1548e-09,  1.1787e-09,  1.1981e-09,  1.2166e-09,  1.2618e-09,\n",
      "         1.3075e-09,  1.3469e-09,  1.3760e-09,  1.4236e-09,  1.6283e-09,\n",
      "         1.8998e-09,  2.0330e-09,  2.5206e-09,  8.1214e-09,  2.3228e-08,\n",
      "         2.1689e-07,  1.7525e-06,  1.9994e-05,  1.7567e-04,  7.6160e-04,\n",
      "         1.1415e-03,  2.0733e-03,  7.9031e-03,  1.1611e-02,  6.1416e-02])\n",
      "wout = tensor([-7.6152e-03, -5.3054e-03, -5.1054e-03, -5.0125e-03, -4.7287e-03,\n",
      "        -4.6598e-03, -4.1178e-03, -3.3372e-03, -3.0334e-03, -2.9007e-03,\n",
      "        -2.7956e-03, -2.7390e-03, -2.7191e-03, -2.6960e-03, -2.6242e-03,\n",
      "        -2.5320e-03, -2.3546e-03, -2.2903e-03, -2.2249e-03, -2.0618e-03,\n",
      "        -2.0154e-03, -1.9855e-03, -1.9627e-03, -1.8912e-03, -1.8250e-03,\n",
      "        -1.7851e-03, -1.7154e-03, -1.6706e-03, -1.5428e-03, -1.4831e-03,\n",
      "        -1.4423e-03, -1.3834e-03, -1.3352e-03, -1.2694e-03, -1.2272e-03,\n",
      "        -1.1823e-03, -1.1035e-03, -1.0533e-03, -1.0198e-03, -9.5769e-04,\n",
      "        -9.3751e-04, -9.1318e-04, -8.9425e-04, -8.8577e-04, -8.1637e-04,\n",
      "        -8.0276e-04, -7.9219e-04, -7.4569e-04, -7.3337e-04, -7.1062e-04,\n",
      "        -6.8695e-04, -6.7500e-04, -6.5996e-04, -6.4817e-04, -6.2658e-04,\n",
      "        -6.0890e-04, -5.8560e-04, -5.7113e-04, -5.6437e-04, -5.5642e-04,\n",
      "        -5.3937e-04, -4.7443e-04, -4.5196e-04, -4.4018e-04, -4.2531e-04,\n",
      "        -4.0198e-04, -3.7931e-04, -3.7224e-04, -3.2127e-04, -3.0854e-04,\n",
      "        -3.0312e-04, -2.8873e-04, -2.5947e-04, -2.5336e-04, -2.2500e-04,\n",
      "        -1.9803e-04, -1.5963e-04, -1.5027e-04, -1.4738e-04, -1.1871e-04,\n",
      "        -1.1464e-04, -1.0156e-04, -8.8764e-05, -8.1932e-05, -7.3859e-05,\n",
      "        -7.0124e-05, -6.1742e-05, -5.5260e-05, -4.4904e-05, -3.6931e-05,\n",
      "        -1.9662e-05, -1.1268e-05, -9.7141e-06, -2.1760e-06,  1.6683e-06,\n",
      "         3.9208e-06,  2.0463e-05,  2.7049e-05,  4.6234e-05,  7.2030e-05,\n",
      "         7.5899e-05,  8.1477e-05,  1.0666e-04,  1.1050e-04,  1.2111e-04,\n",
      "         1.4203e-04,  1.5334e-04,  1.6604e-04,  1.7654e-04,  1.8424e-04,\n",
      "         1.8730e-04,  1.9751e-04,  2.1478e-04,  2.2686e-04,  2.4038e-04,\n",
      "         2.4224e-04,  2.4506e-04,  2.6217e-04,  2.8050e-04,  3.0851e-04,\n",
      "         3.3729e-04,  3.4358e-04,  3.4739e-04,  3.5235e-04,  3.6162e-04,\n",
      "         3.7728e-04,  3.8584e-04,  3.9841e-04,  4.2321e-04,  4.3795e-04,\n",
      "         4.4429e-04,  4.7173e-04,  4.8251e-04,  4.8617e-04,  4.9480e-04,\n",
      "         5.1160e-04,  5.4205e-04,  5.5612e-04,  5.6284e-04,  6.0209e-04,\n",
      "         6.0863e-04,  6.1887e-04,  6.2942e-04,  6.5796e-04,  6.6655e-04,\n",
      "         7.3326e-04,  7.7735e-04,  8.2002e-04,  8.3625e-04,  8.4655e-04,\n",
      "         8.8241e-04,  8.9814e-04,  9.1592e-04,  1.0117e-03,  1.0307e-03,\n",
      "         1.0420e-03,  1.0520e-03,  1.0978e-03,  1.1341e-03,  1.1652e-03,\n",
      "         1.2311e-03,  1.2573e-03,  1.3018e-03,  1.3211e-03,  1.3651e-03,\n",
      "         1.4530e-03,  1.4617e-03,  1.5063e-03,  1.5284e-03,  1.5901e-03,\n",
      "         1.6684e-03,  1.7206e-03,  1.7845e-03,  1.8195e-03,  1.8732e-03,\n",
      "         1.8865e-03,  2.0579e-03,  2.1144e-03,  2.1571e-03,  2.1904e-03,\n",
      "         2.2274e-03,  2.3923e-03,  2.5657e-03,  2.6394e-03,  2.6940e-03,\n",
      "         2.7347e-03,  2.7754e-03,  2.8673e-03,  2.9380e-03,  3.0432e-03,\n",
      "         3.1110e-03,  3.2407e-03,  3.9110e-03,  3.9759e-03,  4.1493e-03,\n",
      "         4.4118e-03,  4.7192e-03,  5.1908e-03,  7.0439e-03,  1.1136e-02])\n",
      "Task 1 Epoch 15/15 - Loss: 0.9991\n",
      "  Acc (Task 1): 56.23%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.1911, Euclidean: 2.9222\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9910\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9915\n",
      "Before coupling: cosine=0.1911, distance=2.9222\n",
      "After coupling: cosine=0.9910, distance=0.2196\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.1071\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 28.73%\n",
      "  Unit similarity - Cosine: 0.6625, Euclidean: 1.8543\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.526 \n",
      "bias = 0\n",
      "win = tensor([-1.8396e-02, -8.3938e-03, -3.1621e-03, -2.8001e-03, -1.9118e-03,\n",
      "        -1.3917e-03, -1.2881e-03, -9.8669e-04, -8.9119e-04, -6.4104e-04,\n",
      "        -6.0451e-04, -4.8943e-04, -4.1401e-04, -3.4177e-04, -2.5251e-04,\n",
      "        -2.4118e-04, -2.1008e-04, -1.9283e-04, -1.8101e-04, -1.7323e-04,\n",
      "        -1.5715e-04, -1.4227e-04, -1.1746e-04, -1.1135e-04, -1.0096e-04,\n",
      "        -9.2163e-05, -8.0560e-05, -6.9199e-05, -6.3244e-05, -5.3844e-05,\n",
      "        -4.8654e-05, -4.7321e-05, -4.4098e-05, -3.9908e-05, -3.7069e-05,\n",
      "        -3.5680e-05, -3.4676e-05, -3.0734e-05, -2.9474e-05, -2.8200e-05,\n",
      "        -2.5387e-05, -2.1294e-05, -1.9477e-05, -1.6794e-05, -1.5003e-05,\n",
      "        -1.4149e-05, -1.3601e-05, -1.2817e-05, -1.2065e-05, -1.1533e-05,\n",
      "        -1.0239e-05, -9.5940e-06, -8.5351e-06, -7.9391e-06, -7.3057e-06,\n",
      "        -6.4630e-06, -5.7013e-06, -5.2817e-06, -4.8039e-06, -3.9378e-06,\n",
      "        -3.5908e-06, -3.3745e-06, -2.9666e-06, -2.7882e-06, -2.5763e-06,\n",
      "        -2.4093e-06, -1.9837e-06, -1.9506e-06, -1.6551e-06, -1.5008e-06,\n",
      "        -1.3756e-06, -1.2776e-06, -1.1690e-06, -9.6492e-07, -9.3351e-07,\n",
      "        -8.5801e-07, -8.3023e-07, -6.8604e-07, -5.8244e-07, -5.6275e-07,\n",
      "        -5.1484e-07, -4.4924e-07, -4.2300e-07, -3.5988e-07, -3.1436e-07,\n",
      "        -2.9705e-07, -2.8083e-07, -2.1817e-07, -1.8726e-07, -1.6576e-07,\n",
      "        -1.4496e-07, -1.2870e-07, -1.1849e-07, -9.7543e-08, -8.7239e-08,\n",
      "        -7.5553e-08, -6.4189e-08, -5.5263e-08, -4.8674e-08, -4.0448e-08,\n",
      "        -2.7284e-08, -1.9327e-08, -1.4575e-08, -1.2727e-08, -1.1844e-08,\n",
      "        -1.3438e-09,  4.0173e-09,  1.2966e-08,  1.7635e-08,  2.3448e-08,\n",
      "         3.1293e-08,  7.3397e-08,  9.5321e-08,  1.0140e-07,  1.4963e-07,\n",
      "         1.5981e-07,  2.2231e-07,  2.9581e-07,  3.1779e-07,  4.7504e-07,\n",
      "         5.5590e-07,  6.3861e-07,  6.9976e-07,  9.1368e-07,  9.4215e-07,\n",
      "         1.1136e-06,  1.3743e-06,  1.4263e-06,  1.7868e-06,  1.8789e-06,\n",
      "         2.0767e-06,  2.3551e-06,  2.6634e-06,  2.7783e-06,  3.5799e-06,\n",
      "         3.9106e-06,  4.6700e-06,  4.7969e-06,  5.2380e-06,  6.1975e-06,\n",
      "         6.9022e-06,  7.8884e-06,  8.7691e-06,  1.0866e-05,  1.1513e-05,\n",
      "         1.3186e-05,  1.4965e-05,  1.6231e-05,  1.8253e-05,  1.8624e-05,\n",
      "         2.1320e-05,  2.2977e-05,  2.3853e-05,  2.5804e-05,  3.0088e-05,\n",
      "         3.3425e-05,  3.8492e-05,  4.0420e-05,  4.1762e-05,  4.4873e-05,\n",
      "         5.1456e-05,  5.4257e-05,  5.7414e-05,  6.3857e-05,  6.5741e-05,\n",
      "         7.7887e-05,  8.6721e-05,  9.7442e-05,  1.0940e-04,  1.2177e-04,\n",
      "         1.2461e-04,  1.2921e-04,  1.3605e-04,  1.6373e-04,  1.6488e-04,\n",
      "         1.8909e-04,  2.1577e-04,  2.4065e-04,  2.5876e-04,  2.7418e-04,\n",
      "         3.0902e-04,  3.5388e-04,  3.7594e-04,  4.0206e-04,  4.3881e-04,\n",
      "         4.9899e-04,  5.2089e-04,  5.6157e-04,  6.7916e-04,  8.2169e-04,\n",
      "         8.7490e-04,  9.1211e-04,  1.2406e-03,  1.4376e-03,  1.6560e-03,\n",
      "         1.8134e-03,  2.9440e-03,  4.4604e-03,  1.4576e-02,  3.5502e-02])\n",
      "wout = tensor([-1.8178e-03, -1.6979e-03, -1.5976e-03, -1.4748e-03, -1.4209e-03,\n",
      "        -1.3018e-03, -1.2028e-03, -1.1357e-03, -1.1058e-03, -1.0187e-03,\n",
      "        -9.4847e-04, -9.3293e-04, -9.0800e-04, -8.6911e-04, -8.4627e-04,\n",
      "        -8.2590e-04, -7.8993e-04, -7.4006e-04, -7.1763e-04, -6.8865e-04,\n",
      "        -6.3499e-04, -6.3050e-04, -6.0501e-04, -5.8352e-04, -5.6887e-04,\n",
      "        -5.5298e-04, -5.3595e-04, -5.3372e-04, -5.2206e-04, -5.1225e-04,\n",
      "        -5.0610e-04, -4.7388e-04, -4.6699e-04, -4.5915e-04, -4.5583e-04,\n",
      "        -4.4084e-04, -4.2861e-04, -4.2379e-04, -4.0768e-04, -3.8836e-04,\n",
      "        -3.6918e-04, -3.5500e-04, -3.4867e-04, -3.4216e-04, -3.3486e-04,\n",
      "        -3.1342e-04, -3.0543e-04, -3.0286e-04, -2.9039e-04, -2.8828e-04,\n",
      "        -2.7908e-04, -2.7305e-04, -2.6532e-04, -2.5461e-04, -2.4832e-04,\n",
      "        -2.4376e-04, -2.2909e-04, -2.1944e-04, -2.1306e-04, -2.0761e-04,\n",
      "        -2.0355e-04, -1.9103e-04, -1.8107e-04, -1.6894e-04, -1.6605e-04,\n",
      "        -1.5286e-04, -1.4304e-04, -1.3604e-04, -1.3390e-04, -1.2475e-04,\n",
      "        -1.1201e-04, -1.0709e-04, -9.8116e-05, -8.7627e-05, -8.5001e-05,\n",
      "        -7.4801e-05, -7.0025e-05, -6.7618e-05, -6.5862e-05, -5.5347e-05,\n",
      "        -4.7016e-05, -3.8570e-05, -3.6858e-05, -2.8766e-05, -2.3764e-05,\n",
      "        -9.5669e-06, -5.5680e-06, -2.0095e-06,  5.7927e-07,  5.5628e-06,\n",
      "         9.8067e-06,  1.3894e-05,  2.4453e-05,  3.2011e-05,  3.8417e-05,\n",
      "         4.2797e-05,  5.1580e-05,  5.2725e-05,  5.3943e-05,  5.7626e-05,\n",
      "         6.5510e-05,  7.2497e-05,  8.1579e-05,  8.6622e-05,  8.9310e-05,\n",
      "         9.3803e-05,  9.7734e-05,  1.0057e-04,  1.1017e-04,  1.1564e-04,\n",
      "         1.2170e-04,  1.3173e-04,  1.3609e-04,  1.5490e-04,  1.5557e-04,\n",
      "         1.6034e-04,  1.7060e-04,  1.7353e-04,  1.7943e-04,  1.8231e-04,\n",
      "         1.9079e-04,  1.9593e-04,  2.0418e-04,  2.2068e-04,  2.2426e-04,\n",
      "         2.3365e-04,  2.4413e-04,  2.5150e-04,  2.6122e-04,  2.8088e-04,\n",
      "         2.9238e-04,  2.9896e-04,  3.0715e-04,  3.1407e-04,  3.3322e-04,\n",
      "         3.4000e-04,  3.4209e-04,  3.6181e-04,  3.7034e-04,  3.9197e-04,\n",
      "         4.0326e-04,  4.1628e-04,  4.3626e-04,  4.5079e-04,  4.5319e-04,\n",
      "         4.5947e-04,  4.6508e-04,  4.7082e-04,  4.8084e-04,  4.9444e-04,\n",
      "         5.0710e-04,  5.2346e-04,  5.3691e-04,  5.4188e-04,  5.5669e-04,\n",
      "         5.7031e-04,  5.7049e-04,  5.8784e-04,  6.0167e-04,  6.1924e-04,\n",
      "         6.5152e-04,  6.5278e-04,  6.7170e-04,  6.9242e-04,  7.1176e-04,\n",
      "         7.1342e-04,  7.2783e-04,  7.4857e-04,  7.6253e-04,  7.7943e-04,\n",
      "         7.9297e-04,  8.0374e-04,  8.3696e-04,  8.7542e-04,  8.9643e-04,\n",
      "         9.3711e-04,  9.3995e-04,  9.5522e-04,  9.6510e-04,  1.0278e-03,\n",
      "         1.0820e-03,  1.0926e-03,  1.1111e-03,  1.1554e-03,  1.1873e-03,\n",
      "         1.2263e-03,  1.2438e-03,  1.2926e-03,  1.3193e-03,  1.3299e-03,\n",
      "         1.4557e-03,  1.5340e-03,  1.5956e-03,  1.6952e-03,  1.7467e-03,\n",
      "         1.9156e-03,  2.0218e-03,  3.5813e-03,  4.2103e-03,  5.0243e-03])\n",
      "Task 2 Epoch 2/5 - Loss: 1.7157\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 30.32%\n",
      "  Unit similarity - Cosine: 0.4996, Euclidean: 2.4924\n",
      "Task 2 Epoch 3/5 - Loss: 1.6674\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 32.55%\n",
      "  Unit similarity - Cosine: 0.4005, Euclidean: 2.8622\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.472 \n",
      "bias = 0\n",
      "win = tensor([-1.9890e-02, -7.8810e-03, -5.4161e-03, -3.0887e-03, -2.6356e-03,\n",
      "        -2.1151e-03, -1.5854e-03, -1.4807e-03, -1.3449e-03, -1.0285e-03,\n",
      "        -9.3625e-04, -8.1406e-04, -6.4211e-04, -5.9735e-04, -5.3901e-04,\n",
      "        -5.1627e-04, -4.6987e-04, -4.5066e-04, -3.9829e-04, -3.8002e-04,\n",
      "        -3.2940e-04, -3.1954e-04, -2.8665e-04, -2.6622e-04, -2.4313e-04,\n",
      "        -2.4041e-04, -2.0739e-04, -1.9205e-04, -1.7827e-04, -1.6557e-04,\n",
      "        -1.5665e-04, -1.4617e-04, -1.3406e-04, -1.2426e-04, -1.2167e-04,\n",
      "        -1.1832e-04, -1.0778e-04, -9.6489e-05, -8.9579e-05, -8.5648e-05,\n",
      "        -8.0730e-05, -7.6080e-05, -7.3222e-05, -6.9867e-05, -6.3515e-05,\n",
      "        -5.5924e-05, -5.0215e-05, -5.0036e-05, -4.8750e-05, -4.6013e-05,\n",
      "        -4.0800e-05, -3.6847e-05, -3.3927e-05, -3.1705e-05, -3.0007e-05,\n",
      "        -2.8028e-05, -2.5674e-05, -2.4224e-05, -2.2966e-05, -1.9081e-05,\n",
      "        -1.8238e-05, -1.6453e-05, -1.4639e-05, -1.4049e-05, -1.2248e-05,\n",
      "        -1.1488e-05, -1.1212e-05, -1.0543e-05, -9.9432e-06, -9.0068e-06,\n",
      "        -8.2411e-06, -6.2071e-06, -5.7576e-06, -5.3656e-06, -4.9272e-06,\n",
      "        -4.3787e-06, -4.0366e-06, -3.7468e-06, -3.1882e-06, -3.0228e-06,\n",
      "        -2.5788e-06, -2.0312e-06, -1.9889e-06, -1.8768e-06, -1.7792e-06,\n",
      "        -1.1214e-06, -8.1213e-07, -6.7462e-07, -5.3227e-07, -4.8285e-07,\n",
      "        -2.3195e-07, -1.7640e-07,  5.1085e-08,  1.9123e-07,  2.3312e-07,\n",
      "         3.3151e-07,  4.5365e-07,  4.8689e-07,  6.2167e-07,  7.3160e-07,\n",
      "         1.0223e-06,  1.0414e-06,  1.2041e-06,  1.2495e-06,  1.3894e-06,\n",
      "         1.5136e-06,  1.8525e-06,  2.1874e-06,  2.4921e-06,  2.7240e-06,\n",
      "         3.0620e-06,  3.2904e-06,  3.5186e-06,  4.4036e-06,  4.7274e-06,\n",
      "         5.6496e-06,  6.0759e-06,  6.7397e-06,  7.2252e-06,  7.6435e-06,\n",
      "         8.1537e-06,  9.0202e-06,  1.0523e-05,  1.1639e-05,  1.2807e-05,\n",
      "         1.3051e-05,  1.4457e-05,  1.5292e-05,  1.5928e-05,  1.6760e-05,\n",
      "         1.8507e-05,  2.0611e-05,  2.0945e-05,  2.2403e-05,  2.4687e-05,\n",
      "         2.6516e-05,  3.2120e-05,  3.3865e-05,  3.5171e-05,  3.6763e-05,\n",
      "         3.8774e-05,  4.2769e-05,  4.4791e-05,  4.8176e-05,  5.0055e-05,\n",
      "         5.3422e-05,  5.8061e-05,  6.2656e-05,  6.5428e-05,  6.8793e-05,\n",
      "         7.4050e-05,  7.9050e-05,  8.4182e-05,  9.0707e-05,  9.6948e-05,\n",
      "         1.0542e-04,  1.0889e-04,  1.2023e-04,  1.2260e-04,  1.2667e-04,\n",
      "         1.3895e-04,  1.5734e-04,  1.6489e-04,  1.7357e-04,  1.9836e-04,\n",
      "         2.0492e-04,  2.1717e-04,  2.3326e-04,  2.4892e-04,  2.9189e-04,\n",
      "         3.1163e-04,  3.3448e-04,  3.5176e-04,  3.7111e-04,  4.0367e-04,\n",
      "         4.3278e-04,  4.5543e-04,  5.1774e-04,  5.4629e-04,  6.4422e-04,\n",
      "         6.5024e-04,  6.7841e-04,  7.7344e-04,  8.4734e-04,  9.5115e-04,\n",
      "         1.0173e-03,  1.0854e-03,  1.2144e-03,  1.3104e-03,  1.8129e-03,\n",
      "         2.1990e-03,  2.3745e-03,  3.2538e-03,  3.7402e-03,  4.5818e-03,\n",
      "         4.8452e-03,  5.1672e-03,  9.9453e-03,  2.5215e-02,  7.5740e-02])\n",
      "wout = tensor([-2.8505e-03, -2.6151e-03, -1.7515e-03, -1.7073e-03, -1.5286e-03,\n",
      "        -1.4269e-03, -1.3706e-03, -1.2895e-03, -1.2450e-03, -1.1577e-03,\n",
      "        -1.1111e-03, -1.0572e-03, -9.9210e-04, -9.4370e-04, -8.8864e-04,\n",
      "        -8.5943e-04, -8.3151e-04, -8.0530e-04, -7.8138e-04, -7.6953e-04,\n",
      "        -7.5128e-04, -7.1060e-04, -6.8495e-04, -6.7495e-04, -6.6721e-04,\n",
      "        -6.2864e-04, -6.1618e-04, -6.1289e-04, -5.9992e-04, -5.8579e-04,\n",
      "        -5.6308e-04, -5.5728e-04, -5.3829e-04, -5.0755e-04, -4.9017e-04,\n",
      "        -4.5996e-04, -4.4133e-04, -4.3599e-04, -4.2389e-04, -4.1583e-04,\n",
      "        -4.0752e-04, -3.5824e-04, -3.5010e-04, -3.3775e-04, -3.1780e-04,\n",
      "        -3.1400e-04, -3.0137e-04, -2.9681e-04, -2.9136e-04, -2.7629e-04,\n",
      "        -2.6487e-04, -2.5027e-04, -2.4525e-04, -2.3765e-04, -2.3204e-04,\n",
      "        -2.2666e-04, -2.2236e-04, -2.1562e-04, -2.0632e-04, -2.0149e-04,\n",
      "        -1.9485e-04, -1.8754e-04, -1.7880e-04, -1.7738e-04, -1.6864e-04,\n",
      "        -1.5234e-04, -1.5066e-04, -1.4202e-04, -1.3305e-04, -1.2324e-04,\n",
      "        -1.0275e-04, -1.0054e-04, -9.1753e-05, -8.3689e-05, -8.0666e-05,\n",
      "        -7.1451e-05, -6.9849e-05, -6.7744e-05, -6.0874e-05, -5.1885e-05,\n",
      "        -4.7075e-05, -3.6764e-05, -3.6185e-05, -2.8254e-05, -2.2262e-05,\n",
      "        -1.4937e-05, -5.0392e-06, -2.1412e-06,  6.5932e-06,  1.1159e-05,\n",
      "         1.6666e-05,  1.8728e-05,  3.4170e-05,  3.6806e-05,  4.4900e-05,\n",
      "         5.3313e-05,  5.8495e-05,  6.4379e-05,  7.3841e-05,  8.1177e-05,\n",
      "         8.2437e-05,  8.8081e-05,  9.6649e-05,  1.0018e-04,  1.0978e-04,\n",
      "         1.1190e-04,  1.1491e-04,  1.2749e-04,  1.2826e-04,  1.4088e-04,\n",
      "         1.4607e-04,  1.5338e-04,  1.5871e-04,  1.6297e-04,  1.7260e-04,\n",
      "         1.7859e-04,  1.8735e-04,  1.9251e-04,  2.0441e-04,  2.1022e-04,\n",
      "         2.1599e-04,  2.2039e-04,  2.2601e-04,  2.3128e-04,  2.4110e-04,\n",
      "         2.4650e-04,  2.5523e-04,  2.6438e-04,  2.7666e-04,  2.7850e-04,\n",
      "         2.8354e-04,  2.8950e-04,  2.9811e-04,  3.0783e-04,  3.1553e-04,\n",
      "         3.3225e-04,  3.3430e-04,  3.4335e-04,  3.5165e-04,  3.6203e-04,\n",
      "         3.7251e-04,  3.7699e-04,  3.8575e-04,  3.9365e-04,  4.0607e-04,\n",
      "         4.0935e-04,  4.1573e-04,  4.2675e-04,  4.4692e-04,  4.5137e-04,\n",
      "         4.5713e-04,  4.6535e-04,  4.6915e-04,  4.7935e-04,  5.0507e-04,\n",
      "         5.0975e-04,  5.2708e-04,  5.3531e-04,  5.4852e-04,  5.5885e-04,\n",
      "         5.6698e-04,  5.9636e-04,  6.1017e-04,  6.1441e-04,  6.2073e-04,\n",
      "         6.4552e-04,  6.5005e-04,  6.7345e-04,  6.8535e-04,  7.0066e-04,\n",
      "         7.1765e-04,  7.2575e-04,  7.4444e-04,  7.5723e-04,  7.6728e-04,\n",
      "         8.1080e-04,  8.2995e-04,  8.4905e-04,  8.9780e-04,  9.2819e-04,\n",
      "         9.5644e-04,  9.6515e-04,  9.9613e-04,  1.0472e-03,  1.0848e-03,\n",
      "         1.1133e-03,  1.1350e-03,  1.1658e-03,  1.2452e-03,  1.2825e-03,\n",
      "         1.3121e-03,  1.4059e-03,  1.4372e-03,  1.4784e-03,  1.5755e-03,\n",
      "         1.7639e-03,  1.9910e-03,  2.2099e-03,  2.4178e-03,  2.8263e-03])\n",
      "Task 2 Epoch 4/5 - Loss: 1.6569\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 31.07%\n",
      "  Unit similarity - Cosine: 0.4839, Euclidean: 2.8481\n",
      "Task 2 Epoch 5/5 - Loss: 1.6333\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 32.92%\n",
      "  Unit similarity - Cosine: 0.5049, Euclidean: 2.9783\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9910 (after coupling) -> 0.5049 (final)\n",
      "Euclidean distance: 0.2196 (after coupling) -> 2.9783 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 0.00% (before Task 2) -> 0.00% (after Task 2)\n",
      "No forgetting observed. Task 1 performance improved by -0.00%.\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 15, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0007, Euclidean distance: 0.8364\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/15 - Loss: 1.6809\n",
      "  Acc (Task 1): 47.45%, Acc (Task 2): 0.02%\n",
      "  Unit similarity - Cosine: -0.4828, Euclidean: 1.5965\n",
      "Task 1 Epoch 2/15 - Loss: 1.1503\n",
      "  Acc (Task 1): 48.85%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5282, Euclidean: 1.6755\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 2, step 100, cos_sim = -0.560 \n",
      "bias = 0\n",
      "win = tensor([-8.7297e-04, -4.9855e-04, -3.8229e-04, -1.4189e-04, -1.2716e-04,\n",
      "        -9.2920e-05, -7.0057e-05, -6.4312e-05, -4.7376e-05, -3.7987e-05,\n",
      "        -2.8970e-05, -2.3868e-05, -2.1397e-05, -1.9318e-05, -1.7210e-05,\n",
      "        -1.4800e-05, -1.4207e-05, -1.1004e-05, -1.0644e-05, -1.0276e-05,\n",
      "        -8.4850e-06, -8.2351e-06, -6.7854e-06, -6.0139e-06, -5.5053e-06,\n",
      "        -4.1470e-06, -3.4585e-06, -3.3391e-06, -2.8231e-06, -2.3127e-06,\n",
      "        -2.1176e-06, -1.8311e-06, -1.6454e-06, -1.5052e-06, -1.2698e-06,\n",
      "        -1.1566e-06, -1.0403e-06, -8.9476e-07, -7.6877e-07, -7.0659e-07,\n",
      "        -6.0322e-07, -5.6022e-07, -5.0202e-07, -4.4665e-07, -3.7638e-07,\n",
      "        -3.3859e-07, -3.0488e-07, -2.3637e-07, -2.2413e-07, -1.8983e-07,\n",
      "        -1.6936e-07, -1.4264e-07, -1.1169e-07, -9.9856e-08, -8.7502e-08,\n",
      "        -7.2491e-08, -5.6264e-08, -4.9638e-08, -4.6965e-08, -4.0095e-08,\n",
      "        -3.8095e-08, -3.1351e-08, -2.7355e-08, -2.2998e-08, -1.9138e-08,\n",
      "        -1.3759e-08, -1.2260e-08, -7.8728e-09, -7.6503e-09, -5.3858e-09,\n",
      "        -4.1310e-09, -3.6025e-09, -3.0858e-09, -1.9350e-09, -1.4127e-09,\n",
      "        -1.1415e-09, -5.8554e-10, -5.1994e-10, -2.8164e-10, -4.1849e-11,\n",
      "         2.0342e-10,  4.5172e-10,  5.2227e-10,  9.0319e-10,  1.0997e-09,\n",
      "         1.5278e-09,  2.1193e-09,  2.5201e-09,  3.4622e-09,  3.8672e-09,\n",
      "         4.2405e-09,  5.1821e-09,  6.3713e-09,  7.1730e-09,  8.6590e-09,\n",
      "         1.1501e-08,  1.2219e-08,  1.5077e-08,  1.6812e-08,  2.0192e-08,\n",
      "         2.2975e-08,  2.4047e-08,  2.7511e-08,  3.0031e-08,  3.3847e-08,\n",
      "         3.6955e-08,  3.9775e-08,  4.4144e-08,  4.5001e-08,  5.1648e-08,\n",
      "         6.0141e-08,  6.8513e-08,  7.3314e-08,  8.6544e-08,  9.6447e-08,\n",
      "         1.0658e-07,  1.0991e-07,  1.1874e-07,  1.2306e-07,  1.3597e-07,\n",
      "         1.5559e-07,  1.7143e-07,  1.8052e-07,  1.9337e-07,  2.2473e-07,\n",
      "         2.4001e-07,  2.5138e-07,  3.0707e-07,  3.1871e-07,  3.4791e-07,\n",
      "         3.7477e-07,  4.0100e-07,  4.2383e-07,  4.6826e-07,  4.9391e-07,\n",
      "         4.9726e-07,  5.2896e-07,  5.8393e-07,  6.2605e-07,  6.8400e-07,\n",
      "         7.0120e-07,  8.3665e-07,  8.6547e-07,  9.6114e-07,  1.0992e-06,\n",
      "         1.1151e-06,  1.2662e-06,  1.3998e-06,  1.5236e-06,  1.7579e-06,\n",
      "         1.8559e-06,  1.9374e-06,  2.0076e-06,  2.1408e-06,  2.2945e-06,\n",
      "         2.5129e-06,  2.8441e-06,  3.0606e-06,  3.3575e-06,  3.8871e-06,\n",
      "         4.0798e-06,  4.7819e-06,  5.7252e-06,  6.5239e-06,  6.5995e-06,\n",
      "         7.0970e-06,  8.6283e-06,  9.0283e-06,  9.6330e-06,  1.0478e-05,\n",
      "         1.2024e-05,  1.3285e-05,  1.3481e-05,  1.4934e-05,  1.7980e-05,\n",
      "         2.1056e-05,  2.2693e-05,  2.6147e-05,  2.9711e-05,  3.4650e-05,\n",
      "         4.2869e-05,  5.4859e-05,  6.0204e-05,  6.5813e-05,  7.5669e-05,\n",
      "         8.6616e-05,  1.0032e-04,  1.0667e-04,  1.4683e-04,  2.2685e-04,\n",
      "         2.6284e-04,  3.3082e-04,  5.3409e-04,  6.7799e-04,  9.5698e-04,\n",
      "         1.0641e-03,  3.2585e-03,  3.5998e-03,  1.8719e-02,  1.5094e-01])\n",
      "wout = tensor([-1.2043e-02, -1.1314e-02, -1.0518e-02, -9.7962e-03, -9.4853e-03,\n",
      "        -7.7539e-03, -7.1904e-03, -5.9014e-03, -5.4847e-03, -5.1479e-03,\n",
      "        -4.9050e-03, -4.7606e-03, -4.5956e-03, -4.4308e-03, -4.3390e-03,\n",
      "        -4.2720e-03, -4.1018e-03, -3.6784e-03, -3.4462e-03, -3.3900e-03,\n",
      "        -3.2873e-03, -3.2561e-03, -3.2056e-03, -3.1478e-03, -3.0286e-03,\n",
      "        -2.9773e-03, -2.9417e-03, -2.8580e-03, -2.7858e-03, -2.7515e-03,\n",
      "        -2.6649e-03, -2.6118e-03, -2.4176e-03, -2.3001e-03, -2.2581e-03,\n",
      "        -2.1467e-03, -2.1140e-03, -2.0286e-03, -1.9644e-03, -1.9497e-03,\n",
      "        -1.8290e-03, -1.7989e-03, -1.7264e-03, -1.6815e-03, -1.5767e-03,\n",
      "        -1.4195e-03, -1.3220e-03, -1.2002e-03, -1.1754e-03, -1.1417e-03,\n",
      "        -1.0975e-03, -1.0066e-03, -9.8366e-04, -9.3651e-04, -9.2597e-04,\n",
      "        -8.6376e-04, -8.4002e-04, -8.3192e-04, -7.6508e-04, -7.5344e-04,\n",
      "        -7.2907e-04, -6.4103e-04, -6.3572e-04, -6.3050e-04, -6.1187e-04,\n",
      "        -5.9661e-04, -5.5302e-04, -5.3658e-04, -5.2790e-04, -5.1564e-04,\n",
      "        -4.9412e-04, -4.7567e-04, -4.3987e-04, -4.1838e-04, -4.0820e-04,\n",
      "        -3.9068e-04, -3.8823e-04, -3.6780e-04, -3.2314e-04, -2.9845e-04,\n",
      "        -2.9483e-04, -2.8355e-04, -2.7589e-04, -2.7079e-04, -2.6138e-04,\n",
      "        -2.5721e-04, -2.4533e-04, -2.4209e-04, -2.2392e-04, -2.1701e-04,\n",
      "        -1.9903e-04, -1.7946e-04, -1.7074e-04, -1.5966e-04, -1.4908e-04,\n",
      "        -1.2158e-04, -1.1826e-04, -1.0615e-04, -9.0102e-05, -8.2699e-05,\n",
      "        -7.9548e-05, -7.3615e-05, -6.6380e-05, -4.5844e-05, -2.9637e-05,\n",
      "        -2.5637e-05, -1.8840e-05,  1.0282e-05,  1.9581e-05,  3.7398e-05,\n",
      "         5.0074e-05,  5.6138e-05,  6.7808e-05,  1.0100e-04,  1.1491e-04,\n",
      "         1.2279e-04,  1.2536e-04,  1.3180e-04,  1.4425e-04,  1.5599e-04,\n",
      "         1.5743e-04,  1.7307e-04,  2.0874e-04,  2.1574e-04,  2.2880e-04,\n",
      "         2.3217e-04,  2.5094e-04,  2.6109e-04,  2.7512e-04,  2.8378e-04,\n",
      "         2.9324e-04,  3.5393e-04,  3.6389e-04,  3.6625e-04,  5.0060e-04,\n",
      "         5.6797e-04,  6.2939e-04,  6.8387e-04,  7.1065e-04,  7.5131e-04,\n",
      "         7.7642e-04,  9.4892e-04,  1.0974e-03,  1.1806e-03,  1.2224e-03,\n",
      "         1.2761e-03,  1.4537e-03,  1.5831e-03,  1.8312e-03,  1.9056e-03,\n",
      "         1.9311e-03,  1.9836e-03,  2.0062e-03,  2.0825e-03,  2.2582e-03,\n",
      "         2.2807e-03,  2.3169e-03,  2.3589e-03,  2.5996e-03,  2.6477e-03,\n",
      "         2.6612e-03,  2.7272e-03,  2.7493e-03,  2.8759e-03,  2.9288e-03,\n",
      "         3.0571e-03,  3.1389e-03,  3.2973e-03,  3.8346e-03,  4.0124e-03,\n",
      "         4.1736e-03,  4.3643e-03,  4.4443e-03,  4.5252e-03,  4.6253e-03,\n",
      "         4.7578e-03,  4.8983e-03,  5.0410e-03,  5.3384e-03,  5.4897e-03,\n",
      "         5.7325e-03,  5.8536e-03,  6.0414e-03,  6.5772e-03,  6.6671e-03,\n",
      "         6.7058e-03,  7.3831e-03,  7.7719e-03,  7.9621e-03,  8.2603e-03,\n",
      "         9.5261e-03,  1.0195e-02,  1.0421e-02,  1.1264e-02,  1.1889e-02,\n",
      "         1.4721e-02,  1.5219e-02,  1.6795e-02,  1.9597e-02,  2.9277e-02])\n",
      "Task 1 Epoch 3/15 - Loss: 1.3043\n",
      "  Acc (Task 1): 45.05%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5557, Euclidean: 1.9193\n",
      "Task 1 Epoch 4/15 - Loss: 1.1297\n",
      "  Acc (Task 1): 52.80%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5737, Euclidean: 1.9835\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 4, step 200, cos_sim = -0.515 \n",
      "bias = 0\n",
      "win = tensor([-6.6022e-02, -1.9775e-02, -3.4001e-03, -1.6329e-03, -6.7465e-04,\n",
      "        -5.5287e-05, -1.0543e-05, -8.8819e-06, -7.4280e-07, -4.2263e-07,\n",
      "        -1.4319e-07, -7.6365e-08, -3.1934e-08, -9.5174e-09, -7.8789e-09,\n",
      "        -3.2363e-09, -1.2226e-09, -1.1280e-09, -1.0209e-09, -7.2702e-10,\n",
      "        -7.1324e-10, -6.7678e-10, -6.6942e-10, -6.2329e-10, -6.0373e-10,\n",
      "        -5.7981e-10, -5.7640e-10, -5.6785e-10, -5.4952e-10, -5.3614e-10,\n",
      "        -5.2795e-10, -5.1655e-10, -5.0241e-10, -4.8537e-10, -4.7385e-10,\n",
      "        -4.6766e-10, -4.6523e-10, -4.4137e-10, -4.3591e-10, -4.3093e-10,\n",
      "        -4.1922e-10, -4.0534e-10, -3.9210e-10, -3.8838e-10, -3.7909e-10,\n",
      "        -3.7682e-10, -3.6021e-10, -3.5183e-10, -3.5055e-10, -3.4299e-10,\n",
      "        -3.3655e-10, -3.2489e-10, -3.1682e-10, -2.9903e-10, -2.9512e-10,\n",
      "        -2.8238e-10, -2.8048e-10, -2.6915e-10, -2.6836e-10, -2.6145e-10,\n",
      "        -2.5535e-10, -2.4606e-10, -2.3324e-10, -2.3279e-10, -2.1414e-10,\n",
      "        -2.0660e-10, -1.9582e-10, -1.9255e-10, -1.8965e-10, -1.8143e-10,\n",
      "        -1.7265e-10, -1.6378e-10, -1.6135e-10, -1.5220e-10, -1.4825e-10,\n",
      "        -1.4162e-10, -1.3295e-10, -1.3122e-10, -1.1918e-10, -1.1697e-10,\n",
      "        -1.0697e-10, -1.0197e-10, -9.9894e-11, -8.8082e-11, -7.8961e-11,\n",
      "        -7.2815e-11, -6.8323e-11, -5.6835e-11, -5.5097e-11, -5.1880e-11,\n",
      "        -4.5365e-11, -4.2400e-11, -3.7793e-11, -2.6398e-11, -2.2419e-11,\n",
      "        -1.8764e-11, -1.8465e-11, -9.2422e-12, -4.3972e-12, -1.5979e-12,\n",
      "         6.9516e-12,  1.6773e-11,  1.9641e-11,  2.8103e-11,  3.3151e-11,\n",
      "         4.2451e-11,  4.5130e-11,  5.2097e-11,  5.5721e-11,  5.8773e-11,\n",
      "         6.2920e-11,  7.7516e-11,  8.7364e-11,  8.9480e-11,  9.7633e-11,\n",
      "         1.0370e-10,  1.0924e-10,  1.1494e-10,  1.1874e-10,  1.2416e-10,\n",
      "         1.3128e-10,  1.4023e-10,  1.4643e-10,  1.4848e-10,  1.5352e-10,\n",
      "         1.5771e-10,  1.6188e-10,  1.7512e-10,  1.8334e-10,  1.9054e-10,\n",
      "         1.9650e-10,  2.0489e-10,  2.1463e-10,  2.2144e-10,  2.2626e-10,\n",
      "         2.3634e-10,  2.3840e-10,  2.4336e-10,  2.6107e-10,  2.6647e-10,\n",
      "         2.7350e-10,  2.7714e-10,  2.8356e-10,  2.9764e-10,  3.0260e-10,\n",
      "         3.1488e-10,  3.1846e-10,  3.3199e-10,  3.3381e-10,  3.4263e-10,\n",
      "         3.4618e-10,  3.5395e-10,  3.7865e-10,  3.8768e-10,  3.9791e-10,\n",
      "         4.0705e-10,  4.1498e-10,  4.1700e-10,  4.2769e-10,  4.4406e-10,\n",
      "         4.4710e-10,  4.5312e-10,  4.7166e-10,  4.9025e-10,  5.0264e-10,\n",
      "         5.1186e-10,  5.3100e-10,  5.4959e-10,  5.5904e-10,  5.6575e-10,\n",
      "         5.7460e-10,  5.9194e-10,  6.4085e-10,  6.4578e-10,  6.7986e-10,\n",
      "         6.8973e-10,  7.1147e-10,  7.2772e-10,  7.6543e-10,  8.8739e-10,\n",
      "         9.9021e-10,  1.3904e-09,  2.0255e-09,  4.9382e-09,  6.6745e-09,\n",
      "         1.0110e-08,  2.5239e-08,  4.5516e-08,  2.3765e-07,  6.3452e-07,\n",
      "         1.8913e-06,  5.2137e-06,  2.5010e-05,  8.2362e-05,  1.3047e-04,\n",
      "         3.2036e-04,  5.2589e-04,  1.8367e-03,  1.2104e-02,  1.9531e-02])\n",
      "wout = tensor([-1.1554e-02, -9.5876e-03, -8.5625e-03, -6.5136e-03, -4.5771e-03,\n",
      "        -4.5032e-03, -4.3819e-03, -4.1547e-03, -4.1329e-03, -3.8615e-03,\n",
      "        -3.7529e-03, -3.6812e-03, -3.5465e-03, -3.4138e-03, -3.1937e-03,\n",
      "        -3.0973e-03, -3.0347e-03, -2.9758e-03, -2.8882e-03, -2.6816e-03,\n",
      "        -2.6407e-03, -2.4934e-03, -2.4493e-03, -2.4146e-03, -2.3441e-03,\n",
      "        -2.2698e-03, -2.2149e-03, -2.1564e-03, -1.9453e-03, -1.8943e-03,\n",
      "        -1.7037e-03, -1.6501e-03, -1.6092e-03, -1.5441e-03, -1.5070e-03,\n",
      "        -1.5001e-03, -1.4721e-03, -1.4449e-03, -1.4339e-03, -1.4027e-03,\n",
      "        -1.3797e-03, -1.3400e-03, -1.3086e-03, -1.2645e-03, -1.2416e-03,\n",
      "        -1.1575e-03, -1.1400e-03, -1.1297e-03, -1.1123e-03, -1.0983e-03,\n",
      "        -1.0333e-03, -9.5429e-04, -9.1956e-04, -8.8956e-04, -8.8609e-04,\n",
      "        -8.5035e-04, -8.4952e-04, -8.2715e-04, -7.8645e-04, -7.5426e-04,\n",
      "        -7.5301e-04, -7.3711e-04, -7.2552e-04, -7.1951e-04, -6.9231e-04,\n",
      "        -6.6538e-04, -6.3534e-04, -6.1499e-04, -5.9432e-04, -5.8818e-04,\n",
      "        -5.6249e-04, -5.2909e-04, -5.0195e-04, -4.9362e-04, -4.8923e-04,\n",
      "        -4.8452e-04, -4.6397e-04, -4.2540e-04, -4.1735e-04, -4.1283e-04,\n",
      "        -3.9337e-04, -3.7006e-04, -3.2500e-04, -2.8578e-04, -2.7919e-04,\n",
      "        -2.7021e-04, -2.5474e-04, -2.2108e-04, -2.1459e-04, -2.0617e-04,\n",
      "        -1.7275e-04, -1.6039e-04, -1.5070e-04, -1.4939e-04, -1.4105e-04,\n",
      "        -1.3322e-04, -1.2895e-04, -1.2567e-04, -1.1958e-04, -1.1888e-04,\n",
      "        -1.1708e-04, -1.0213e-04, -1.0085e-04, -9.5222e-05, -8.6242e-05,\n",
      "        -7.6343e-05, -6.9147e-05, -6.5787e-05, -6.0221e-05, -5.0969e-05,\n",
      "        -4.5256e-05, -3.0992e-05, -2.7141e-05, -2.0357e-05, -1.7757e-05,\n",
      "        -8.6146e-06,  1.1141e-05,  1.3683e-05,  1.6378e-05,  1.7750e-05,\n",
      "         2.4047e-05,  5.3465e-05,  6.0816e-05,  7.9767e-05,  8.5927e-05,\n",
      "         9.2043e-05,  1.1482e-04,  1.1607e-04,  1.2600e-04,  1.4939e-04,\n",
      "         1.5129e-04,  1.6894e-04,  1.9473e-04,  1.9544e-04,  1.9868e-04,\n",
      "         2.1973e-04,  2.2554e-04,  3.0201e-04,  3.1412e-04,  3.2365e-04,\n",
      "         3.4647e-04,  3.5784e-04,  4.2359e-04,  4.6795e-04,  4.7940e-04,\n",
      "         5.1100e-04,  5.2281e-04,  5.5822e-04,  5.7597e-04,  6.0963e-04,\n",
      "         6.2801e-04,  7.2496e-04,  7.5605e-04,  7.9235e-04,  8.2600e-04,\n",
      "         8.4359e-04,  8.7959e-04,  9.0225e-04,  9.2367e-04,  9.8079e-04,\n",
      "         1.0046e-03,  1.0554e-03,  1.1287e-03,  1.1700e-03,  1.2285e-03,\n",
      "         1.3081e-03,  1.3495e-03,  1.3605e-03,  1.3786e-03,  1.4450e-03,\n",
      "         1.4616e-03,  1.5268e-03,  1.5851e-03,  1.6233e-03,  1.6968e-03,\n",
      "         1.7327e-03,  1.7868e-03,  1.8525e-03,  1.9187e-03,  2.0265e-03,\n",
      "         2.0440e-03,  2.0756e-03,  2.5282e-03,  2.5510e-03,  2.7365e-03,\n",
      "         2.7893e-03,  3.1416e-03,  3.2435e-03,  3.4305e-03,  3.5185e-03,\n",
      "         3.5471e-03,  3.7989e-03,  4.4592e-03,  4.8220e-03,  5.3324e-03,\n",
      "         5.3975e-03,  6.2140e-03,  7.2300e-03,  7.8771e-03,  9.0422e-03])\n",
      "Task 1 Epoch 5/15 - Loss: 1.0949\n",
      "  Acc (Task 1): 51.08%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5153, Euclidean: 1.9884\n",
      "Task 1 Epoch 6/15 - Loss: 1.0817\n",
      "  Acc (Task 1): 54.48%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5242, Euclidean: 2.0210\n",
      "Task 1 Epoch 7/15 - Loss: 1.0560\n",
      "  Acc (Task 1): 56.30%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.5514, Euclidean: 2.0700\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 7, step 300, cos_sim = -0.496 \n",
      "bias = 0\n",
      "win = tensor([-1.3843e-02, -3.0741e-03, -2.4297e-03, -1.4860e-03, -1.1243e-03,\n",
      "        -1.0794e-03, -8.5053e-04, -8.4311e-04, -5.5730e-04, -4.2731e-04,\n",
      "        -3.2638e-04, -2.9505e-04, -2.6599e-04, -2.3725e-04, -2.0656e-04,\n",
      "        -1.7630e-04, -1.5310e-04, -1.4473e-04, -1.3662e-04, -1.2191e-04,\n",
      "        -1.0758e-04, -1.0021e-04, -8.7931e-05, -8.2917e-05, -7.7973e-05,\n",
      "        -7.2197e-05, -7.0413e-05, -5.8803e-05, -5.7062e-05, -5.5282e-05,\n",
      "        -4.6197e-05, -4.4760e-05, -4.2971e-05, -4.1239e-05, -3.8054e-05,\n",
      "        -3.4515e-05, -3.3970e-05, -2.9080e-05, -2.7086e-05, -2.5581e-05,\n",
      "        -2.3789e-05, -2.1156e-05, -1.9932e-05, -1.9347e-05, -1.7172e-05,\n",
      "        -1.6586e-05, -1.5344e-05, -1.4830e-05, -1.4165e-05, -1.3266e-05,\n",
      "        -1.2495e-05, -1.1498e-05, -1.1160e-05, -1.0744e-05, -1.0138e-05,\n",
      "        -8.7651e-06, -8.3057e-06, -7.8995e-06, -7.6309e-06, -7.0737e-06,\n",
      "        -6.2494e-06, -6.0665e-06, -5.5978e-06, -5.0830e-06, -4.6924e-06,\n",
      "        -4.3796e-06, -4.0918e-06, -3.9399e-06, -3.7039e-06, -3.5431e-06,\n",
      "        -3.1687e-06, -3.0770e-06, -2.6402e-06, -2.5887e-06, -2.3849e-06,\n",
      "        -2.2899e-06, -1.8849e-06, -1.8753e-06, -1.7611e-06, -1.6781e-06,\n",
      "        -1.4687e-06, -1.3469e-06, -1.2898e-06, -1.1243e-06, -1.0285e-06,\n",
      "        -9.7894e-07, -8.6538e-07, -8.2485e-07, -7.6318e-07, -7.1778e-07,\n",
      "        -6.1777e-07, -5.5107e-07, -4.6557e-07, -4.3278e-07, -3.9678e-07,\n",
      "        -3.5526e-07, -3.3161e-07, -2.8300e-07, -2.6374e-07, -1.9044e-07,\n",
      "        -1.7854e-07, -1.3640e-07, -1.3372e-07, -1.0484e-07, -8.5023e-08,\n",
      "        -6.2984e-08, -5.6356e-08, -4.9742e-08, -3.9147e-08, -2.4367e-08,\n",
      "        -5.9377e-09,  8.3110e-09,  2.1407e-08,  4.9479e-08,  6.0631e-08,\n",
      "         6.9396e-08,  1.1720e-07,  1.2194e-07,  1.7880e-07,  1.8394e-07,\n",
      "         2.4172e-07,  2.6763e-07,  2.9657e-07,  3.6133e-07,  3.7496e-07,\n",
      "         4.1851e-07,  4.6373e-07,  5.2800e-07,  5.6739e-07,  5.7898e-07,\n",
      "         8.0164e-07,  8.6843e-07,  9.3127e-07,  9.7626e-07,  1.0018e-06,\n",
      "         1.1736e-06,  1.4152e-06,  1.4365e-06,  1.6226e-06,  1.9382e-06,\n",
      "         2.1277e-06,  2.3858e-06,  2.5628e-06,  3.1321e-06,  3.4641e-06,\n",
      "         3.5464e-06,  3.9970e-06,  4.1999e-06,  4.6676e-06,  4.8758e-06,\n",
      "         5.5698e-06,  6.2649e-06,  6.5832e-06,  7.1201e-06,  7.8703e-06,\n",
      "         8.8877e-06,  9.8209e-06,  1.0138e-05,  1.1113e-05,  1.2179e-05,\n",
      "         1.3443e-05,  1.4301e-05,  1.5250e-05,  1.5925e-05,  1.7630e-05,\n",
      "         1.7971e-05,  1.9935e-05,  2.2965e-05,  2.3579e-05,  2.5778e-05,\n",
      "         2.5965e-05,  3.1180e-05,  3.3194e-05,  3.9525e-05,  4.2036e-05,\n",
      "         4.9907e-05,  5.2906e-05,  6.1161e-05,  6.7420e-05,  7.7696e-05,\n",
      "         8.2961e-05,  9.3462e-05,  1.0584e-04,  1.1462e-04,  1.2524e-04,\n",
      "         1.4013e-04,  1.6735e-04,  1.8744e-04,  2.4398e-04,  2.5709e-04,\n",
      "         2.8670e-04,  3.6603e-04,  4.4216e-04,  5.4527e-04,  6.7708e-04,\n",
      "         1.8040e-03,  2.2882e-03,  2.5429e-03,  7.0404e-03,  2.1664e-02])\n",
      "wout = tensor([-2.7180e-03, -1.8894e-03, -1.8167e-03, -1.3752e-03, -1.2221e-03,\n",
      "        -8.9804e-04, -8.6947e-04, -8.3638e-04, -8.2294e-04, -7.9294e-04,\n",
      "        -7.7561e-04, -7.4557e-04, -7.2165e-04, -6.8461e-04, -6.6784e-04,\n",
      "        -6.3528e-04, -6.2609e-04, -6.0638e-04, -5.8836e-04, -5.8409e-04,\n",
      "        -5.3140e-04, -5.2397e-04, -5.0750e-04, -5.0534e-04, -5.0001e-04,\n",
      "        -4.9443e-04, -4.8340e-04, -4.7708e-04, -4.6428e-04, -4.5635e-04,\n",
      "        -4.5488e-04, -4.4782e-04, -4.3716e-04, -4.2963e-04, -4.2317e-04,\n",
      "        -4.1970e-04, -4.0467e-04, -3.9754e-04, -3.9508e-04, -3.8354e-04,\n",
      "        -3.7753e-04, -3.7143e-04, -3.5479e-04, -3.4273e-04, -3.3416e-04,\n",
      "        -3.2660e-04, -3.2214e-04, -2.9897e-04, -2.7571e-04, -2.6595e-04,\n",
      "        -2.5871e-04, -2.4577e-04, -2.4160e-04, -2.3609e-04, -2.2777e-04,\n",
      "        -2.2709e-04, -2.2036e-04, -2.1531e-04, -2.0877e-04, -1.9877e-04,\n",
      "        -1.9174e-04, -1.9001e-04, -1.8323e-04, -1.7632e-04, -1.7245e-04,\n",
      "        -1.6747e-04, -1.5626e-04, -1.4702e-04, -1.3969e-04, -1.3854e-04,\n",
      "        -1.2840e-04, -1.2417e-04, -1.1825e-04, -1.1589e-04, -1.0790e-04,\n",
      "        -9.6025e-05, -9.0455e-05, -8.4663e-05, -8.1505e-05, -7.8546e-05,\n",
      "        -7.1699e-05, -6.2695e-05, -6.2368e-05, -5.7370e-05, -5.5918e-05,\n",
      "        -5.1297e-05, -4.6562e-05, -3.1567e-05, -2.9107e-05, -2.2452e-05,\n",
      "        -2.0546e-05, -1.3841e-05, -9.2748e-06, -7.3341e-06, -3.2635e-06,\n",
      "        -5.0633e-07,  3.9012e-06,  5.3978e-06,  1.2137e-05,  1.6406e-05,\n",
      "         2.1115e-05,  2.6722e-05,  3.0101e-05,  3.4447e-05,  4.2025e-05,\n",
      "         5.0020e-05,  5.2319e-05,  5.5737e-05,  6.1009e-05,  7.0147e-05,\n",
      "         7.7416e-05,  1.0100e-04,  1.0503e-04,  1.0763e-04,  1.1475e-04,\n",
      "         1.2746e-04,  1.3262e-04,  1.3406e-04,  1.3601e-04,  1.4262e-04,\n",
      "         1.4849e-04,  1.5453e-04,  1.5633e-04,  1.6355e-04,  1.7043e-04,\n",
      "         1.7134e-04,  1.8006e-04,  1.8209e-04,  1.9462e-04,  2.0583e-04,\n",
      "         2.0675e-04,  2.1125e-04,  2.1681e-04,  2.3257e-04,  2.3572e-04,\n",
      "         2.4606e-04,  2.6212e-04,  2.6530e-04,  2.7559e-04,  2.7862e-04,\n",
      "         2.8975e-04,  3.0534e-04,  3.1835e-04,  3.2476e-04,  3.3015e-04,\n",
      "         3.3820e-04,  3.4709e-04,  3.5510e-04,  3.5749e-04,  3.6693e-04,\n",
      "         3.7911e-04,  3.9031e-04,  3.9263e-04,  4.0612e-04,  4.1408e-04,\n",
      "         4.2422e-04,  4.2798e-04,  4.4034e-04,  4.4843e-04,  4.6183e-04,\n",
      "         4.7798e-04,  4.9505e-04,  5.0402e-04,  5.2191e-04,  5.3005e-04,\n",
      "         5.3582e-04,  5.5578e-04,  5.7129e-04,  5.8778e-04,  6.0251e-04,\n",
      "         6.2797e-04,  6.5242e-04,  6.6969e-04,  7.0045e-04,  7.4496e-04,\n",
      "         7.6372e-04,  7.6772e-04,  7.8526e-04,  7.9312e-04,  8.5465e-04,\n",
      "         8.6492e-04,  8.7683e-04,  9.0236e-04,  9.3283e-04,  9.9252e-04,\n",
      "         1.0590e-03,  1.1206e-03,  1.1620e-03,  1.2965e-03,  1.3618e-03,\n",
      "         1.4161e-03,  1.4927e-03,  1.6529e-03,  1.6839e-03,  1.7194e-03,\n",
      "         1.8630e-03,  1.9878e-03,  2.1429e-03,  2.3951e-03,  4.0237e-03])\n",
      "Task 1 Epoch 8/15 - Loss: 1.0712\n",
      "  Acc (Task 1): 51.15%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.4739, Euclidean: 2.0703\n",
      "Task 1 Epoch 9/15 - Loss: 1.0604\n",
      "  Acc (Task 1): 54.25%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.4663, Euclidean: 2.1150\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 9, step 400, cos_sim = -0.441 \n",
      "bias = 0\n",
      "win = tensor([-2.9472e-02, -7.5558e-03, -3.5666e-03, -2.1755e-03, -8.1373e-04,\n",
      "        -5.1583e-04, -3.5760e-04, -2.5325e-04, -8.4224e-05, -4.0195e-05,\n",
      "        -2.9950e-05, -1.9376e-05, -1.5886e-05, -2.4252e-06, -7.8898e-07,\n",
      "        -5.4499e-08, -4.4826e-08, -3.7926e-08, -1.5354e-08, -9.0897e-09,\n",
      "        -3.9266e-09, -1.7210e-09, -1.4117e-09, -1.3144e-09, -1.2756e-09,\n",
      "        -1.2471e-09, -1.2399e-09, -1.2048e-09, -1.1501e-09, -1.0905e-09,\n",
      "        -1.0773e-09, -1.0590e-09, -1.0166e-09, -1.0057e-09, -9.8455e-10,\n",
      "        -9.6588e-10, -9.4139e-10, -9.2763e-10, -9.0142e-10, -8.7551e-10,\n",
      "        -8.5310e-10, -8.4125e-10, -8.2353e-10, -8.0746e-10, -7.8714e-10,\n",
      "        -7.7066e-10, -7.5719e-10, -7.4069e-10, -7.0617e-10, -6.9559e-10,\n",
      "        -6.8670e-10, -6.6948e-10, -6.5941e-10, -6.3758e-10, -6.1956e-10,\n",
      "        -6.0207e-10, -5.8906e-10, -5.5470e-10, -5.4544e-10, -5.3815e-10,\n",
      "        -5.2485e-10, -5.1378e-10, -5.0559e-10, -4.8800e-10, -4.6770e-10,\n",
      "        -4.5649e-10, -4.4508e-10, -4.2638e-10, -4.0943e-10, -4.0239e-10,\n",
      "        -3.7595e-10, -3.6572e-10, -3.5680e-10, -3.4456e-10, -3.3609e-10,\n",
      "        -3.2041e-10, -3.1348e-10, -3.0544e-10, -2.9708e-10, -2.8161e-10,\n",
      "        -2.6461e-10, -2.5638e-10, -2.5113e-10, -2.3223e-10, -2.2739e-10,\n",
      "        -2.2092e-10, -2.1065e-10, -1.8595e-10, -1.6810e-10, -1.5969e-10,\n",
      "        -1.4771e-10, -1.3766e-10, -1.2653e-10, -1.1249e-10, -9.4675e-11,\n",
      "        -8.1040e-11, -7.1623e-11, -6.4595e-11, -5.4700e-11, -5.1040e-11,\n",
      "        -3.4710e-11, -2.0191e-11, -9.5448e-12, -3.7635e-12,  3.9450e-12,\n",
      "         1.3611e-11,  1.9818e-11,  3.6436e-11,  6.0694e-11,  8.0984e-11,\n",
      "         8.4787e-11,  8.7752e-11,  1.0764e-10,  1.2033e-10,  1.3379e-10,\n",
      "         1.3821e-10,  1.5259e-10,  1.6054e-10,  1.7270e-10,  1.7963e-10,\n",
      "         1.9788e-10,  2.2547e-10,  2.2622e-10,  2.3718e-10,  2.4711e-10,\n",
      "         2.6183e-10,  2.8170e-10,  2.9226e-10,  2.9958e-10,  3.1836e-10,\n",
      "         3.2875e-10,  3.4960e-10,  3.5339e-10,  3.5995e-10,  3.7888e-10,\n",
      "         3.8585e-10,  4.0605e-10,  4.2962e-10,  4.4256e-10,  4.5581e-10,\n",
      "         4.6511e-10,  4.8484e-10,  4.8659e-10,  5.0085e-10,  5.1320e-10,\n",
      "         5.2748e-10,  5.3935e-10,  5.6224e-10,  5.6917e-10,  5.7763e-10,\n",
      "         5.9888e-10,  6.1117e-10,  6.3133e-10,  6.4225e-10,  6.5190e-10,\n",
      "         6.5952e-10,  6.8262e-10,  7.0545e-10,  7.2169e-10,  7.4777e-10,\n",
      "         7.5837e-10,  7.7894e-10,  7.9032e-10,  8.1186e-10,  8.3189e-10,\n",
      "         8.4794e-10,  8.6358e-10,  8.6601e-10,  8.8932e-10,  9.2302e-10,\n",
      "         9.3730e-10,  9.8242e-10,  9.9747e-10,  1.0124e-09,  1.0482e-09,\n",
      "         1.0781e-09,  1.1133e-09,  1.1263e-09,  1.1812e-09,  1.2090e-09,\n",
      "         1.2340e-09,  1.2352e-09,  1.3056e-09,  1.3421e-09,  1.4023e-09,\n",
      "         4.9884e-09,  6.7461e-09,  1.0800e-08,  1.8271e-08,  4.5268e-08,\n",
      "         1.0649e-07,  3.6694e-07,  6.4138e-07,  3.0384e-06,  8.8878e-04,\n",
      "         1.1889e-03,  3.2677e-03,  5.9662e-03,  1.7060e-02,  8.3004e-02])\n",
      "wout = tensor([-6.4028e-03, -6.2824e-03, -5.9215e-03, -4.9952e-03, -4.6989e-03,\n",
      "        -4.1453e-03, -3.5997e-03, -3.1266e-03, -2.8525e-03, -2.6712e-03,\n",
      "        -2.5338e-03, -2.3165e-03, -2.2684e-03, -2.0955e-03, -1.8874e-03,\n",
      "        -1.8075e-03, -1.7756e-03, -1.7245e-03, -1.6892e-03, -1.5634e-03,\n",
      "        -1.5538e-03, -1.4849e-03, -1.4678e-03, -1.4372e-03, -1.4038e-03,\n",
      "        -1.3220e-03, -1.3024e-03, -1.2378e-03, -1.2171e-03, -1.1800e-03,\n",
      "        -1.1666e-03, -1.0955e-03, -1.0690e-03, -1.0636e-03, -1.0410e-03,\n",
      "        -1.0365e-03, -1.0226e-03, -9.7957e-04, -9.5144e-04, -9.4735e-04,\n",
      "        -9.0371e-04, -8.8307e-04, -8.4992e-04, -8.3337e-04, -8.1321e-04,\n",
      "        -7.6304e-04, -7.5665e-04, -7.5074e-04, -7.2403e-04, -6.7778e-04,\n",
      "        -6.4111e-04, -6.2454e-04, -5.8412e-04, -5.7422e-04, -5.6145e-04,\n",
      "        -5.3870e-04, -5.1562e-04, -4.8333e-04, -4.6277e-04, -4.5582e-04,\n",
      "        -4.4710e-04, -4.4233e-04, -4.3888e-04, -4.2889e-04, -4.1910e-04,\n",
      "        -4.0671e-04, -3.7862e-04, -3.7448e-04, -3.4532e-04, -3.3202e-04,\n",
      "        -3.2089e-04, -3.1634e-04, -3.1370e-04, -3.0570e-04, -2.9905e-04,\n",
      "        -2.9133e-04, -2.8301e-04, -2.8127e-04, -2.5959e-04, -2.5682e-04,\n",
      "        -2.2953e-04, -2.0982e-04, -1.7599e-04, -1.5657e-04, -1.5167e-04,\n",
      "        -1.4517e-04, -1.1640e-04, -8.8717e-05, -6.4124e-05, -5.7525e-05,\n",
      "        -4.7614e-05, -4.0897e-05, -3.4152e-05, -2.3271e-05, -2.1591e-05,\n",
      "         3.6373e-06,  1.4227e-05,  2.2324e-05,  3.5422e-05,  5.0344e-05,\n",
      "         5.9681e-05,  6.5838e-05,  8.6824e-05,  1.0995e-04,  1.1363e-04,\n",
      "         1.1622e-04,  1.2581e-04,  1.5436e-04,  1.6754e-04,  1.6840e-04,\n",
      "         1.8198e-04,  1.8950e-04,  2.2611e-04,  2.3721e-04,  2.6016e-04,\n",
      "         2.8050e-04,  2.9468e-04,  3.0142e-04,  3.4345e-04,  3.5889e-04,\n",
      "         3.6333e-04,  3.7726e-04,  3.8987e-04,  4.1389e-04,  4.3335e-04,\n",
      "         4.6588e-04,  4.7634e-04,  5.0005e-04,  5.2389e-04,  5.3146e-04,\n",
      "         5.5703e-04,  5.7143e-04,  5.8595e-04,  6.0210e-04,  6.1967e-04,\n",
      "         6.5129e-04,  6.7202e-04,  6.8379e-04,  7.1172e-04,  7.1822e-04,\n",
      "         7.2420e-04,  7.3963e-04,  7.4768e-04,  7.7962e-04,  7.9621e-04,\n",
      "         8.0435e-04,  8.2568e-04,  8.2921e-04,  8.5280e-04,  8.8322e-04,\n",
      "         8.9614e-04,  9.4722e-04,  9.8355e-04,  1.0435e-03,  1.0684e-03,\n",
      "         1.1175e-03,  1.1736e-03,  1.2190e-03,  1.2239e-03,  1.3057e-03,\n",
      "         1.3274e-03,  1.3288e-03,  1.3963e-03,  1.4297e-03,  1.4443e-03,\n",
      "         1.4708e-03,  1.4873e-03,  1.5192e-03,  1.6022e-03,  1.6088e-03,\n",
      "         1.6573e-03,  1.6792e-03,  1.7163e-03,  1.7461e-03,  1.7664e-03,\n",
      "         1.8050e-03,  1.8594e-03,  1.9485e-03,  2.0040e-03,  2.0433e-03,\n",
      "         2.0900e-03,  2.2053e-03,  2.2262e-03,  2.2586e-03,  2.3402e-03,\n",
      "         2.4378e-03,  2.5120e-03,  2.7392e-03,  2.8281e-03,  2.8644e-03,\n",
      "         3.1994e-03,  3.6006e-03,  4.1358e-03,  4.3245e-03,  4.6757e-03,\n",
      "         6.0611e-03,  6.1855e-03,  6.8257e-03,  8.5639e-03,  1.2449e-02])\n",
      "Task 1 Epoch 10/15 - Loss: 1.0347\n",
      "  Acc (Task 1): 53.83%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.4409, Euclidean: 2.1518\n",
      "Task 1 Epoch 11/15 - Loss: 1.0456\n",
      "  Acc (Task 1): 48.55%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.4500, Euclidean: 2.2766\n",
      "Task 1 Epoch 12/15 - Loss: 1.0495\n",
      "  Acc (Task 1): 53.98%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.4029, Euclidean: 2.3482\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 12, step 500, cos_sim = -0.331 \n",
      "bias = 0\n",
      "win = tensor([-1.4055e-02, -5.6507e-03, -3.8313e-03, -2.6224e-03, -1.6640e-03,\n",
      "        -1.5072e-03, -1.1650e-03, -9.3633e-04, -8.2192e-04, -6.5183e-04,\n",
      "        -5.4672e-04, -4.5077e-04, -3.7903e-04, -3.3586e-04, -2.9168e-04,\n",
      "        -2.7258e-04, -2.5368e-04, -2.1870e-04, -2.0470e-04, -1.8284e-04,\n",
      "        -1.5605e-04, -1.5360e-04, -1.3659e-04, -1.3026e-04, -1.2378e-04,\n",
      "        -1.1493e-04, -1.1291e-04, -1.0133e-04, -8.9858e-05, -7.8550e-05,\n",
      "        -7.5738e-05, -7.1992e-05, -6.5274e-05, -5.9963e-05, -5.3657e-05,\n",
      "        -5.3125e-05, -5.1769e-05, -4.7300e-05, -4.0502e-05, -3.8848e-05,\n",
      "        -3.5822e-05, -3.2575e-05, -3.1463e-05, -3.0479e-05, -2.7322e-05,\n",
      "        -2.6564e-05, -2.4883e-05, -2.0693e-05, -1.9662e-05, -1.8556e-05,\n",
      "        -1.7830e-05, -1.7374e-05, -1.6098e-05, -1.4021e-05, -1.2557e-05,\n",
      "        -1.1220e-05, -1.0779e-05, -1.0042e-05, -9.2503e-06, -8.3510e-06,\n",
      "        -7.4478e-06, -7.2677e-06, -6.6338e-06, -6.4192e-06, -5.7366e-06,\n",
      "        -5.3668e-06, -4.8561e-06, -4.5671e-06, -3.9942e-06, -3.7833e-06,\n",
      "        -3.5445e-06, -3.3635e-06, -2.7758e-06, -2.6863e-06, -2.2921e-06,\n",
      "        -2.0128e-06, -1.8945e-06, -1.5361e-06, -1.4462e-06, -1.2692e-06,\n",
      "        -1.2517e-06, -1.1460e-06, -1.0346e-06, -7.5187e-07, -6.7458e-07,\n",
      "        -6.2609e-07, -4.9900e-07, -4.8516e-07, -3.8024e-07, -2.7847e-07,\n",
      "        -2.5599e-07, -2.0555e-07, -1.0012e-07, -4.2430e-08, -1.2949e-08,\n",
      "        -4.1232e-09,  5.9937e-08,  8.6299e-08,  1.6551e-07,  1.8093e-07,\n",
      "         2.8946e-07,  3.1588e-07,  4.1822e-07,  5.3395e-07,  5.7256e-07,\n",
      "         6.2801e-07,  7.3112e-07,  7.8767e-07,  8.8932e-07,  1.0453e-06,\n",
      "         1.0893e-06,  1.1479e-06,  1.3402e-06,  1.5475e-06,  1.6921e-06,\n",
      "         1.8072e-06,  2.1687e-06,  2.3026e-06,  2.3717e-06,  2.5706e-06,\n",
      "         3.0077e-06,  3.0564e-06,  3.1713e-06,  3.6381e-06,  3.9285e-06,\n",
      "         4.1687e-06,  4.6539e-06,  4.7474e-06,  5.4532e-06,  5.5040e-06,\n",
      "         5.7993e-06,  6.0055e-06,  6.8012e-06,  6.9805e-06,  7.3879e-06,\n",
      "         8.3391e-06,  9.0076e-06,  1.0134e-05,  1.0386e-05,  1.1240e-05,\n",
      "         1.2551e-05,  1.2794e-05,  1.3207e-05,  1.4315e-05,  1.5938e-05,\n",
      "         1.7816e-05,  1.8849e-05,  1.9414e-05,  2.0547e-05,  2.2326e-05,\n",
      "         2.3394e-05,  2.4983e-05,  2.6900e-05,  2.8413e-05,  2.9500e-05,\n",
      "         3.1494e-05,  3.3825e-05,  3.5992e-05,  3.8429e-05,  3.8785e-05,\n",
      "         4.3472e-05,  4.6657e-05,  4.8139e-05,  5.0535e-05,  5.5311e-05,\n",
      "         6.0716e-05,  6.4487e-05,  7.1951e-05,  7.6982e-05,  8.3570e-05,\n",
      "         8.9556e-05,  9.4020e-05,  1.0034e-04,  1.0754e-04,  1.2283e-04,\n",
      "         1.2940e-04,  1.3473e-04,  1.3504e-04,  1.5244e-04,  1.6015e-04,\n",
      "         1.8432e-04,  2.0081e-04,  2.1603e-04,  2.4214e-04,  2.8467e-04,\n",
      "         2.9732e-04,  3.1157e-04,  3.9919e-04,  4.1335e-04,  5.6139e-04,\n",
      "         6.2935e-04,  7.4695e-04,  8.9745e-04,  9.5102e-04,  1.0947e-03,\n",
      "         1.3090e-03,  1.9244e-03,  3.0976e-03,  5.1209e-03,  1.3328e-02])\n",
      "wout = tensor([-1.8035e-03, -1.4734e-03, -1.4480e-03, -1.2957e-03, -1.0945e-03,\n",
      "        -8.8073e-04, -8.3002e-04, -8.1614e-04, -7.8676e-04, -6.8708e-04,\n",
      "        -6.7613e-04, -6.0650e-04, -5.9329e-04, -5.8783e-04, -5.7961e-04,\n",
      "        -5.6810e-04, -5.2798e-04, -5.1561e-04, -4.7574e-04, -4.7148e-04,\n",
      "        -4.4602e-04, -4.3643e-04, -4.2339e-04, -4.1439e-04, -4.1144e-04,\n",
      "        -4.0234e-04, -3.8673e-04, -3.7920e-04, -3.7158e-04, -3.6750e-04,\n",
      "        -3.6370e-04, -3.5323e-04, -3.5226e-04, -3.4860e-04, -3.4265e-04,\n",
      "        -3.3453e-04, -3.2766e-04, -3.2347e-04, -3.2190e-04, -3.1228e-04,\n",
      "        -3.0042e-04, -2.9225e-04, -2.8465e-04, -2.7395e-04, -2.6117e-04,\n",
      "        -2.5628e-04, -2.5150e-04, -2.4972e-04, -2.4526e-04, -2.3619e-04,\n",
      "        -2.2709e-04, -2.2483e-04, -2.1818e-04, -2.1137e-04, -2.0176e-04,\n",
      "        -1.8608e-04, -1.8424e-04, -1.7635e-04, -1.7042e-04, -1.6804e-04,\n",
      "        -1.5320e-04, -1.4981e-04, -1.4760e-04, -1.3441e-04, -1.3340e-04,\n",
      "        -1.2703e-04, -1.2446e-04, -1.2117e-04, -1.1984e-04, -1.1642e-04,\n",
      "        -1.0475e-04, -1.0096e-04, -9.2157e-05, -8.6793e-05, -8.5344e-05,\n",
      "        -8.0711e-05, -7.6158e-05, -7.1695e-05, -7.0887e-05, -6.4449e-05,\n",
      "        -6.1839e-05, -6.0710e-05, -5.5866e-05, -5.2104e-05, -4.3510e-05,\n",
      "        -3.6194e-05, -2.8714e-05, -2.5782e-05, -2.2376e-05, -2.0463e-05,\n",
      "        -1.3648e-05, -1.1924e-05, -1.1297e-05, -1.5199e-06,  8.6221e-07,\n",
      "         4.9938e-06,  7.6279e-06,  8.2338e-06,  1.0963e-05,  1.3866e-05,\n",
      "         1.9183e-05,  2.1282e-05,  2.5138e-05,  2.8382e-05,  3.6951e-05,\n",
      "         3.9201e-05,  4.0779e-05,  4.3243e-05,  5.1243e-05,  5.4600e-05,\n",
      "         6.2952e-05,  6.8929e-05,  7.1578e-05,  7.7584e-05,  8.0435e-05,\n",
      "         8.9652e-05,  9.6477e-05,  1.0030e-04,  1.0416e-04,  1.0672e-04,\n",
      "         1.0826e-04,  1.0955e-04,  1.1290e-04,  1.1783e-04,  1.2068e-04,\n",
      "         1.2325e-04,  1.3199e-04,  1.3535e-04,  1.3740e-04,  1.3963e-04,\n",
      "         1.4468e-04,  1.4811e-04,  1.5462e-04,  1.5527e-04,  1.5952e-04,\n",
      "         1.7311e-04,  1.7531e-04,  1.7926e-04,  1.8724e-04,  1.9817e-04,\n",
      "         2.0112e-04,  2.0193e-04,  2.0807e-04,  2.1450e-04,  2.1890e-04,\n",
      "         2.2283e-04,  2.2596e-04,  2.3044e-04,  2.3244e-04,  2.4347e-04,\n",
      "         2.5352e-04,  2.5619e-04,  2.7141e-04,  2.7865e-04,  2.8561e-04,\n",
      "         2.9729e-04,  3.0746e-04,  3.1814e-04,  3.2242e-04,  3.3458e-04,\n",
      "         3.4002e-04,  3.4649e-04,  3.5641e-04,  3.6116e-04,  3.6735e-04,\n",
      "         3.7381e-04,  3.7540e-04,  3.8381e-04,  3.9172e-04,  3.9753e-04,\n",
      "         4.1076e-04,  4.2315e-04,  4.4643e-04,  4.4983e-04,  4.6639e-04,\n",
      "         4.7192e-04,  4.7938e-04,  4.8274e-04,  4.9699e-04,  5.5408e-04,\n",
      "         5.9506e-04,  6.0522e-04,  6.2421e-04,  6.6745e-04,  6.7897e-04,\n",
      "         6.9768e-04,  7.1811e-04,  7.4491e-04,  7.7330e-04,  8.2405e-04,\n",
      "         8.4385e-04,  9.0694e-04,  1.0041e-03,  1.3049e-03,  1.3661e-03,\n",
      "         1.3806e-03,  1.8398e-03,  2.4108e-03,  3.0358e-03,  4.1605e-03])\n",
      "Task 1 Epoch 13/15 - Loss: 1.0211\n",
      "  Acc (Task 1): 56.65%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.2708, Euclidean: 2.3313\n",
      "Task 1 Epoch 14/15 - Loss: 1.0089\n",
      "  Acc (Task 1): 57.75%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.2933, Euclidean: 2.4024\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 14, step 600, cos_sim = -0.270 \n",
      "bias = 0\n",
      "win = tensor([-1.4182e-01, -1.4584e-02, -8.0829e-03, -2.2998e-03, -2.0155e-03,\n",
      "        -1.1931e-03, -9.5531e-04, -2.9677e-04, -1.1249e-04, -5.1611e-05,\n",
      "        -3.3254e-05, -2.5692e-05, -1.7156e-05, -8.7808e-06, -3.1475e-06,\n",
      "        -2.4911e-06, -3.2495e-07, -1.4795e-07, -8.3576e-08, -4.6906e-08,\n",
      "        -1.3314e-08, -4.0450e-09, -3.4845e-09, -3.1064e-09, -2.8083e-09,\n",
      "        -2.6456e-09, -2.5540e-09, -2.4348e-09, -2.3944e-09, -2.3119e-09,\n",
      "        -2.2163e-09, -2.2100e-09, -2.1355e-09, -2.1067e-09, -2.0688e-09,\n",
      "        -2.0139e-09, -1.9767e-09, -1.9198e-09, -1.8981e-09, -1.8667e-09,\n",
      "        -1.8087e-09, -1.7888e-09, -1.7087e-09, -1.6718e-09, -1.6377e-09,\n",
      "        -1.6122e-09, -1.5720e-09, -1.5361e-09, -1.4750e-09, -1.4504e-09,\n",
      "        -1.4032e-09, -1.3810e-09, -1.3532e-09, -1.3316e-09, -1.3101e-09,\n",
      "        -1.2723e-09, -1.2471e-09, -1.2187e-09, -1.1999e-09, -1.1218e-09,\n",
      "        -1.0982e-09, -1.0877e-09, -1.0639e-09, -1.0486e-09, -1.0246e-09,\n",
      "        -9.6155e-10, -9.1371e-10, -8.9784e-10, -8.5715e-10, -8.4793e-10,\n",
      "        -8.1848e-10, -7.9327e-10, -7.8033e-10, -7.4785e-10, -7.0106e-10,\n",
      "        -6.7774e-10, -6.5640e-10, -6.2518e-10, -6.1007e-10, -5.8487e-10,\n",
      "        -5.6183e-10, -5.4957e-10, -5.2449e-10, -5.0713e-10, -4.8212e-10,\n",
      "        -4.5586e-10, -4.1909e-10, -3.8939e-10, -3.8287e-10, -3.4452e-10,\n",
      "        -3.2008e-10, -2.8861e-10, -2.7940e-10, -2.6067e-10, -2.3517e-10,\n",
      "        -2.0145e-10, -1.6155e-10, -1.3573e-10, -1.2880e-10, -1.1028e-10,\n",
      "        -9.1043e-11, -4.1694e-11, -2.5368e-11, -2.1014e-11,  2.4015e-11,\n",
      "         3.1561e-11,  5.0840e-11,  8.4592e-11,  9.4744e-11,  1.3072e-10,\n",
      "         1.6261e-10,  1.9203e-10,  2.3848e-10,  2.4801e-10,  2.6942e-10,\n",
      "         2.7579e-10,  3.1311e-10,  3.2938e-10,  3.4607e-10,  3.5848e-10,\n",
      "         4.0637e-10,  4.2475e-10,  4.3600e-10,  4.5371e-10,  5.2488e-10,\n",
      "         5.3540e-10,  5.5159e-10,  5.7391e-10,  5.8172e-10,  5.9323e-10,\n",
      "         6.3428e-10,  7.0686e-10,  7.2783e-10,  7.3366e-10,  7.5279e-10,\n",
      "         8.0842e-10,  8.2677e-10,  8.4730e-10,  8.9376e-10,  9.0924e-10,\n",
      "         9.1842e-10,  9.5641e-10,  9.7286e-10,  9.8326e-10,  1.0303e-09,\n",
      "         1.0448e-09,  1.0793e-09,  1.0904e-09,  1.1329e-09,  1.1483e-09,\n",
      "         1.1668e-09,  1.2027e-09,  1.2457e-09,  1.2635e-09,  1.3087e-09,\n",
      "         1.3256e-09,  1.3635e-09,  1.4050e-09,  1.4708e-09,  1.5003e-09,\n",
      "         1.5374e-09,  1.5889e-09,  1.5946e-09,  1.6619e-09,  1.7052e-09,\n",
      "         1.7258e-09,  1.7604e-09,  1.7776e-09,  1.8118e-09,  1.8414e-09,\n",
      "         1.8770e-09,  1.9183e-09,  1.9919e-09,  2.0594e-09,  2.1094e-09,\n",
      "         2.1441e-09,  2.2030e-09,  2.3541e-09,  2.3936e-09,  2.5276e-09,\n",
      "         2.5976e-09,  2.7350e-09,  2.9642e-09,  3.0339e-09,  3.2181e-09,\n",
      "         3.7633e-09,  4.9119e-09,  1.5529e-08,  2.7584e-07,  9.4168e-07,\n",
      "         4.6888e-06,  8.0633e-06,  6.1753e-05,  6.8183e-04,  1.1100e-03,\n",
      "         2.6363e-03,  3.2618e-03,  1.5300e-02,  3.1139e-02,  1.3005e-01])\n",
      "wout = tensor([-5.6238e-03, -4.0072e-03, -3.9370e-03, -3.5319e-03, -3.1214e-03,\n",
      "        -2.8934e-03, -2.6563e-03, -2.5995e-03, -2.3143e-03, -2.1741e-03,\n",
      "        -2.0667e-03, -1.9387e-03, -1.8105e-03, -1.7593e-03, -1.7366e-03,\n",
      "        -1.5743e-03, -1.5167e-03, -1.4844e-03, -1.4555e-03, -1.3935e-03,\n",
      "        -1.3563e-03, -1.2284e-03, -1.1857e-03, -1.0727e-03, -1.0288e-03,\n",
      "        -1.0183e-03, -9.9710e-04, -9.7638e-04, -9.6541e-04, -9.4878e-04,\n",
      "        -9.3066e-04, -8.7213e-04, -8.4833e-04, -8.2222e-04, -8.1198e-04,\n",
      "        -7.8944e-04, -7.3540e-04, -7.2277e-04, -7.0178e-04, -6.7422e-04,\n",
      "        -6.5197e-04, -6.3086e-04, -6.1479e-04, -6.0089e-04, -5.5831e-04,\n",
      "        -5.4434e-04, -5.3576e-04, -5.3266e-04, -5.3244e-04, -5.0819e-04,\n",
      "        -4.9756e-04, -4.9090e-04, -4.8962e-04, -4.8435e-04, -4.5476e-04,\n",
      "        -4.5376e-04, -4.4736e-04, -4.3146e-04, -4.1312e-04, -4.0293e-04,\n",
      "        -3.9479e-04, -3.9082e-04, -3.8569e-04, -3.8304e-04, -3.6787e-04,\n",
      "        -3.6617e-04, -3.6120e-04, -3.2177e-04, -3.1114e-04, -3.1080e-04,\n",
      "        -2.8530e-04, -2.7424e-04, -2.6293e-04, -2.5319e-04, -2.2937e-04,\n",
      "        -2.1361e-04, -2.0063e-04, -1.9521e-04, -1.7118e-04, -1.5884e-04,\n",
      "        -1.5242e-04, -1.3398e-04, -1.3132e-04, -1.1404e-04, -1.1033e-04,\n",
      "        -1.0197e-04, -8.1505e-05, -8.0823e-05, -7.9538e-05, -7.2226e-05,\n",
      "        -6.3555e-05, -6.1164e-05, -5.5468e-05, -5.3890e-05, -4.4428e-05,\n",
      "        -3.6536e-05, -3.2833e-05, -2.9217e-05, -2.5310e-05, -1.2988e-05,\n",
      "        -9.3326e-06, -3.3702e-06,  2.6647e-06,  5.5154e-06,  2.1469e-05,\n",
      "         2.5269e-05,  3.0555e-05,  3.2799e-05,  3.8421e-05,  4.2863e-05,\n",
      "         4.8843e-05,  5.2910e-05,  5.6700e-05,  7.6009e-05,  8.5675e-05,\n",
      "         9.7648e-05,  1.0194e-04,  1.1060e-04,  1.1308e-04,  1.3127e-04,\n",
      "         1.4186e-04,  1.4723e-04,  1.5757e-04,  1.6989e-04,  1.7675e-04,\n",
      "         1.9413e-04,  2.0276e-04,  2.1086e-04,  2.2059e-04,  2.3151e-04,\n",
      "         2.3492e-04,  2.4953e-04,  2.5283e-04,  2.6774e-04,  3.0209e-04,\n",
      "         3.1301e-04,  3.3009e-04,  3.3632e-04,  3.6449e-04,  3.8662e-04,\n",
      "         3.9976e-04,  4.2544e-04,  4.3132e-04,  4.3823e-04,  4.6283e-04,\n",
      "         4.7107e-04,  4.8052e-04,  5.0603e-04,  5.1955e-04,  5.3013e-04,\n",
      "         5.4882e-04,  5.6552e-04,  5.7170e-04,  5.7834e-04,  6.1367e-04,\n",
      "         6.4448e-04,  6.6010e-04,  6.6600e-04,  7.0813e-04,  7.3689e-04,\n",
      "         7.6661e-04,  7.9806e-04,  8.2513e-04,  8.3571e-04,  8.4559e-04,\n",
      "         8.7283e-04,  8.7558e-04,  8.7778e-04,  9.1475e-04,  9.2872e-04,\n",
      "         9.8292e-04,  1.0090e-03,  1.0145e-03,  1.0415e-03,  1.0595e-03,\n",
      "         1.1018e-03,  1.1049e-03,  1.1125e-03,  1.1575e-03,  1.1803e-03,\n",
      "         1.2464e-03,  1.3013e-03,  1.3226e-03,  1.3536e-03,  1.4228e-03,\n",
      "         1.7555e-03,  1.7988e-03,  1.9518e-03,  2.0841e-03,  2.1270e-03,\n",
      "         2.4059e-03,  2.6016e-03,  3.2227e-03,  3.4011e-03,  3.5319e-03,\n",
      "         3.9102e-03,  4.1187e-03,  4.1420e-03,  4.3115e-03,  5.2273e-03])\n",
      "Task 1 Epoch 15/15 - Loss: 1.0116\n",
      "  Acc (Task 1): 57.05%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: -0.2697, Euclidean: 2.4666\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9916\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9908\n",
      "Before coupling: cosine=-0.2697, distance=2.4666\n",
      "After coupling: cosine=0.9916, distance=0.2051\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.1972\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 26.50%\n",
      "  Unit similarity - Cosine: 0.9053, Euclidean: 0.8108\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.908 \n",
      "bias = 0\n",
      "win = tensor([-4.1169e-03, -2.4151e-03, -1.2177e-03, -1.0696e-03, -6.3846e-04,\n",
      "        -5.6680e-04, -4.2859e-04, -3.5256e-04, -2.7935e-04, -2.4564e-04,\n",
      "        -2.0332e-04, -1.5171e-04, -1.3638e-04, -1.0791e-04, -9.0793e-05,\n",
      "        -7.5571e-05, -7.3603e-05, -5.5931e-05, -4.9942e-05, -4.3106e-05,\n",
      "        -3.1140e-05, -2.7769e-05, -2.5154e-05, -2.5039e-05, -1.9575e-05,\n",
      "        -1.8444e-05, -1.6747e-05, -1.5003e-05, -1.1746e-05, -9.3168e-06,\n",
      "        -8.7631e-06, -7.8970e-06, -6.6157e-06, -6.3292e-06, -5.3777e-06,\n",
      "        -5.2046e-06, -4.7577e-06, -4.1936e-06, -3.4958e-06, -3.1074e-06,\n",
      "        -3.0905e-06, -2.9351e-06, -2.3922e-06, -2.0813e-06, -1.8857e-06,\n",
      "        -1.6321e-06, -1.4161e-06, -1.3398e-06, -1.2682e-06, -1.0965e-06,\n",
      "        -9.2426e-07, -7.4272e-07, -6.3354e-07, -5.5389e-07, -4.7243e-07,\n",
      "        -4.0565e-07, -3.5077e-07, -2.7696e-07, -2.2935e-07, -2.0297e-07,\n",
      "        -1.8181e-07, -1.3423e-07, -1.1343e-07, -8.4915e-08, -7.8387e-08,\n",
      "        -5.8718e-08, -5.4084e-08, -3.6162e-08, -2.9534e-08, -2.2586e-08,\n",
      "        -1.4911e-08, -1.1427e-08, -7.7526e-09, -6.7716e-09, -3.2754e-09,\n",
      "        -3.0511e-09, -2.1627e-09, -1.9934e-09, -1.3187e-09, -5.9673e-10,\n",
      "        -5.1511e-10, -4.9104e-10, -4.0689e-10, -3.4860e-10, -2.7518e-10,\n",
      "        -2.5942e-10, -2.3345e-10, -1.8886e-10, -1.7871e-10, -1.6387e-10,\n",
      "        -1.3952e-10, -1.2972e-10, -1.1541e-10, -9.1309e-11, -8.5030e-11,\n",
      "        -5.0444e-11, -4.0863e-11, -1.8317e-11, -1.6829e-11, -6.4923e-12,\n",
      "         4.1823e-13,  3.2015e-11,  5.4472e-11,  6.6841e-11,  8.4360e-11,\n",
      "         9.9470e-11,  1.1384e-10,  1.2012e-10,  1.4260e-10,  1.5902e-10,\n",
      "         2.0841e-10,  2.2303e-10,  2.4657e-10,  2.9098e-10,  3.4465e-10,\n",
      "         3.6074e-10,  4.8888e-10,  6.0377e-10,  1.1466e-09,  1.5438e-09,\n",
      "         2.0504e-09,  2.7893e-09,  4.3301e-09,  5.0506e-09,  9.5977e-09,\n",
      "         1.1236e-08,  1.3262e-08,  1.4021e-08,  1.9122e-08,  2.6929e-08,\n",
      "         3.1404e-08,  3.6522e-08,  4.3512e-08,  5.0511e-08,  5.1420e-08,\n",
      "         8.2079e-08,  1.0079e-07,  1.0707e-07,  1.8237e-07,  2.2022e-07,\n",
      "         2.4536e-07,  2.9861e-07,  3.3743e-07,  4.3823e-07,  6.0304e-07,\n",
      "         7.3124e-07,  7.7576e-07,  9.6275e-07,  1.1176e-06,  1.3261e-06,\n",
      "         1.5846e-06,  1.8788e-06,  2.2048e-06,  2.5138e-06,  2.8171e-06,\n",
      "         3.3172e-06,  4.0970e-06,  4.7625e-06,  5.5221e-06,  5.6679e-06,\n",
      "         6.2836e-06,  7.0658e-06,  7.6071e-06,  8.6741e-06,  1.0322e-05,\n",
      "         1.0572e-05,  1.2068e-05,  1.5247e-05,  1.6719e-05,  1.9235e-05,\n",
      "         2.1781e-05,  2.5898e-05,  3.0463e-05,  3.2036e-05,  3.4354e-05,\n",
      "         4.3607e-05,  4.6177e-05,  5.3873e-05,  5.7662e-05,  5.9736e-05,\n",
      "         7.1897e-05,  8.1108e-05,  8.5064e-05,  1.0276e-04,  1.2118e-04,\n",
      "         1.3562e-04,  1.3719e-04,  1.7279e-04,  2.1517e-04,  3.6504e-04,\n",
      "         4.1924e-04,  5.4344e-04,  7.9794e-04,  1.3107e-03,  1.3488e-03,\n",
      "         2.0185e-03,  2.7727e-03,  4.1288e-03,  9.5051e-03,  6.2083e-02])\n",
      "wout = tensor([-1.2915e-03, -1.1112e-03, -1.0116e-03, -9.7120e-04, -9.5595e-04,\n",
      "        -9.1193e-04, -8.9405e-04, -8.4437e-04, -8.1493e-04, -7.7868e-04,\n",
      "        -7.0622e-04, -6.8326e-04, -6.2865e-04, -5.8775e-04, -4.8283e-04,\n",
      "        -4.5924e-04, -4.3743e-04, -4.0288e-04, -3.8150e-04, -3.6663e-04,\n",
      "        -3.0898e-04, -2.9131e-04, -2.7629e-04, -2.5579e-04, -2.4570e-04,\n",
      "        -2.3452e-04, -2.2226e-04, -2.1377e-04, -2.1274e-04, -1.9072e-04,\n",
      "        -1.8188e-04, -1.7977e-04, -1.5748e-04, -1.5222e-04, -1.4863e-04,\n",
      "        -1.3587e-04, -1.2605e-04, -1.1956e-04, -1.0650e-04, -9.2799e-05,\n",
      "        -8.4424e-05, -7.7725e-05, -7.7308e-05, -7.2324e-05, -7.0333e-05,\n",
      "        -6.9587e-05, -6.5902e-05, -6.0179e-05, -5.7892e-05, -5.7522e-05,\n",
      "        -5.3029e-05, -5.0158e-05, -4.9955e-05, -4.3278e-05, -3.8474e-05,\n",
      "        -3.4439e-05, -2.9422e-05, -2.6568e-05, -2.5948e-05, -2.5547e-05,\n",
      "        -2.0808e-05, -2.0140e-05, -1.9550e-05, -1.5911e-05, -1.3944e-05,\n",
      "        -1.2641e-05, -1.2482e-05, -1.2184e-05, -1.1385e-05, -9.3432e-06,\n",
      "        -7.2676e-06, -5.6043e-06, -4.1869e-06, -3.6974e-06, -3.4580e-06,\n",
      "        -3.4411e-06, -2.1365e-06, -1.4933e-06, -5.8122e-07, -3.3789e-07,\n",
      "        -3.3490e-07, -2.9506e-07, -2.8896e-07, -2.6617e-07, -2.2852e-07,\n",
      "        -2.2720e-07, -1.8915e-07, -4.4004e-08, -1.0205e-08, -8.1160e-09,\n",
      "        -2.0695e-09, -9.1607e-10, -3.5520e-10, -2.9490e-10, -6.6457e-11,\n",
      "        -6.4106e-11, -3.9308e-11, -3.5555e-11, -2.0905e-11, -8.0726e-12,\n",
      "         5.8273e-12,  1.4551e-11,  1.6040e-11,  1.7157e-11,  2.1388e-11,\n",
      "         2.8559e-11,  3.3737e-11,  4.1112e-11,  9.4739e-11,  1.0550e-10,\n",
      "         1.8937e-10,  3.5688e-10,  6.4454e-10,  9.9152e-10,  1.1677e-09,\n",
      "         1.5291e-09,  2.1011e-09,  2.7414e-09,  6.7887e-09,  9.2733e-09,\n",
      "         9.9042e-09,  8.4461e-08,  1.0388e-07,  1.4374e-07,  2.2683e-07,\n",
      "         8.0828e-07,  8.8119e-07,  9.6920e-07,  1.3675e-06,  1.6443e-06,\n",
      "         2.0784e-06,  2.8441e-06,  3.0969e-06,  3.4697e-06,  3.9713e-06,\n",
      "         6.5034e-06,  7.1292e-06,  8.8130e-06,  9.1892e-06,  9.6058e-06,\n",
      "         1.0100e-05,  1.1468e-05,  1.2878e-05,  1.3563e-05,  1.4101e-05,\n",
      "         1.5930e-05,  1.7063e-05,  1.7907e-05,  1.8801e-05,  2.2802e-05,\n",
      "         2.5046e-05,  2.6078e-05,  2.6921e-05,  2.8039e-05,  3.0217e-05,\n",
      "         3.1481e-05,  3.2229e-05,  3.3301e-05,  3.6164e-05,  4.2981e-05,\n",
      "         4.7588e-05,  5.2638e-05,  5.8643e-05,  5.9223e-05,  6.0411e-05,\n",
      "         6.2486e-05,  6.4883e-05,  6.9354e-05,  7.0254e-05,  7.0436e-05,\n",
      "         7.6891e-05,  8.1124e-05,  8.4972e-05,  8.8441e-05,  8.9938e-05,\n",
      "         9.2512e-05,  1.0214e-04,  1.0467e-04,  1.1303e-04,  1.2157e-04,\n",
      "         1.4763e-04,  1.5306e-04,  1.5800e-04,  1.7484e-04,  1.9481e-04,\n",
      "         2.1413e-04,  2.3793e-04,  2.5378e-04,  2.7926e-04,  2.8189e-04,\n",
      "         3.0437e-04,  4.0308e-04,  4.1675e-04,  4.8507e-04,  5.2551e-04,\n",
      "         6.1465e-04,  6.6282e-04,  7.1839e-04,  1.1704e-03,  1.2698e-03])\n",
      "Task 2 Epoch 2/5 - Loss: 1.7606\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 28.70%\n",
      "  Unit similarity - Cosine: 0.8876, Euclidean: 0.9303\n",
      "Task 2 Epoch 3/5 - Loss: 1.7264\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 30.07%\n",
      "  Unit similarity - Cosine: 0.9223, Euclidean: 0.7998\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.932 \n",
      "bias = 0\n",
      "win = tensor([-6.0711e-03, -3.8420e-03, -2.6836e-03, -1.9769e-03, -1.2872e-03,\n",
      "        -9.8043e-04, -6.9265e-04, -6.1603e-04, -5.4345e-04, -4.3792e-04,\n",
      "        -2.4225e-04, -2.2738e-04, -1.9022e-04, -1.3595e-04, -1.1353e-04,\n",
      "        -1.0305e-04, -7.7378e-05, -6.0600e-05, -5.8611e-05, -5.3728e-05,\n",
      "        -3.8205e-05, -3.6671e-05, -3.2652e-05, -2.6099e-05, -2.4720e-05,\n",
      "        -1.8844e-05, -1.5050e-05, -1.4810e-05, -1.0435e-05, -8.6185e-06,\n",
      "        -7.5747e-06, -6.5176e-06, -6.0373e-06, -4.6733e-06, -4.2535e-06,\n",
      "        -3.6615e-06, -2.3969e-06, -2.2096e-06, -1.8740e-06, -1.8036e-06,\n",
      "        -1.5295e-06, -1.2976e-06, -9.9432e-07, -8.6736e-07, -7.9604e-07,\n",
      "        -6.6708e-07, -5.4720e-07, -5.3886e-07, -3.4220e-07, -2.9114e-07,\n",
      "        -1.8603e-07, -1.4575e-07, -1.1620e-07, -8.1790e-08, -7.4075e-08,\n",
      "        -6.7605e-08, -4.9239e-08, -3.9743e-08, -3.6716e-08, -2.9086e-08,\n",
      "        -2.3234e-08, -1.4786e-08, -1.0403e-08, -6.5469e-09, -5.6587e-09,\n",
      "        -3.4363e-09, -1.6397e-09, -1.5101e-09, -1.0185e-09, -4.8738e-10,\n",
      "        -3.8932e-10, -3.2693e-10, -2.8914e-10, -2.6588e-10, -2.4390e-10,\n",
      "        -2.0518e-10, -1.9236e-10, -1.8093e-10, -1.6357e-10, -1.5106e-10,\n",
      "        -1.3367e-10, -1.2895e-10, -1.0078e-10, -9.5142e-11, -8.2929e-11,\n",
      "        -7.3442e-11, -4.7467e-11, -4.5451e-11, -2.6042e-11, -1.6075e-11,\n",
      "        -6.2247e-12,  5.6060e-12,  1.2013e-11,  1.6890e-11,  3.0253e-11,\n",
      "         3.3616e-11,  3.8207e-11,  6.3820e-11,  6.9484e-11,  8.8283e-11,\n",
      "         9.7761e-11,  1.0146e-10,  1.2539e-10,  1.4070e-10,  1.5244e-10,\n",
      "         1.6291e-10,  1.8419e-10,  1.9458e-10,  1.9995e-10,  2.1438e-10,\n",
      "         2.7709e-10,  2.9170e-10,  3.2200e-10,  3.4948e-10,  4.3555e-10,\n",
      "         5.2810e-10,  7.9581e-10,  1.2450e-09,  1.8159e-09,  2.5984e-09,\n",
      "         3.5304e-09,  4.2484e-09,  4.8190e-09,  5.4271e-09,  6.9352e-09,\n",
      "         1.0663e-08,  1.2504e-08,  1.3692e-08,  1.8079e-08,  2.4053e-08,\n",
      "         2.9359e-08,  3.2959e-08,  4.0171e-08,  4.7211e-08,  6.4866e-08,\n",
      "         8.4533e-08,  9.3128e-08,  1.2804e-07,  1.6992e-07,  1.9447e-07,\n",
      "         2.5227e-07,  3.0697e-07,  3.9046e-07,  4.2009e-07,  4.5798e-07,\n",
      "         5.6000e-07,  6.9733e-07,  8.4187e-07,  8.6183e-07,  1.0171e-06,\n",
      "         1.1206e-06,  1.3926e-06,  1.4490e-06,  2.3590e-06,  2.6399e-06,\n",
      "         2.7517e-06,  3.2822e-06,  3.4317e-06,  5.1595e-06,  5.4351e-06,\n",
      "         6.0250e-06,  6.3571e-06,  7.0175e-06,  9.9257e-06,  1.0788e-05,\n",
      "         1.2497e-05,  1.4046e-05,  1.7669e-05,  1.8100e-05,  2.1130e-05,\n",
      "         2.4863e-05,  2.8318e-05,  3.2655e-05,  4.1089e-05,  4.3933e-05,\n",
      "         4.5355e-05,  4.9506e-05,  5.6195e-05,  6.2664e-05,  7.2225e-05,\n",
      "         7.6092e-05,  9.1744e-05,  1.2074e-04,  1.4571e-04,  1.5212e-04,\n",
      "         2.0167e-04,  2.0526e-04,  2.9749e-04,  3.3728e-04,  4.4718e-04,\n",
      "         5.4817e-04,  6.1353e-04,  8.7383e-04,  1.1608e-03,  1.3421e-03,\n",
      "         1.7317e-03,  2.4277e-03,  4.6965e-03,  4.9129e-03,  4.9404e-02])\n",
      "wout = tensor([-4.4803e-04, -4.2717e-04, -3.9149e-04, -2.6349e-04, -2.3551e-04,\n",
      "        -2.2822e-04, -1.8832e-04, -1.7738e-04, -1.7663e-04, -1.6975e-04,\n",
      "        -1.3940e-04, -1.2853e-04, -1.2527e-04, -1.2291e-04, -1.1805e-04,\n",
      "        -9.4067e-05, -8.1345e-05, -7.3678e-05, -7.2616e-05, -7.0220e-05,\n",
      "        -6.9230e-05, -6.5324e-05, -6.2101e-05, -6.0726e-05, -5.1608e-05,\n",
      "        -4.7397e-05, -4.5479e-05, -4.4921e-05, -3.9552e-05, -3.5935e-05,\n",
      "        -3.4414e-05, -3.4152e-05, -3.2183e-05, -3.0779e-05, -3.0095e-05,\n",
      "        -2.8255e-05, -2.7069e-05, -2.6377e-05, -2.5782e-05, -2.5394e-05,\n",
      "        -2.3784e-05, -2.2782e-05, -2.2591e-05, -2.1119e-05, -1.9624e-05,\n",
      "        -1.9206e-05, -1.8606e-05, -1.4858e-05, -1.4753e-05, -1.3791e-05,\n",
      "        -1.3584e-05, -1.2342e-05, -1.1356e-05, -1.1008e-05, -1.0211e-05,\n",
      "        -9.8405e-06, -9.1502e-06, -7.8625e-06, -7.6435e-06, -7.4869e-06,\n",
      "        -7.0640e-06, -6.8961e-06, -6.5368e-06, -5.8446e-06, -5.6883e-06,\n",
      "        -5.5280e-06, -5.3104e-06, -5.2305e-06, -5.1398e-06, -4.4518e-06,\n",
      "        -4.2504e-06, -4.1149e-06, -3.8042e-06, -3.7365e-06, -3.7017e-06,\n",
      "        -3.3495e-06, -3.2426e-06, -3.0930e-06, -2.9592e-06, -2.5562e-06,\n",
      "        -2.5335e-06, -2.4133e-06, -2.1923e-06, -1.7341e-06, -1.6960e-06,\n",
      "        -1.6635e-06, -1.5053e-06, -1.3612e-06, -1.1056e-06, -1.0379e-06,\n",
      "        -1.0121e-06, -9.3117e-07, -8.3402e-07, -6.6783e-07, -5.9497e-07,\n",
      "        -5.6830e-07, -3.2052e-07, -3.1535e-07, -2.3818e-07, -1.7196e-07,\n",
      "        -1.4964e-07, -1.1643e-07, -7.9389e-08, -7.2404e-08, -4.2224e-08,\n",
      "        -3.6146e-08, -7.7226e-09, -1.2567e-09, -1.2555e-10,  2.4696e-10,\n",
      "         1.2615e-08,  2.0150e-08,  8.4537e-08,  9.5762e-08,  1.2302e-07,\n",
      "         3.1495e-07,  3.9312e-07,  4.0263e-07,  4.8332e-07,  5.5373e-07,\n",
      "         6.6586e-07,  7.4916e-07,  7.5777e-07,  7.6877e-07,  7.7402e-07,\n",
      "         8.1789e-07,  9.8874e-07,  1.0450e-06,  1.8811e-06,  2.1125e-06,\n",
      "         3.1542e-06,  3.4146e-06,  3.4363e-06,  3.8897e-06,  4.1314e-06,\n",
      "         4.4392e-06,  4.4968e-06,  4.5837e-06,  4.8270e-06,  4.9613e-06,\n",
      "         5.2922e-06,  6.0337e-06,  7.2136e-06,  7.7108e-06,  8.0508e-06,\n",
      "         8.7142e-06,  9.5658e-06,  1.0105e-05,  1.0455e-05,  1.1047e-05,\n",
      "         1.1693e-05,  1.2459e-05,  1.4040e-05,  1.4626e-05,  1.5160e-05,\n",
      "         1.5781e-05,  1.6518e-05,  1.7820e-05,  1.9277e-05,  2.0232e-05,\n",
      "         2.1503e-05,  2.1637e-05,  2.2215e-05,  2.5909e-05,  2.6831e-05,\n",
      "         2.7443e-05,  3.1040e-05,  3.2462e-05,  3.3148e-05,  3.5486e-05,\n",
      "         3.9073e-05,  3.9357e-05,  4.4353e-05,  4.7123e-05,  5.3933e-05,\n",
      "         6.3384e-05,  6.5088e-05,  6.8189e-05,  7.1586e-05,  7.6233e-05,\n",
      "         7.9285e-05,  8.7156e-05,  9.1488e-05,  1.0036e-04,  1.0977e-04,\n",
      "         1.1968e-04,  1.3109e-04,  1.3526e-04,  1.3653e-04,  1.4754e-04,\n",
      "         1.5573e-04,  2.1381e-04,  2.1644e-04,  2.4532e-04,  2.7938e-04,\n",
      "         3.0498e-04,  3.6895e-04,  3.7469e-04,  4.2857e-04,  1.4899e-03])\n",
      "Task 2 Epoch 4/5 - Loss: 1.7124\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 30.18%\n",
      "  Unit similarity - Cosine: 0.8910, Euclidean: 1.0015\n",
      "Task 2 Epoch 5/5 - Loss: 1.6853\n",
      "  Acc (Task 1): 0.03%, Acc (Task 2): 29.98%\n",
      "  Unit similarity - Cosine: 0.8850, Euclidean: 1.0681\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9916 (after coupling) -> 0.8850 (final)\n",
      "Euclidean distance: 0.2051 (after coupling) -> 1.0681 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 0.00% (before Task 2) -> 0.03% (after Task 2)\n",
      "No forgetting observed. Task 1 performance improved by 0.03%.\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 15, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0734, Euclidean distance: 0.8403\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/15 - Loss: 1.7567\n",
      "  Acc (Task 1): 43.80%, Acc (Task 2): 0.80%\n",
      "  Unit similarity - Cosine: 0.4466, Euclidean: 1.1882\n",
      "Task 1 Epoch 2/15 - Loss: 1.2134\n",
      "  Acc (Task 1): 34.23%, Acc (Task 2): 1.02%\n",
      "  Unit similarity - Cosine: 0.5490, Euclidean: 1.1092\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 2, step 100, cos_sim = 0.602 \n",
      "bias = 0\n",
      "win = tensor([-4.2419e-02, -4.4072e-03, -2.4737e-03, -1.0417e-03, -7.2276e-04,\n",
      "        -5.3967e-04, -4.9385e-04, -1.8067e-04, -1.5735e-04, -1.2798e-04,\n",
      "        -1.0243e-04, -9.3883e-05, -7.9682e-05, -7.2780e-05, -5.2412e-05,\n",
      "        -4.7824e-05, -4.6563e-05, -3.3828e-05, -3.0137e-05, -2.6521e-05,\n",
      "        -2.4508e-05, -2.2199e-05, -1.9014e-05, -1.6123e-05, -1.3328e-05,\n",
      "        -1.1953e-05, -1.1105e-05, -8.7221e-06, -8.0571e-06, -7.1374e-06,\n",
      "        -6.9491e-06, -6.3765e-06, -5.0567e-06, -4.4030e-06, -4.0733e-06,\n",
      "        -3.7797e-06, -3.5373e-06, -3.2793e-06, -3.1222e-06, -2.8220e-06,\n",
      "        -2.4031e-06, -2.1169e-06, -1.7776e-06, -1.7199e-06, -1.6213e-06,\n",
      "        -1.4342e-06, -1.1435e-06, -1.1369e-06, -1.0439e-06, -8.9050e-07,\n",
      "        -7.6242e-07, -7.0128e-07, -6.1922e-07, -5.2178e-07, -4.2422e-07,\n",
      "        -3.9688e-07, -3.5628e-07, -2.9831e-07, -2.7938e-07, -2.4678e-07,\n",
      "        -2.3005e-07, -2.0822e-07, -2.0082e-07, -1.8556e-07, -1.4054e-07,\n",
      "        -1.3461e-07, -1.1082e-07, -1.0291e-07, -9.4669e-08, -8.6930e-08,\n",
      "        -6.5643e-08, -5.3648e-08, -5.2012e-08, -4.4673e-08, -4.0409e-08,\n",
      "        -3.2822e-08, -2.5213e-08, -1.9492e-08, -1.6855e-08, -1.3556e-08,\n",
      "        -1.2536e-08, -9.2188e-09, -6.8623e-09, -6.5857e-09, -5.8724e-09,\n",
      "        -4.8403e-09, -3.6521e-09, -2.7030e-09, -2.4289e-09, -1.6442e-09,\n",
      "        -1.2006e-09, -1.0894e-09, -8.9826e-10, -6.1609e-10, -5.6558e-10,\n",
      "        -5.3234e-10, -4.9571e-10, -3.8643e-10, -3.5933e-10, -3.1966e-10,\n",
      "        -2.6924e-10, -2.1895e-10, -2.0667e-10, -1.7641e-10, -1.5415e-10,\n",
      "        -1.3028e-10, -1.0550e-10, -8.7934e-11, -7.0204e-11, -5.4403e-11,\n",
      "        -2.9808e-11, -1.2994e-11,  3.0581e-13,  1.0167e-11,  3.8953e-11,\n",
      "         4.2808e-11,  7.0222e-11,  9.0853e-11,  9.6673e-11,  1.2139e-10,\n",
      "         1.2838e-10,  1.5460e-10,  1.7960e-10,  2.0091e-10,  2.2128e-10,\n",
      "         2.3903e-10,  2.7979e-10,  3.5623e-10,  3.7278e-10,  4.2706e-10,\n",
      "         5.1484e-10,  6.5386e-10,  8.4356e-10,  1.0249e-09,  1.2860e-09,\n",
      "         1.6310e-09,  2.1740e-09,  2.8214e-09,  3.9526e-09,  6.8793e-09,\n",
      "         7.5034e-09,  1.0624e-08,  1.1153e-08,  1.4231e-08,  1.6981e-08,\n",
      "         2.1004e-08,  2.6161e-08,  4.0087e-08,  4.7841e-08,  5.4420e-08,\n",
      "         5.9548e-08,  7.0891e-08,  8.4821e-08,  9.9000e-08,  1.3605e-07,\n",
      "         1.4482e-07,  1.6458e-07,  1.7370e-07,  2.0442e-07,  2.2321e-07,\n",
      "         2.3089e-07,  2.9029e-07,  3.1690e-07,  3.8083e-07,  4.2479e-07,\n",
      "         4.4561e-07,  5.4124e-07,  5.5588e-07,  6.2681e-07,  8.6047e-07,\n",
      "         1.2044e-06,  1.2188e-06,  1.4104e-06,  1.5117e-06,  1.9614e-06,\n",
      "         2.7195e-06,  3.1308e-06,  3.8877e-06,  4.7307e-06,  5.5786e-06,\n",
      "         7.0124e-06,  1.0175e-05,  1.4513e-05,  1.6937e-05,  1.9013e-05,\n",
      "         2.0500e-05,  2.8108e-05,  2.8339e-05,  3.7502e-05,  4.1692e-05,\n",
      "         4.4923e-05,  4.9300e-05,  5.6000e-05,  9.5634e-05,  1.6885e-04,\n",
      "         1.9198e-04,  3.8321e-04,  9.1032e-04,  2.3783e-03,  4.0890e-03])\n",
      "wout = tensor([-3.7168e-04, -2.9044e-04, -2.6347e-04, -2.5364e-04, -2.4269e-04,\n",
      "        -2.3708e-04, -2.1385e-04, -1.6564e-04, -1.5750e-04, -1.5033e-04,\n",
      "        -1.4830e-04, -1.3379e-04, -1.2000e-04, -1.1608e-04, -1.1364e-04,\n",
      "        -1.0969e-04, -1.0300e-04, -9.9313e-05, -9.8308e-05, -8.8581e-05,\n",
      "        -8.1689e-05, -7.9816e-05, -7.9057e-05, -7.5910e-05, -7.3522e-05,\n",
      "        -7.1783e-05, -7.0358e-05, -6.8115e-05, -6.7580e-05, -6.6929e-05,\n",
      "        -6.5417e-05, -6.5204e-05, -6.4595e-05, -6.3354e-05, -6.2908e-05,\n",
      "        -6.1484e-05, -5.9755e-05, -5.7293e-05, -5.3957e-05, -5.2994e-05,\n",
      "        -5.1753e-05, -5.0882e-05, -4.9438e-05, -4.9165e-05, -4.5895e-05,\n",
      "        -4.3977e-05, -4.1148e-05, -4.0632e-05, -3.9930e-05, -3.9234e-05,\n",
      "        -3.8041e-05, -3.7242e-05, -3.7066e-05, -3.4379e-05, -3.3218e-05,\n",
      "        -3.2998e-05, -3.0908e-05, -2.9939e-05, -2.8820e-05, -2.8571e-05,\n",
      "        -2.7094e-05, -2.6497e-05, -2.6039e-05, -2.5358e-05, -2.3909e-05,\n",
      "        -2.1908e-05, -2.0226e-05, -1.9527e-05, -1.9297e-05, -1.8511e-05,\n",
      "        -1.8004e-05, -1.7023e-05, -1.6912e-05, -1.6045e-05, -1.4853e-05,\n",
      "        -1.2711e-05, -1.1991e-05, -1.1887e-05, -1.1448e-05, -1.0925e-05,\n",
      "        -1.0509e-05, -9.9057e-06, -8.9257e-06, -8.5128e-06, -7.3844e-06,\n",
      "        -6.8100e-06, -6.2908e-06, -5.4591e-06, -5.3367e-06, -4.5754e-06,\n",
      "        -3.9559e-06, -2.8183e-06, -4.4080e-07, -2.0819e-07,  8.7877e-07,\n",
      "         1.2765e-06,  1.7872e-06,  2.9560e-06,  3.2961e-06,  4.3024e-06,\n",
      "         5.7140e-06,  6.5966e-06,  6.9972e-06,  8.0973e-06,  8.7115e-06,\n",
      "         9.9459e-06,  1.0946e-05,  1.2016e-05,  1.2981e-05,  1.3349e-05,\n",
      "         1.3671e-05,  1.4595e-05,  1.6029e-05,  1.6500e-05,  1.6654e-05,\n",
      "         1.6763e-05,  1.7414e-05,  1.8189e-05,  1.8394e-05,  1.9682e-05,\n",
      "         2.0689e-05,  2.1681e-05,  2.2651e-05,  2.3481e-05,  2.4156e-05,\n",
      "         2.5566e-05,  2.6409e-05,  2.7127e-05,  2.8628e-05,  2.9033e-05,\n",
      "         3.0471e-05,  3.1067e-05,  3.2224e-05,  3.3023e-05,  3.5073e-05,\n",
      "         3.6158e-05,  3.7693e-05,  3.8535e-05,  3.9215e-05,  4.1674e-05,\n",
      "         4.4083e-05,  4.4484e-05,  4.5661e-05,  4.6941e-05,  4.8630e-05,\n",
      "         4.9904e-05,  5.2359e-05,  5.2673e-05,  5.4453e-05,  5.4617e-05,\n",
      "         5.5258e-05,  5.7437e-05,  5.9095e-05,  6.0580e-05,  6.1276e-05,\n",
      "         6.3407e-05,  6.5779e-05,  6.8769e-05,  7.0289e-05,  7.1199e-05,\n",
      "         7.3351e-05,  7.4358e-05,  7.6611e-05,  7.7894e-05,  7.8428e-05,\n",
      "         7.9553e-05,  8.2441e-05,  8.3168e-05,  8.5335e-05,  8.8730e-05,\n",
      "         9.3729e-05,  9.7277e-05,  9.8645e-05,  1.0116e-04,  1.0876e-04,\n",
      "         1.0950e-04,  1.1237e-04,  1.1502e-04,  1.1795e-04,  1.2025e-04,\n",
      "         1.2641e-04,  1.2920e-04,  1.3886e-04,  1.4230e-04,  1.4425e-04,\n",
      "         1.5019e-04,  1.5840e-04,  1.6563e-04,  1.7570e-04,  1.7739e-04,\n",
      "         1.9117e-04,  2.0068e-04,  2.1492e-04,  2.5782e-04,  2.8199e-04,\n",
      "         2.9602e-04,  3.2447e-04,  4.0596e-04,  5.1927e-04,  6.4800e-04])\n",
      "Task 1 Epoch 3/15 - Loss: 1.4024\n",
      "  Acc (Task 1): 44.95%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6447, Euclidean: 1.0979\n",
      "Task 1 Epoch 4/15 - Loss: 1.1575\n",
      "  Acc (Task 1): 50.58%, Acc (Task 2): 0.32%\n",
      "  Unit similarity - Cosine: 0.6957, Euclidean: 1.1776\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 4, step 200, cos_sim = 0.615 \n",
      "bias = 0\n",
      "win = tensor([-4.3353e-02, -1.7881e-02, -3.8983e-05, -6.7658e-06, -2.5610e-06,\n",
      "        -1.6740e-06, -7.2022e-07, -5.3781e-08, -4.5873e-08, -1.1852e-08,\n",
      "        -6.2396e-09, -5.8346e-09, -5.0933e-09, -4.5299e-09, -4.2633e-09,\n",
      "        -4.0770e-09, -3.9546e-09, -3.8685e-09, -3.5364e-09, -3.4425e-09,\n",
      "        -3.3853e-09, -3.1876e-09, -3.1396e-09, -3.0848e-09, -2.9651e-09,\n",
      "        -2.9440e-09, -2.8652e-09, -2.8118e-09, -2.7581e-09, -2.6516e-09,\n",
      "        -2.6123e-09, -2.5572e-09, -2.4842e-09, -2.4512e-09, -2.4067e-09,\n",
      "        -2.3141e-09, -2.2457e-09, -2.1987e-09, -2.1224e-09, -2.0890e-09,\n",
      "        -2.0002e-09, -1.9759e-09, -1.9246e-09, -1.8870e-09, -1.8461e-09,\n",
      "        -1.7870e-09, -1.7361e-09, -1.7083e-09, -1.6712e-09, -1.6336e-09,\n",
      "        -1.6078e-09, -1.5872e-09, -1.4962e-09, -1.4868e-09, -1.4564e-09,\n",
      "        -1.4362e-09, -1.4218e-09, -1.3675e-09, -1.3388e-09, -1.2559e-09,\n",
      "        -1.2378e-09, -1.1330e-09, -1.0945e-09, -1.0708e-09, -1.0483e-09,\n",
      "        -1.0146e-09, -9.7920e-10, -9.5778e-10, -9.4326e-10, -8.8510e-10,\n",
      "        -8.1177e-10, -7.8288e-10, -7.4252e-10, -7.1167e-10, -6.9144e-10,\n",
      "        -6.6981e-10, -6.5638e-10, -6.0517e-10, -5.4524e-10, -5.1851e-10,\n",
      "        -5.1120e-10, -4.6447e-10, -4.3213e-10, -3.8502e-10, -3.2946e-10,\n",
      "        -2.8833e-10, -2.7491e-10, -2.5141e-10, -1.9320e-10, -1.8156e-10,\n",
      "        -1.2567e-10, -1.1507e-10, -8.0448e-11, -6.0596e-11, -1.7312e-11,\n",
      "         3.5526e-11,  6.0608e-11,  9.8081e-11,  1.4741e-10,  1.8537e-10,\n",
      "         2.2050e-10,  2.2747e-10,  3.1381e-10,  3.3219e-10,  3.5101e-10,\n",
      "         4.0093e-10,  4.4893e-10,  4.5159e-10,  5.3627e-10,  5.4618e-10,\n",
      "         5.7644e-10,  5.9882e-10,  6.2416e-10,  6.4505e-10,  7.0584e-10,\n",
      "         7.6793e-10,  8.1008e-10,  8.3339e-10,  8.5137e-10,  8.8147e-10,\n",
      "         9.0435e-10,  9.4138e-10,  9.5382e-10,  1.0247e-09,  1.0720e-09,\n",
      "         1.0844e-09,  1.1094e-09,  1.1759e-09,  1.1803e-09,  1.2658e-09,\n",
      "         1.2967e-09,  1.3326e-09,  1.3579e-09,  1.3994e-09,  1.4318e-09,\n",
      "         1.4599e-09,  1.5303e-09,  1.5504e-09,  1.6303e-09,  1.7004e-09,\n",
      "         1.7093e-09,  1.7459e-09,  1.7710e-09,  1.8389e-09,  1.8487e-09,\n",
      "         1.9400e-09,  1.9930e-09,  2.0351e-09,  2.0529e-09,  2.1051e-09,\n",
      "         2.1874e-09,  2.2349e-09,  2.2555e-09,  2.2910e-09,  2.3654e-09,\n",
      "         2.4157e-09,  2.5063e-09,  2.6100e-09,  2.6546e-09,  2.7099e-09,\n",
      "         2.7259e-09,  2.8013e-09,  2.8620e-09,  2.9279e-09,  3.0767e-09,\n",
      "         3.1016e-09,  3.2368e-09,  3.3188e-09,  3.3570e-09,  3.4538e-09,\n",
      "         3.5069e-09,  3.6404e-09,  3.7401e-09,  4.0240e-09,  4.3882e-09,\n",
      "         4.5980e-09,  6.3768e-09,  7.5250e-09,  3.2426e-08,  3.4805e-08,\n",
      "         3.9892e-08,  1.0089e-07,  1.7159e-07,  1.9675e-07,  4.7028e-07,\n",
      "         6.4379e-07,  3.3176e-06,  6.4211e-06,  7.6847e-06,  2.8705e-05,\n",
      "         6.7068e-05,  2.2685e-04,  2.9317e-04,  5.6293e-04,  1.1680e-03,\n",
      "         2.1349e-03,  4.1457e-03,  1.1332e-02,  5.2842e-02,  4.7006e-01])\n",
      "wout = tensor([-1.0619e-02, -5.1540e-03, -4.1724e-03, -4.1014e-03, -3.3852e-03,\n",
      "        -2.9199e-03, -2.8902e-03, -2.5059e-03, -2.2981e-03, -2.2298e-03,\n",
      "        -1.9410e-03, -1.8508e-03, -1.6235e-03, -1.5024e-03, -1.4409e-03,\n",
      "        -1.4001e-03, -1.3234e-03, -1.1425e-03, -9.7775e-04, -8.4573e-04,\n",
      "        -8.0063e-04, -7.9590e-04, -7.1372e-04, -6.9073e-04, -4.1214e-04,\n",
      "        -2.9649e-04, -2.1615e-04, -1.8844e-04, -1.0690e-04, -9.6747e-05,\n",
      "        -8.9978e-05, -8.2284e-05, -4.0553e-05, -3.5503e-05, -3.4188e-05,\n",
      "        -2.7481e-05, -2.1739e-05, -1.6879e-05, -1.3897e-05, -9.9429e-06,\n",
      "        -7.6972e-06, -7.3811e-06, -4.6991e-06, -4.6510e-06, -4.2333e-06,\n",
      "        -3.3525e-06, -2.0773e-06, -1.6233e-06, -1.5078e-06, -1.3416e-06,\n",
      "        -1.2379e-06, -1.2200e-06, -1.0725e-06, -9.1439e-07, -7.6024e-07,\n",
      "        -7.0295e-07, -6.8817e-07, -6.1757e-07, -6.1450e-07, -6.0379e-07,\n",
      "        -5.2632e-07, -5.2243e-07, -5.1594e-07, -3.4241e-07, -3.2573e-07,\n",
      "        -3.2525e-07, -3.0675e-07, -2.0748e-07, -1.8445e-07, -1.6659e-07,\n",
      "        -1.5295e-07, -1.2304e-07, -1.0701e-07, -1.0406e-07, -7.1479e-08,\n",
      "        -4.0923e-08, -3.2318e-08, -3.2210e-08, -3.0648e-08, -3.0146e-08,\n",
      "        -2.8415e-08, -2.2604e-08, -1.8152e-08, -1.4662e-08, -1.3555e-08,\n",
      "        -1.1751e-08, -9.7819e-09, -8.5321e-09, -8.2559e-09, -7.9890e-09,\n",
      "        -7.2396e-09, -4.7980e-09, -4.7348e-09, -4.1373e-09, -2.8994e-09,\n",
      "        -2.7106e-09, -1.9786e-09, -9.4650e-10, -1.6465e-11,  7.2854e-14,\n",
      "         7.2172e-11,  9.9183e-11,  6.8016e-09,  1.6294e-08,  1.9725e-08,\n",
      "         2.2080e-08,  2.7055e-08,  2.7634e-08,  3.6926e-08,  3.7171e-08,\n",
      "         4.0295e-08,  4.9906e-08,  5.8547e-08,  6.5892e-08,  7.0867e-08,\n",
      "         7.9014e-08,  7.9781e-08,  2.8803e-07,  5.1901e-07,  5.5515e-07,\n",
      "         6.7228e-07,  8.4746e-07,  9.4357e-07,  1.0805e-06,  1.1395e-06,\n",
      "         1.1981e-06,  1.9314e-06,  3.9652e-06,  4.3686e-06,  4.5852e-06,\n",
      "         5.7094e-06,  7.3332e-06,  8.0084e-06,  8.4972e-06,  1.0505e-05,\n",
      "         1.1976e-05,  1.5654e-05,  1.7920e-05,  1.8260e-05,  2.1859e-05,\n",
      "         2.2246e-05,  2.4405e-05,  2.6069e-05,  2.8825e-05,  2.9389e-05,\n",
      "         5.0205e-05,  5.4969e-05,  5.8928e-05,  6.0476e-05,  6.4298e-05,\n",
      "         9.1992e-05,  9.5124e-05,  1.7740e-04,  2.6699e-04,  3.0354e-04,\n",
      "         4.6486e-04,  5.6535e-04,  6.0004e-04,  7.6365e-04,  8.0554e-04,\n",
      "         8.2452e-04,  8.4231e-04,  8.6702e-04,  9.5665e-04,  1.0371e-03,\n",
      "         1.0643e-03,  1.1020e-03,  1.2108e-03,  1.2684e-03,  1.6626e-03,\n",
      "         1.7016e-03,  1.7410e-03,  1.7537e-03,  1.8892e-03,  1.9614e-03,\n",
      "         1.9862e-03,  2.0123e-03,  2.0445e-03,  2.0861e-03,  2.2163e-03,\n",
      "         2.2691e-03,  2.6285e-03,  2.6581e-03,  2.9318e-03,  2.9880e-03,\n",
      "         3.0374e-03,  3.1911e-03,  3.3357e-03,  3.3646e-03,  3.5117e-03,\n",
      "         3.5961e-03,  3.8752e-03,  4.0942e-03,  4.1912e-03,  4.5498e-03,\n",
      "         4.8039e-03,  5.2824e-03,  5.8960e-03,  6.4412e-03,  1.0037e-02])\n",
      "Task 1 Epoch 5/15 - Loss: 1.1326\n",
      "  Acc (Task 1): 52.52%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6146, Euclidean: 1.4921\n",
      "Task 1 Epoch 6/15 - Loss: 1.1181\n",
      "  Acc (Task 1): 53.00%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.5816, Euclidean: 1.7026\n",
      "Task 1 Epoch 7/15 - Loss: 1.1041\n",
      "  Acc (Task 1): 48.70%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6769, Euclidean: 1.5889\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 7, step 300, cos_sim = 0.691 \n",
      "bias = 0\n",
      "win = tensor([-9.6282e-03, -3.3784e-03, -1.7215e-03, -1.0284e-03, -7.6070e-04,\n",
      "        -5.3043e-04, -4.6201e-04, -4.1677e-04, -3.7575e-04, -3.2976e-04,\n",
      "        -2.7351e-04, -2.1774e-04, -1.3546e-04, -1.1625e-04, -9.8753e-05,\n",
      "        -7.9696e-05, -7.2565e-05, -5.9923e-05, -5.3155e-05, -4.7147e-05,\n",
      "        -4.6306e-05, -3.6259e-05, -3.1571e-05, -2.8951e-05, -2.4523e-05,\n",
      "        -2.1557e-05, -1.6904e-05, -1.5912e-05, -1.3791e-05, -9.2574e-06,\n",
      "        -8.4998e-06, -5.8668e-06, -5.3310e-06, -4.9818e-06, -4.5953e-06,\n",
      "        -3.9557e-06, -3.6059e-06, -3.1284e-06, -2.6382e-06, -2.4271e-06,\n",
      "        -2.0089e-06, -1.7378e-06, -1.6164e-06, -1.2977e-06, -1.1791e-06,\n",
      "        -8.3756e-07, -7.1821e-07, -6.9604e-07, -5.6326e-07, -4.6340e-07,\n",
      "        -3.7905e-07, -3.0942e-07, -2.8107e-07, -2.0457e-07, -1.9130e-07,\n",
      "        -1.6211e-07, -1.1969e-07, -8.5350e-08, -6.9887e-08, -6.0855e-08,\n",
      "        -5.3006e-08, -4.2911e-08, -3.7037e-08, -3.1791e-08, -2.3061e-08,\n",
      "        -1.9807e-08, -1.7029e-08, -1.0418e-08, -9.6543e-09, -6.5944e-09,\n",
      "        -3.2094e-09, -8.0461e-10, -5.0528e-10, -3.3533e-10, -2.3053e-10,\n",
      "        -2.0354e-10, -1.7784e-10, -1.7032e-10, -1.6376e-10, -1.4979e-10,\n",
      "        -1.3931e-10, -1.1680e-10, -1.1408e-10, -1.0624e-10, -9.2571e-11,\n",
      "        -8.6969e-11, -7.7435e-11, -7.1877e-11, -5.9033e-11, -5.1760e-11,\n",
      "        -4.8547e-11, -4.6348e-11, -3.9032e-11, -3.2235e-11, -2.8448e-11,\n",
      "        -2.6110e-11, -2.0232e-11, -6.9790e-12, -3.4853e-12, -1.0200e-12,\n",
      "         7.5674e-12,  1.2098e-11,  1.9623e-11,  2.1993e-11,  2.5249e-11,\n",
      "         3.7391e-11,  4.4706e-11,  5.4171e-11,  5.9644e-11,  6.4136e-11,\n",
      "         6.8016e-11,  7.1431e-11,  9.0434e-11,  1.0262e-10,  1.1302e-10,\n",
      "         1.2364e-10,  1.2535e-10,  1.3614e-10,  1.4563e-10,  1.5769e-10,\n",
      "         1.7309e-10,  1.8396e-10,  2.1635e-10,  2.2947e-10,  2.9396e-10,\n",
      "         3.5161e-10,  4.6912e-10,  8.2533e-10,  1.1661e-09,  1.2207e-09,\n",
      "         1.9853e-09,  2.9071e-09,  5.3269e-09,  7.8289e-09,  9.0226e-09,\n",
      "         1.7805e-08,  2.6481e-08,  4.1747e-08,  4.6548e-08,  7.1249e-08,\n",
      "         7.9070e-08,  1.0976e-07,  1.2397e-07,  1.6044e-07,  2.2271e-07,\n",
      "         2.3213e-07,  2.8294e-07,  3.4501e-07,  3.9624e-07,  4.1486e-07,\n",
      "         5.7313e-07,  6.4599e-07,  7.2651e-07,  7.9334e-07,  9.3304e-07,\n",
      "         1.2634e-06,  1.3582e-06,  1.5002e-06,  1.8634e-06,  2.2218e-06,\n",
      "         2.6757e-06,  2.9511e-06,  3.3924e-06,  4.0206e-06,  4.1366e-06,\n",
      "         4.9077e-06,  5.7067e-06,  7.6086e-06,  8.2938e-06,  1.1260e-05,\n",
      "         1.1721e-05,  1.5294e-05,  1.5993e-05,  2.0290e-05,  2.3767e-05,\n",
      "         2.4189e-05,  2.8425e-05,  3.5790e-05,  4.4390e-05,  5.1402e-05,\n",
      "         5.6946e-05,  5.9116e-05,  6.5772e-05,  7.5567e-05,  8.2949e-05,\n",
      "         8.4821e-05,  1.1349e-04,  1.1635e-04,  1.2895e-04,  1.7048e-04,\n",
      "         1.7292e-04,  2.3170e-04,  3.7550e-04,  6.0385e-04,  8.1973e-04,\n",
      "         9.2314e-04,  1.4221e-03,  2.2990e-03,  9.0408e-03,  1.4160e-02])\n",
      "wout = tensor([-1.1363e-03, -7.0037e-04, -6.1901e-04, -4.3634e-04, -4.0118e-04,\n",
      "        -3.4334e-04, -3.0342e-04, -2.9193e-04, -2.7534e-04, -2.6543e-04,\n",
      "        -2.3248e-04, -2.2497e-04, -2.1323e-04, -2.0364e-04, -1.9184e-04,\n",
      "        -1.8722e-04, -1.7730e-04, -1.6930e-04, -1.6179e-04, -1.3745e-04,\n",
      "        -1.3395e-04, -1.3083e-04, -1.2775e-04, -1.1443e-04, -1.1396e-04,\n",
      "        -1.0276e-04, -9.7164e-05, -8.9572e-05, -8.5093e-05, -8.1428e-05,\n",
      "        -7.6818e-05, -6.4810e-05, -5.6237e-05, -4.9476e-05, -4.6885e-05,\n",
      "        -4.4321e-05, -4.2169e-05, -3.9826e-05, -3.6890e-05, -2.3363e-05,\n",
      "        -2.2506e-05, -2.1716e-05, -2.0192e-05, -1.7665e-05, -1.4228e-05,\n",
      "        -1.2748e-05, -1.1331e-05, -1.0516e-05, -9.4401e-06, -5.5105e-06,\n",
      "        -4.4487e-06, -4.2456e-06, -2.6791e-06, -2.1854e-06, -1.5574e-06,\n",
      "        -1.0957e-06, -5.4092e-07, -3.4757e-07, -1.9979e-07, -7.6915e-08,\n",
      "        -3.0786e-08, -2.9063e-08, -1.5090e-08, -9.1279e-09, -7.8567e-09,\n",
      "        -6.5868e-09, -5.6413e-09, -5.3487e-09, -4.7767e-09, -4.7539e-09,\n",
      "        -4.7517e-09, -3.5431e-09, -3.2180e-09, -1.1115e-09, -9.7720e-10,\n",
      "        -9.6745e-10, -6.8242e-10, -5.8162e-10, -5.7642e-10, -3.5624e-10,\n",
      "        -3.2248e-10, -1.9663e-10, -6.0454e-11, -5.4570e-11,  6.3526e-11,\n",
      "         1.0633e-10,  1.3966e-10,  1.8428e-10,  2.7854e-10,  5.1441e-10,\n",
      "         5.8856e-10,  2.3065e-09,  2.3248e-09,  2.6358e-09,  3.1281e-09,\n",
      "         3.3128e-09,  3.3268e-09,  4.0757e-09,  4.4923e-09,  5.2588e-09,\n",
      "         5.8469e-09,  6.0463e-09,  6.8817e-09,  1.3347e-08,  1.6859e-08,\n",
      "         1.7237e-08,  2.1186e-08,  2.2427e-08,  2.5243e-08,  2.9217e-08,\n",
      "         4.3697e-08,  4.4720e-08,  4.8859e-08,  5.0657e-08,  5.3348e-08,\n",
      "         5.8591e-08,  6.0262e-08,  7.5376e-08,  7.5832e-08,  7.9516e-08,\n",
      "         8.2340e-08,  9.8507e-08,  1.4298e-07,  1.5749e-07,  1.7086e-07,\n",
      "         2.1490e-07,  2.2007e-07,  2.6131e-07,  3.2921e-07,  3.6411e-07,\n",
      "         4.2635e-07,  5.2228e-07,  6.2232e-07,  6.7777e-07,  7.0055e-07,\n",
      "         7.4114e-07,  7.9326e-07,  8.5725e-07,  9.9214e-07,  1.0797e-06,\n",
      "         1.5593e-06,  1.6274e-06,  2.2011e-06,  2.3850e-06,  2.5822e-06,\n",
      "         3.6963e-06,  4.7151e-06,  4.8858e-06,  7.0276e-06,  8.6740e-06,\n",
      "         1.3245e-05,  1.6886e-05,  2.6009e-05,  2.7769e-05,  3.6545e-05,\n",
      "         3.9488e-05,  4.2301e-05,  4.6469e-05,  5.2084e-05,  5.4354e-05,\n",
      "         5.7571e-05,  6.1294e-05,  6.3947e-05,  7.0020e-05,  7.7821e-05,\n",
      "         8.5064e-05,  9.0246e-05,  9.0985e-05,  9.9792e-05,  1.0057e-04,\n",
      "         1.0554e-04,  1.1821e-04,  1.2009e-04,  1.2767e-04,  1.3908e-04,\n",
      "         1.4643e-04,  1.5122e-04,  1.5411e-04,  1.5586e-04,  1.6892e-04,\n",
      "         1.8397e-04,  1.8814e-04,  2.0901e-04,  2.2277e-04,  2.3487e-04,\n",
      "         2.5468e-04,  2.6719e-04,  2.7303e-04,  2.8498e-04,  2.9852e-04,\n",
      "         3.1351e-04,  3.2898e-04,  3.6836e-04,  3.6998e-04,  4.4889e-04,\n",
      "         4.8211e-04,  5.8782e-04,  7.4445e-04,  9.1816e-04,  3.4097e-03])\n",
      "Task 1 Epoch 8/15 - Loss: 1.1121\n",
      "  Acc (Task 1): 50.75%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6652, Euclidean: 1.6897\n",
      "Task 1 Epoch 9/15 - Loss: 1.1076\n",
      "  Acc (Task 1): 52.77%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6718, Euclidean: 1.7293\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 9, step 400, cos_sim = 0.597 \n",
      "bias = 0\n",
      "win = tensor([-2.4446e-02, -1.0046e-03, -2.3626e-05, -1.3303e-05, -8.3494e-06,\n",
      "        -6.8024e-06, -1.0786e-06, -7.3574e-07, -2.6528e-08, -3.5129e-09,\n",
      "        -2.9535e-09, -2.0525e-09, -1.6721e-09, -1.1861e-09, -1.0550e-09,\n",
      "        -8.9560e-10, -8.3408e-10, -7.2834e-10, -6.7957e-10, -6.4194e-10,\n",
      "        -6.3533e-10, -6.0282e-10, -5.8489e-10, -5.7447e-10, -5.5277e-10,\n",
      "        -5.3820e-10, -5.2396e-10, -5.2135e-10, -5.0592e-10, -4.9914e-10,\n",
      "        -4.8118e-10, -4.7246e-10, -4.6235e-10, -4.5028e-10, -4.3267e-10,\n",
      "        -4.1925e-10, -4.1159e-10, -3.9598e-10, -3.9288e-10, -3.8501e-10,\n",
      "        -3.7563e-10, -3.6411e-10, -3.5271e-10, -3.4837e-10, -3.3682e-10,\n",
      "        -3.2818e-10, -3.2586e-10, -3.2268e-10, -3.0489e-10, -3.0336e-10,\n",
      "        -2.9645e-10, -2.8227e-10, -2.7731e-10, -2.7018e-10, -2.5979e-10,\n",
      "        -2.5117e-10, -2.4688e-10, -2.4236e-10, -2.3567e-10, -2.2718e-10,\n",
      "        -2.2021e-10, -2.1481e-10, -2.0802e-10, -2.0087e-10, -1.9832e-10,\n",
      "        -1.9315e-10, -1.8881e-10, -1.8479e-10, -1.7558e-10, -1.6873e-10,\n",
      "        -1.6272e-10, -1.5296e-10, -1.5154e-10, -1.4760e-10, -1.4520e-10,\n",
      "        -1.4114e-10, -1.2581e-10, -1.2105e-10, -1.1709e-10, -1.1071e-10,\n",
      "        -1.0787e-10, -1.0531e-10, -9.7547e-11, -9.0613e-11, -8.7933e-11,\n",
      "        -7.7000e-11, -6.7224e-11, -6.5278e-11, -5.5161e-11, -5.3850e-11,\n",
      "        -4.7205e-11, -4.3639e-11, -4.0334e-11, -3.5812e-11, -3.3063e-11,\n",
      "        -2.7700e-11, -2.0996e-11, -1.5761e-11, -1.0487e-11, -4.1959e-12,\n",
      "        -1.3320e-12,  3.8197e-12,  1.4104e-11,  1.6043e-11,  2.0845e-11,\n",
      "         3.0081e-11,  3.4450e-11,  3.7285e-11,  4.0186e-11,  5.1031e-11,\n",
      "         5.2688e-11,  5.9070e-11,  6.3583e-11,  6.7754e-11,  7.0439e-11,\n",
      "         7.7203e-11,  7.8857e-11,  8.5729e-11,  8.6903e-11,  9.3303e-11,\n",
      "         1.0824e-10,  1.1348e-10,  1.1938e-10,  1.2638e-10,  1.3061e-10,\n",
      "         1.3838e-10,  1.4091e-10,  1.4514e-10,  1.4759e-10,  1.5293e-10,\n",
      "         1.6086e-10,  1.6538e-10,  1.7172e-10,  1.7558e-10,  1.8074e-10,\n",
      "         1.9336e-10,  1.9569e-10,  1.9975e-10,  2.0792e-10,  2.1645e-10,\n",
      "         2.1804e-10,  2.2592e-10,  2.3773e-10,  2.5198e-10,  2.5668e-10,\n",
      "         2.5705e-10,  2.6880e-10,  2.7045e-10,  2.8227e-10,  2.8639e-10,\n",
      "         2.8863e-10,  3.0659e-10,  3.1037e-10,  3.1310e-10,  3.1464e-10,\n",
      "         3.2680e-10,  3.3115e-10,  3.4418e-10,  3.4932e-10,  3.5569e-10,\n",
      "         3.6996e-10,  3.7650e-10,  3.8790e-10,  3.9270e-10,  4.0653e-10,\n",
      "         4.1218e-10,  4.2181e-10,  4.2935e-10,  4.3859e-10,  4.4539e-10,\n",
      "         4.5388e-10,  4.7487e-10,  4.8496e-10,  4.9850e-10,  5.3278e-10,\n",
      "         5.5344e-10,  5.6694e-10,  5.9398e-10,  6.2206e-10,  6.2915e-10,\n",
      "         6.6058e-10,  6.7795e-10,  7.1565e-10,  8.4220e-10,  9.6858e-10,\n",
      "         1.0565e-09,  1.2191e-09,  2.5678e-09,  1.4493e-08,  1.4719e-08,\n",
      "         6.3854e-08,  1.6214e-07,  5.5456e-07,  9.5606e-06,  2.8248e-05,\n",
      "         1.0779e-04,  1.1083e-03,  1.1624e-03,  1.8351e-02,  5.8298e-02])\n",
      "wout = tensor([-7.9988e-03, -6.9244e-03, -4.4982e-03, -3.5199e-03, -3.2461e-03,\n",
      "        -3.0932e-03, -2.9686e-03, -2.6513e-03, -2.0004e-03, -1.5397e-03,\n",
      "        -1.3841e-03, -1.3009e-03, -1.1172e-03, -1.0728e-03, -9.9878e-04,\n",
      "        -7.9522e-04, -6.9395e-04, -6.8816e-04, -6.6677e-04, -6.4673e-04,\n",
      "        -6.2543e-04, -5.7551e-04, -5.6998e-04, -5.5787e-04, -5.2556e-04,\n",
      "        -5.0010e-04, -4.5047e-04, -4.2961e-04, -3.9568e-04, -3.7528e-04,\n",
      "        -3.4813e-04, -3.4272e-04, -3.2979e-04, -3.0349e-04, -2.9281e-04,\n",
      "        -2.8664e-04, -2.7978e-04, -2.5991e-04, -2.1825e-04, -1.9847e-04,\n",
      "        -1.9142e-04, -1.8197e-04, -1.7602e-04, -1.5701e-04, -1.5072e-04,\n",
      "        -1.4457e-04, -1.2166e-04, -1.0617e-04, -1.0306e-04, -9.8322e-05,\n",
      "        -9.7025e-05, -8.5000e-05, -7.0412e-05, -6.4810e-05, -5.4205e-05,\n",
      "        -5.2326e-05, -5.0076e-05, -4.7898e-05, -4.5286e-05, -4.3976e-05,\n",
      "        -4.2064e-05, -4.0183e-05, -3.8764e-05, -2.9990e-05, -2.3770e-05,\n",
      "        -2.2164e-05, -1.7991e-05, -1.5188e-05, -1.3393e-05, -1.0767e-05,\n",
      "        -8.7959e-06, -7.2392e-06, -5.7406e-06, -2.6485e-06, -2.3127e-06,\n",
      "        -1.6521e-06, -5.1296e-07, -2.3805e-07, -2.0066e-07, -6.9300e-08,\n",
      "         2.5330e-09,  4.9446e-08,  5.3229e-08,  8.7649e-08,  9.4010e-08,\n",
      "         1.3000e-07,  2.4934e-07,  3.3355e-07,  4.8336e-07,  9.8037e-07,\n",
      "         1.2472e-06,  1.2988e-06,  1.3629e-06,  1.6651e-06,  2.7247e-06,\n",
      "         2.7378e-06,  2.8886e-06,  3.1116e-06,  3.1375e-06,  3.2777e-06,\n",
      "         3.4307e-06,  4.1582e-06,  4.6792e-06,  5.4621e-06,  5.5773e-06,\n",
      "         5.7973e-06,  7.0991e-06,  8.6359e-06,  9.1186e-06,  9.4368e-06,\n",
      "         9.6122e-06,  1.0824e-05,  1.2529e-05,  1.2940e-05,  1.4875e-05,\n",
      "         1.7601e-05,  1.8086e-05,  1.8284e-05,  2.2367e-05,  2.2529e-05,\n",
      "         2.3313e-05,  2.3854e-05,  2.4036e-05,  2.5117e-05,  2.5616e-05,\n",
      "         2.6410e-05,  2.7032e-05,  2.7208e-05,  2.7731e-05,  2.9912e-05,\n",
      "         3.4730e-05,  3.7564e-05,  4.1052e-05,  4.4881e-05,  4.6779e-05,\n",
      "         5.8988e-05,  6.0815e-05,  6.3842e-05,  6.5095e-05,  7.3722e-05,\n",
      "         8.8120e-05,  9.3082e-05,  9.4486e-05,  1.0305e-04,  1.0548e-04,\n",
      "         1.0923e-04,  1.1326e-04,  1.1558e-04,  1.3180e-04,  1.3597e-04,\n",
      "         1.3696e-04,  1.4154e-04,  1.4594e-04,  1.5592e-04,  1.7125e-04,\n",
      "         2.0985e-04,  2.2417e-04,  2.4875e-04,  2.5571e-04,  2.6903e-04,\n",
      "         2.8112e-04,  2.9277e-04,  3.0082e-04,  3.1973e-04,  3.3932e-04,\n",
      "         3.4085e-04,  3.4483e-04,  3.6010e-04,  3.9085e-04,  4.0860e-04,\n",
      "         4.3810e-04,  4.7295e-04,  4.9448e-04,  5.0193e-04,  5.3532e-04,\n",
      "         5.5697e-04,  5.6628e-04,  6.1468e-04,  6.3659e-04,  6.8110e-04,\n",
      "         7.0477e-04,  7.1129e-04,  7.3828e-04,  8.5105e-04,  8.6614e-04,\n",
      "         8.9547e-04,  9.5428e-04,  1.2268e-03,  1.4161e-03,  1.4400e-03,\n",
      "         1.4434e-03,  1.5934e-03,  1.9654e-03,  2.0203e-03,  2.4283e-03,\n",
      "         2.7293e-03,  2.8259e-03,  4.6214e-03,  7.2777e-03,  9.0535e-03])\n",
      "Task 1 Epoch 10/15 - Loss: 1.0869\n",
      "  Acc (Task 1): 54.23%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.5967, Euclidean: 1.9352\n",
      "Task 1 Epoch 11/15 - Loss: 1.0724\n",
      "  Acc (Task 1): 51.62%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.6139, Euclidean: 1.9331\n",
      "Task 1 Epoch 12/15 - Loss: 1.0724\n",
      "  Acc (Task 1): 54.45%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.5660, Euclidean: 2.0920\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 12, step 500, cos_sim = 0.523 \n",
      "bias = 0\n",
      "win = tensor([-5.8450e-03, -1.7706e-03, -1.4085e-03, -9.7697e-04, -8.3982e-04,\n",
      "        -8.0323e-04, -5.4031e-04, -4.6761e-04, -3.7744e-04, -3.0747e-04,\n",
      "        -2.6980e-04, -2.5293e-04, -1.9902e-04, -1.7016e-04, -1.6581e-04,\n",
      "        -1.4320e-04, -1.3219e-04, -1.0176e-04, -8.8356e-05, -7.9218e-05,\n",
      "        -7.2177e-05, -6.1897e-05, -5.9661e-05, -5.4605e-05, -5.0508e-05,\n",
      "        -4.4021e-05, -4.3369e-05, -4.1932e-05, -3.9573e-05, -3.4347e-05,\n",
      "        -3.1714e-05, -2.9587e-05, -2.6478e-05, -2.4921e-05, -2.2365e-05,\n",
      "        -2.1599e-05, -2.0712e-05, -1.9316e-05, -1.8026e-05, -1.6867e-05,\n",
      "        -1.5749e-05, -1.4285e-05, -1.2818e-05, -1.1839e-05, -1.1311e-05,\n",
      "        -9.6606e-06, -9.4310e-06, -8.4989e-06, -8.0350e-06, -7.6303e-06,\n",
      "        -5.9387e-06, -5.7437e-06, -5.3948e-06, -5.0482e-06, -4.5500e-06,\n",
      "        -4.1117e-06, -3.7175e-06, -3.1095e-06, -2.9242e-06, -2.6422e-06,\n",
      "        -2.2575e-06, -2.2108e-06, -2.0569e-06, -1.8458e-06, -1.6852e-06,\n",
      "        -1.4529e-06, -1.3184e-06, -1.1938e-06, -1.1312e-06, -9.9613e-07,\n",
      "        -8.2031e-07, -7.6319e-07, -7.0742e-07, -6.0904e-07, -5.8799e-07,\n",
      "        -5.1613e-07, -4.4072e-07, -3.8198e-07, -3.4922e-07, -2.8704e-07,\n",
      "        -2.5205e-07, -2.0647e-07, -1.7283e-07, -1.2532e-07, -1.1067e-07,\n",
      "        -9.1307e-08, -8.0528e-08, -6.6518e-08, -4.9439e-08, -4.4741e-08,\n",
      "        -4.1680e-08, -2.8948e-08, -2.4324e-08, -1.7719e-08, -9.6334e-09,\n",
      "        -6.0188e-09, -4.5413e-10,  3.0872e-09,  5.8305e-09,  1.2407e-08,\n",
      "         1.4681e-08,  3.3756e-08,  4.3964e-08,  5.1031e-08,  5.9037e-08,\n",
      "         7.1424e-08,  8.9326e-08,  1.0532e-07,  1.2169e-07,  1.5300e-07,\n",
      "         1.8065e-07,  2.1600e-07,  2.4510e-07,  2.9941e-07,  3.5826e-07,\n",
      "         3.8260e-07,  4.1247e-07,  4.8374e-07,  5.3728e-07,  6.0165e-07,\n",
      "         6.8180e-07,  7.5120e-07,  7.6290e-07,  8.2628e-07,  8.4904e-07,\n",
      "         1.0503e-06,  1.2103e-06,  1.3391e-06,  1.3540e-06,  1.3899e-06,\n",
      "         1.6156e-06,  1.6951e-06,  1.7264e-06,  2.1069e-06,  2.2317e-06,\n",
      "         2.2903e-06,  2.5047e-06,  2.7958e-06,  3.0370e-06,  3.2664e-06,\n",
      "         3.5063e-06,  3.8174e-06,  4.2527e-06,  4.4498e-06,  5.2098e-06,\n",
      "         5.3118e-06,  5.7481e-06,  6.1370e-06,  6.7824e-06,  7.1446e-06,\n",
      "         7.6934e-06,  8.1917e-06,  8.9762e-06,  9.4100e-06,  1.0330e-05,\n",
      "         1.1321e-05,  1.1522e-05,  1.2757e-05,  1.4138e-05,  1.4235e-05,\n",
      "         1.5633e-05,  1.7197e-05,  1.9659e-05,  2.2501e-05,  2.4993e-05,\n",
      "         2.6133e-05,  2.8679e-05,  3.1321e-05,  3.5249e-05,  3.5426e-05,\n",
      "         4.2487e-05,  4.6067e-05,  5.3211e-05,  5.7366e-05,  6.0585e-05,\n",
      "         6.5192e-05,  7.2725e-05,  8.3454e-05,  9.2194e-05,  1.1645e-04,\n",
      "         1.2075e-04,  1.5322e-04,  1.6625e-04,  1.8792e-04,  2.4268e-04,\n",
      "         2.8748e-04,  3.1251e-04,  3.6121e-04,  4.5269e-04,  5.0041e-04,\n",
      "         5.6257e-04,  7.5195e-04,  8.3303e-04,  9.5694e-04,  1.1890e-03,\n",
      "         1.8498e-03,  2.0994e-03,  2.9908e-03,  1.4693e-02,  2.1548e-02])\n",
      "wout = tensor([-3.5886e-03, -2.2791e-03, -2.1467e-03, -1.8423e-03, -1.3315e-03,\n",
      "        -1.0921e-03, -9.6028e-04, -9.0239e-04, -8.4588e-04, -7.3683e-04,\n",
      "        -7.0869e-04, -6.6096e-04, -6.3224e-04, -5.8879e-04, -5.6052e-04,\n",
      "        -4.7080e-04, -4.4105e-04, -4.2345e-04, -3.9319e-04, -3.4095e-04,\n",
      "        -3.3296e-04, -3.1367e-04, -3.0998e-04, -2.8834e-04, -2.6669e-04,\n",
      "        -2.6430e-04, -2.5583e-04, -2.3332e-04, -2.2987e-04, -2.1131e-04,\n",
      "        -1.9904e-04, -1.7539e-04, -1.6080e-04, -1.3643e-04, -1.2852e-04,\n",
      "        -1.1949e-04, -1.1449e-04, -1.1164e-04, -9.5407e-05, -7.7097e-05,\n",
      "        -7.4827e-05, -7.1111e-05, -6.7440e-05, -6.6930e-05, -6.5982e-05,\n",
      "        -6.4226e-05, -6.2296e-05, -6.0166e-05, -5.9233e-05, -5.5413e-05,\n",
      "        -5.5226e-05, -5.3086e-05, -5.2160e-05, -5.1369e-05, -4.6811e-05,\n",
      "        -4.2796e-05, -4.2447e-05, -4.0483e-05, -3.8495e-05, -3.6624e-05,\n",
      "        -3.6170e-05, -3.5587e-05, -3.1979e-05, -2.8337e-05, -2.7708e-05,\n",
      "        -2.6300e-05, -2.4060e-05, -2.3373e-05, -2.2286e-05, -2.2273e-05,\n",
      "        -2.0509e-05, -1.9174e-05, -1.7775e-05, -1.6919e-05, -1.5645e-05,\n",
      "        -1.5212e-05, -1.4512e-05, -1.3763e-05, -1.3476e-05, -1.0670e-05,\n",
      "        -1.0326e-05, -8.9269e-06, -7.9653e-06, -7.4860e-06, -5.6467e-06,\n",
      "        -5.1338e-06, -3.0168e-06, -2.3089e-06, -2.1256e-06,  3.4967e-07,\n",
      "         6.0719e-07,  7.7338e-07,  3.8775e-06,  4.3058e-06,  4.5608e-06,\n",
      "         5.4546e-06,  5.5685e-06,  6.3520e-06,  6.8678e-06,  6.9730e-06,\n",
      "         8.4827e-06,  9.0398e-06,  9.9588e-06,  1.0330e-05,  1.1336e-05,\n",
      "         1.3903e-05,  1.5578e-05,  1.5953e-05,  1.6021e-05,  1.7005e-05,\n",
      "         1.7663e-05,  1.9665e-05,  2.1085e-05,  2.2248e-05,  2.2413e-05,\n",
      "         2.4777e-05,  2.6061e-05,  2.8327e-05,  3.0671e-05,  3.2942e-05,\n",
      "         3.9968e-05,  4.0689e-05,  4.4126e-05,  4.4457e-05,  4.5198e-05,\n",
      "         4.6470e-05,  4.8783e-05,  5.3665e-05,  5.9747e-05,  6.0305e-05,\n",
      "         6.5610e-05,  7.0715e-05,  7.2220e-05,  7.2820e-05,  7.3554e-05,\n",
      "         7.5287e-05,  7.5972e-05,  7.6978e-05,  8.0151e-05,  8.7697e-05,\n",
      "         8.8921e-05,  9.1328e-05,  9.1917e-05,  9.7219e-05,  1.0233e-04,\n",
      "         1.0441e-04,  1.0539e-04,  1.0729e-04,  1.1111e-04,  1.1441e-04,\n",
      "         1.2257e-04,  1.2474e-04,  1.3301e-04,  1.5106e-04,  1.5658e-04,\n",
      "         1.7277e-04,  1.8226e-04,  1.9817e-04,  2.2643e-04,  2.3202e-04,\n",
      "         2.6138e-04,  3.0479e-04,  3.1473e-04,  3.2131e-04,  3.4011e-04,\n",
      "         3.5992e-04,  3.6187e-04,  3.8753e-04,  4.1095e-04,  4.3693e-04,\n",
      "         4.7886e-04,  4.8937e-04,  5.2024e-04,  5.5268e-04,  5.8311e-04,\n",
      "         6.1038e-04,  7.1926e-04,  7.3679e-04,  7.5116e-04,  7.6003e-04,\n",
      "         8.2040e-04,  8.6621e-04,  8.9345e-04,  9.2779e-04,  9.5859e-04,\n",
      "         1.0066e-03,  1.1510e-03,  1.1542e-03,  1.2740e-03,  1.3775e-03,\n",
      "         1.5014e-03,  1.6071e-03,  1.7107e-03,  1.9653e-03,  2.0651e-03,\n",
      "         2.2506e-03,  2.3595e-03,  2.8175e-03,  3.0811e-03,  3.4570e-03])\n",
      "Task 1 Epoch 13/15 - Loss: 1.0640\n",
      "  Acc (Task 1): 45.20%, Acc (Task 2): 0.03%\n",
      "  Unit similarity - Cosine: 0.4746, Euclidean: 2.3621\n",
      "Task 1 Epoch 14/15 - Loss: 1.0777\n",
      "  Acc (Task 1): 55.50%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.4252, Euclidean: 2.5794\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 14, step 600, cos_sim = 0.333 \n",
      "bias = 0\n",
      "win = tensor([-1.8825e-01, -1.9761e-02, -5.9279e-03, -6.4683e-04, -4.3912e-04,\n",
      "        -1.2035e-04, -2.9241e-06, -1.7289e-07, -1.3571e-07, -3.8981e-08,\n",
      "        -3.1340e-08, -6.5167e-09, -5.7740e-09, -4.6935e-09, -3.0983e-09,\n",
      "        -2.7139e-09, -2.4412e-09, -2.2993e-09, -2.0927e-09, -2.0569e-09,\n",
      "        -1.9996e-09, -1.9110e-09, -1.8302e-09, -1.8087e-09, -1.7662e-09,\n",
      "        -1.7300e-09, -1.6590e-09, -1.6028e-09, -1.5809e-09, -1.5605e-09,\n",
      "        -1.4386e-09, -1.4327e-09, -1.3957e-09, -1.3875e-09, -1.3714e-09,\n",
      "        -1.3191e-09, -1.3082e-09, -1.2485e-09, -1.2260e-09, -1.1659e-09,\n",
      "        -1.1467e-09, -1.1244e-09, -1.1000e-09, -1.0496e-09, -1.0281e-09,\n",
      "        -1.0240e-09, -9.8551e-10, -9.6626e-10, -9.4196e-10, -9.0372e-10,\n",
      "        -8.8758e-10, -8.6761e-10, -8.6179e-10, -8.4714e-10, -8.2898e-10,\n",
      "        -8.0729e-10, -7.9244e-10, -7.7156e-10, -7.4183e-10, -7.3027e-10,\n",
      "        -6.7908e-10, -6.4497e-10, -6.3895e-10, -6.2330e-10, -5.9896e-10,\n",
      "        -5.9046e-10, -5.7741e-10, -5.6272e-10, -5.2658e-10, -5.1665e-10,\n",
      "        -5.0220e-10, -4.9138e-10, -4.8022e-10, -4.5585e-10, -4.2601e-10,\n",
      "        -3.9521e-10, -3.7009e-10, -3.6261e-10, -3.3882e-10, -3.2132e-10,\n",
      "        -2.8553e-10, -2.7391e-10, -2.5930e-10, -2.4344e-10, -2.2527e-10,\n",
      "        -2.0265e-10, -1.9669e-10, -1.6076e-10, -1.4327e-10, -1.2573e-10,\n",
      "        -9.7523e-11, -7.8101e-11, -6.3236e-11, -4.8979e-11, -4.3660e-11,\n",
      "        -1.3728e-11,  3.5483e-12,  1.7880e-11,  2.8399e-11,  6.2237e-11,\n",
      "         7.6967e-11,  8.8860e-11,  1.0280e-10,  1.1894e-10,  1.3369e-10,\n",
      "         1.6466e-10,  1.9360e-10,  2.0847e-10,  2.1343e-10,  2.3361e-10,\n",
      "         2.4764e-10,  2.8539e-10,  2.9564e-10,  3.0284e-10,  3.2443e-10,\n",
      "         3.5171e-10,  3.7344e-10,  3.7594e-10,  3.9865e-10,  4.0851e-10,\n",
      "         4.2499e-10,  4.4722e-10,  4.8724e-10,  5.0682e-10,  5.1904e-10,\n",
      "         5.2493e-10,  5.5367e-10,  5.8682e-10,  5.9315e-10,  6.1598e-10,\n",
      "         6.3406e-10,  6.6107e-10,  6.7297e-10,  7.0594e-10,  7.2846e-10,\n",
      "         7.4354e-10,  7.4829e-10,  7.9813e-10,  8.0319e-10,  8.2089e-10,\n",
      "         8.3462e-10,  8.6659e-10,  8.7803e-10,  8.9450e-10,  9.2332e-10,\n",
      "         9.5419e-10,  9.7414e-10,  1.0172e-09,  1.0619e-09,  1.0798e-09,\n",
      "         1.0874e-09,  1.1227e-09,  1.1580e-09,  1.2061e-09,  1.2315e-09,\n",
      "         1.2620e-09,  1.2891e-09,  1.3020e-09,  1.3356e-09,  1.3558e-09,\n",
      "         1.4242e-09,  1.4312e-09,  1.4699e-09,  1.5007e-09,  1.5361e-09,\n",
      "         1.5907e-09,  1.6302e-09,  1.6381e-09,  1.7431e-09,  1.7903e-09,\n",
      "         1.8295e-09,  1.8516e-09,  1.8844e-09,  1.9892e-09,  2.1079e-09,\n",
      "         2.1973e-09,  2.3145e-09,  2.4716e-09,  2.9185e-09,  3.3639e-09,\n",
      "         3.7714e-09,  5.1288e-09,  9.0664e-09,  3.1472e-08,  4.2412e-08,\n",
      "         5.7608e-08,  1.7592e-07,  2.0988e-07,  2.7271e-07,  1.1376e-06,\n",
      "         7.9767e-06,  1.6240e-05,  6.5545e-05,  1.6054e-04,  1.1287e-03,\n",
      "         1.5902e-03,  4.7406e-03,  1.4406e-02,  2.5491e-02,  4.0551e-02])\n",
      "wout = tensor([-1.3497e-02, -1.0876e-02, -9.7611e-03, -9.3558e-03, -8.5201e-03,\n",
      "        -7.5358e-03, -6.6368e-03, -6.1292e-03, -5.5883e-03, -5.5236e-03,\n",
      "        -5.3214e-03, -5.1423e-03, -4.7780e-03, -4.4406e-03, -4.0669e-03,\n",
      "        -3.6149e-03, -3.5494e-03, -3.5072e-03, -3.4173e-03, -3.1278e-03,\n",
      "        -2.9932e-03, -2.9821e-03, -2.9463e-03, -2.8395e-03, -2.7919e-03,\n",
      "        -2.7354e-03, -2.6578e-03, -2.6089e-03, -2.5434e-03, -2.5266e-03,\n",
      "        -2.4635e-03, -2.4274e-03, -2.4080e-03, -2.2659e-03, -2.1888e-03,\n",
      "        -2.0226e-03, -1.9986e-03, -1.9190e-03, -1.8493e-03, -1.8251e-03,\n",
      "        -1.7769e-03, -1.7440e-03, -1.7304e-03, -1.7031e-03, -1.6001e-03,\n",
      "        -1.4756e-03, -1.3942e-03, -1.3393e-03, -1.3187e-03, -1.2577e-03,\n",
      "        -1.2253e-03, -1.1912e-03, -1.1247e-03, -1.0951e-03, -1.0834e-03,\n",
      "        -1.0477e-03, -1.0133e-03, -9.5895e-04, -9.0587e-04, -8.6619e-04,\n",
      "        -8.5744e-04, -8.3746e-04, -7.6656e-04, -6.9623e-04, -6.7833e-04,\n",
      "        -6.7674e-04, -6.6387e-04, -6.3113e-04, -6.0385e-04, -5.5131e-04,\n",
      "        -4.5631e-04, -4.3846e-04, -4.1396e-04, -3.7308e-04, -3.4223e-04,\n",
      "        -3.1561e-04, -2.7362e-04, -2.5261e-04, -2.2181e-04, -2.0373e-04,\n",
      "        -1.9996e-04, -1.8054e-04, -1.5330e-04, -1.2928e-04, -1.1849e-04,\n",
      "        -1.1289e-04, -1.0866e-04, -9.7756e-05, -9.0622e-05, -7.0954e-05,\n",
      "        -6.1218e-05, -5.6398e-05, -4.6163e-05, -2.9974e-05, -1.8171e-05,\n",
      "        -1.3375e-05, -3.5616e-06, -8.5020e-07,  2.8056e-06,  1.4680e-05,\n",
      "         1.8824e-05,  2.5138e-05,  2.7105e-05,  3.2137e-05,  4.4479e-05,\n",
      "         4.9432e-05,  6.4048e-05,  6.9296e-05,  9.2615e-05,  1.0617e-04,\n",
      "         1.0888e-04,  1.2764e-04,  1.3961e-04,  1.5828e-04,  1.8148e-04,\n",
      "         1.9129e-04,  1.9754e-04,  2.1618e-04,  2.2181e-04,  2.2558e-04,\n",
      "         2.9005e-04,  3.1744e-04,  3.2317e-04,  3.7833e-04,  4.3866e-04,\n",
      "         4.4485e-04,  4.4897e-04,  4.6445e-04,  4.7185e-04,  4.7946e-04,\n",
      "         5.2599e-04,  5.7537e-04,  6.0052e-04,  6.2092e-04,  6.4990e-04,\n",
      "         6.7308e-04,  7.2337e-04,  7.3380e-04,  7.5771e-04,  7.7562e-04,\n",
      "         8.0414e-04,  8.1744e-04,  8.3691e-04,  8.4808e-04,  9.0156e-04,\n",
      "         9.1656e-04,  9.6289e-04,  9.9559e-04,  1.0343e-03,  1.0374e-03,\n",
      "         1.1179e-03,  1.1649e-03,  1.1728e-03,  1.2306e-03,  1.2601e-03,\n",
      "         1.3304e-03,  1.3352e-03,  1.3645e-03,  1.4525e-03,  1.5605e-03,\n",
      "         1.5941e-03,  1.6664e-03,  1.6684e-03,  1.6792e-03,  1.7013e-03,\n",
      "         1.7516e-03,  1.7620e-03,  1.8279e-03,  1.8964e-03,  1.9683e-03,\n",
      "         2.0559e-03,  2.0670e-03,  2.0813e-03,  2.1180e-03,  2.1360e-03,\n",
      "         2.1458e-03,  2.2453e-03,  2.3614e-03,  2.4779e-03,  2.5418e-03,\n",
      "         2.5961e-03,  2.6247e-03,  2.6783e-03,  2.9436e-03,  3.1300e-03,\n",
      "         3.1776e-03,  3.3217e-03,  3.4109e-03,  3.6117e-03,  3.9460e-03,\n",
      "         4.0459e-03,  4.8177e-03,  5.4555e-03,  5.8053e-03,  6.1816e-03,\n",
      "         6.9657e-03,  7.2068e-03,  9.0950e-03,  9.6646e-03,  1.0106e-02])\n",
      "Task 1 Epoch 15/15 - Loss: 1.0760\n",
      "  Acc (Task 1): 51.42%, Acc (Task 2): 0.00%\n",
      "  Unit similarity - Cosine: 0.3333, Euclidean: 2.8834\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9905\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9897\n",
      "Before coupling: cosine=0.3333, distance=2.8834\n",
      "After coupling: cosine=0.9905, distance=0.3517\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.2378\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 19.30%\n",
      "  Unit similarity - Cosine: 0.9238, Euclidean: 1.1022\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.923 \n",
      "bias = 0\n",
      "win = tensor([-7.9780e-02, -9.1269e-03, -7.3339e-03, -3.7204e-03, -1.9919e-03,\n",
      "        -1.2869e-03, -1.0711e-03, -7.9427e-04, -5.5492e-04, -4.4865e-04,\n",
      "        -3.5431e-04, -2.9043e-04, -2.6885e-04, -2.1411e-04, -2.0210e-04,\n",
      "        -1.8951e-04, -1.5467e-04, -1.3304e-04, -1.1588e-04, -1.0508e-04,\n",
      "        -9.2557e-05, -8.2994e-05, -7.0030e-05, -6.5704e-05, -5.9252e-05,\n",
      "        -5.3670e-05, -4.2130e-05, -4.0939e-05, -3.6715e-05, -3.4081e-05,\n",
      "        -2.9161e-05, -2.5092e-05, -2.3344e-05, -2.0452e-05, -1.8513e-05,\n",
      "        -1.6632e-05, -1.4068e-05, -1.3640e-05, -1.2017e-05, -1.0608e-05,\n",
      "        -8.7405e-06, -8.0522e-06, -6.9616e-06, -6.0945e-06, -5.4784e-06,\n",
      "        -4.4015e-06, -3.7753e-06, -3.4819e-06, -3.0744e-06, -2.7163e-06,\n",
      "        -2.4653e-06, -1.9262e-06, -1.6503e-06, -1.4934e-06, -1.2246e-06,\n",
      "        -1.2100e-06, -9.6374e-07, -9.0425e-07, -8.0971e-07, -7.5308e-07,\n",
      "        -6.1937e-07, -3.8978e-07, -2.9964e-07, -2.4436e-07, -2.1552e-07,\n",
      "        -1.5813e-07, -1.4458e-07, -1.1694e-07, -1.0162e-07, -9.4824e-08,\n",
      "        -6.4225e-08, -5.3936e-08, -5.2644e-08, -4.0154e-08, -2.9718e-08,\n",
      "        -2.2458e-08, -2.0525e-08, -1.6858e-08, -1.1125e-08, -1.0333e-08,\n",
      "        -8.4779e-09, -5.4871e-09, -5.1730e-09, -3.5725e-09, -1.7387e-09,\n",
      "        -1.4167e-09, -1.0812e-09, -7.0695e-10, -5.4634e-10, -3.9543e-10,\n",
      "        -3.2640e-10, -3.1602e-10, -2.9922e-10, -2.3256e-10, -2.1225e-10,\n",
      "        -1.7103e-10, -1.4826e-10, -1.2689e-10, -1.0333e-10, -8.3554e-11,\n",
      "        -6.9538e-11, -4.0785e-11, -1.6989e-11,  9.7216e-14,  1.3607e-11,\n",
      "         6.8113e-11,  8.3040e-11,  1.0960e-10,  1.4135e-10,  1.7592e-10,\n",
      "         1.9481e-10,  2.1482e-10,  2.7355e-10,  2.8639e-10,  3.1854e-10,\n",
      "         3.4811e-10,  3.6593e-10,  5.7118e-10,  8.1425e-10,  9.5676e-10,\n",
      "         1.4199e-09,  1.8213e-09,  2.6663e-09,  3.2625e-09,  3.9714e-09,\n",
      "         4.8439e-09,  5.3435e-09,  7.3776e-09,  9.2897e-09,  1.2329e-08,\n",
      "         1.3160e-08,  1.7897e-08,  1.9887e-08,  2.6820e-08,  2.7911e-08,\n",
      "         3.4383e-08,  3.8668e-08,  5.1473e-08,  5.5023e-08,  8.4830e-08,\n",
      "         9.6474e-08,  1.1069e-07,  1.4407e-07,  1.7050e-07,  1.9770e-07,\n",
      "         2.1186e-07,  2.5660e-07,  3.1070e-07,  3.3947e-07,  4.0049e-07,\n",
      "         5.0856e-07,  7.5585e-07,  7.8466e-07,  8.1325e-07,  1.0564e-06,\n",
      "         1.3750e-06,  1.4870e-06,  1.6752e-06,  2.0522e-06,  2.2500e-06,\n",
      "         2.7874e-06,  3.2091e-06,  4.2362e-06,  4.6743e-06,  4.8737e-06,\n",
      "         5.4855e-06,  6.4972e-06,  7.3707e-06,  7.7398e-06,  8.4706e-06,\n",
      "         1.1781e-05,  1.6846e-05,  1.7427e-05,  1.9326e-05,  2.4760e-05,\n",
      "         2.7514e-05,  2.9529e-05,  3.6666e-05,  3.7910e-05,  4.7071e-05,\n",
      "         5.1627e-05,  5.6001e-05,  7.1557e-05,  8.7929e-05,  1.3509e-04,\n",
      "         1.5472e-04,  1.9338e-04,  2.0774e-04,  2.5133e-04,  4.0547e-04,\n",
      "         4.7732e-04,  6.2859e-04,  7.8453e-04,  9.5759e-04,  1.3817e-03,\n",
      "         1.8285e-03,  2.0064e-03,  2.4703e-03,  4.0693e-03,  8.4846e-03])\n",
      "wout = tensor([-5.4070e-04, -2.5141e-04, -1.6777e-04, -1.3992e-04, -1.2848e-04,\n",
      "        -1.1633e-04, -9.1331e-05, -8.8547e-05, -8.3557e-05, -8.0304e-05,\n",
      "        -7.5450e-05, -6.6782e-05, -5.9494e-05, -5.7025e-05, -4.9935e-05,\n",
      "        -4.9014e-05, -4.7183e-05, -4.0252e-05, -3.6889e-05, -3.4811e-05,\n",
      "        -3.4623e-05, -3.1642e-05, -3.1420e-05, -2.9891e-05, -2.9074e-05,\n",
      "        -2.8109e-05, -2.7663e-05, -2.7290e-05, -2.6406e-05, -2.6048e-05,\n",
      "        -2.5837e-05, -2.5023e-05, -2.4012e-05, -2.3546e-05, -2.2811e-05,\n",
      "        -2.1698e-05, -2.1504e-05, -2.0505e-05, -2.0243e-05, -1.9662e-05,\n",
      "        -1.9478e-05, -1.9208e-05, -1.8318e-05, -1.7837e-05, -1.7716e-05,\n",
      "        -1.6296e-05, -1.5746e-05, -1.4002e-05, -1.3290e-05, -1.2284e-05,\n",
      "        -1.2211e-05, -1.0923e-05, -1.0508e-05, -1.0249e-05, -9.9322e-06,\n",
      "        -9.3013e-06, -8.8437e-06, -8.5216e-06, -8.0355e-06, -6.8963e-06,\n",
      "        -6.4023e-06, -6.0697e-06, -5.7224e-06, -5.6422e-06, -5.0414e-06,\n",
      "        -4.7296e-06, -4.3154e-06, -3.6016e-06, -2.9887e-06, -2.8513e-06,\n",
      "        -2.7715e-06, -2.5388e-06, -2.4946e-06, -2.2155e-06, -1.4820e-06,\n",
      "        -1.4107e-06, -1.0979e-06, -7.9574e-07, -7.2594e-07, -7.2335e-07,\n",
      "        -6.8816e-07, -5.8739e-07, -4.2209e-07, -3.5633e-07, -3.1546e-07,\n",
      "        -2.9816e-07, -2.5169e-07, -1.4175e-07, -1.0518e-09, -4.3862e-11,\n",
      "         6.1256e-10,  5.8541e-09,  7.3338e-09,  2.1214e-08,  2.1860e-08,\n",
      "         3.6413e-08,  4.7143e-08,  1.2644e-07,  1.6155e-07,  2.3539e-07,\n",
      "         2.3605e-07,  2.5328e-07,  2.6252e-07,  2.7875e-07,  3.1023e-07,\n",
      "         3.5299e-07,  4.1025e-07,  4.3851e-07,  4.6722e-07,  5.3159e-07,\n",
      "         6.2182e-07,  7.2583e-07,  7.7980e-07,  9.9949e-07,  1.0668e-06,\n",
      "         1.2763e-06,  1.3120e-06,  1.4550e-06,  1.5060e-06,  1.5139e-06,\n",
      "         1.5718e-06,  1.5942e-06,  1.7958e-06,  1.9486e-06,  2.2423e-06,\n",
      "         2.5916e-06,  2.6633e-06,  2.9368e-06,  3.0921e-06,  3.2400e-06,\n",
      "         3.4050e-06,  3.5154e-06,  4.3199e-06,  4.5341e-06,  4.9760e-06,\n",
      "         5.5555e-06,  5.6740e-06,  6.5627e-06,  6.6277e-06,  6.9000e-06,\n",
      "         7.1426e-06,  7.4238e-06,  7.5889e-06,  7.7114e-06,  8.1071e-06,\n",
      "         8.7367e-06,  1.0023e-05,  1.0068e-05,  1.0472e-05,  1.0756e-05,\n",
      "         1.1397e-05,  1.2527e-05,  1.2927e-05,  1.3010e-05,  1.4317e-05,\n",
      "         1.5130e-05,  1.6185e-05,  1.6482e-05,  1.7050e-05,  1.7369e-05,\n",
      "         1.7732e-05,  1.7766e-05,  1.8116e-05,  1.9079e-05,  1.9929e-05,\n",
      "         2.1796e-05,  2.4762e-05,  2.5469e-05,  2.6212e-05,  2.6222e-05,\n",
      "         2.7247e-05,  2.8134e-05,  3.0309e-05,  3.1577e-05,  3.2012e-05,\n",
      "         3.2976e-05,  3.3689e-05,  3.4986e-05,  3.8579e-05,  3.9552e-05,\n",
      "         4.2934e-05,  4.6896e-05,  4.7646e-05,  4.9286e-05,  5.3427e-05,\n",
      "         5.6706e-05,  5.8184e-05,  6.0542e-05,  6.3966e-05,  6.8317e-05,\n",
      "         7.0529e-05,  8.0820e-05,  8.7230e-05,  9.9796e-05,  1.1258e-04,\n",
      "         1.3058e-04,  1.4344e-04,  1.7940e-04,  2.0706e-04,  2.5632e-04])\n",
      "Task 2 Epoch 2/5 - Loss: 1.7902\n",
      "  Acc (Task 1): 0.05%, Acc (Task 2): 26.38%\n",
      "  Unit similarity - Cosine: 0.9196, Euclidean: 1.1812\n",
      "Task 2 Epoch 3/5 - Loss: 1.7472\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 25.08%\n",
      "  Unit similarity - Cosine: 0.9216, Euclidean: 1.2086\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.913 \n",
      "bias = 0\n",
      "win = tensor([-1.0239e-02, -5.1095e-03, -3.1797e-03, -1.8889e-03, -1.5304e-03,\n",
      "        -1.1382e-03, -1.0522e-03, -8.5200e-04, -6.3773e-04, -6.1642e-04,\n",
      "        -5.4035e-04, -3.4809e-04, -2.0406e-04, -1.5463e-04, -1.4377e-04,\n",
      "        -1.1097e-04, -8.6500e-05, -7.6750e-05, -7.0135e-05, -6.7291e-05,\n",
      "        -5.6376e-05, -4.5343e-05, -4.1546e-05, -3.7387e-05, -3.2228e-05,\n",
      "        -3.1419e-05, -2.8585e-05, -2.4948e-05, -2.3047e-05, -2.0476e-05,\n",
      "        -1.8980e-05, -1.7792e-05, -1.6612e-05, -1.5424e-05, -1.2202e-05,\n",
      "        -1.1802e-05, -1.0799e-05, -9.8646e-06, -8.7893e-06, -7.8184e-06,\n",
      "        -7.4525e-06, -6.3361e-06, -5.9873e-06, -5.2764e-06, -4.9490e-06,\n",
      "        -4.7158e-06, -4.0830e-06, -3.3193e-06, -2.8787e-06, -2.2555e-06,\n",
      "        -2.1351e-06, -2.0477e-06, -1.6818e-06, -1.3531e-06, -1.1399e-06,\n",
      "        -1.0006e-06, -8.1260e-07, -7.7283e-07, -6.1691e-07, -5.0726e-07,\n",
      "        -4.8269e-07, -4.6426e-07, -4.0457e-07, -3.6009e-07, -3.2631e-07,\n",
      "        -3.0060e-07, -2.5485e-07, -2.2391e-07, -2.1361e-07, -1.9531e-07,\n",
      "        -1.4156e-07, -1.1967e-07, -8.8697e-08, -7.7758e-08, -6.7424e-08,\n",
      "        -5.1081e-08, -4.6774e-08, -4.1768e-08, -3.5190e-08, -3.2036e-08,\n",
      "        -2.0730e-08, -1.4659e-08, -1.1944e-08, -9.4543e-09, -7.9995e-09,\n",
      "        -6.8988e-09, -6.2201e-09, -4.9557e-09, -3.8008e-09, -3.5132e-09,\n",
      "        -3.0125e-09, -2.2130e-09, -1.5721e-09, -1.0795e-09, -8.4887e-10,\n",
      "        -6.7759e-10, -5.9082e-10, -4.3407e-10, -2.1842e-10, -1.7918e-10,\n",
      "        -1.2843e-10, -6.0252e-11, -4.1265e-11, -1.0648e-11,  2.1940e-11,\n",
      "         9.0879e-11,  1.2531e-10,  1.7562e-10,  3.4176e-10,  4.6213e-10,\n",
      "         7.1813e-10,  9.0061e-10,  1.3577e-09,  1.6734e-09,  2.4884e-09,\n",
      "         3.0393e-09,  3.4119e-09,  6.2087e-09,  6.8648e-09,  8.7297e-09,\n",
      "         9.1435e-09,  1.4007e-08,  1.7874e-08,  2.1112e-08,  2.3330e-08,\n",
      "         3.3965e-08,  3.8132e-08,  4.7141e-08,  5.3490e-08,  5.7960e-08,\n",
      "         6.8113e-08,  7.7969e-08,  8.4274e-08,  9.1526e-08,  1.0237e-07,\n",
      "         1.4527e-07,  1.6470e-07,  2.6288e-07,  2.8117e-07,  3.6322e-07,\n",
      "         4.1904e-07,  4.6871e-07,  6.2446e-07,  6.6698e-07,  7.5540e-07,\n",
      "         9.8092e-07,  1.3501e-06,  1.4683e-06,  1.5951e-06,  1.9363e-06,\n",
      "         2.1860e-06,  2.5590e-06,  3.6548e-06,  3.9463e-06,  4.1638e-06,\n",
      "         4.7486e-06,  5.5305e-06,  5.8673e-06,  7.8610e-06,  9.4806e-06,\n",
      "         1.0986e-05,  1.3421e-05,  1.5653e-05,  1.6450e-05,  1.9581e-05,\n",
      "         2.0051e-05,  2.3266e-05,  2.8011e-05,  3.6346e-05,  3.9686e-05,\n",
      "         4.1129e-05,  4.3460e-05,  4.5967e-05,  5.3147e-05,  5.8883e-05,\n",
      "         6.2570e-05,  8.7591e-05,  9.4342e-05,  1.0533e-04,  1.1442e-04,\n",
      "         1.2419e-04,  1.6665e-04,  1.8418e-04,  1.8971e-04,  2.2370e-04,\n",
      "         2.8820e-04,  3.4783e-04,  3.7812e-04,  4.6968e-04,  5.2921e-04,\n",
      "         5.5852e-04,  8.4235e-04,  9.1877e-04,  1.0597e-03,  1.3715e-03,\n",
      "         1.4972e-03,  2.5631e-03,  3.7019e-03,  8.6029e-03,  2.6663e-02])\n",
      "wout = tensor([-1.2236e-03, -7.7022e-04, -6.5888e-04, -4.3043e-04, -4.2538e-04,\n",
      "        -4.1754e-04, -3.6764e-04, -3.6228e-04, -3.0932e-04, -3.0185e-04,\n",
      "        -2.9545e-04, -2.7749e-04, -2.6512e-04, -2.3378e-04, -2.2300e-04,\n",
      "        -2.1175e-04, -1.9482e-04, -1.8606e-04, -1.7054e-04, -1.6749e-04,\n",
      "        -1.6427e-04, -1.6104e-04, -1.4629e-04, -1.2561e-04, -1.2522e-04,\n",
      "        -1.1935e-04, -1.1481e-04, -1.1348e-04, -1.1001e-04, -9.2053e-05,\n",
      "        -8.9415e-05, -8.4337e-05, -8.2244e-05, -8.1087e-05, -7.9351e-05,\n",
      "        -7.4242e-05, -6.8603e-05, -6.3170e-05, -6.2130e-05, -5.5816e-05,\n",
      "        -5.4297e-05, -5.2492e-05, -4.7476e-05, -4.6651e-05, -4.5182e-05,\n",
      "        -4.2891e-05, -4.0610e-05, -3.9823e-05, -3.8728e-05, -3.8097e-05,\n",
      "        -3.6685e-05, -3.5167e-05, -3.4946e-05, -3.2307e-05, -3.1681e-05,\n",
      "        -3.1019e-05, -3.0206e-05, -2.9526e-05, -2.6232e-05, -2.3046e-05,\n",
      "        -2.1639e-05, -2.1137e-05, -2.0229e-05, -1.8184e-05, -1.7893e-05,\n",
      "        -1.4863e-05, -1.3399e-05, -1.0867e-05, -1.0199e-05, -9.9444e-06,\n",
      "        -9.6619e-06, -9.2625e-06, -8.4152e-06, -5.6701e-06, -4.3876e-06,\n",
      "        -4.2436e-06, -3.9222e-06, -3.6273e-06, -3.3747e-06, -2.1987e-06,\n",
      "        -1.9741e-06, -1.8850e-06, -1.8819e-06, -1.8224e-06, -1.4675e-06,\n",
      "        -1.3086e-06, -1.1561e-06, -8.3592e-07, -6.0018e-07, -4.6796e-07,\n",
      "        -2.5376e-07, -2.0231e-07, -1.2560e-07, -5.4301e-08,  6.0789e-09,\n",
      "         1.7261e-07,  2.7682e-07,  3.4107e-07,  9.8224e-07,  1.1158e-06,\n",
      "         1.2839e-06,  1.7902e-06,  2.1158e-06,  2.2757e-06,  3.1094e-06,\n",
      "         3.9778e-06,  4.2124e-06,  4.5034e-06,  5.6563e-06,  6.0054e-06,\n",
      "         6.2819e-06,  6.9883e-06,  8.5205e-06,  9.9742e-06,  1.1584e-05,\n",
      "         1.1975e-05,  1.2456e-05,  1.2911e-05,  1.5149e-05,  1.5298e-05,\n",
      "         1.5471e-05,  1.6547e-05,  1.7382e-05,  1.8225e-05,  2.0441e-05,\n",
      "         2.0814e-05,  2.2002e-05,  2.4149e-05,  2.4388e-05,  2.5997e-05,\n",
      "         2.8279e-05,  2.8997e-05,  3.0198e-05,  3.1657e-05,  3.2401e-05,\n",
      "         3.3024e-05,  3.5481e-05,  3.6038e-05,  3.7370e-05,  3.7937e-05,\n",
      "         3.8310e-05,  4.1136e-05,  4.1785e-05,  4.3092e-05,  4.5479e-05,\n",
      "         4.8075e-05,  4.8580e-05,  5.0178e-05,  5.0504e-05,  5.1189e-05,\n",
      "         5.2581e-05,  5.3958e-05,  5.6142e-05,  5.7366e-05,  5.9172e-05,\n",
      "         6.0436e-05,  6.1574e-05,  6.4770e-05,  6.6714e-05,  7.2875e-05,\n",
      "         7.5081e-05,  7.5523e-05,  7.9376e-05,  8.2711e-05,  8.7834e-05,\n",
      "         8.9908e-05,  9.7637e-05,  9.8705e-05,  1.0343e-04,  1.1311e-04,\n",
      "         1.1377e-04,  1.2689e-04,  1.3518e-04,  1.5085e-04,  1.6317e-04,\n",
      "         1.6817e-04,  1.7389e-04,  1.9195e-04,  2.1330e-04,  2.3083e-04,\n",
      "         2.3858e-04,  2.5076e-04,  2.5608e-04,  2.6246e-04,  2.8259e-04,\n",
      "         2.9093e-04,  2.9516e-04,  3.3361e-04,  3.3771e-04,  3.4733e-04,\n",
      "         3.7185e-04,  4.0264e-04,  4.4408e-04,  4.8809e-04,  5.1611e-04,\n",
      "         6.2808e-04,  6.8459e-04,  7.3356e-04,  9.3583e-04,  1.4333e-03])\n",
      "Task 2 Epoch 4/5 - Loss: 1.7371\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 28.27%\n",
      "  Unit similarity - Cosine: 0.8542, Euclidean: 1.7259\n",
      "Task 2 Epoch 5/5 - Loss: 1.6999\n",
      "  Acc (Task 1): 0.03%, Acc (Task 2): 30.35%\n",
      "  Unit similarity - Cosine: 0.8488, Euclidean: 1.8492\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9905 (after coupling) -> 0.8488 (final)\n",
      "Euclidean distance: 0.3517 (after coupling) -> 1.8492 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 0.00% (before Task 2) -> 0.03% (after Task 2)\n",
      "No forgetting observed. Task 1 performance improved by 0.03%.\n",
      "\n",
      "\n",
      "================= shorter trainig =================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 1, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0344, Euclidean distance: 0.8073\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/1 - Loss: 1.6452\n",
      "  Acc (Task 1): 45.73%, Acc (Task 2): 0.20%\n",
      "  Unit similarity - Cosine: -0.1020, Euclidean: 1.1380\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9901\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9888\n",
      "Before coupling: cosine=-0.1020, distance=1.1380\n",
      "After coupling: cosine=0.9901, distance=0.1019\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.1357\n",
      "  Acc (Task 1): 0.15%, Acc (Task 2): 24.67%\n",
      "  Unit similarity - Cosine: 0.7278, Euclidean: 0.9205\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.655 \n",
      "bias = 0\n",
      "win = tensor([-6.3395e-03, -1.0356e-03, -4.9996e-04, -4.4066e-04, -3.4224e-04,\n",
      "        -2.2165e-04, -1.1455e-04, -8.5081e-05, -6.1155e-05, -5.9094e-05,\n",
      "        -4.1129e-05, -3.8978e-05, -3.7218e-05, -3.3906e-05, -2.8974e-05,\n",
      "        -2.5416e-05, -2.2800e-05, -2.1662e-05, -1.6131e-05, -1.2228e-05,\n",
      "        -1.0611e-05, -9.6930e-06, -8.0185e-06, -6.8752e-06, -6.5663e-06,\n",
      "        -4.9179e-06, -4.4192e-06, -3.7371e-06, -3.4602e-06, -3.2127e-06,\n",
      "        -2.7898e-06, -2.4899e-06, -2.2435e-06, -1.8401e-06, -1.7716e-06,\n",
      "        -1.7132e-06, -1.4024e-06, -1.3120e-06, -1.1833e-06, -1.1591e-06,\n",
      "        -1.0040e-06, -9.5207e-07, -9.0001e-07, -7.4919e-07, -7.0891e-07,\n",
      "        -6.6907e-07, -5.5443e-07, -5.0296e-07, -4.3765e-07, -4.1370e-07,\n",
      "        -3.4880e-07, -3.3042e-07, -3.0724e-07, -2.6710e-07, -2.6296e-07,\n",
      "        -2.3706e-07, -2.1147e-07, -1.7895e-07, -1.7134e-07, -1.6398e-07,\n",
      "        -1.3288e-07, -1.2314e-07, -1.0376e-07, -9.7869e-08, -9.0689e-08,\n",
      "        -8.1225e-08, -7.1353e-08, -7.0607e-08, -5.6367e-08, -5.2150e-08,\n",
      "        -4.4117e-08, -4.2468e-08, -3.7142e-08, -3.2450e-08, -3.0787e-08,\n",
      "        -2.5417e-08, -2.1461e-08, -1.9044e-08, -1.6204e-08, -1.5195e-08,\n",
      "        -1.2882e-08, -1.0645e-08, -9.3839e-09, -8.3366e-09, -6.6798e-09,\n",
      "        -5.1631e-09, -3.8966e-09, -2.9877e-09, -2.6283e-09, -1.8365e-09,\n",
      "        -1.4864e-09, -1.0667e-09, -7.0105e-10, -4.5862e-10, -3.6472e-10,\n",
      "        -1.6114e-10, -5.7581e-11,  9.0142e-11,  1.9128e-10,  4.1013e-10,\n",
      "         5.6797e-10,  7.6892e-10,  1.2397e-09,  1.7011e-09,  2.1857e-09,\n",
      "         2.3250e-09,  2.8142e-09,  3.5058e-09,  4.6618e-09,  5.1403e-09,\n",
      "         7.0835e-09,  7.9431e-09,  8.7423e-09,  9.7357e-09,  1.1782e-08,\n",
      "         1.3558e-08,  1.4003e-08,  1.7069e-08,  1.8006e-08,  2.3492e-08,\n",
      "         2.7787e-08,  3.1291e-08,  3.8639e-08,  4.5160e-08,  4.8354e-08,\n",
      "         6.2438e-08,  6.8176e-08,  7.4370e-08,  7.5143e-08,  8.4872e-08,\n",
      "         8.9285e-08,  9.8253e-08,  1.2393e-07,  1.3162e-07,  1.5174e-07,\n",
      "         1.6475e-07,  1.7528e-07,  2.0225e-07,  2.2388e-07,  2.6559e-07,\n",
      "         2.7943e-07,  3.1627e-07,  3.7563e-07,  4.1973e-07,  4.8390e-07,\n",
      "         5.0654e-07,  5.8029e-07,  5.9361e-07,  7.3898e-07,  8.4322e-07,\n",
      "         8.9647e-07,  1.0829e-06,  1.1247e-06,  1.3423e-06,  1.3796e-06,\n",
      "         1.6133e-06,  1.7116e-06,  2.2079e-06,  2.3013e-06,  2.4840e-06,\n",
      "         2.8450e-06,  3.1230e-06,  3.3718e-06,  3.8419e-06,  4.4013e-06,\n",
      "         4.6725e-06,  4.8953e-06,  6.8235e-06,  7.5486e-06,  7.9998e-06,\n",
      "         8.0887e-06,  9.2363e-06,  1.0172e-05,  1.2394e-05,  1.3104e-05,\n",
      "         1.4516e-05,  1.6942e-05,  1.8522e-05,  2.2496e-05,  2.3355e-05,\n",
      "         2.5970e-05,  2.8690e-05,  3.0666e-05,  3.9069e-05,  4.2227e-05,\n",
      "         4.6536e-05,  5.8138e-05,  6.1763e-05,  7.6124e-05,  8.6587e-05,\n",
      "         1.2287e-04,  1.5674e-04,  1.8159e-04,  2.4800e-04,  2.6589e-04,\n",
      "         4.6261e-04,  7.5895e-04,  1.8878e-03,  2.6120e-03,  1.5964e-02])\n",
      "wout = tensor([-5.2568e-04, -4.2119e-04, -2.8396e-04, -2.7856e-04, -2.3707e-04,\n",
      "        -2.1087e-04, -1.8214e-04, -1.7422e-04, -1.6101e-04, -1.5248e-04,\n",
      "        -1.4568e-04, -1.3794e-04, -1.2270e-04, -1.1875e-04, -1.1434e-04,\n",
      "        -1.1150e-04, -1.0412e-04, -9.7515e-05, -9.5252e-05, -9.3189e-05,\n",
      "        -8.4432e-05, -8.1524e-05, -7.8931e-05, -7.6843e-05, -7.6212e-05,\n",
      "        -7.5338e-05, -7.3176e-05, -6.6758e-05, -6.4295e-05, -5.8917e-05,\n",
      "        -5.6370e-05, -5.5323e-05, -4.9877e-05, -4.8806e-05, -4.5698e-05,\n",
      "        -4.4766e-05, -4.2851e-05, -4.1357e-05, -4.0101e-05, -3.6792e-05,\n",
      "        -3.1353e-05, -2.4393e-05, -2.2826e-05, -2.2429e-05, -1.9747e-05,\n",
      "        -1.8771e-05, -1.7221e-05, -1.7042e-05, -1.6740e-05, -1.6030e-05,\n",
      "        -1.3096e-05, -1.2141e-05, -1.0248e-05, -9.8289e-06, -8.5781e-06,\n",
      "        -8.3145e-06, -7.9060e-06, -6.9130e-06, -6.7020e-06, -6.6340e-06,\n",
      "        -6.2298e-06, -6.0019e-06, -5.3203e-06, -4.8315e-06, -4.3995e-06,\n",
      "        -4.3248e-06, -4.1492e-06, -3.8578e-06, -3.7682e-06, -3.5394e-06,\n",
      "        -3.4264e-06, -3.3713e-06, -3.2301e-06, -2.9627e-06, -2.7305e-06,\n",
      "        -2.2579e-06, -2.0814e-06, -1.9620e-06, -1.7490e-06, -1.6823e-06,\n",
      "        -1.4947e-06, -1.4466e-06, -1.4077e-06, -1.3656e-06, -1.1653e-06,\n",
      "        -1.0305e-06, -1.0164e-06, -7.2926e-07, -6.9770e-07, -4.2780e-07,\n",
      "        -2.9693e-07, -1.4710e-07, -6.8666e-09,  1.4766e-07,  2.4065e-07,\n",
      "         3.2961e-07,  3.9445e-07,  5.2423e-07,  6.1067e-07,  7.4269e-07,\n",
      "         1.1029e-06,  1.2106e-06,  1.3249e-06,  1.3675e-06,  1.5240e-06,\n",
      "         1.7956e-06,  2.0364e-06,  2.4422e-06,  2.7295e-06,  2.8421e-06,\n",
      "         2.8718e-06,  2.9256e-06,  2.9525e-06,  3.0852e-06,  3.1809e-06,\n",
      "         3.2662e-06,  3.3033e-06,  3.4177e-06,  3.6891e-06,  3.8135e-06,\n",
      "         4.4602e-06,  4.5282e-06,  4.8900e-06,  4.9793e-06,  5.3882e-06,\n",
      "         5.5031e-06,  5.8186e-06,  5.9759e-06,  6.0452e-06,  6.1936e-06,\n",
      "         6.8342e-06,  7.7900e-06,  8.7394e-06,  9.4011e-06,  1.0045e-05,\n",
      "         1.0255e-05,  1.1395e-05,  1.2363e-05,  1.2460e-05,  1.8990e-05,\n",
      "         2.0939e-05,  2.1337e-05,  2.2002e-05,  2.2456e-05,  2.3247e-05,\n",
      "         2.4349e-05,  2.6273e-05,  2.6713e-05,  2.7528e-05,  2.7764e-05,\n",
      "         2.9663e-05,  3.0480e-05,  3.1125e-05,  3.1582e-05,  3.4470e-05,\n",
      "         3.6286e-05,  4.1427e-05,  4.4418e-05,  4.5630e-05,  4.7678e-05,\n",
      "         4.9343e-05,  5.1371e-05,  5.4465e-05,  5.5963e-05,  6.0671e-05,\n",
      "         6.4032e-05,  6.4693e-05,  6.6253e-05,  6.7723e-05,  6.9174e-05,\n",
      "         7.3366e-05,  7.9987e-05,  8.0501e-05,  8.0907e-05,  9.0103e-05,\n",
      "         9.3138e-05,  9.6648e-05,  9.9530e-05,  1.0062e-04,  1.0180e-04,\n",
      "         1.0211e-04,  1.0693e-04,  1.1100e-04,  1.1829e-04,  1.1922e-04,\n",
      "         1.2208e-04,  1.2286e-04,  1.2694e-04,  1.2755e-04,  1.2909e-04,\n",
      "         1.4305e-04,  1.8018e-04,  2.1444e-04,  2.2846e-04,  2.4725e-04,\n",
      "         2.6849e-04,  2.7534e-04,  3.1634e-04,  4.5769e-04,  5.2078e-04])\n",
      "Task 2 Epoch 2/5 - Loss: 1.7826\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 27.58%\n",
      "  Unit similarity - Cosine: 0.6531, Euclidean: 1.0753\n",
      "Task 2 Epoch 3/5 - Loss: 1.7222\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 30.37%\n",
      "  Unit similarity - Cosine: 0.3027, Euclidean: 1.5334\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.377 \n",
      "bias = 0\n",
      "win = tensor([-7.2145e-03, -3.2735e-03, -2.5433e-03, -1.3208e-03, -7.9739e-04,\n",
      "        -4.0323e-04, -3.0897e-04, -2.5094e-04, -2.1776e-04, -1.9134e-04,\n",
      "        -1.5383e-04, -1.2014e-04, -1.1358e-04, -9.4930e-05, -8.6469e-05,\n",
      "        -7.4125e-05, -6.1935e-05, -5.8310e-05, -5.1311e-05, -4.4296e-05,\n",
      "        -3.7248e-05, -3.3269e-05, -3.0761e-05, -2.7259e-05, -2.4804e-05,\n",
      "        -2.0011e-05, -1.8513e-05, -1.7877e-05, -1.5668e-05, -1.3539e-05,\n",
      "        -1.2142e-05, -1.1090e-05, -1.0746e-05, -8.6985e-06, -8.3550e-06,\n",
      "        -7.4969e-06, -6.8661e-06, -6.2013e-06, -5.1682e-06, -4.2624e-06,\n",
      "        -3.9758e-06, -3.7311e-06, -3.4040e-06, -2.9783e-06, -2.7056e-06,\n",
      "        -2.6504e-06, -2.3460e-06, -2.1808e-06, -1.8474e-06, -1.5581e-06,\n",
      "        -1.4033e-06, -1.3186e-06, -1.0429e-06, -9.6165e-07, -8.3310e-07,\n",
      "        -7.3940e-07, -6.2521e-07, -5.7028e-07, -5.1201e-07, -4.4349e-07,\n",
      "        -3.8794e-07, -3.5431e-07, -3.1154e-07, -2.7574e-07, -2.5195e-07,\n",
      "        -2.1136e-07, -1.9916e-07, -1.8557e-07, -1.4013e-07, -1.2482e-07,\n",
      "        -1.1571e-07, -1.0250e-07, -7.6392e-08, -6.3981e-08, -5.9266e-08,\n",
      "        -5.5182e-08, -5.0007e-08, -4.3873e-08, -2.8077e-08, -2.5396e-08,\n",
      "        -2.1407e-08, -1.7749e-08, -1.5917e-08, -1.3682e-08, -1.2072e-08,\n",
      "        -8.7851e-09, -5.5961e-09, -5.3301e-09, -3.1483e-09, -2.5238e-09,\n",
      "        -1.9817e-09, -4.8980e-10, -1.9832e-10, -1.2243e-10,  1.8446e-10,\n",
      "         7.8087e-10,  1.2952e-09,  2.1282e-09,  2.6078e-09,  4.1946e-09,\n",
      "         4.7876e-09,  7.1480e-09,  8.1325e-09,  1.1043e-08,  1.3133e-08,\n",
      "         1.6897e-08,  1.9085e-08,  2.1641e-08,  2.3263e-08,  2.9861e-08,\n",
      "         3.1616e-08,  4.1609e-08,  4.2984e-08,  4.8571e-08,  4.9510e-08,\n",
      "         5.6260e-08,  6.7475e-08,  7.2493e-08,  8.0930e-08,  8.8929e-08,\n",
      "         9.4399e-08,  1.0234e-07,  1.3568e-07,  1.4709e-07,  1.6015e-07,\n",
      "         1.7510e-07,  1.8411e-07,  2.1412e-07,  2.1649e-07,  2.5947e-07,\n",
      "         2.7207e-07,  2.8961e-07,  3.2328e-07,  3.8412e-07,  4.4009e-07,\n",
      "         4.7952e-07,  5.0011e-07,  5.7690e-07,  6.1261e-07,  6.5776e-07,\n",
      "         7.6216e-07,  9.0686e-07,  9.9022e-07,  1.1325e-06,  1.1896e-06,\n",
      "         1.3313e-06,  1.4335e-06,  1.6192e-06,  1.7324e-06,  1.8130e-06,\n",
      "         1.9515e-06,  2.1319e-06,  2.2887e-06,  2.6297e-06,  2.7870e-06,\n",
      "         3.0923e-06,  3.3070e-06,  3.7098e-06,  4.0131e-06,  4.1999e-06,\n",
      "         4.7511e-06,  5.0466e-06,  5.5495e-06,  6.0699e-06,  6.9674e-06,\n",
      "         7.2942e-06,  7.8514e-06,  8.4687e-06,  9.3254e-06,  1.0031e-05,\n",
      "         1.0347e-05,  1.1006e-05,  1.2728e-05,  1.4316e-05,  1.5399e-05,\n",
      "         1.6487e-05,  1.8917e-05,  1.9991e-05,  2.1565e-05,  2.3977e-05,\n",
      "         3.1489e-05,  3.5138e-05,  3.9539e-05,  4.2644e-05,  5.2276e-05,\n",
      "         5.8669e-05,  7.0132e-05,  7.3618e-05,  8.5591e-05,  1.0302e-04,\n",
      "         1.1833e-04,  1.4187e-04,  1.9882e-04,  2.1166e-04,  3.0385e-04,\n",
      "         5.1702e-04,  8.9658e-04,  1.5434e-03,  8.8581e-03,  2.1677e-02])\n",
      "wout = tensor([-1.2519e-03, -1.1937e-03, -1.0089e-03, -9.3027e-04, -8.8287e-04,\n",
      "        -8.3882e-04, -7.8744e-04, -7.3104e-04, -7.1908e-04, -6.4863e-04,\n",
      "        -5.7095e-04, -5.5021e-04, -5.4338e-04, -5.2228e-04, -5.1086e-04,\n",
      "        -4.9425e-04, -4.6835e-04, -4.4968e-04, -4.3957e-04, -4.2756e-04,\n",
      "        -4.1344e-04, -4.0636e-04, -3.8973e-04, -3.6787e-04, -3.6063e-04,\n",
      "        -3.4670e-04, -3.3700e-04, -3.3233e-04, -3.0662e-04, -2.9577e-04,\n",
      "        -2.8056e-04, -2.7339e-04, -2.7089e-04, -2.5767e-04, -2.5019e-04,\n",
      "        -2.4044e-04, -2.2851e-04, -2.1671e-04, -2.0822e-04, -2.0030e-04,\n",
      "        -1.9661e-04, -1.5937e-04, -1.5590e-04, -1.4331e-04, -1.2987e-04,\n",
      "        -1.2087e-04, -1.1139e-04, -1.0672e-04, -1.0048e-04, -8.6976e-05,\n",
      "        -8.2792e-05, -7.8720e-05, -6.3397e-05, -5.2446e-05, -5.1547e-05,\n",
      "        -4.6956e-05, -4.4561e-05, -4.3432e-05, -3.9785e-05, -3.6077e-05,\n",
      "        -3.4509e-05, -3.3599e-05, -3.3283e-05, -3.1901e-05, -2.7948e-05,\n",
      "        -2.7597e-05, -2.6453e-05, -2.3141e-05, -2.2689e-05, -2.0387e-05,\n",
      "        -2.0283e-05, -1.9159e-05, -1.8936e-05, -1.7941e-05, -1.7322e-05,\n",
      "        -1.5323e-05, -1.2681e-05, -1.2345e-05, -9.7043e-06, -8.9157e-06,\n",
      "        -8.1475e-06, -6.9182e-06, -6.5337e-06, -6.4242e-06, -6.0054e-06,\n",
      "        -5.8143e-06, -5.5125e-06, -5.3249e-06, -4.7745e-06, -4.4879e-06,\n",
      "        -4.4695e-06, -4.3302e-06, -4.1441e-06, -4.0837e-06, -4.0353e-06,\n",
      "        -3.9984e-06, -3.1986e-06, -2.7617e-06, -2.5196e-06, -2.2935e-06,\n",
      "        -2.2231e-06, -2.1762e-06, -2.0794e-06, -1.8130e-06, -1.4928e-06,\n",
      "        -1.3478e-06, -5.0293e-07, -4.3849e-07, -4.2907e-07, -2.4702e-07,\n",
      "        -1.5736e-07, -1.1429e-07,  4.9318e-08,  2.8128e-07,  4.3563e-07,\n",
      "         9.6073e-07,  1.5817e-06,  2.2086e-06,  2.8437e-06,  4.2846e-06,\n",
      "         5.3600e-06,  7.2209e-06,  9.5480e-06,  1.0595e-05,  1.1693e-05,\n",
      "         1.2787e-05,  1.3722e-05,  1.3949e-05,  1.4281e-05,  1.4406e-05,\n",
      "         1.5960e-05,  1.8021e-05,  1.9759e-05,  2.1016e-05,  2.2135e-05,\n",
      "         2.2541e-05,  2.3007e-05,  2.3775e-05,  2.3872e-05,  2.5663e-05,\n",
      "         2.7198e-05,  3.0134e-05,  3.3055e-05,  3.3162e-05,  3.4126e-05,\n",
      "         3.6413e-05,  3.6516e-05,  3.8738e-05,  3.9873e-05,  4.3766e-05,\n",
      "         4.5505e-05,  4.6390e-05,  4.7431e-05,  5.0227e-05,  5.0367e-05,\n",
      "         5.2965e-05,  5.3936e-05,  5.5929e-05,  5.7630e-05,  6.2474e-05,\n",
      "         7.0483e-05,  7.1950e-05,  7.5503e-05,  7.7977e-05,  7.8928e-05,\n",
      "         8.5201e-05,  9.8698e-05,  1.0324e-04,  1.0489e-04,  1.2095e-04,\n",
      "         1.2743e-04,  1.3047e-04,  1.3840e-04,  1.4534e-04,  1.6543e-04,\n",
      "         1.7287e-04,  1.7678e-04,  1.8678e-04,  1.8842e-04,  1.9554e-04,\n",
      "         2.0302e-04,  2.1186e-04,  2.2060e-04,  2.3336e-04,  2.4623e-04,\n",
      "         2.7140e-04,  2.8894e-04,  3.0329e-04,  4.2856e-04,  4.6687e-04,\n",
      "         5.6878e-04,  6.5700e-04,  6.8756e-04,  7.2341e-04,  7.8198e-04,\n",
      "         9.4088e-04,  1.0170e-03,  1.1310e-03,  1.3002e-03,  3.2883e-03])\n",
      "Task 2 Epoch 4/5 - Loss: 1.7208\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 29.47%\n",
      "  Unit similarity - Cosine: 0.5452, Euclidean: 1.3552\n",
      "Task 2 Epoch 5/5 - Loss: 1.7018\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 27.28%\n",
      "  Unit similarity - Cosine: 0.6765, Euclidean: 1.2423\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9901 (after coupling) -> 0.6765 (final)\n",
      "Euclidean distance: 0.1019 (after coupling) -> 1.2423 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 0.15% (before Task 2) -> 0.00% (after Task 2)\n",
      "Catastrophic forgetting observed: 0.15% drop in Task 1 performance.\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 1, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0431, Euclidean distance: 0.8468\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/1 - Loss: 1.6277\n",
      "  Acc (Task 1): 45.55%, Acc (Task 2): 0.03%\n",
      "  Unit similarity - Cosine: -0.3013, Euclidean: 1.4591\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9898\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9912\n",
      "Before coupling: cosine=-0.3013, distance=1.4591\n",
      "After coupling: cosine=0.9898, distance=0.1247\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.0151\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 26.17%\n",
      "  Unit similarity - Cosine: 0.9279, Euclidean: 0.3545\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.708 \n",
      "bias = 0\n",
      "win = tensor([-6.0712e-02, -9.2974e-03, -4.1200e-03, -3.8116e-03, -1.6619e-03,\n",
      "        -7.4493e-04, -4.3386e-04, -4.2045e-04, -3.3256e-04, -3.0369e-04,\n",
      "        -2.4745e-04, -2.2404e-04, -1.9218e-04, -1.7466e-04, -1.4335e-04,\n",
      "        -1.3484e-04, -1.1420e-04, -1.0413e-04, -8.4540e-05, -8.0312e-05,\n",
      "        -7.4140e-05, -6.0548e-05, -5.2580e-05, -4.7088e-05, -4.2015e-05,\n",
      "        -3.7081e-05, -3.4510e-05, -3.0571e-05, -2.6268e-05, -2.5020e-05,\n",
      "        -2.3558e-05, -2.0227e-05, -1.9318e-05, -1.7100e-05, -1.5095e-05,\n",
      "        -1.3975e-05, -1.2360e-05, -1.2176e-05, -1.1364e-05, -9.3803e-06,\n",
      "        -8.6645e-06, -7.6467e-06, -6.9029e-06, -5.9068e-06, -5.4026e-06,\n",
      "        -4.9701e-06, -4.2287e-06, -3.7905e-06, -3.4947e-06, -3.0334e-06,\n",
      "        -2.9218e-06, -2.7816e-06, -2.4790e-06, -2.1619e-06, -2.1224e-06,\n",
      "        -1.8892e-06, -1.7137e-06, -1.6184e-06, -1.4923e-06, -1.3396e-06,\n",
      "        -1.2650e-06, -1.1214e-06, -9.6584e-07, -8.7876e-07, -7.7429e-07,\n",
      "        -7.2549e-07, -6.1112e-07, -5.7604e-07, -5.3374e-07, -4.3050e-07,\n",
      "        -3.6429e-07, -3.4224e-07, -3.3151e-07, -3.0262e-07, -2.5558e-07,\n",
      "        -2.1230e-07, -1.9905e-07, -1.7399e-07, -1.3752e-07, -1.1419e-07,\n",
      "        -1.1165e-07, -7.8217e-08, -6.7659e-08, -5.6072e-08, -4.3112e-08,\n",
      "        -4.2238e-08, -3.8964e-08, -3.2711e-08, -2.6579e-08, -2.4946e-08,\n",
      "        -2.0547e-08, -1.5566e-08, -1.2661e-08, -1.0229e-08, -7.2231e-09,\n",
      "        -3.3274e-09, -2.1804e-09, -1.4029e-09, -6.5336e-10, -3.5318e-10,\n",
      "        -1.2003e-10,  1.9416e-10,  3.7474e-10,  1.2160e-09,  2.7816e-09,\n",
      "         4.5370e-09,  7.4544e-09,  9.9578e-09,  1.2793e-08,  1.3949e-08,\n",
      "         2.0114e-08,  2.3034e-08,  3.0170e-08,  4.6845e-08,  5.2449e-08,\n",
      "         6.1789e-08,  6.6134e-08,  6.9201e-08,  9.1582e-08,  9.7282e-08,\n",
      "         1.2081e-07,  1.3589e-07,  1.5862e-07,  2.0836e-07,  2.1488e-07,\n",
      "         2.9505e-07,  3.0325e-07,  3.4507e-07,  3.6988e-07,  3.9510e-07,\n",
      "         4.4164e-07,  4.8495e-07,  5.5098e-07,  6.0406e-07,  6.2826e-07,\n",
      "         7.8970e-07,  9.0707e-07,  1.0885e-06,  1.1258e-06,  1.3228e-06,\n",
      "         1.5214e-06,  1.6482e-06,  1.7008e-06,  1.8926e-06,  2.0200e-06,\n",
      "         2.1917e-06,  2.3706e-06,  2.5057e-06,  2.7444e-06,  2.8524e-06,\n",
      "         2.9394e-06,  3.4805e-06,  3.6760e-06,  4.2088e-06,  4.4508e-06,\n",
      "         5.7051e-06,  6.5645e-06,  6.7426e-06,  7.1350e-06,  7.6732e-06,\n",
      "         8.6774e-06,  9.6224e-06,  1.0277e-05,  1.0800e-05,  1.2245e-05,\n",
      "         1.3589e-05,  1.4471e-05,  1.6215e-05,  1.7985e-05,  2.2844e-05,\n",
      "         2.4330e-05,  2.7715e-05,  3.0812e-05,  3.1438e-05,  3.2330e-05,\n",
      "         3.8788e-05,  3.9721e-05,  4.9350e-05,  5.4188e-05,  6.1575e-05,\n",
      "         6.4504e-05,  7.1188e-05,  7.3837e-05,  9.4013e-05,  1.0630e-04,\n",
      "         1.1893e-04,  1.3377e-04,  1.6479e-04,  1.7882e-04,  2.2574e-04,\n",
      "         2.7034e-04,  3.2104e-04,  3.4735e-04,  4.2015e-04,  5.7590e-04,\n",
      "         9.5699e-04,  1.3021e-03,  1.3585e-03,  3.0960e-03,  6.9167e-03])\n",
      "wout = tensor([-3.0325e-03, -2.4106e-03, -2.2819e-03, -1.7547e-03, -1.6870e-03,\n",
      "        -1.4785e-03, -1.3226e-03, -1.2648e-03, -1.2570e-03, -1.1999e-03,\n",
      "        -1.1415e-03, -1.1245e-03, -1.0567e-03, -1.0108e-03, -9.4205e-04,\n",
      "        -8.4309e-04, -7.2126e-04, -4.8619e-04, -4.6666e-04, -4.5830e-04,\n",
      "        -4.4207e-04, -3.9527e-04, -3.5992e-04, -3.4013e-04, -3.0495e-04,\n",
      "        -2.7077e-04, -2.6027e-04, -2.4236e-04, -2.0679e-04, -1.6815e-04,\n",
      "        -1.5223e-04, -1.4880e-04, -1.4351e-04, -1.2332e-04, -1.1939e-04,\n",
      "        -1.0061e-04, -8.4908e-05, -7.6557e-05, -7.1978e-05, -7.0312e-05,\n",
      "        -6.8664e-05, -6.5046e-05, -5.8283e-05, -5.4219e-05, -4.5785e-05,\n",
      "        -4.5165e-05, -3.3288e-05, -3.2420e-05, -3.1235e-05, -3.0015e-05,\n",
      "        -2.7983e-05, -2.7039e-05, -2.5504e-05, -2.4600e-05, -2.4366e-05,\n",
      "        -2.2331e-05, -2.2108e-05, -2.1529e-05, -1.8207e-05, -1.6609e-05,\n",
      "        -1.6444e-05, -1.5218e-05, -1.4645e-05, -1.4214e-05, -1.3826e-05,\n",
      "        -1.3239e-05, -1.2963e-05, -1.2302e-05, -1.1847e-05, -1.0845e-05,\n",
      "        -9.3800e-06, -9.2437e-06, -8.5606e-06, -6.7587e-06, -6.4150e-06,\n",
      "        -6.0318e-06, -5.3802e-06, -4.8363e-06, -3.8683e-06, -3.6921e-06,\n",
      "        -3.2424e-06, -2.6545e-06, -2.1946e-06, -2.1361e-06, -1.9392e-06,\n",
      "        -1.9030e-06, -1.7991e-06, -1.3101e-06, -1.1829e-06, -9.9767e-07,\n",
      "        -9.1705e-07, -6.3073e-07, -5.3094e-07, -4.0032e-07, -3.4623e-07,\n",
      "        -2.9939e-07, -4.0801e-08, -3.1419e-08, -1.0382e-08, -6.1922e-09,\n",
      "        -4.3477e-09,  2.0581e-09,  1.5189e-08,  1.7228e-08,  3.5727e-08,\n",
      "         7.0181e-08,  7.7645e-08,  1.7333e-07,  1.8589e-07,  2.2003e-07,\n",
      "         2.2915e-07,  2.4930e-07,  2.7924e-07,  4.1747e-07,  4.8227e-07,\n",
      "         5.4451e-07,  8.2463e-07,  1.1443e-06,  1.1597e-06,  1.3166e-06,\n",
      "         1.3467e-06,  1.4032e-06,  1.4775e-06,  1.4950e-06,  1.7505e-06,\n",
      "         1.9083e-06,  2.0195e-06,  2.0679e-06,  2.3008e-06,  2.5999e-06,\n",
      "         2.6629e-06,  2.7854e-06,  3.3522e-06,  3.5139e-06,  3.7720e-06,\n",
      "         3.8513e-06,  4.1611e-06,  4.2906e-06,  5.2394e-06,  5.3587e-06,\n",
      "         5.7661e-06,  6.1126e-06,  6.4155e-06,  6.7706e-06,  7.0512e-06,\n",
      "         7.1396e-06,  7.4595e-06,  8.1692e-06,  8.2741e-06,  8.9503e-06,\n",
      "         9.4925e-06,  1.0331e-05,  1.1613e-05,  1.1751e-05,  1.2333e-05,\n",
      "         1.2466e-05,  1.3032e-05,  1.3243e-05,  1.3856e-05,  1.6482e-05,\n",
      "         1.7208e-05,  1.7877e-05,  1.8233e-05,  2.0223e-05,  2.1590e-05,\n",
      "         2.3712e-05,  2.5173e-05,  2.6251e-05,  2.7430e-05,  3.2775e-05,\n",
      "         4.2743e-05,  5.9007e-05,  7.5718e-05,  1.0255e-04,  1.0306e-04,\n",
      "         1.1418e-04,  1.3195e-04,  1.5503e-04,  1.5579e-04,  1.5881e-04,\n",
      "         1.6905e-04,  2.0379e-04,  2.3442e-04,  2.5072e-04,  2.8162e-04,\n",
      "         3.0915e-04,  3.3685e-04,  3.5174e-04,  3.6105e-04,  4.4049e-04,\n",
      "         4.6669e-04,  5.4840e-04,  5.9920e-04,  6.7272e-04,  9.5470e-04,\n",
      "         9.8856e-04,  1.4260e-03,  1.6570e-03,  2.9476e-03,  6.7381e-03])\n",
      "Task 2 Epoch 2/5 - Loss: 1.7456\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 29.38%\n",
      "  Unit similarity - Cosine: 0.7005, Euclidean: 0.8106\n",
      "Task 2 Epoch 3/5 - Loss: 1.7125\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 25.45%\n",
      "  Unit similarity - Cosine: 0.7915, Euclidean: 0.7116\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.789 \n",
      "bias = 0\n",
      "win = tensor([-3.2932e-02, -8.2847e-03, -4.6648e-03, -2.6111e-03, -1.7248e-03,\n",
      "        -1.2708e-03, -9.0506e-04, -6.7500e-04, -5.7232e-04, -4.7686e-04,\n",
      "        -3.7249e-04, -3.2427e-04, -2.9530e-04, -2.5593e-04, -2.2625e-04,\n",
      "        -2.0143e-04, -1.8904e-04, -1.6858e-04, -1.5936e-04, -1.3261e-04,\n",
      "        -1.1124e-04, -1.0664e-04, -9.0798e-05, -8.4879e-05, -8.0606e-05,\n",
      "        -7.4342e-05, -7.0036e-05, -6.2681e-05, -5.7960e-05, -5.2847e-05,\n",
      "        -5.0711e-05, -4.8576e-05, -4.0137e-05, -3.4279e-05, -3.3304e-05,\n",
      "        -2.8944e-05, -2.7749e-05, -2.4940e-05, -2.4408e-05, -2.1749e-05,\n",
      "        -1.9503e-05, -1.8853e-05, -1.6764e-05, -1.5249e-05, -1.4144e-05,\n",
      "        -1.2716e-05, -1.1812e-05, -1.1697e-05, -9.8164e-06, -9.2160e-06,\n",
      "        -8.3953e-06, -7.9828e-06, -7.5315e-06, -6.9606e-06, -6.2474e-06,\n",
      "        -5.9276e-06, -4.7833e-06, -4.4733e-06, -4.2538e-06, -3.6478e-06,\n",
      "        -3.2784e-06, -2.8782e-06, -2.6431e-06, -2.3790e-06, -2.1406e-06,\n",
      "        -1.8764e-06, -1.4369e-06, -1.3536e-06, -1.3164e-06, -9.8806e-07,\n",
      "        -9.0630e-07, -8.4740e-07, -7.6146e-07, -5.8886e-07, -5.3200e-07,\n",
      "        -4.0977e-07, -3.3802e-07, -3.0596e-07, -2.7904e-07, -2.6356e-07,\n",
      "        -2.1588e-07, -1.8755e-07, -1.7323e-07, -1.4105e-07, -1.0707e-07,\n",
      "        -8.9914e-08, -7.5617e-08, -6.8826e-08, -5.1010e-08, -4.2336e-08,\n",
      "        -3.4348e-08, -2.9459e-08, -2.5386e-08, -2.0314e-08, -1.6976e-08,\n",
      "        -1.4242e-08, -8.3399e-09, -6.3352e-09, -4.7258e-09, -3.0485e-09,\n",
      "        -1.9422e-09, -1.6954e-09, -1.4700e-09, -8.5028e-10, -4.8707e-10,\n",
      "        -3.4482e-10, -1.5677e-10,  3.8653e-11,  9.1802e-11,  1.6198e-10,\n",
      "         3.2159e-10,  7.4047e-10,  8.4030e-10,  1.0309e-09,  1.9863e-09,\n",
      "         2.6346e-09,  3.3584e-09,  4.1679e-09,  5.4350e-09,  8.5260e-09,\n",
      "         1.0261e-08,  1.1806e-08,  1.2881e-08,  2.1126e-08,  2.3830e-08,\n",
      "         2.8703e-08,  2.9260e-08,  4.6272e-08,  5.2822e-08,  6.1978e-08,\n",
      "         8.3128e-08,  1.0467e-07,  1.2610e-07,  1.3822e-07,  1.6816e-07,\n",
      "         2.1592e-07,  3.1563e-07,  3.5628e-07,  4.0711e-07,  4.7155e-07,\n",
      "         5.9441e-07,  7.2241e-07,  7.4904e-07,  1.1084e-06,  1.2353e-06,\n",
      "         1.4046e-06,  1.5961e-06,  1.8245e-06,  1.9487e-06,  2.4061e-06,\n",
      "         2.7471e-06,  3.2921e-06,  3.3293e-06,  3.9304e-06,  4.5222e-06,\n",
      "         4.9062e-06,  5.4439e-06,  6.5966e-06,  7.0309e-06,  7.3969e-06,\n",
      "         8.1675e-06,  8.4851e-06,  1.0524e-05,  1.1803e-05,  1.3356e-05,\n",
      "         1.3844e-05,  1.4575e-05,  1.5709e-05,  1.7481e-05,  2.0470e-05,\n",
      "         2.1963e-05,  2.2940e-05,  2.7718e-05,  3.1695e-05,  3.2895e-05,\n",
      "         3.8824e-05,  4.1637e-05,  4.3390e-05,  5.4567e-05,  5.7948e-05,\n",
      "         6.7204e-05,  7.3076e-05,  8.1496e-05,  9.8645e-05,  1.0606e-04,\n",
      "         1.2976e-04,  1.4469e-04,  1.5259e-04,  1.6698e-04,  1.6932e-04,\n",
      "         2.0911e-04,  2.4443e-04,  3.0973e-04,  3.4876e-04,  3.7083e-04,\n",
      "         4.9862e-04,  7.2211e-04,  9.4580e-04,  1.9217e-03,  1.3085e-02])\n",
      "wout = tensor([-1.5689e-03, -1.2348e-03, -1.1908e-03, -1.1788e-03, -1.0992e-03,\n",
      "        -1.0523e-03, -1.0275e-03, -9.5066e-04, -8.5304e-04, -7.7751e-04,\n",
      "        -7.6108e-04, -7.2565e-04, -7.1101e-04, -6.3776e-04, -6.0960e-04,\n",
      "        -5.9450e-04, -4.8717e-04, -4.5225e-04, -4.4901e-04, -4.3045e-04,\n",
      "        -3.7457e-04, -3.5406e-04, -3.3567e-04, -3.1871e-04, -2.9162e-04,\n",
      "        -2.5779e-04, -2.2742e-04, -2.1323e-04, -1.9496e-04, -1.7419e-04,\n",
      "        -1.6558e-04, -1.5095e-04, -1.4331e-04, -1.3466e-04, -1.0874e-04,\n",
      "        -1.0350e-04, -8.8861e-05, -7.2984e-05, -7.0099e-05, -5.4318e-05,\n",
      "        -5.3425e-05, -4.8732e-05, -4.5807e-05, -4.1831e-05, -4.1365e-05,\n",
      "        -4.1110e-05, -3.7928e-05, -3.6941e-05, -3.5463e-05, -3.5050e-05,\n",
      "        -3.1562e-05, -2.8956e-05, -2.4782e-05, -2.4282e-05, -2.1046e-05,\n",
      "        -1.7306e-05, -1.6844e-05, -1.5264e-05, -1.4348e-05, -1.3338e-05,\n",
      "        -1.2595e-05, -1.2581e-05, -1.2240e-05, -1.2109e-05, -1.0006e-05,\n",
      "        -9.5809e-06, -8.7288e-06, -6.4904e-06, -6.1429e-06, -5.9585e-06,\n",
      "        -5.6850e-06, -5.0685e-06, -5.0138e-06, -3.7820e-06, -2.6440e-06,\n",
      "        -2.5587e-06, -2.4382e-06, -1.9641e-06, -1.6987e-06, -1.3756e-06,\n",
      "        -1.0772e-06, -9.9100e-07, -9.1489e-07, -8.1903e-07, -7.8586e-07,\n",
      "        -7.5802e-07, -7.3283e-07, -4.7924e-07, -4.7324e-07, -1.6660e-07,\n",
      "        -1.1922e-07, -9.6143e-08,  2.5681e-09,  3.4457e-09,  3.3210e-08,\n",
      "         6.0064e-08,  6.8606e-08,  1.0829e-07,  1.4701e-07,  1.7109e-07,\n",
      "         1.7270e-07,  2.6056e-07,  6.1448e-07,  6.3743e-07,  7.4837e-07,\n",
      "         9.4211e-07,  1.1690e-06,  1.3694e-06,  1.8170e-06,  2.0098e-06,\n",
      "         2.0172e-06,  2.2493e-06,  2.2917e-06,  2.3179e-06,  2.6855e-06,\n",
      "         3.1916e-06,  3.2923e-06,  3.7383e-06,  3.8258e-06,  3.8315e-06,\n",
      "         4.2102e-06,  4.4692e-06,  4.9527e-06,  6.6373e-06,  7.0404e-06,\n",
      "         7.3089e-06,  7.4879e-06,  8.0485e-06,  9.8979e-06,  1.0997e-05,\n",
      "         1.2513e-05,  1.2757e-05,  1.2839e-05,  1.3130e-05,  1.3682e-05,\n",
      "         1.4479e-05,  1.4836e-05,  1.4939e-05,  1.5578e-05,  1.5602e-05,\n",
      "         1.6285e-05,  1.8440e-05,  1.9853e-05,  2.3751e-05,  2.4054e-05,\n",
      "         2.4719e-05,  2.5149e-05,  2.7516e-05,  2.8908e-05,  2.9736e-05,\n",
      "         3.0458e-05,  3.0924e-05,  3.2086e-05,  3.2540e-05,  3.3501e-05,\n",
      "         3.4062e-05,  3.4716e-05,  3.5182e-05,  3.5852e-05,  3.8365e-05,\n",
      "         4.1016e-05,  4.3702e-05,  4.6675e-05,  4.8120e-05,  5.0255e-05,\n",
      "         5.1463e-05,  5.3812e-05,  5.4435e-05,  5.6472e-05,  5.7629e-05,\n",
      "         6.2252e-05,  7.4101e-05,  7.6036e-05,  8.3400e-05,  9.3473e-05,\n",
      "         9.4277e-05,  1.0458e-04,  1.1842e-04,  1.2163e-04,  1.2350e-04,\n",
      "         1.3570e-04,  1.8140e-04,  2.3322e-04,  2.4478e-04,  3.1405e-04,\n",
      "         3.4287e-04,  4.3949e-04,  4.9486e-04,  5.3911e-04,  5.9425e-04,\n",
      "         6.6238e-04,  7.5714e-04,  8.0460e-04,  8.3253e-04,  9.2911e-04,\n",
      "         1.0723e-03,  1.4658e-03,  1.7283e-03,  1.9265e-03,  2.8253e-03])\n",
      "Task 2 Epoch 4/5 - Loss: 1.6872\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 32.00%\n",
      "  Unit similarity - Cosine: 0.6243, Euclidean: 1.0427\n",
      "Task 2 Epoch 5/5 - Loss: 1.6778\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 32.68%\n",
      "  Unit similarity - Cosine: 0.4225, Euclidean: 1.3709\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9898 (after coupling) -> 0.4225 (final)\n",
      "Euclidean distance: 0.1247 (after coupling) -> 1.3709 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 0.00% (before Task 2) -> 0.00% (after Task 2)\n",
      "No forgetting observed. Task 1 performance improved by -0.00%.\n",
      "\n",
      "\n",
      "\n",
      "experimental config =  {'first_classes': [0, 1, 8, 9], 'second_classes': [2, 5, 6, 7, 3, 4], 'hidden_sizes': [200, 200, 200, 200, 200, 200], 'activation_type': 'tanh', 'couple_layer': 3, 'couple_units': (0, 1), 'epsilon': 0.01, 'epochs_first_task': 1, 'epochs_second_task': 5, 'learning_rate': 0.01, 'batch_size': 512, 'print_model_step': 100, 'train_steps': 10000, 'train2_steps': 10000}\n",
      "First task classes: ['airplane', 'automobile', 'ship', 'truck']\n",
      "Second task classes: ['bird', 'dog', 'frog', 'horse', 'cat', 'deer']\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "First task: 20000 training samples, 4000 test samples\n",
      "Second task: 30000 training samples, 6000 test samples\n",
      "Created MLP with architecture: 3072 -> 200 -> 200 -> 200 -> 200 -> 200 -> 200 -> 10\n",
      "Activation function: tanh\n",
      "Will couple units 0 and 1 in layer 3, meaning we couple \n",
      "rows  0 and 1 of weight matrix layer 3\n",
      "and columns 0 and 1 of weight matrix layer 4\n",
      " using 0.010 perturbation\n",
      "Initial cosine similarity: -0.0206, Euclidean distance: 0.8360\n",
      "\n",
      "===== Phase 1: Training on first set of classes =====\n",
      "Task 1 Epoch 1/1 - Loss: 1.7876\n",
      "  Acc (Task 1): 42.45%, Acc (Task 2): 0.40%\n",
      "  Unit similarity - Cosine: -0.1431, Euclidean: 1.3504\n",
      "\n",
      "===== Applying unit coupling =====\n",
      "DEBUG - After coupling incoming weights: cosine similarity = 0.9886\n",
      "DEBUG - After coupling outgoing weights: cosine similarity = 0.9908\n",
      "Before coupling: cosine=-0.1431, distance=1.3504\n",
      "After coupling: cosine=0.9886, distance=0.1332\n",
      "\n",
      "===== Phase 2: Training on second set of classes =====\n",
      "Task 2 Epoch 1/5 - Loss: 2.3457\n",
      "  Acc (Task 1): 12.62%, Acc (Task 2): 18.60%\n",
      "  Unit similarity - Cosine: 0.8896, Euclidean: 0.5301\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 1, step 100, cos_sim = 0.895 \n",
      "bias = 0\n",
      "win = tensor([-8.8402e-03, -2.1673e-03, -7.2804e-04, -3.1121e-04, -2.3758e-04,\n",
      "        -1.2637e-04, -8.7806e-05, -8.1555e-05, -4.0039e-05, -3.7413e-05,\n",
      "        -2.5701e-05, -2.2322e-05, -1.9935e-05, -1.7044e-05, -1.4574e-05,\n",
      "        -1.0664e-05, -9.4086e-06, -8.9971e-06, -7.6324e-06, -7.5762e-06,\n",
      "        -6.5591e-06, -5.8779e-06, -4.9345e-06, -3.6070e-06, -3.3645e-06,\n",
      "        -3.0685e-06, -3.0103e-06, -2.6135e-06, -2.0509e-06, -2.0298e-06,\n",
      "        -1.6690e-06, -1.5305e-06, -1.3733e-06, -1.1940e-06, -1.1473e-06,\n",
      "        -8.6205e-07, -7.6529e-07, -7.0195e-07, -6.4069e-07, -6.2163e-07,\n",
      "        -5.4821e-07, -5.1068e-07, -4.4166e-07, -3.7585e-07, -3.5839e-07,\n",
      "        -3.3859e-07, -2.7310e-07, -2.2847e-07, -1.8936e-07, -1.8129e-07,\n",
      "        -1.4784e-07, -1.3157e-07, -1.2201e-07, -1.0420e-07, -9.2266e-08,\n",
      "        -8.8683e-08, -7.2038e-08, -6.4661e-08, -6.0880e-08, -5.8179e-08,\n",
      "        -4.8422e-08, -3.8484e-08, -3.2991e-08, -2.7883e-08, -2.3783e-08,\n",
      "        -2.0351e-08, -1.9927e-08, -1.6030e-08, -1.2826e-08, -1.1892e-08,\n",
      "        -1.1596e-08, -9.3586e-09, -7.9835e-09, -7.2465e-09, -5.8025e-09,\n",
      "        -5.4967e-09, -4.4252e-09, -4.2553e-09, -3.6177e-09, -2.9088e-09,\n",
      "        -2.5582e-09, -2.1972e-09, -1.9539e-09, -1.4699e-09, -1.4361e-09,\n",
      "        -1.0915e-09, -9.9800e-10, -7.5939e-10, -6.4639e-10, -5.5773e-10,\n",
      "        -4.9381e-10, -4.4394e-10, -4.0776e-10, -2.6907e-10, -2.0486e-10,\n",
      "        -1.6899e-10, -1.6291e-10, -1.0059e-10, -9.6639e-11, -7.9390e-11,\n",
      "        -4.8688e-11, -3.3691e-11, -2.2802e-11, -1.0194e-11, -3.2299e-12,\n",
      "        -8.0975e-13,  1.4822e-11,  1.9979e-11,  3.3445e-11,  4.0668e-11,\n",
      "         5.6745e-11,  6.7443e-11,  1.0244e-10,  1.3227e-10,  1.9008e-10,\n",
      "         2.8390e-10,  3.3342e-10,  4.1607e-10,  5.1102e-10,  6.1172e-10,\n",
      "         8.1587e-10,  1.1266e-09,  1.2430e-09,  1.4876e-09,  1.6427e-09,\n",
      "         2.1728e-09,  2.5369e-09,  3.1231e-09,  3.6039e-09,  3.9319e-09,\n",
      "         4.4208e-09,  5.0201e-09,  6.0042e-09,  6.6526e-09,  7.8357e-09,\n",
      "         9.4618e-09,  9.8385e-09,  1.1310e-08,  1.5037e-08,  1.7321e-08,\n",
      "         2.0566e-08,  2.1166e-08,  2.2430e-08,  2.5149e-08,  2.9547e-08,\n",
      "         3.0574e-08,  3.4326e-08,  3.5058e-08,  3.9533e-08,  4.6739e-08,\n",
      "         5.4032e-08,  5.9760e-08,  7.0690e-08,  8.8346e-08,  1.0901e-07,\n",
      "         1.2062e-07,  1.3763e-07,  1.5377e-07,  1.6720e-07,  1.7905e-07,\n",
      "         2.0829e-07,  2.3098e-07,  2.6965e-07,  2.9180e-07,  3.2233e-07,\n",
      "         3.5547e-07,  4.4747e-07,  4.8643e-07,  5.2158e-07,  7.1772e-07,\n",
      "         7.6512e-07,  8.4116e-07,  1.0334e-06,  1.1720e-06,  1.4648e-06,\n",
      "         1.5348e-06,  1.8874e-06,  1.9501e-06,  2.1845e-06,  2.4393e-06,\n",
      "         2.7146e-06,  3.4988e-06,  3.7298e-06,  4.6425e-06,  5.0020e-06,\n",
      "         6.8807e-06,  7.7900e-06,  9.1389e-06,  1.0867e-05,  1.6415e-05,\n",
      "         1.9188e-05,  2.6116e-05,  2.8653e-05,  2.9499e-05,  4.4039e-05,\n",
      "         6.3718e-05,  6.7306e-05,  8.4749e-05,  1.7195e-04,  7.0067e-04])\n",
      "wout = tensor([-5.1653e-05, -1.7970e-05, -1.4892e-05, -1.4215e-05, -1.2384e-05,\n",
      "        -1.1466e-05, -8.0837e-06, -7.8736e-06, -7.2721e-06, -6.1849e-06,\n",
      "        -5.9134e-06, -5.5335e-06, -5.4481e-06, -5.1080e-06, -4.8890e-06,\n",
      "        -3.9195e-06, -3.8141e-06, -3.3537e-06, -2.8345e-06, -2.3555e-06,\n",
      "        -2.3276e-06, -2.2036e-06, -1.7861e-06, -1.6499e-06, -1.3060e-06,\n",
      "        -1.1880e-06, -1.1858e-06, -1.1135e-06, -9.3138e-07, -9.0736e-07,\n",
      "        -7.2649e-07, -7.0382e-07, -6.6341e-07, -6.3165e-07, -5.1235e-07,\n",
      "        -4.9956e-07, -4.4343e-07, -4.3607e-07, -2.6750e-07, -2.2260e-07,\n",
      "        -2.0547e-07, -1.9625e-07, -1.8745e-07, -1.7986e-07, -1.7558e-07,\n",
      "        -1.7395e-07, -1.7161e-07, -1.6456e-07, -1.6319e-07, -1.5835e-07,\n",
      "        -1.5744e-07, -1.4980e-07, -1.4385e-07, -1.4183e-07, -1.3430e-07,\n",
      "        -1.3215e-07, -1.3043e-07, -1.2999e-07, -1.2863e-07, -1.2523e-07,\n",
      "        -1.1903e-07, -1.0847e-07, -9.5585e-08, -9.3658e-08, -8.7717e-08,\n",
      "        -8.5209e-08, -8.4296e-08, -8.1532e-08, -6.9668e-08, -6.8283e-08,\n",
      "        -6.6579e-08, -6.0785e-08, -5.8303e-08, -5.3503e-08, -5.0455e-08,\n",
      "        -4.4112e-08, -4.2932e-08, -3.6959e-08, -3.4904e-08, -3.1695e-08,\n",
      "        -3.0282e-08, -2.8617e-08, -2.6520e-08, -2.3068e-08, -2.2705e-08,\n",
      "        -1.9587e-08, -1.9471e-08, -1.9203e-08, -1.8970e-08, -1.8469e-08,\n",
      "        -1.7781e-08, -1.7188e-08, -1.6264e-08, -1.5146e-08, -1.4333e-08,\n",
      "        -1.4055e-08, -1.2877e-08, -1.2461e-08, -9.3348e-09, -6.6301e-09,\n",
      "        -5.8953e-09, -4.8738e-09, -3.6742e-09, -3.3292e-09, -2.5025e-09,\n",
      "        -1.7464e-09, -1.1770e-09, -1.0726e-09, -6.9908e-11,  1.3618e-10,\n",
      "         1.7838e-10,  1.1648e-09,  2.5285e-09,  3.6563e-09,  4.1504e-09,\n",
      "         4.8642e-09,  8.3167e-09,  9.1818e-09,  1.0990e-08,  1.3248e-08,\n",
      "         1.4211e-08,  1.6449e-08,  1.7444e-08,  2.2005e-08,  2.4095e-08,\n",
      "         2.4403e-08,  2.6894e-08,  2.8293e-08,  3.1505e-08,  3.5101e-08,\n",
      "         4.0324e-08,  4.1842e-08,  4.6391e-08,  4.7949e-08,  5.1256e-08,\n",
      "         5.1685e-08,  5.2012e-08,  5.5910e-08,  5.6909e-08,  5.9105e-08,\n",
      "         6.1888e-08,  6.3991e-08,  6.4895e-08,  6.6280e-08,  6.8699e-08,\n",
      "         6.9864e-08,  7.1701e-08,  7.4118e-08,  8.6540e-08,  8.9966e-08,\n",
      "         1.0194e-07,  1.0488e-07,  1.0706e-07,  1.1247e-07,  1.1525e-07,\n",
      "         1.2163e-07,  1.2768e-07,  1.3136e-07,  1.4032e-07,  1.6165e-07,\n",
      "         1.6885e-07,  1.8875e-07,  1.8940e-07,  2.9667e-07,  3.2674e-07,\n",
      "         3.8167e-07,  3.8836e-07,  5.1845e-07,  5.4873e-07,  5.6616e-07,\n",
      "         6.5697e-07,  7.4924e-07,  1.0124e-06,  1.0796e-06,  1.0862e-06,\n",
      "         1.7200e-06,  1.8823e-06,  2.6962e-06,  3.0936e-06,  3.3817e-06,\n",
      "         3.9862e-06,  4.3588e-06,  5.3508e-06,  6.1369e-06,  7.1830e-06,\n",
      "         7.4348e-06,  1.0811e-05,  1.1776e-05,  1.2467e-05,  1.4930e-05,\n",
      "         1.6798e-05,  1.7765e-05,  1.7860e-05,  2.1298e-05,  2.8142e-05,\n",
      "         3.1894e-05,  4.7405e-05,  6.3927e-05,  1.0660e-04,  1.8514e-04])\n",
      "Task 2 Epoch 2/5 - Loss: 1.8174\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 24.32%\n",
      "  Unit similarity - Cosine: 0.8946, Euclidean: 0.5385\n",
      "Task 2 Epoch 3/5 - Loss: 1.7496\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 26.02%\n",
      "  Unit similarity - Cosine: 0.8850, Euclidean: 0.5635\n",
      "testing coupling of neurons 0 & 1 in layer 3\n",
      "Computing Hessian for layer 3 - this may take a moment...\n",
      "Computing Hessian for layer 4 - this may take a moment...\n",
      "####################epoch 3, step 200, cos_sim = 0.873 \n",
      "bias = 0\n",
      "win = tensor([-1.5482e-02, -4.5966e-03, -1.3634e-03, -9.7618e-04, -6.7319e-04,\n",
      "        -6.2448e-04, -3.6440e-04, -3.0117e-04, -2.7602e-04, -2.3979e-04,\n",
      "        -1.8405e-04, -1.6413e-04, -1.1160e-04, -1.0325e-04, -8.6547e-05,\n",
      "        -8.4236e-05, -7.9910e-05, -7.4606e-05, -6.5644e-05, -5.4416e-05,\n",
      "        -5.1129e-05, -4.8526e-05, -4.2952e-05, -3.8269e-05, -3.2143e-05,\n",
      "        -2.9544e-05, -2.7858e-05, -2.3388e-05, -2.1902e-05, -1.9417e-05,\n",
      "        -1.6103e-05, -1.5089e-05, -1.4222e-05, -1.2828e-05, -1.1331e-05,\n",
      "        -1.0259e-05, -9.3366e-06, -8.6612e-06, -8.0244e-06, -7.7774e-06,\n",
      "        -6.6040e-06, -6.2001e-06, -5.5608e-06, -4.9069e-06, -4.4617e-06,\n",
      "        -4.0219e-06, -3.7296e-06, -3.6076e-06, -3.1566e-06, -2.6455e-06,\n",
      "        -2.4618e-06, -1.8930e-06, -1.7653e-06, -1.6771e-06, -1.3839e-06,\n",
      "        -1.2208e-06, -9.8395e-07, -9.0368e-07, -8.5511e-07, -7.3603e-07,\n",
      "        -5.9050e-07, -5.5330e-07, -4.4265e-07, -3.9524e-07, -3.3152e-07,\n",
      "        -3.1197e-07, -2.7571e-07, -2.5498e-07, -1.9062e-07, -1.6903e-07,\n",
      "        -1.4978e-07, -1.2287e-07, -1.1394e-07, -8.6046e-08, -6.7351e-08,\n",
      "        -6.3463e-08, -6.0430e-08, -5.0297e-08, -4.2253e-08, -3.6041e-08,\n",
      "        -3.1220e-08, -2.5993e-08, -2.5228e-08, -2.0550e-08, -1.8348e-08,\n",
      "        -1.5360e-08, -1.2352e-08, -8.6705e-09, -6.8577e-09, -5.9317e-09,\n",
      "        -5.4586e-09, -3.5475e-09, -2.4715e-09, -1.3627e-09, -1.1170e-09,\n",
      "        -9.6013e-10, -8.5474e-10, -4.8209e-10, -4.1265e-10, -3.5535e-10,\n",
      "        -3.2419e-10, -2.6525e-10, -2.2241e-10, -1.5058e-10, -1.3556e-10,\n",
      "        -1.1195e-10, -6.8039e-11, -1.8891e-11, -3.6474e-13,  7.8728e-11,\n",
      "         9.5125e-11,  1.4523e-10,  1.6739e-10,  2.1477e-10,  2.5080e-10,\n",
      "         3.0427e-10,  3.4583e-10,  3.6790e-10,  5.2509e-10,  7.2908e-10,\n",
      "         8.3070e-10,  8.7553e-10,  1.3173e-09,  1.7944e-09,  2.0133e-09,\n",
      "         3.0474e-09,  3.5042e-09,  3.8151e-09,  6.7512e-09,  7.3255e-09,\n",
      "         8.2343e-09,  1.0429e-08,  1.1877e-08,  2.1161e-08,  2.2550e-08,\n",
      "         3.5996e-08,  4.0016e-08,  4.5964e-08,  5.7003e-08,  8.6445e-08,\n",
      "         1.1299e-07,  1.5927e-07,  2.0402e-07,  2.4727e-07,  2.8773e-07,\n",
      "         3.2809e-07,  3.7157e-07,  3.9010e-07,  4.1060e-07,  5.1062e-07,\n",
      "         5.8202e-07,  6.4557e-07,  6.6214e-07,  7.9491e-07,  9.4284e-07,\n",
      "         1.1522e-06,  1.3828e-06,  1.4812e-06,  1.7526e-06,  1.9246e-06,\n",
      "         2.0738e-06,  2.3751e-06,  2.8460e-06,  3.1174e-06,  3.2414e-06,\n",
      "         3.8635e-06,  4.8008e-06,  6.2345e-06,  6.6623e-06,  6.9219e-06,\n",
      "         8.6034e-06,  8.9810e-06,  1.0967e-05,  1.2269e-05,  1.4382e-05,\n",
      "         1.5584e-05,  1.7978e-05,  2.4684e-05,  2.7135e-05,  3.3819e-05,\n",
      "         3.9802e-05,  5.6237e-05,  5.9454e-05,  6.4712e-05,  8.1619e-05,\n",
      "         8.3785e-05,  8.8616e-05,  1.0176e-04,  1.3074e-04,  1.4136e-04,\n",
      "         1.8055e-04,  2.1504e-04,  3.0182e-04,  3.6381e-04,  5.0838e-04,\n",
      "         5.6041e-04,  8.4543e-04,  1.8997e-03,  7.1468e-03,  2.4579e-02])\n",
      "wout = tensor([-7.3699e-05, -6.7536e-05, -6.6852e-05, -6.2666e-05, -5.9405e-05,\n",
      "        -5.3404e-05, -5.2339e-05, -4.6471e-05, -4.5158e-05, -4.1259e-05,\n",
      "        -4.0415e-05, -3.7626e-05, -3.6893e-05, -3.2715e-05, -3.1968e-05,\n",
      "        -3.0131e-05, -2.9528e-05, -2.8431e-05, -2.7321e-05, -2.6397e-05,\n",
      "        -2.5090e-05, -2.4939e-05, -2.4831e-05, -2.4642e-05, -2.2849e-05,\n",
      "        -2.2544e-05, -2.2272e-05, -2.1830e-05, -2.0960e-05, -2.0438e-05,\n",
      "        -1.9715e-05, -1.9554e-05, -1.9377e-05, -1.9131e-05, -1.9056e-05,\n",
      "        -1.8741e-05, -1.8538e-05, -1.8293e-05, -1.8143e-05, -1.7810e-05,\n",
      "        -1.7459e-05, -1.7268e-05, -1.6915e-05, -1.6414e-05, -1.6130e-05,\n",
      "        -1.5910e-05, -1.5551e-05, -1.5372e-05, -1.4789e-05, -1.4458e-05,\n",
      "        -1.4388e-05, -1.4201e-05, -1.3625e-05, -1.3190e-05, -1.2995e-05,\n",
      "        -1.2667e-05, -1.2089e-05, -1.1916e-05, -1.1804e-05, -1.1629e-05,\n",
      "        -1.1071e-05, -1.0712e-05, -1.0491e-05, -1.0328e-05, -1.0239e-05,\n",
      "        -1.0127e-05, -9.6256e-06, -9.2547e-06, -8.5285e-06, -8.5193e-06,\n",
      "        -8.3594e-06, -7.9394e-06, -7.6809e-06, -7.4907e-06, -7.1644e-06,\n",
      "        -7.0610e-06, -6.4671e-06, -5.8449e-06, -5.7038e-06, -5.6899e-06,\n",
      "        -5.5320e-06, -5.4314e-06, -5.3746e-06, -5.1229e-06, -4.6958e-06,\n",
      "        -4.4160e-06, -4.1617e-06, -4.0765e-06, -3.5897e-06, -3.4830e-06,\n",
      "        -2.9124e-06, -2.8682e-06, -2.7539e-06, -2.5667e-06, -2.5012e-06,\n",
      "        -2.2304e-06, -2.0880e-06, -2.0596e-06, -1.9746e-06, -1.9539e-06,\n",
      "        -1.8660e-06, -1.6774e-06, -1.5461e-06, -1.4004e-06, -1.2618e-06,\n",
      "        -1.1280e-06, -1.0110e-06, -8.7704e-07, -7.4691e-07, -4.8649e-07,\n",
      "        -4.7339e-07, -3.0994e-07, -2.2312e-07, -1.7357e-07, -1.0076e-07,\n",
      "         2.6068e-08,  1.7815e-07,  3.4947e-07,  5.3576e-07,  5.8975e-07,\n",
      "         7.0317e-07,  7.4446e-07,  8.0678e-07,  8.4705e-07,  1.0230e-06,\n",
      "         1.1358e-06,  1.1870e-06,  1.4355e-06,  1.9866e-06,  2.1712e-06,\n",
      "         2.2590e-06,  2.8699e-06,  3.1297e-06,  3.5599e-06,  4.0159e-06,\n",
      "         4.5284e-06,  4.8294e-06,  5.1731e-06,  5.6779e-06,  5.7294e-06,\n",
      "         6.1117e-06,  6.6413e-06,  7.3783e-06,  7.8237e-06,  8.0282e-06,\n",
      "         8.3797e-06,  8.3934e-06,  1.0399e-05,  1.0634e-05,  1.1689e-05,\n",
      "         1.2190e-05,  1.2545e-05,  1.4397e-05,  1.4829e-05,  1.4934e-05,\n",
      "         1.5524e-05,  1.5901e-05,  1.6975e-05,  1.7340e-05,  1.8354e-05,\n",
      "         1.8885e-05,  1.9080e-05,  1.9708e-05,  2.0713e-05,  2.0941e-05,\n",
      "         2.1305e-05,  2.2064e-05,  2.2640e-05,  2.3363e-05,  2.3687e-05,\n",
      "         2.5791e-05,  2.5888e-05,  2.6552e-05,  2.7343e-05,  2.8082e-05,\n",
      "         2.8166e-05,  2.8846e-05,  2.9258e-05,  2.9567e-05,  3.2080e-05,\n",
      "         3.3066e-05,  3.3872e-05,  3.5836e-05,  3.7229e-05,  3.8358e-05,\n",
      "         3.9605e-05,  4.0261e-05,  4.0503e-05,  4.2652e-05,  4.2945e-05,\n",
      "         4.4827e-05,  4.5923e-05,  4.6942e-05,  5.1814e-05,  5.5490e-05,\n",
      "         5.9027e-05,  6.1960e-05,  6.8457e-05,  8.2486e-05,  1.2461e-04])\n",
      "Task 2 Epoch 4/5 - Loss: 1.7196\n",
      "  Acc (Task 1): 0.03%, Acc (Task 2): 28.42%\n",
      "  Unit similarity - Cosine: 0.8361, Euclidean: 0.6586\n",
      "Task 2 Epoch 5/5 - Loss: 1.6862\n",
      "  Acc (Task 1): 0.00%, Acc (Task 2): 31.75%\n",
      "  Unit similarity - Cosine: 0.7354, Euclidean: 0.8120\n",
      "\n",
      "===== Final Results =====\n",
      "Cosine similarity: 0.9886 (after coupling) -> 0.7354 (final)\n",
      "Euclidean distance: 0.1332 (after coupling) -> 0.8120 (final)\n",
      "Units became LESS similar in terms of cosine similarity during training on second task.\n",
      "Units moved FURTHER APART in terms of Euclidean distance during training on second task.\n",
      "\n",
      "===== Catastrophic Forgetting Analysis =====\n",
      "Task 1 accuracy: 12.62% (before Task 2) -> 0.00% (after Task 2)\n",
      "Catastrophic forgetting observed: 12.62% drop in Task 1 performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import hessian\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_sizes, activation_type='relu'):\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        \n",
    "        # Set activation function based on input parameter\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_type == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation type: {activation_type}\")\n",
    "        \n",
    "        # Create layers list starting with flattening the input\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Build layer architecture\n",
    "        layer_sizes = [input_dim] + hidden_sizes + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation to all but the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def get_layer_weights(self, layer_idx):\n",
    "        \"\"\"Return the weight matrix of a specific layer\"\"\"\n",
    "        return self.layers[layer_idx].weight\n",
    "        \n",
    "    def couple_units(self, layer_idx, unit_i, unit_j, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Couple units i and j in layer layer_idx by making their incoming \n",
    "        and outgoing weights similar with a small perturbation.\n",
    "        \"\"\"\n",
    "        # Handle incoming weights (weights of the specified layer)\n",
    "        if layer_idx < len(self.layers):\n",
    "            # Get the weights of the specified layer (but don't modify directly)\n",
    "            weights = self.get_layer_weights(layer_idx).clone()\n",
    "            \n",
    "            # Make unit_j similar to unit_i with small perturbation\n",
    "            # We keep unit_i as is and set unit_j to be similar\n",
    "            noise = torch.randn_like(weights[unit_i]) * epsilon * torch.norm(weights[unit_i])\n",
    "            \n",
    "            # Create new weights tensor with the modified values\n",
    "            new_weights = weights.clone()\n",
    "            new_weights[unit_j] = weights[unit_i] + noise\n",
    "            \n",
    "            # Update the layer weights with the new tensor\n",
    "            self.layers[layer_idx].weight.data = new_weights\n",
    "            \n",
    "            # Debug: verify the coupling\n",
    "            cosine_sim = self.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "            print(f\"DEBUG - After coupling incoming weights: cosine similarity = {cosine_sim:.4f}\")\n",
    "        \n",
    "        # Handle outgoing weights (weights of the next layer)\n",
    "        if layer_idx + 1 < len(self.layers):\n",
    "            # Get the weights of the next layer (but don't modify directly)\n",
    "            next_weights = self.get_layer_weights(layer_idx + 1).clone()\n",
    "            \n",
    "            # For outgoing weights, we need to process the columns\n",
    "            # Extract the columns corresponding to unit_i and unit_j\n",
    "            outgoing_i = next_weights[:, unit_i].clone()\n",
    "            \n",
    "            # Create perturbation for outgoing weights\n",
    "            out_noise = torch.randn_like(outgoing_i) * epsilon * torch.norm(outgoing_i)\n",
    "            \n",
    "            # Set unit_j's outgoing weights based on unit_i with noise\n",
    "            next_weights[:, unit_j] = outgoing_i + out_noise\n",
    "            \n",
    "            # Update the next layer weights\n",
    "            self.layers[layer_idx + 1].weight.data = next_weights\n",
    "            \n",
    "            # Show the outgoing similarity\n",
    "            cos_sim_out = torch.dot(next_weights[:, unit_i], next_weights[:, unit_j]) / (\n",
    "                torch.norm(next_weights[:, unit_i]) * torch.norm(next_weights[:, unit_j]))\n",
    "            print(f\"DEBUG - After coupling outgoing weights: cosine similarity = {cos_sim_out.item():.4f}\")\n",
    "    \n",
    "    def measure_unit_similarity(self, layer_idx, unit_i, unit_j, metric='cosine'):\n",
    "        \"\"\"\n",
    "        Measure the similarity between units i and j in layer layer_idx.\n",
    "        \"\"\"\n",
    "        weights = self.get_layer_weights(layer_idx)\n",
    "        \n",
    "        if metric == 'cosine':\n",
    "            # Compute cosine similarity between incoming weight vectors\n",
    "            norm_i = torch.norm(weights[unit_i])\n",
    "            norm_j = torch.norm(weights[unit_j])\n",
    "            \n",
    "            if norm_i > 0 and norm_j > 0:\n",
    "                cos_sim = torch.dot(weights[unit_i], weights[unit_j]) / (norm_i * norm_j)\n",
    "                return cos_sim.item()\n",
    "            else:\n",
    "                return 0.0\n",
    "        elif metric == 'euclidean':\n",
    "            # Compute Euclidean distance between incoming weight vectors\n",
    "            distance = torch.norm(weights[unit_i] - weights[unit_j])\n",
    "            return distance.item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "\n",
    "def compute_layer_hessian(model, inputs, targets, loss_fn, layer_idx):\n",
    "    # Move inputs and targets to device\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Check if layer_idx is valid\n",
    "    if layer_idx < 0 or layer_idx >= len(model.layers):\n",
    "        raise ValueError(f\"Invalid layer index: {layer_idx}. Model has {len(model.layers)} layers.\")\n",
    "    \n",
    "    # Create a pure function to compute loss with respect to the layer parameters\n",
    "    def layer_params_to_loss(flat_params):\n",
    "        \"\"\"\n",
    "        Function that takes flattened parameters for the specified layer and returns the loss.\n",
    "        \"\"\"\n",
    "        # Get the dimensions of the layer's parameters\n",
    "        layer = model.layers[layer_idx]\n",
    "        weight_size = layer.weight.numel()\n",
    "        \n",
    "        # Reshape parameters into weight and bias\n",
    "        weight = flat_params[:weight_size].reshape(layer.weight.shape)\n",
    "        bias = flat_params[weight_size:]\n",
    "        \n",
    "        # Make a forward pass through the model with our custom parameters\n",
    "        x = model.flatten(inputs)\n",
    "        \n",
    "        # Pass through layers before our target layer using original parameters\n",
    "        for i in range(layer_idx):\n",
    "            x = model.layers[i](x)\n",
    "            if i < len(model.layers) - 1:  # Apply activation if not the last layer\n",
    "                x = model.activation(x)\n",
    "        \n",
    "        # Pass through our target layer with custom parameters\n",
    "        x = F.linear(x, weight, bias)\n",
    "        if layer_idx < len(model.layers) - 1:  # Apply activation if not the last layer\n",
    "            x = model.activation(x)\n",
    "        \n",
    "        # Pass through remaining layers using original parameters\n",
    "        for i in range(layer_idx + 1, len(model.layers)):\n",
    "            x = model.layers[i](x)\n",
    "            if i < len(model.layers) - 1:  # Apply activation if not the last layer\n",
    "                x = model.activation(x)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(x, targets)\n",
    "        return loss\n",
    "    \n",
    "    # Get and flatten the layer parameters\n",
    "    layer = model.layers[layer_idx]\n",
    "    layer_params = torch.cat([layer.weight.flatten(), layer.bias])\n",
    "    layer_params = layer_params.detach().requires_grad_(True)\n",
    "    \n",
    "    # Compute Hessian - potentially memory intensive operation\n",
    "    print(f\"Computing Hessian for layer {layer_idx} - this may take a moment...\")\n",
    "    \n",
    "    try:\n",
    "        # Move to CPU for hessian computation to avoid GPU memory issues\n",
    "        cpu_layer_params = layer_params.cpu()\n",
    "        \n",
    "        # Define a cpu version of the loss function\n",
    "        def cpu_params_to_loss(cpu_params):\n",
    "            # Move params to device, compute loss, then get result back to CPU\n",
    "            device_params = cpu_params.to(device)\n",
    "            loss_value = layer_params_to_loss(device_params)\n",
    "            return loss_value.cpu()\n",
    "        \n",
    "        # Compute Hessian on CPU\n",
    "        H = hessian(cpu_params_to_loss, cpu_layer_params)\n",
    "        \n",
    "        # Calculate dimensions for weight and bias parts\n",
    "        weight_size = layer.weight.numel()\n",
    "        bias_size = layer.bias.numel()\n",
    "        \n",
    "        # Extract subblocks of the Hessian\n",
    "        H_ww = H[:weight_size, :weight_size]  # weight-weight block\n",
    "        input_dim = layer.weight.shape[1]   # din\n",
    "        output_dim = layer.weight.shape[0]  # dout\n",
    "        H_ww_4d = H_ww.reshape(output_dim, input_dim, output_dim, input_dim)\n",
    "        H_wb = H[:weight_size, weight_size:]  # weight-bias block\n",
    "        H_bw = H[weight_size:, :weight_size]  # bias-weight block\n",
    "        H_bb = H[weight_size:, weight_size:]  # bias-bias block\n",
    "        \n",
    "        # Compute eigenvalues and condition number\n",
    "        # eigenvalues = torch.linalg.eigvalsh(H)\n",
    "        \n",
    "        # Create result dictionary\n",
    "        hessian_dict = {\n",
    "            # 'full': H,\n",
    "            'H_ww': H_ww,\n",
    "            'H_ww_4d': H_ww_4d,\n",
    "            'H_wb': H_wb,\n",
    "            'H_bw': H_bw,\n",
    "            'H_bb': H_bb,\n",
    "            # 'eigenvalues': eigenvalues,\n",
    "            'layer_index': layer_idx,\n",
    "            'bias_shape': layer.bias.shape,\n",
    "            'weight_shape': layer.weight.shape,\n",
    "        }\n",
    "        \n",
    "        return hessian_dict\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error computing Hessian: {e}\")\n",
    "        print(\"Trying with a smaller batch or on CPU...\")\n",
    "        # Here you could implement a fallback method for computing the Hessian\n",
    "        raise e\n",
    "    finally:\n",
    "        # Clean up to free memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def extract_incoming_block(H_ww_4d, i, j):\n",
    "    \"\"\"\n",
    "    H_ww_4d: 4D Hessian of shape [out_dim, in_dim, out_dim, in_dim].\n",
    "    i,j: two neuron indices in [0, out_dim).\n",
    "    Returns a (2*in_dim) x (2*in_dim) torch.Tensor sub-block\n",
    "    that captures the Hessian w.r.t. rows i and j only.\n",
    "    \"\"\"\n",
    "    out_dim, in_dim, _, _ = H_ww_4d.shape\n",
    "    block = torch.zeros(2*in_dim, 2*in_dim, dtype=H_ww_4d.dtype, device=H_ww_4d.device)\n",
    "\n",
    "    # top-left\n",
    "    block[0:in_dim, 0:in_dim] = H_ww_4d[i, :, i, :]\n",
    "\n",
    "    # top-right\n",
    "    block[0:in_dim, in_dim:] = H_ww_4d[i, :, j, :]\n",
    "\n",
    "    # bottom-left\n",
    "    block[in_dim:, 0:in_dim] = H_ww_4d[j, :, i, :]\n",
    "\n",
    "    # bottom-right\n",
    "    block[in_dim:, in_dim:] = H_ww_4d[j, :, j, :]\n",
    "\n",
    "    return block\n",
    "\n",
    "def extract_outgoing_block(H_ww_4d, i, j):\n",
    "    \"\"\"\n",
    "    H_ww_4d: 4D Hessian of shape [out_dim, in_dim, out_dim, in_dim].\n",
    "    i,j: two neuron indices in [0, out_dim).\n",
    "    Returns a (2*in_dim) x (2*in_dim) torch.Tensor sub-block\n",
    "    that captures the Hessian w.r.t. rows i and j only.\n",
    "    \"\"\"\n",
    "    out_dim, in_dim, _, _ = H_ww_4d.shape\n",
    "    block = torch.zeros(2*in_dim, 2*in_dim, dtype=H_ww_4d.dtype, device=H_ww_4d.device)\n",
    "\n",
    "    # top-left\n",
    "    block[0:in_dim, 0:in_dim] = H_ww_4d[:, i, :, i,]\n",
    "\n",
    "    # top-right\n",
    "    block[0:in_dim, in_dim:] = H_ww_4d[:, i, :, j,]\n",
    "\n",
    "    # bottom-left\n",
    "    block[in_dim:, 0:in_dim] = H_ww_4d[:, j, :, i,]\n",
    "\n",
    "    # bottom-right\n",
    "    block[in_dim:, in_dim:] = H_ww_4d[:, j, :, j,]\n",
    "\n",
    "    return block\n",
    "\n",
    "def build_difference_matrix(in_dim, device=None, dtype=None):\n",
    "    \"\"\"\n",
    "    Returns D of shape (2*in_dim, in_dim)\n",
    "    where each column is e_{i,k} - e_{j,k}.\n",
    "    Top half has +1 in row k,\n",
    "    Bottom half has -1 in row k,\n",
    "    zeros otherwise.\n",
    "    \"\"\"\n",
    "    D = torch.zeros(2*in_dim, in_dim, device=device, dtype=dtype)\n",
    "    for k in range(in_dim):\n",
    "        D[k, k] = 1.0             # the \"top half\" (i's row)\n",
    "        D[in_dim + k, k] = -1.0   # the \"bottom half\" (j's row)\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_coupling_stability(i, j, layer_idx, model, inputs, targets, loss_fn):\n",
    "    print(f\"testing coupling of neurons {i} & {j} in layer {layer_idx}\")\n",
    "    \n",
    "    # Move inputs and targets to device\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    bias = []\n",
    "    win = []\n",
    "    wout = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # check incoming weight coupling conditions \n",
    "        hessian_dict = compute_layer_hessian(model, inputs, targets, loss_fn, layer_idx=layer_idx)\n",
    "        # H_ww = hessian_dict['H_ww_4d']\n",
    "        H_in = extract_incoming_block(hessian_dict['H_ww_4d'], i, j, )\n",
    "        in_dim = H_in.shape[0]//2\n",
    "        D = build_difference_matrix(in_dim, device=H_in.device, dtype=H_in.dtype)\n",
    "        H_in = D.T @ H_in @ D    # shape (in_dim, in_dim)\n",
    "\n",
    "        win = torch.linalg.eigvalsh(H_in) \n",
    "        # for k in range(H_ww.shape[1]):\n",
    "        #     b = [[H_ww[i,k,i,k], H_ww[j,k,i,k]],[H_ww[i,k,j,k], H_ww[j,k,j,k]]]\n",
    "        #     win.append((b[0][0] + b[1][1]-b[0][1]-b[1][0]).item())\n",
    "            \n",
    "        # check outgoing weight coupling conditions \n",
    "        hessian_dict = compute_layer_hessian(model, inputs, targets, loss_fn, layer_idx=layer_idx+1)\n",
    "        H_out = extract_outgoing_block(hessian_dict['H_ww_4d'], i, j, )\n",
    "        out_dim = H_out.shape[0]//2\n",
    "        D = build_difference_matrix(out_dim, device=H_out.device, dtype=H_out.dtype)\n",
    "        H_out = D.T @ H_out @ D    # shape (in_dim, in_dim)\n",
    "        wout =  torch.linalg.eigvalsh(H_out) \n",
    "        # H_ww = hessian_dict['H_ww_4d']\n",
    "        # Hb = hessian_dict['H_bb']\n",
    "        # b = [[Hb[i,i],Hb[i,j]],[Hb[j,i],Hb[j,j]]]\n",
    "        # bias  = ((b[0][0] + b[1][1]-b[0][1]-b[1][0]).item() )\n",
    "        # for k in range(H_ww.shape[0]):\n",
    "        #     b = [[H_ww[k,i,k,i], H_ww[k,j,k,i]],[H_ww[k,i,k,j], H_ww[k,j,k,j]]]\n",
    "        #     wout.append((b[0][0] + b[1][1]-b[0][1]-b[1][0]).item())\n",
    "\n",
    "    finally:\n",
    "        # Clean up to free memory\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    # win = sorted(win)\n",
    "    # wout = sorted(wout)\n",
    "    return 0, win, wout\n",
    "\n",
    "\n",
    "def pretty_print_tensor(tensor, precision=3):\n",
    "    # Convert tensor to numpy array - move to CPU first if it's on GPU\n",
    "    if tensor.is_cuda:\n",
    "        arr = tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = tensor.detach().numpy()\n",
    "    # Create a DataFrame for better visual formatting\n",
    "    df = pd.DataFrame(arr)\n",
    "    # Set a custom float formatter for the DataFrame\n",
    "    pd.options.display.float_format = f\"{{:.{precision}f}}\".format\n",
    "    print(df)\n",
    "\n",
    "def print_model(model, layer_idx, precision=3):\n",
    "    # Print only the coupled layers (layer_idx and layer_idx+1)\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if i == layer_idx or i == layer_idx + 1:\n",
    "            print(f'Layer {i} Bias:')\n",
    "            pretty_print_tensor(layer.bias.data, precision=precision)\n",
    "            print(f'\\nLayer {i} Weight:')\n",
    "            pretty_print_tensor(layer.weight.data, precision=precision)\n",
    "            print('\\n' + '-'*40 + '\\n')\n",
    "\n",
    "def load_cifar10_continual(first_classes, second_classes, batch_size=128):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 dataset split for continual learning.\n",
    "    first_classes and second_classes should be lists of class indices.\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Download and load the training dataset\n",
    "    full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "    full_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "    \n",
    "    # Create subsets for first task (first set of classes)\n",
    "    first_train_indices = [i for i, (_, label) in enumerate(full_trainset) if label in first_classes]\n",
    "    first_test_indices = [i for i, (_, label) in enumerate(full_testset) if label in first_classes]\n",
    "    \n",
    "    first_trainset = Subset(full_trainset, first_train_indices)\n",
    "    first_testset = Subset(full_testset, first_test_indices)\n",
    "    \n",
    "    # Create subsets for second task (second set of classes)\n",
    "    second_train_indices = [i for i, (_, label) in enumerate(full_trainset) if label in second_classes]\n",
    "    second_test_indices = [i for i, (_, label) in enumerate(full_testset) if label in second_classes]\n",
    "    \n",
    "    second_trainset = Subset(full_trainset, second_train_indices)\n",
    "    second_testset = Subset(full_testset, second_test_indices)\n",
    "    \n",
    "    # Determine number of workers based on device\n",
    "    # When using CUDA, we want to use multiple workers for the DataLoader\n",
    "    # When using CPU, fewer workers might be better\n",
    "    num_workers = 2 if device.type == 'cuda' else 0\n",
    "    \n",
    "    # Pin memory for faster data transfer to GPU\n",
    "    pin_memory = device.type == 'cuda'\n",
    "    \n",
    "    # Create dataloaders\n",
    "    first_trainloader = DataLoader(\n",
    "        first_trainset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    first_testloader = DataLoader(\n",
    "        first_testset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    second_trainloader = DataLoader(\n",
    "        second_trainset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    second_testloader = DataLoader(\n",
    "        second_testset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    print(f\"First task: {len(first_trainset)} training samples, {len(first_testset)} test samples\")\n",
    "    print(f\"Second task: {len(second_trainset)} training samples, {len(second_testset)} test samples\")\n",
    "    \n",
    "    return (first_trainloader, first_testloader), (second_trainloader, second_testloader)\n",
    "\n",
    "\n",
    "def train_continual_learning(model, data_loaders, coupling_info, \n",
    "                           epochs_first_task=10, epochs_second_task=10, \n",
    "                           learning_rate=0.001, print_model_step=10, train_steps=20, train2_steps=40):\n",
    "    \"\"\"\n",
    "    Train the model with a continual learning approach:\n",
    "    1. Train on first set of classes\n",
    "    2. Apply coupling\n",
    "    3. Train on second set of classes\n",
    "    \"\"\"\n",
    "    # Unpack data loaders\n",
    "    (first_trainloader, first_testloader), (second_trainloader, second_testloader) = data_loaders\n",
    "    \n",
    "    # Extract coupling parameters\n",
    "    layer_idx = coupling_info['layer']\n",
    "    unit_i = coupling_info['unit_i']\n",
    "    unit_j = coupling_info['unit_j']\n",
    "    epsilon = coupling_info['epsilon']\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track metrics over time\n",
    "    metrics = {\n",
    "        'train_losses': [],\n",
    "        'test_accuracies_first': [],\n",
    "        'test_accuracies_second': [],\n",
    "        'cosine_similarity': [],\n",
    "        'euclidean_distance': [],\n",
    "        'phase': []  # 'first' or 'second' to track which task we're on\n",
    "    }\n",
    "    \n",
    "    # Initial unit similarity\n",
    "    cos_sim = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "    eucl_dist = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'euclidean')\n",
    "    \n",
    "    metrics['cosine_similarity'].append(cos_sim)\n",
    "    metrics['euclidean_distance'].append(eucl_dist)\n",
    "    metrics['phase'].append('initial')\n",
    "    \n",
    "    print(f\"Initial cosine similarity: {cos_sim:.4f}, Euclidean distance: {eucl_dist:.4f}\")\n",
    "    \n",
    "    # Phase 1: Train on first task\n",
    "    print(\"\\n===== Phase 1: Training on first set of classes =====\")\n",
    "    step = 0 \n",
    "    for epoch in range(epochs_first_task):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(first_trainloader):\n",
    "            step += 1 \n",
    "            \n",
    "            # Move input data to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % print_model_step == 0:\n",
    "                bias, win, wout = test_coupling_stability(unit_i, unit_j, layer_idx, model, inputs, labels, criterion)\n",
    "                cos_sim = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "                print('#'*20 + f'epoch {epoch}, step {step}, cos_sim = {cos_sim:.3f} ' )\n",
    "                print(f\"bias = {bias}\\nwin = {win}\\nwout = {wout}\")\n",
    "                \n",
    "                # Free memory after computation\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            if step >= train_steps:\n",
    "                break\n",
    "        if step >= train_steps:\n",
    "            break\n",
    "            \n",
    "        epoch_loss = running_loss / len(first_trainloader)\n",
    "        metrics['train_losses'].append(epoch_loss)\n",
    "        metrics['phase'].append('first')\n",
    "        \n",
    "        # Evaluate on both tasks\n",
    "        model.eval()\n",
    "        acc_first = evaluate_model(model, first_testloader)\n",
    "        acc_second = evaluate_model(model, second_testloader)\n",
    "        \n",
    "        metrics['test_accuracies_first'].append(acc_first)\n",
    "        metrics['test_accuracies_second'].append(acc_second)\n",
    "        \n",
    "        # Measure unit similarity\n",
    "        cos_sim = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "        eucl_dist = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'euclidean')\n",
    "        \n",
    "        metrics['cosine_similarity'].append(cos_sim)\n",
    "        metrics['euclidean_distance'].append(eucl_dist)\n",
    "        \n",
    "        print(f\"Task 1 Epoch {epoch+1}/{epochs_first_task} - Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"  Acc (Task 1): {acc_first:.2f}%, Acc (Task 2): {acc_second:.2f}%\")\n",
    "        print(f\"  Unit similarity - Cosine: {cos_sim:.4f}, Euclidean: {eucl_dist:.4f}\")\n",
    "    \n",
    "    print(\"\\n===== Applying unit coupling =====\")\n",
    "    \n",
    "    # Measure similarity before coupling\n",
    "    cos_sim_before = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "    eucl_dist_before = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'euclidean')\n",
    "    \n",
    "    # Apply coupling\n",
    "    model.couple_units(layer_idx, unit_i, unit_j, epsilon)\n",
    "    \n",
    "    # Measure similarity after coupling\n",
    "    cos_sim_after = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "    eucl_dist_after = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'euclidean')\n",
    "\n",
    "    \n",
    "    print(f\"Before coupling: cosine={cos_sim_before:.4f}, distance={eucl_dist_before:.4f}\")\n",
    "    print(f\"After coupling: cosine={cos_sim_after:.4f}, distance={eucl_dist_after:.4f}\")\n",
    "    \n",
    "    # Add coupling point to metrics\n",
    "    metrics['cosine_similarity'].append(cos_sim_after)\n",
    "    metrics['euclidean_distance'].append(eucl_dist_after)\n",
    "    metrics['phase'].append('coupling')\n",
    "    \n",
    "    # Phase 2: Train on second task\n",
    "    print(\"\\n===== Phase 2: Training on second set of classes =====\")\n",
    "    step = 0 \n",
    "    for epoch in range(epochs_second_task):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(second_trainloader):\n",
    "            step += 1\n",
    "            \n",
    "            # Move input data to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % print_model_step == 0:\n",
    "                bias, win, wout = test_coupling_stability(unit_i, unit_j, layer_idx, model, inputs, labels, criterion)\n",
    "                cos_sim = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "                print('#'*20 + f'epoch {epoch}, step {step}, cos_sim = {cos_sim:.3f} ' )\n",
    "                print(f\"bias = {bias}\\nwin = {win}\\nwout = {wout}\")\n",
    "                \n",
    "                # Free memory after computation\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            if step >= train2_steps:\n",
    "                break\n",
    "        if step >= train2_steps:\n",
    "            break\n",
    "            \n",
    "        epoch_loss = running_loss / len(second_trainloader)\n",
    "        metrics['train_losses'].append(epoch_loss)\n",
    "        metrics['phase'].append('second')\n",
    "        \n",
    "        # Evaluate on both tasks\n",
    "        model.eval()\n",
    "        acc_first = evaluate_model(model, first_testloader)\n",
    "        acc_second = evaluate_model(model, second_testloader)\n",
    "        \n",
    "        metrics['test_accuracies_first'].append(acc_first)\n",
    "        metrics['test_accuracies_second'].append(acc_second)\n",
    "        \n",
    "        # Measure unit similarity\n",
    "        cos_sim = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "        eucl_dist = model.measure_unit_similarity(layer_idx, unit_i, unit_j, 'euclidean')\n",
    "\n",
    "        metrics['cosine_similarity'].append(cos_sim)\n",
    "        metrics['euclidean_distance'].append(eucl_dist)\n",
    "        \n",
    "        print(f\"Task 2 Epoch {epoch+1}/{epochs_second_task} - Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"  Acc (Task 1): {acc_first:.2f}%, Acc (Task 2): {acc_second:.2f}%\")\n",
    "        print(f\"  Unit similarity - Cosine: {cos_sim:.4f}, Euclidean: {eucl_dist:.4f}\")\n",
    "        \n",
    "    # print_model(model, layer_idx)\n",
    "    \n",
    "    # Clean up to ensure all GPU memory is released\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model accuracy on a dataset\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to the appropriate device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Free memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return 100 * correct / total if total > 0 else 0\n",
    "\n",
    "def plot_continual_learning_results(metrics):\n",
    "    \"\"\"Plot the results of the continual learning experiment\"\"\"\n",
    "    # Find phase transition indices\n",
    "    phases = metrics['phase']\n",
    "    first_phase_end = phases.index('coupling') - 1\n",
    "    coupling_idx = phases.index('coupling')\n",
    "    second_phase_start = coupling_idx + 1\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    \n",
    "    # Use CPU for matplotlib operations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12, 18))\n",
    "    \n",
    "    # Plot training loss\n",
    "    axs[0].plot(metrics['train_losses'], 'b-')\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].set_xlabel('Training Step')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    \n",
    "    # Add vertical lines for phase transitions\n",
    "    axs[0].axvline(x=first_phase_end, color='r', linestyle='--', label='End of Task 1')\n",
    "    axs[0].axvline(x=coupling_idx, color='g', linestyle='--', label='Coupling Applied')\n",
    "    axs[0].axvline(x=second_phase_start, color='m', linestyle='--', label='Start of Task 2')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    axs[1].plot(metrics['test_accuracies_first'], 'b-', label='Task 1 (First Classes)')\n",
    "    axs[1].plot(metrics['test_accuracies_second'], 'g-', label='Task 2 (Second Classes)')\n",
    "    axs[1].set_title('Test Accuracy by Task')\n",
    "    axs[1].set_xlabel('Training Step')\n",
    "    axs[1].set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    # Add vertical lines for phase transitions\n",
    "    axs[1].axvline(x=first_phase_end, color='r', linestyle='--', label='End of Task 1')\n",
    "    axs[1].axvline(x=coupling_idx, color='g', linestyle='--', label='Coupling Applied')\n",
    "    axs[1].axvline(x=second_phase_start, color='m', linestyle='--', label='Start of Task 2')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Plot similarity metrics\n",
    "    axs[2].plot(metrics['cosine_similarity'], 'b-', label='Cosine Similarity')\n",
    "    axs[2].set_title('Unit Similarity Metrics')\n",
    "    axs[2].set_xlabel('Training Step')\n",
    "    axs[2].set_ylabel('Cosine Similarity')\n",
    "    \n",
    "    \n",
    "    # Add vertical lines for phase transitions\n",
    "    axs[2].axvline(x=first_phase_end, color='r', linestyle='--', label='End of Task 1')\n",
    "    axs[2].axvline(x=coupling_idx, color='g', linestyle='--', label='Coupling Applied')\n",
    "    axs[2].axvline(x=second_phase_start, color='m', linestyle='--', label='Start of Task 2')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = axs[2].get_legend_handles_labels()\n",
    "    axs[2].legend(lines1, labels1, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('continual_learning_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up matplotlib memory\n",
    "    plt.close('all')\n",
    "\n",
    "def run_continual_learning_experiment(\n",
    "    hidden_sizes=[7]*5, \n",
    "    activation_type='relu',\n",
    "    first_classes=[0, 1, 2, 3, 4],  # First 5 CIFAR-10 classes\n",
    "    second_classes=[5, 6, 7, 8, 9],  # Second 5 CIFAR-10 classes\n",
    "    couple_layer=1,\n",
    "    couple_units=(10, 20),\n",
    "    epsilon=0.01,\n",
    "    epochs_first_task=5,\n",
    "    epochs_second_task=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    print_model_step=10, \n",
    "    train_steps=20,\n",
    "    train2_steps=40,\n",
    "):\n",
    "    \"\"\"Run the complete continual learning experiment\"\"\"\n",
    "    # Get class names for better reporting\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    first_class_names = [class_names[i] for i in first_classes]\n",
    "    second_class_names = [class_names[i] for i in second_classes]\n",
    "    \n",
    "    print(f\"First task classes: {first_class_names}\")\n",
    "    print(f\"Second task classes: {second_class_names}\")\n",
    "    \n",
    "    try:\n",
    "        # Load datasets for continual learning\n",
    "        data_loaders = load_cifar10_continual(first_classes, second_classes, batch_size=batch_size)\n",
    "        \n",
    "        # Calculate input dimensions for CIFAR-10 (3 channels, 32x32 images)\n",
    "        input_dim = 3 * 32 * 32\n",
    "        output_dim = 10  # All CIFAR-10 classes\n",
    "        \n",
    "        # Create the model\n",
    "        model = ConfigurableMLP(input_dim, output_dim, hidden_sizes, activation_type)\n",
    "        print(f\"Created MLP with architecture: {input_dim} -> {' -> '.join(map(str, hidden_sizes))} -> {output_dim}\")\n",
    "        print(f\"Activation function: {activation_type}\")\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Coupling parameters\n",
    "        coupling_info = {\n",
    "            'layer': couple_layer,\n",
    "            'unit_i': couple_units[0],\n",
    "            'unit_j': couple_units[1],\n",
    "            'epsilon': epsilon\n",
    "        }\n",
    "        \n",
    "        print(f\"Will couple units {couple_units[0]} and {couple_units[1]} in layer {couple_layer}, meaning we couple \")\n",
    "        print(f\"rows  {couple_units[0]} and {couple_units[1]} of weight matrix layer {couple_layer}\")\n",
    "        print(f\"and columns {couple_units[0]} and {couple_units[1]} of weight matrix layer {couple_layer+1}\")\n",
    "        print(f\" using {epsilon:.3f} perturbation\")\n",
    "        \n",
    "        # Train with continual learning approach\n",
    "        metrics = train_continual_learning(\n",
    "            model, \n",
    "            data_loaders,\n",
    "            coupling_info,\n",
    "            epochs_first_task=epochs_first_task,\n",
    "            epochs_second_task=epochs_second_task,\n",
    "            learning_rate=learning_rate,\n",
    "            print_model_step=print_model_step, \n",
    "            train_steps=train_steps,\n",
    "            train2_steps=train2_steps,\n",
    "        )\n",
    "        \n",
    "        # Plot and save results - do this on CPU to avoid memory issues\n",
    "        # Move model to CPU for final analysis\n",
    "        model = model.to('cpu')\n",
    "        # plot_continual_learning_results(metrics)\n",
    "        \n",
    "        # Analyze results of correlation after coupling\n",
    "        coupling_idx = metrics['phase'].index('coupling')\n",
    "        post_coupling_cosine = metrics['cosine_similarity'][coupling_idx]\n",
    "        final_cosine = metrics['cosine_similarity'][-1]\n",
    "        \n",
    "        post_coupling_distance = metrics['euclidean_distance'][coupling_idx]\n",
    "        final_distance = metrics['euclidean_distance'][-1]\n",
    "        \n",
    "        print(\"\\n===== Final Results =====\")\n",
    "        print(f\"Cosine similarity: {post_coupling_cosine:.4f} (after coupling) -> {final_cosine:.4f} (final)\")\n",
    "        print(f\"Euclidean distance: {post_coupling_distance:.4f} (after coupling) -> {final_distance:.4f} (final)\")\n",
    "        \n",
    "        if final_cosine > post_coupling_cosine:\n",
    "            print(\"Units became MORE similar in terms of cosine similarity during training on second task.\")\n",
    "        else:\n",
    "            print(\"Units became LESS similar in terms of cosine similarity during training on second task.\")\n",
    "        \n",
    "        if final_distance < post_coupling_distance:\n",
    "            print(\"Units became CLOSER in terms of Euclidean distance during training on second task.\")\n",
    "        else:\n",
    "            print(\"Units moved FURTHER APART in terms of Euclidean distance during training on second task.\")\n",
    "        \n",
    "        # Also measure catastrophic forgetting\n",
    "        first_task_acc_before = metrics['test_accuracies_first'][coupling_idx - 1]\n",
    "        first_task_acc_after = metrics['test_accuracies_first'][-1]\n",
    "        \n",
    "        print(\"\\n===== Catastrophic Forgetting Analysis =====\")\n",
    "        print(f\"Task 1 accuracy: {first_task_acc_before:.2f}% (before Task 2) -> {first_task_acc_after:.2f}% (after Task 2)\")\n",
    "        \n",
    "        forgetting = first_task_acc_before - first_task_acc_after\n",
    "        if forgetting > 0:\n",
    "            print(f\"Catastrophic forgetting observed: {forgetting:.2f}% drop in Task 1 performance.\")\n",
    "        else:\n",
    "            print(f\"No forgetting observed. Task 1 performance improved by {-forgetting:.2f}%.\")\n",
    "        \n",
    "        return model, metrics\n",
    "    \n",
    "    finally:\n",
    "        # Make sure memory is cleaned up\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the continual learning experiment\n",
    "    config = dict(\n",
    "        first_classes=[0, 1,8,9],  # First 5 CIFAR-10 classes (airplane, auto, bird, cat, deer)\n",
    "        second_classes=[2, 5, 6, 7, 3,4],  # Second 5 CIFAR-10 classes (dog, frog, horse, ship, truck)\n",
    "        hidden_sizes=[200,]*6, \n",
    "        activation_type='tanh',\n",
    "        couple_layer=3,  # Second layer (0-indexed)\n",
    "        couple_units=(0, 1),  # Couple units 0 and 1\n",
    "        epsilon=0.01,  # Perturbation factor\n",
    "        epochs_first_task=15,\n",
    "        epochs_second_task=5,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=512,\n",
    "        print_model_step=100, \n",
    "        train_steps=10000,\n",
    "        train2_steps=10000,\n",
    "    )\n",
    "    print('\\n\\n================= longer trainig =================\\n\\n') \n",
    "    for _ in range(3):\n",
    "        print(\"\\n\\n\\nexperimental config = \", config)\n",
    "        model, metrics = run_continual_learning_experiment(**config)\n",
    "    # config['train_steps'] = 10\n",
    "    config['epochs_first_task'] = 1\n",
    "    print('\\n\\n================= shorter trainig =================\\n\\n') \n",
    "    for _ in range(3):\n",
    "        print(\"\\n\\n\\nexperimental config = \", config)\n",
    "        model, metrics = run_continual_learning_experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b3a3968-dd9f-483a-8c82-569eba91e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: longer, Seed: 1, Phase 1, Epoch: 2, Step: 100\n",
      "    cos_sim = 0.226\n",
      "    win = [-0.010057, -0.0051312, -0.0021936, -0.001356, -0.00085231, -0.00062902, -0.00038814, -0.00030525, -0.0002361, -0.0001716, -0.00013853, -0.00011702, -8.7452e-05, -6.8232e-05, -6.2275e-05, -5.4817e-05, -4.9105e-05, -4.4498e-05, -4.0845e-05, -3.488e-05, -3.1727e-05, -2.9022e-05, -2.7472e-05, -2.4894e-05, -2.3911e-05, -2.3127e-05, -1.9648e-05, -1.9302e-05, -1.6461e-05, -1.5482e-05, -1.3889e-05, -1.3249e-05, -1.1376e-05, -1.0827e-05, -1.0226e-05, -9.613e-06, -9.1119e-06, -7.9667e-06, -7.788e-06, -6.7689e-06, -6.1904e-06, -5.7642e-06, -5.3821e-06, -5.103e-06, -4.6747e-06, -4.5487e-06, -4.0443e-06, -3.9421e-06, -3.6772e-06, -3.3436e-06, -2.9826e-06, -2.56e-06, -2.4867e-06, -2.2529e-06, -2.2487e-06, -2.1313e-06, -1.9475e-06, -1.8693e-06, -1.7487e-06, -1.6174e-06, -1.5541e-06, -1.3737e-06, -1.3085e-06, -1.2977e-06, -1.1998e-06, -1.0187e-06, -9.8918e-07, -8.8717e-07, -7.9938e-07, -7.7126e-07, -6.795e-07, -6.0994e-07, -6.0029e-07, -5.6114e-07, -5.2655e-07, -4.7183e-07, -4.2981e-07, -3.9606e-07, -3.5178e-07, -3.1202e-07, -3.0348e-07, -2.6678e-07, -2.555e-07, -2.4668e-07, -2.0309e-07, -1.9182e-07, -1.6957e-07, -1.5407e-07, -1.2975e-07, -1.1366e-07, -1.0295e-07, -8.5751e-08, -7.1628e-08, -6.6532e-08, -4.6866e-08, -4.101e-08, -2.6711e-08, -1.789e-08, -1.3557e-08, -8.8876e-09, 2.0638e-09, 8.3925e-09, 1.0655e-08, 1.8861e-08, 2.466e-08, 3.2114e-08, 3.5867e-08, 4.7037e-08, 5.679e-08, 6.2467e-08, 7.7749e-08, 8.3655e-08, 8.9018e-08, 1.0117e-07, 1.0948e-07, 1.2022e-07, 1.4213e-07, 1.4547e-07, 1.5629e-07, 1.8649e-07, 2.0425e-07, 2.2747e-07, 2.6712e-07, 2.8202e-07, 3.1227e-07, 3.5448e-07, 4.0429e-07, 4.2277e-07, 4.9357e-07, 5.265e-07, 5.3872e-07, 5.9698e-07, 6.2708e-07, 6.7523e-07, 7.1746e-07, 8.2204e-07, 9.0339e-07, 9.6861e-07, 1.1276e-06, 1.2028e-06, 1.4386e-06, 1.4928e-06, 1.4983e-06, 1.6612e-06, 1.9446e-06, 1.9932e-06, 2.1099e-06, 2.2221e-06, 2.4097e-06, 2.7161e-06, 3.1099e-06, 3.3205e-06, 3.618e-06, 3.6573e-06, 4.0603e-06, 4.2753e-06, 4.6583e-06, 5.1534e-06, 5.6185e-06, 6.0334e-06, 6.534e-06, 6.5718e-06, 6.9418e-06, 8.2012e-06, 8.5788e-06, 9.4842e-06, 9.7684e-06, 1.0356e-05, 1.1737e-05, 1.309e-05, 1.4761e-05, 1.5105e-05, 1.62e-05, 1.7876e-05, 1.9788e-05, 2.1155e-05, 2.4244e-05, 2.5977e-05, 2.908e-05, 3.2113e-05, 3.5684e-05, 3.9754e-05, 4.3807e-05, 5.0937e-05, 6.9808e-05, 7.6612e-05, 7.7409e-05, 0.00010836, 0.00012, 0.0001621, 0.00017023, 0.0002336, 0.00030635, 0.00036824, 0.00060974, 0.0007866, 0.00095023, 0.0025522, 0.0029793, 0.0078267]\n",
      "    wout = [-0.0041889, -0.0025049, -0.0021425, -0.0020004, -0.001661, -0.0016325, -0.0016167, -0.001419, -0.0013886, -0.0013453, -0.0012898, -0.0012558, -0.0012167, -0.0011874, -0.0010712, -0.0010055, -0.00097643, -0.00096065, -0.00093378, -0.00090063, -0.00088003, -0.00087469, -0.00081025, -0.00079755, -0.00078159, -0.00077395, -0.00075348, -0.00074634, -0.00072069, -0.00071176, -0.00070247, -0.00069839, -0.00068567, -0.00067485, -0.00065642, -0.00064734, -0.00063878, -0.00063576, -0.00061525, -0.00061081, -0.00060668, -0.00060371, -0.00060078, -0.00059264, -0.00057707, -0.00056232, -0.00055597, -0.00054456, -0.00053344, -0.00053011, -0.00052433, -0.00050846, -0.00049927, -0.00049472, -0.00048515, -0.00048009, -0.00047099, -0.00046927, -0.00045216, -0.00044543, -0.00043297, -0.00042649, -0.00042296, -0.00040706, -0.0004027, -0.00037099, -0.00036719, -0.00035518, -0.00035035, -0.00033659, -0.00032008, -0.00030475, -0.00028417, -0.00027307, -0.0002502, -0.00024544, -0.00023856, -0.0002325, -0.00022834, -0.00021722, -0.00020873, -0.00020846, -0.00019828, -0.00018271, -0.00017508, -0.00017286, -0.00016561, -0.00016459, -0.00015883, -0.00015155, -0.00014902, -0.00014057, -0.00013379, -0.00013104, -0.00012211, -0.00011974, -0.00011124, -0.00011055, -0.00010594, -9.806e-05, -9.352e-05, -8.4067e-05, -7.9154e-05, -7.7646e-05, -6.9114e-05, -6.6556e-05, -6.4758e-05, -5.6819e-05, -5.2154e-05, -3.5197e-05, -3.3839e-05, -3.1581e-05, -2.7997e-05, -2.3989e-05, -2.1616e-05, -1.1675e-05, -3.9317e-06, 5.0112e-06, 2.1778e-05, 2.5254e-05, 3.2547e-05, 3.764e-05, 4.1841e-05, 4.2757e-05, 4.7739e-05, 4.9339e-05, 5.047e-05, 5.6751e-05, 6.3962e-05, 6.7687e-05, 7.2942e-05, 7.4932e-05, 8.5498e-05, 0.00010269, 0.00011082, 0.0001165, 0.00012216, 0.00014793, 0.00015467, 0.00017067, 0.00017335, 0.00018267, 0.0001951, 0.00020723, 0.00020891, 0.00022655, 0.00023242, 0.00023746, 0.00024995, 0.00027686, 0.00029441, 0.00029592, 0.00030133, 0.00031722, 0.00032028, 0.00032613, 0.00037316, 0.0003786, 0.00038742, 0.00041536, 0.0004277, 0.00044159, 0.00045381, 0.00046361, 0.0004831, 0.00049197, 0.00051468, 0.00054758, 0.00055926, 0.00056802, 0.00060765, 0.00062335, 0.00066058, 0.00072131, 0.00074027, 0.00077613, 0.0008023, 0.00084708, 0.00086924, 0.00091016, 0.00093606, 0.0010097, 0.0010459, 0.0010758, 0.001123, 0.0011473, 0.0011708, 0.0013123, 0.0013327, 0.0013651, 0.0014308, 0.0014919, 0.001545, 0.0016404, 0.0018393, 0.0021804, 0.0022305, 0.0023874, 0.0026052, 0.0030225]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 1, Epoch: 4, Step: 200\n",
      "    cos_sim = 0.327\n",
      "    win = [-0.0089, -0.006135, -0.0014802, -0.00033769, -0.00017355, -0.00015304, -1.9809e-05, -8.1923e-06, -1.4996e-06, -7.3851e-07, -3.6681e-07, -9.1473e-08, -6.3117e-08, -3.2057e-08, -1.8657e-08, -7.7962e-09, -4.0226e-09, -3.812e-09, -1.361e-09, -1.1447e-09, -9.9895e-10, -9.6391e-10, -9.4408e-10, -9.2264e-10, -8.8079e-10, -8.5963e-10, -8.3422e-10, -7.9557e-10, -7.644e-10, -7.4061e-10, -7.3604e-10, -7.2263e-10, -7.1302e-10, -6.8719e-10, -6.7176e-10, -6.4372e-10, -6.307e-10, -6.0264e-10, -5.9452e-10, -5.8793e-10, -5.7769e-10, -5.6144e-10, -5.3143e-10, -5.2666e-10, -5.148e-10, -5.07e-10, -5.0015e-10, -4.7927e-10, -4.7756e-10, -4.6789e-10, -4.5455e-10, -4.3608e-10, -4.2756e-10, -4.1992e-10, -4.1676e-10, -4.0352e-10, -3.7924e-10, -3.674e-10, -3.648e-10, -3.5847e-10, -3.5229e-10, -3.4408e-10, -3.3967e-10, -3.2416e-10, -3.2121e-10, -3.1499e-10, -3.0552e-10, -2.9848e-10, -2.8305e-10, -2.7255e-10, -2.5137e-10, -2.4402e-10, -2.2917e-10, -2.2714e-10, -2.1784e-10, -2.1349e-10, -1.9728e-10, -1.8625e-10, -1.8266e-10, -1.7242e-10, -1.6201e-10, -1.5393e-10, -1.468e-10, -1.4249e-10, -1.3339e-10, -1.2331e-10, -1.0853e-10, -1.0519e-10, -9.932e-11, -9.3694e-11, -9.1571e-11, -8.3249e-11, -6.846e-11, -6.0927e-11, -5.0324e-11, -4.4371e-11, -4.0897e-11, -3.2875e-11, -2.6813e-11, -1.4521e-11, -7.9215e-12, 4.5404e-12, 8.829e-12, 1.6009e-11, 1.8839e-11, 3.1333e-11, 3.7386e-11, 4.3977e-11, 5.2361e-11, 5.9477e-11, 6.1465e-11, 6.8137e-11, 8.2165e-11, 9.5005e-11, 1.0297e-10, 1.0806e-10, 1.2283e-10, 1.325e-10, 1.3476e-10, 1.4605e-10, 1.5254e-10, 1.602e-10, 1.6536e-10, 1.7652e-10, 1.8054e-10, 1.9177e-10, 2.0705e-10, 2.1762e-10, 2.1926e-10, 2.3162e-10, 2.3544e-10, 2.5067e-10, 2.5554e-10, 2.687e-10, 2.7505e-10, 2.8417e-10, 2.9667e-10, 3.0472e-10, 3.136e-10, 3.1958e-10, 3.2771e-10, 3.4063e-10, 3.4948e-10, 3.5631e-10, 3.6227e-10, 3.8058e-10, 4.0154e-10, 4.034e-10, 4.1683e-10, 4.2093e-10, 4.2986e-10, 4.4249e-10, 4.5603e-10, 4.6965e-10, 4.8221e-10, 4.8983e-10, 4.9722e-10, 5.1449e-10, 5.4361e-10, 5.5054e-10, 5.5471e-10, 5.6805e-10, 5.8228e-10, 5.9044e-10, 6.1755e-10, 6.2e-10, 6.4445e-10, 6.4937e-10, 6.6795e-10, 6.7669e-10, 7.0153e-10, 7.2149e-10, 7.4226e-10, 7.5847e-10, 7.6953e-10, 7.9569e-10, 8.1785e-10, 8.3789e-10, 8.6583e-10, 9.2511e-10, 9.3856e-10, 1.0115e-09, 1.0256e-09, 1.104e-09, 1.1936e-09, 1.2349e-09, 1.4737e-09, 2.3563e-09, 2.9965e-09, 1.7342e-08, 1.9323e-08, 3.0786e-08, 4.747e-08, 1.8604e-07, 0.00012005, 0.00026638, 0.00054098, 0.011015, 0.029481, 0.07222]\n",
      "    wout = [-0.0075057, -0.006141, -0.0033826, -0.0031069, -0.0027272, -0.0018961, -0.0016798, -0.0016467, -0.0016102, -0.0015157, -0.0013859, -0.0012399, -0.0012123, -0.0011632, -0.0010894, -0.0010815, -0.00095126, -0.00090304, -0.00087442, -0.00084899, -0.0008099, -0.00076722, -0.00076333, -0.00072734, -0.0006952, -0.00068845, -0.00065486, -0.00063532, -0.00063159, -0.00062137, -0.00059365, -0.00056293, -0.00055006, -0.00054293, -0.00051428, -0.00050982, -0.00047563, -0.0004656, -0.00044701, -0.0004421, -0.00043463, -0.0004267, -0.00042118, -0.00039565, -0.00037029, -0.00035563, -0.00033361, -0.00033173, -0.00032949, -0.00032326, -0.00031569, -0.00030592, -0.00029718, -0.00029121, -0.0002826, -0.00027211, -0.000265, -0.00025396, -0.00024872, -0.00023733, -0.000227, -0.0002185, -0.00020201, -0.0001721, -0.00015245, -0.00015032, -0.00014189, -0.00013206, -0.00012113, -0.00011539, -0.00010834, -0.00010042, -9.44e-05, -8.2591e-05, -6.6391e-05, -5.7608e-05, -4.4981e-05, -3.2408e-05, -2.4649e-05, -2.3585e-05, -2.2775e-05, -1.7735e-05, -8.0449e-06, -2.9221e-06, -2.1421e-06, -1.6526e-06, -7.362e-07, -4.4987e-07, -7.9348e-08, -5.0698e-08, 7.7545e-08, 1.3376e-07, 4.3845e-06, 5.9053e-06, 9.8915e-06, 1.8562e-05, 2.5369e-05, 3.2373e-05, 3.5421e-05, 3.8796e-05, 4.0209e-05, 4.5745e-05, 4.6916e-05, 5.355e-05, 5.5267e-05, 6.9788e-05, 7.9994e-05, 8.7181e-05, 9.0248e-05, 9.5082e-05, 9.6791e-05, 0.0001073, 0.00011053, 0.00011243, 0.00011891, 0.00012771, 0.00012981, 0.00013369, 0.00014487, 0.00015127, 0.00016823, 0.00017823, 0.00019171, 0.00019644, 0.00020911, 0.00021292, 0.00022753, 0.00023482, 0.00023626, 0.00024657, 0.00024894, 0.00025894, 0.0002672, 0.00027257, 0.00028495, 0.00029869, 0.00030204, 0.00033645, 0.00034643, 0.00034717, 0.00035896, 0.00036194, 0.0003695, 0.00038076, 0.00041543, 0.0004181, 0.00042318, 0.00044246, 0.00045489, 0.0004643, 0.00048421, 0.00049129, 0.00051252, 0.0005257, 0.00053942, 0.0005526, 0.00055731, 0.00057919, 0.00058643, 0.00060352, 0.00061191, 0.00063267, 0.00065438, 0.00067957, 0.00068421, 0.00069109, 0.00071974, 0.00072789, 0.00072966, 0.00075011, 0.00080202, 0.00081712, 0.00082682, 0.00083163, 0.00083954, 0.00084665, 0.00086304, 0.00087375, 0.00089561, 0.00091531, 0.00092832, 0.00093869, 0.00094698, 0.001071, 0.0011351, 0.001165, 0.0011735, 0.0011833, 0.0012426, 0.0012693, 0.0013016, 0.0013597, 0.0013947, 0.0015986, 0.0016474, 0.002084, 0.0022314, 0.0028614, 0.0039225, 0.0045059]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 1, Epoch: 7, Step: 300\n",
      "    cos_sim = 0.119\n",
      "    win = [-0.018042, -0.0095536, -0.0041741, -0.0020024, -0.0012566, -0.0010749, -0.00084785, -0.00071986, -0.00061573, -0.00046832, -0.00034895, -0.00031335, -0.00024156, -0.00021831, -0.00020647, -0.00018915, -0.00015636, -0.00014857, -0.00012014, -0.00011218, -0.00010513, -9.7014e-05, -8.4301e-05, -7.4918e-05, -7.3294e-05, -6.775e-05, -6.2316e-05, -5.5473e-05, -4.9862e-05, -4.8802e-05, -4.042e-05, -3.7591e-05, -3.3643e-05, -3.1666e-05, -2.8097e-05, -2.5183e-05, -2.4948e-05, -2.331e-05, -2.1658e-05, -2.0268e-05, -1.8099e-05, -1.7234e-05, -1.6417e-05, -1.4803e-05, -1.3386e-05, -1.2886e-05, -1.1686e-05, -1.1293e-05, -1.0011e-05, -9.0095e-06, -8.1966e-06, -7.647e-06, -7.3347e-06, -6.6333e-06, -5.717e-06, -5.4347e-06, -5.09e-06, -4.9726e-06, -4.25e-06, -3.9522e-06, -3.6468e-06, -3.3201e-06, -2.9069e-06, -2.6633e-06, -2.5314e-06, -2.3144e-06, -2.2522e-06, -2.0377e-06, -1.9057e-06, -1.6515e-06, -1.5781e-06, -1.4962e-06, -1.4071e-06, -1.3628e-06, -1.1243e-06, -1.0372e-06, -9.7804e-07, -9.4461e-07, -8.1601e-07, -7.3222e-07, -7.0363e-07, -6.8617e-07, -6.0812e-07, -5.1862e-07, -4.3145e-07, -4.2246e-07, -3.9814e-07, -3.4394e-07, -3.1492e-07, -2.5583e-07, -2.4218e-07, -1.9087e-07, -1.6748e-07, -1.4894e-07, -1.3007e-07, -1.1106e-07, -9.3624e-08, -6.1446e-08, -5.4203e-08, -4.3809e-08, -3.3264e-08, -4.039e-09, 2.0852e-09, 2.9193e-08, 3.5647e-08, 6.5747e-08, 9.4229e-08, 1.1625e-07, 1.2983e-07, 1.736e-07, 2.0811e-07, 2.5217e-07, 2.8058e-07, 3.4201e-07, 3.7712e-07, 4.4304e-07, 4.9233e-07, 5.385e-07, 6.4243e-07, 6.7187e-07, 6.8648e-07, 7.9801e-07, 8.4946e-07, 8.5629e-07, 9.8863e-07, 1.1247e-06, 1.1569e-06, 1.3864e-06, 1.5866e-06, 1.6437e-06, 1.7438e-06, 1.7909e-06, 2.1101e-06, 2.3381e-06, 2.4919e-06, 2.7911e-06, 2.9737e-06, 3.1212e-06, 3.3549e-06, 3.4769e-06, 3.9592e-06, 4.5043e-06, 4.7905e-06, 5.0005e-06, 5.2629e-06, 5.6521e-06, 5.6888e-06, 6.5544e-06, 7.1558e-06, 7.6192e-06, 8.2178e-06, 9.314e-06, 9.9092e-06, 1.0865e-05, 1.1727e-05, 1.2444e-05, 1.4668e-05, 1.4952e-05, 1.646e-05, 1.7527e-05, 1.8343e-05, 2.0723e-05, 2.3233e-05, 2.5054e-05, 2.7493e-05, 2.8807e-05, 3.5025e-05, 3.6487e-05, 4.0958e-05, 4.6807e-05, 4.9479e-05, 5.231e-05, 5.777e-05, 6.0746e-05, 6.3647e-05, 7.7044e-05, 8.43e-05, 9.096e-05, 9.2988e-05, 0.00010775, 0.00013288, 0.00013984, 0.00014869, 0.00016205, 0.00018118, 0.00020192, 0.00020938, 0.00022979, 0.00024388, 0.00028643, 0.00034257, 0.00036179, 0.00049924, 0.00051772, 0.00094573, 0.0010913, 0.0013975, 0.0032392, 0.0053611, 0.029932]\n",
      "    wout = [-0.0018215, -0.0016852, -0.001517, -0.0013533, -0.0013284, -0.0011855, -0.001117, -0.0011047, -0.00098364, -0.00091152, -0.00090688, -0.00083305, -0.00081528, -0.00079728, -0.00069743, -0.0006296, -0.00059585, -0.00059095, -0.00057694, -0.00056971, -0.00055799, -0.00054796, -0.0005415, -0.00051522, -0.0005082, -0.00050402, -0.00049851, -0.00049016, -0.0004711, -0.00046327, -0.00046315, -0.00045422, -0.00042008, -0.00041131, -0.00039422, -0.00038455, -0.00037406, -0.00037258, -0.0003615, -0.00035784, -0.0003498, -0.00032495, -0.00031988, -0.00030809, -0.00029262, -0.00028817, -0.00028095, -0.00026605, -0.0002492, -0.00023459, -0.00023225, -0.00022314, -0.00022021, -0.00021716, -0.00021151, -0.00020908, -0.000207, -0.00020029, -0.00019688, -0.00019226, -0.00019092, -0.00018576, -0.00018321, -0.00018004, -0.0001764, -0.00017225, -0.00016894, -0.00016587, -0.00014677, -0.00014001, -0.00013833, -0.00013478, -0.00013057, -0.00012718, -0.00012087, -0.00011203, -0.00010978, -0.00010217, -9.6816e-05, -9.3411e-05, -9.3285e-05, -9.0957e-05, -8.862e-05, -8.1967e-05, -8.0074e-05, -7.6076e-05, -6.8798e-05, -5.9203e-05, -5.3542e-05, -4.9117e-05, -4.6225e-05, -3.7007e-05, -3.2003e-05, -2.9246e-05, -2.6892e-05, -1.7816e-05, -1.3533e-05, -1.1022e-05, -9.0116e-06, -4.4877e-06, -3.2732e-06, 1.8971e-07, 4.667e-06, 9.4552e-06, 1.661e-05, 2.0684e-05, 2.4652e-05, 2.5929e-05, 3.3542e-05, 3.9474e-05, 4.3617e-05, 4.798e-05, 4.9988e-05, 5.799e-05, 6.7829e-05, 7.1116e-05, 7.7014e-05, 8.8324e-05, 9.2102e-05, 0.00010118, 0.00011387, 0.00012546, 0.00013979, 0.00014705, 0.00016508, 0.00016665, 0.00018015, 0.00018175, 0.00018838, 0.00019368, 0.00019626, 0.00020724, 0.00021468, 0.00023353, 0.00026383, 0.00028426, 0.00029132, 0.00029947, 0.000307, 0.00031433, 0.00032463, 0.00033978, 0.00036591, 0.00040523, 0.00041567, 0.00043675, 0.0004546, 0.00046844, 0.0004945, 0.00050653, 0.00050853, 0.00053924, 0.0005479, 0.00054936, 0.00057244, 0.00058575, 0.00060085, 0.00064054, 0.00067307, 0.00072752, 0.00074007, 0.00075426, 0.00076691, 0.00081877, 0.00082654, 0.00083988, 0.00090541, 0.00093504, 0.00096264, 0.00098072, 0.0010165, 0.0010219, 0.0010574, 0.0010731, 0.0011053, 0.0011224, 0.0011496, 0.0011931, 0.001205, 0.001262, 0.0012756, 0.0013524, 0.0013924, 0.0014607, 0.0015093, 0.0015516, 0.0016469, 0.0016746, 0.0017116, 0.0017385, 0.0018582, 0.0018996, 0.0022571, 0.0023324, 0.0031493, 0.0032048, 0.0035121, 0.0035896, 0.0041729, 0.011883]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 1, Epoch: 9, Step: 400\n",
      "    cos_sim = -0.136\n",
      "    win = [-0.03939, -0.011708, -0.0040231, -0.00069799, -0.00038119, -0.00018504, -9.7692e-05, -1.8135e-05, -1.6625e-06, -3.2811e-07, -2.2887e-08, -1.3083e-08, -4.3868e-09, -2.9613e-09, -2.4019e-09, -1.7794e-09, -1.483e-09, -9.8643e-10, -7.4006e-10, -6.7222e-10, -6.2651e-10, -6.1616e-10, -5.7735e-10, -5.5703e-10, -5.4347e-10, -5.4092e-10, -5.1647e-10, -5.0665e-10, -4.8184e-10, -4.6964e-10, -4.6116e-10, -4.4039e-10, -4.3397e-10, -4.2559e-10, -4.2113e-10, -4.1021e-10, -4.0374e-10, -3.936e-10, -3.927e-10, -3.748e-10, -3.6974e-10, -3.5834e-10, -3.5438e-10, -3.4108e-10, -3.3363e-10, -3.1568e-10, -3.0913e-10, -2.9991e-10, -2.9773e-10, -2.894e-10, -2.8501e-10, -2.7554e-10, -2.7203e-10, -2.6454e-10, -2.5974e-10, -2.4802e-10, -2.4032e-10, -2.3409e-10, -2.2359e-10, -2.192e-10, -2.1736e-10, -2.1086e-10, -2.0362e-10, -1.9806e-10, -1.9353e-10, -1.9019e-10, -1.8291e-10, -1.7127e-10, -1.6575e-10, -1.6107e-10, -1.4668e-10, -1.395e-10, -1.3154e-10, -1.2735e-10, -1.2152e-10, -1.2076e-10, -1.1268e-10, -1.0596e-10, -1.0333e-10, -9.5163e-11, -9.2519e-11, -8.6174e-11, -8.3731e-11, -7.5832e-11, -7.1628e-11, -6.6721e-11, -6.2733e-11, -5.4562e-11, -5.3631e-11, -4.4794e-11, -3.9383e-11, -3.5394e-11, -2.871e-11, -2.3997e-11, -1.6467e-11, -1.558e-11, -1.0693e-11, -9.3997e-12, -3.9555e-12, 1.471e-12, 6.0918e-12, 1.2862e-11, 1.7582e-11, 2.3461e-11, 2.6655e-11, 3.5048e-11, 4.0182e-11, 4.6581e-11, 5.2347e-11, 5.2748e-11, 5.6433e-11, 6.4035e-11, 6.6502e-11, 7.6059e-11, 8.3017e-11, 9.3141e-11, 9.4773e-11, 1.0027e-10, 1.0458e-10, 1.1144e-10, 1.1405e-10, 1.1684e-10, 1.2683e-10, 1.2915e-10, 1.3159e-10, 1.3711e-10, 1.429e-10, 1.5074e-10, 1.5708e-10, 1.6507e-10, 1.7231e-10, 1.8002e-10, 1.8222e-10, 1.8977e-10, 1.9707e-10, 1.9816e-10, 2.0243e-10, 2.0701e-10, 2.2116e-10, 2.2451e-10, 2.3264e-10, 2.4094e-10, 2.4498e-10, 2.5243e-10, 2.5706e-10, 2.5919e-10, 2.6935e-10, 2.8395e-10, 2.8665e-10, 3.0003e-10, 3.098e-10, 3.1372e-10, 3.2097e-10, 3.2583e-10, 3.3637e-10, 3.4043e-10, 3.4676e-10, 3.5662e-10, 3.6616e-10, 3.7952e-10, 3.861e-10, 3.9715e-10, 3.9997e-10, 4.0809e-10, 4.1396e-10, 4.2096e-10, 4.4352e-10, 4.5408e-10, 4.649e-10, 4.793e-10, 4.8782e-10, 5.0176e-10, 5.1423e-10, 5.3429e-10, 5.4389e-10, 5.7189e-10, 5.8503e-10, 6.1713e-10, 7.0501e-10, 7.1124e-10, 7.4293e-10, 7.4883e-10, 9.5147e-10, 1.2215e-09, 1.3033e-09, 1.5463e-09, 4.4765e-09, 3.802e-08, 1.2014e-07, 5.2965e-06, 6.7622e-06, 7.554e-06, 1.5507e-05, 7.6261e-05, 9.0224e-05, 9.6088e-05, 0.0021109, 0.0059523, 0.0107, 0.014225]\n",
      "    wout = [-0.007838, -0.0059265, -0.003848, -0.0035815, -0.0026769, -0.0025276, -0.0024527, -0.0021334, -0.0019281, -0.0019196, -0.0019105, -0.0018001, -0.0017184, -0.0016626, -0.0015548, -0.0014551, -0.0013934, -0.0013149, -0.0011747, -0.0011609, -0.0011321, -0.0011102, -0.0010901, -0.0010496, -0.0010418, -0.0009877, -0.00095787, -0.00093346, -0.00085309, -0.0008477, -0.00079191, -0.00078664, -0.00077423, -0.00076653, -0.00073835, -0.00072765, -0.00070917, -0.00069588, -0.00068257, -0.00067966, -0.00064138, -0.00062887, -0.0006064, -0.00059813, -0.00058958, -0.00058788, -0.00055837, -0.00052656, -0.00052216, -0.00047395, -0.00043175, -0.00041313, -0.00039811, -0.00039376, -0.00034875, -0.00033181, -0.00031614, -0.00031285, -0.00029988, -0.00028045, -0.00024452, -0.00022399, -0.00021234, -0.00020762, -0.00020718, -0.00017495, -0.00013683, -0.00011802, -0.00011381, -9.4607e-05, -7.3958e-05, -6.339e-05, -6.0946e-05, -3.7057e-05, -2.5153e-05, -1.8572e-05, 4.2476e-06, 5.3908e-06, 8.307e-06, 2.2766e-05, 2.8106e-05, 5.5593e-05, 6.0783e-05, 8.2796e-05, 9.0947e-05, 9.8746e-05, 0.00010422, 0.00010638, 0.00010724, 0.00013269, 0.00013589, 0.00014262, 0.00015322, 0.00015848, 0.00016718, 0.00018157, 0.00018913, 0.00019704, 0.00020139, 0.00020476, 0.00022494, 0.00023513, 0.00024841, 0.00025441, 0.00025924, 0.00026802, 0.00027993, 0.00030338, 0.00031402, 0.00032139, 0.00034172, 0.00035029, 0.0003549, 0.00036083, 0.00037033, 0.00037452, 0.00037628, 0.00037988, 0.00039074, 0.00040368, 0.000421, 0.00043438, 0.00045127, 0.00046064, 0.00047274, 0.00049263, 0.00050528, 0.00051356, 0.00053904, 0.00057095, 0.00059819, 0.00060826, 0.00063247, 0.00064183, 0.0006511, 0.00065553, 0.00066854, 0.00067885, 0.00069146, 0.00070356, 0.0007093, 0.00074835, 0.00077391, 0.00080936, 0.00082993, 0.00086441, 0.000875, 0.0009086, 0.00092729, 0.00094742, 0.00094995, 0.00096426, 0.00098207, 0.00099672, 0.0010052, 0.0010239, 0.0010703, 0.0011038, 0.0011083, 0.0011346, 0.0011768, 0.0012094, 0.0012242, 0.0012406, 0.0012552, 0.0012664, 0.0012863, 0.0013361, 0.0013549, 0.0013742, 0.0014332, 0.0014562, 0.001504, 0.0015535, 0.0015774, 0.0015973, 0.0016402, 0.0017174, 0.0017461, 0.0017733, 0.0018302, 0.0018544, 0.0019405, 0.0019873, 0.0020287, 0.0020736, 0.0021173, 0.0022504, 0.0022908, 0.0023509, 0.0026269, 0.0028263, 0.0029941, 0.0035333, 0.0038689, 0.0045064, 0.0052537, 0.011972, 0.014385, 0.016054]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 1, Epoch: 12, Step: 500\n",
      "    cos_sim = 0.248\n",
      "    win = [-0.018294, -0.0087189, -0.0041951, -0.003164, -0.0027652, -0.0018977, -0.0014684, -0.0012841, -0.00094647, -0.00088019, -0.00077831, -0.00061625, -0.00051603, -0.00050746, -0.0004429, -0.00040296, -0.00034177, -0.00030977, -0.00030005, -0.00027491, -0.00027001, -0.00026035, -0.00024148, -0.00022299, -0.00021325, -0.00018816, -0.00017337, -0.00016551, -0.00015781, -0.00014616, -0.00013924, -0.0001327, -0.00012393, -0.00011221, -0.00010884, -0.0001028, -9.4374e-05, -8.8374e-05, -8.3856e-05, -7.1803e-05, -6.9116e-05, -6.7128e-05, -6.0043e-05, -5.9469e-05, -5.4558e-05, -5.3193e-05, -5.2735e-05, -4.5273e-05, -4.3431e-05, -4.2471e-05, -4.0634e-05, -3.8749e-05, -3.8008e-05, -3.7233e-05, -3.5205e-05, -3.2023e-05, -3.1429e-05, -2.8205e-05, -2.7087e-05, -2.5747e-05, -2.3673e-05, -2.2694e-05, -2.1353e-05, -2.0325e-05, -1.9778e-05, -1.8503e-05, -1.7384e-05, -1.5906e-05, -1.5116e-05, -1.3912e-05, -1.3663e-05, -1.2873e-05, -1.1962e-05, -1.1586e-05, -9.9578e-06, -9.6919e-06, -9.268e-06, -8.2914e-06, -7.6315e-06, -7.4404e-06, -6.9592e-06, -6.7573e-06, -6.0463e-06, -5.5877e-06, -5.3239e-06, -5.2135e-06, -4.6444e-06, -4.4377e-06, -4.1472e-06, -3.5071e-06, -3.277e-06, -3.1225e-06, -2.9975e-06, -2.8036e-06, -2.6752e-06, -2.5671e-06, -2.2112e-06, -2.0678e-06, -1.7682e-06, -1.6483e-06, -1.3821e-06, -1.0957e-06, -8.9137e-07, -8.6445e-07, -7.9893e-07, -6.8985e-07, -4.8215e-07, -4.3792e-07, -3.7059e-07, -1.4612e-07, -1.3114e-07, 8.4487e-09, 4.7348e-08, 3.13e-07, 3.6219e-07, 4.1755e-07, 5.0378e-07, 5.8474e-07, 7.3831e-07, 8.5054e-07, 9.1399e-07, 1.0991e-06, 1.2605e-06, 1.3658e-06, 1.5205e-06, 1.6185e-06, 1.8492e-06, 2.1667e-06, 2.3208e-06, 2.4776e-06, 2.8681e-06, 3.0075e-06, 3.4322e-06, 3.7213e-06, 3.8196e-06, 4.4638e-06, 4.9303e-06, 5.0866e-06, 5.2132e-06, 5.3096e-06, 6.0559e-06, 7.0995e-06, 7.5346e-06, 8.6836e-06, 8.8771e-06, 9.1924e-06, 9.6075e-06, 1.0866e-05, 1.1688e-05, 1.2233e-05, 1.2969e-05, 1.5052e-05, 1.5727e-05, 1.6851e-05, 1.8029e-05, 2.0841e-05, 2.324e-05, 2.3977e-05, 2.451e-05, 2.6125e-05, 2.8983e-05, 3.1635e-05, 3.4984e-05, 3.6154e-05, 3.9229e-05, 4.1865e-05, 4.6052e-05, 5.428e-05, 5.5659e-05, 6.0062e-05, 6.9194e-05, 7.2376e-05, 7.6745e-05, 8.588e-05, 9.6602e-05, 0.00010437, 0.00010987, 0.00012292, 0.00013211, 0.00015077, 0.00016255, 0.00016677, 0.0001953, 0.00022595, 0.00027157, 0.00031628, 0.0003301, 0.00037971, 0.00045057, 0.00049722, 0.0005686, 0.00063539, 0.00075047, 0.00088965, 0.0010276, 0.0010989, 0.0013178, 0.0018918, 0.0040409, 0.024616]\n",
      "    wout = [-0.0020915, -0.0011944, -0.0011786, -0.0010594, -0.00094262, -0.00083789, -0.00079178, -0.00071907, -0.00068671, -0.00058699, -0.00057486, -0.00056819, -0.00054411, -0.00051754, -0.00046185, -0.00045574, -0.00044029, -0.00043088, -0.00042383, -0.00041581, -0.00039985, -0.00037982, -0.00036182, -0.00034977, -0.0003452, -0.00034152, -0.00033593, -0.0003345, -0.00033073, -0.00032805, -0.00032061, -0.00031341, -0.00030426, -0.00030171, -0.00028617, -0.00028298, -0.00027579, -0.00027372, -0.00026678, -0.00025816, -0.00025237, -0.000248, -0.00024554, -0.00023723, -0.00023197, -0.00021669, -0.00020873, -0.00020451, -0.00020175, -0.00019067, -0.00018885, -0.00018785, -0.00017866, -0.00017386, -0.00016733, -0.00016259, -0.0001605, -0.00015499, -0.00015093, -0.00014711, -0.00014357, -0.00014144, -0.00013686, -0.00013548, -0.00012703, -0.00012163, -0.00011928, -0.00011652, -0.00011092, -0.00010676, -0.00010046, -9.941e-05, -9.5942e-05, -8.9684e-05, -8.5895e-05, -8.129e-05, -7.9059e-05, -7.6917e-05, -7.5765e-05, -7.2352e-05, -6.6695e-05, -6.2335e-05, -5.8362e-05, -5.1172e-05, -5.0029e-05, -4.8215e-05, -4.6824e-05, -4.5708e-05, -4.2294e-05, -4.1269e-05, -3.6693e-05, -3.4703e-05, -3.2358e-05, -2.6999e-05, -2.3558e-05, -2.1945e-05, -1.2349e-05, -1.1733e-05, -7.0836e-06, -1.9973e-06, -1.0242e-06, 3.239e-06, 4.9392e-06, 6.6318e-06, 9.1717e-06, 1.0644e-05, 1.3143e-05, 1.4756e-05, 1.996e-05, 2.2681e-05, 2.5882e-05, 3.4219e-05, 3.8892e-05, 4.5451e-05, 4.8583e-05, 5.047e-05, 5.4462e-05, 6.3968e-05, 6.5275e-05, 7.0083e-05, 7.2602e-05, 8.1028e-05, 8.1346e-05, 8.9445e-05, 9.3412e-05, 0.000103, 0.00010913, 0.00011436, 0.00011842, 0.00012579, 0.00012856, 0.00013088, 0.00013647, 0.00013835, 0.00014272, 0.00014668, 0.00015483, 0.00015736, 0.00016469, 0.00016604, 0.00016955, 0.00018394, 0.00019077, 0.00019589, 0.00020853, 0.00021, 0.00021285, 0.00022054, 0.00022194, 0.00022806, 0.00024128, 0.00024544, 0.00025387, 0.0002561, 0.00026147, 0.00027619, 0.00028093, 0.00028454, 0.0002873, 0.00029557, 0.00031291, 0.00031613, 0.0003238, 0.00033645, 0.00034092, 0.00034667, 0.00035655, 0.00036331, 0.00037334, 0.00037862, 0.00039163, 0.00039476, 0.00041835, 0.00043821, 0.00045006, 0.00051508, 0.00054775, 0.00057201, 0.00058983, 0.00059607, 0.00061084, 0.00064642, 0.0006895, 0.00072214, 0.00072618, 0.00075107, 0.00083102, 0.00084609, 0.00091653, 0.00093766, 0.00098555, 0.0010791, 0.0011424, 0.001184, 0.0011984, 0.0014166, 0.0015404, 0.0016504, 0.0026169, 0.0030524]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 1, Epoch: 14, Step: 600\n",
      "    cos_sim = 0.191\n",
      "    win = [-0.067798, -0.010681, -0.0031075, -0.0024606, -0.0014652, -0.00062563, -0.00041152, -0.00026972, -0.00017655, -0.00012897, -0.00011462, -6.5108e-05, -3.9539e-05, -9.7528e-06, -5.9538e-06, -3.8466e-06, -2.7542e-06, -2.519e-06, -2.0637e-06, -1.3066e-06, -7.0763e-07, -6.6507e-09, -1.838e-09, -1.7391e-09, -1.6497e-09, -1.5531e-09, -1.4213e-09, -1.4009e-09, -1.397e-09, -1.3115e-09, -1.2387e-09, -1.225e-09, -1.2175e-09, -1.2091e-09, -1.1579e-09, -1.1397e-09, -1.1159e-09, -1.0429e-09, -1.0218e-09, -1.0007e-09, -9.8587e-10, -9.7751e-10, -9.2644e-10, -9.2145e-10, -8.932e-10, -8.8981e-10, -8.507e-10, -8.3827e-10, -8.2179e-10, -8.0056e-10, -7.8261e-10, -7.6538e-10, -7.5835e-10, -7.3455e-10, -7.0645e-10, -6.9379e-10, -6.8224e-10, -6.7455e-10, -6.6653e-10, -6.4855e-10, -6.1829e-10, -6.0815e-10, -5.8958e-10, -5.7433e-10, -5.5905e-10, -5.4392e-10, -5.1252e-10, -5.0521e-10, -4.9633e-10, -4.6961e-10, -4.6461e-10, -4.3228e-10, -4.1328e-10, -3.9984e-10, -3.9438e-10, -3.8398e-10, -3.7484e-10, -3.6288e-10, -3.5189e-10, -3.4022e-10, -3.1779e-10, -2.8474e-10, -2.8016e-10, -2.756e-10, -2.6321e-10, -2.507e-10, -2.4295e-10, -2.2069e-10, -2.0546e-10, -1.9611e-10, -1.7702e-10, -1.7623e-10, -1.6628e-10, -1.5399e-10, -1.4744e-10, -1.2877e-10, -1.0494e-10, -9.9695e-11, -8.9686e-11, -7.9216e-11, -6.6504e-11, -5.7082e-11, -2.2826e-11, -7.3233e-12, -1.381e-12, 1.3353e-11, 2.5495e-11, 5.6247e-11, 6.6014e-11, 8.0121e-11, 9.1732e-11, 9.7424e-11, 1.1091e-10, 1.1364e-10, 1.2604e-10, 1.4101e-10, 1.5916e-10, 1.6818e-10, 1.7232e-10, 1.8718e-10, 1.9225e-10, 2.054e-10, 2.3314e-10, 2.3861e-10, 2.5423e-10, 2.6744e-10, 2.8293e-10, 3.0368e-10, 3.1548e-10, 3.1725e-10, 3.3047e-10, 3.4967e-10, 3.7133e-10, 3.8047e-10, 3.9961e-10, 4.0844e-10, 4.2445e-10, 4.3769e-10, 4.4556e-10, 4.6619e-10, 4.6906e-10, 4.7646e-10, 5.0894e-10, 5.2406e-10, 5.3114e-10, 5.5639e-10, 5.6873e-10, 5.7309e-10, 5.9123e-10, 6.0405e-10, 6.2671e-10, 6.3799e-10, 6.4796e-10, 6.8618e-10, 6.9396e-10, 7.2476e-10, 7.3916e-10, 7.6274e-10, 7.7717e-10, 8.0054e-10, 8.0562e-10, 8.2477e-10, 8.5014e-10, 8.5868e-10, 8.917e-10, 9.0257e-10, 9.259e-10, 9.4282e-10, 9.612e-10, 9.7896e-10, 9.961e-10, 1.0307e-09, 1.0807e-09, 1.0911e-09, 1.1285e-09, 1.1548e-09, 1.1787e-09, 1.1981e-09, 1.2166e-09, 1.2618e-09, 1.3075e-09, 1.3469e-09, 1.376e-09, 1.4236e-09, 1.6283e-09, 1.8998e-09, 2.033e-09, 2.5206e-09, 8.1214e-09, 2.3228e-08, 2.1689e-07, 1.7525e-06, 1.9994e-05, 0.00017567, 0.0007616, 0.0011415, 0.0020733, 0.0079031, 0.011611, 0.061416]\n",
      "    wout = [-0.0076152, -0.0053054, -0.0051054, -0.0050125, -0.0047287, -0.0046598, -0.0041178, -0.0033372, -0.0030334, -0.0029007, -0.0027956, -0.002739, -0.0027191, -0.002696, -0.0026242, -0.002532, -0.0023546, -0.0022903, -0.0022249, -0.0020618, -0.0020154, -0.0019855, -0.0019627, -0.0018912, -0.001825, -0.0017851, -0.0017154, -0.0016706, -0.0015428, -0.0014831, -0.0014423, -0.0013834, -0.0013352, -0.0012694, -0.0012272, -0.0011823, -0.0011035, -0.0010533, -0.0010198, -0.00095769, -0.00093751, -0.00091318, -0.00089425, -0.00088577, -0.00081637, -0.00080276, -0.00079219, -0.00074569, -0.00073337, -0.00071062, -0.00068695, -0.000675, -0.00065996, -0.00064817, -0.00062658, -0.0006089, -0.0005856, -0.00057113, -0.00056437, -0.00055642, -0.00053937, -0.00047443, -0.00045196, -0.00044018, -0.00042531, -0.00040198, -0.00037931, -0.00037224, -0.00032127, -0.00030854, -0.00030312, -0.00028873, -0.00025947, -0.00025336, -0.000225, -0.00019803, -0.00015963, -0.00015027, -0.00014738, -0.00011871, -0.00011464, -0.00010156, -8.8764e-05, -8.1932e-05, -7.3859e-05, -7.0124e-05, -6.1742e-05, -5.526e-05, -4.4904e-05, -3.6931e-05, -1.9662e-05, -1.1268e-05, -9.7141e-06, -2.176e-06, 1.6683e-06, 3.9208e-06, 2.0463e-05, 2.7049e-05, 4.6234e-05, 7.203e-05, 7.5899e-05, 8.1477e-05, 0.00010666, 0.0001105, 0.00012111, 0.00014203, 0.00015334, 0.00016604, 0.00017654, 0.00018424, 0.0001873, 0.00019751, 0.00021478, 0.00022686, 0.00024038, 0.00024224, 0.00024506, 0.00026217, 0.0002805, 0.00030851, 0.00033729, 0.00034358, 0.00034739, 0.00035235, 0.00036162, 0.00037728, 0.00038584, 0.00039841, 0.00042321, 0.00043795, 0.00044429, 0.00047173, 0.00048251, 0.00048617, 0.0004948, 0.0005116, 0.00054205, 0.00055612, 0.00056284, 0.00060209, 0.00060863, 0.00061887, 0.00062942, 0.00065796, 0.00066655, 0.00073326, 0.00077735, 0.00082002, 0.00083625, 0.00084655, 0.00088241, 0.00089814, 0.00091592, 0.0010117, 0.0010307, 0.001042, 0.001052, 0.0010978, 0.0011341, 0.0011652, 0.0012311, 0.0012573, 0.0013018, 0.0013211, 0.0013651, 0.001453, 0.0014617, 0.0015063, 0.0015284, 0.0015901, 0.0016684, 0.0017206, 0.0017845, 0.0018195, 0.0018732, 0.0018865, 0.0020579, 0.0021144, 0.0021571, 0.0021904, 0.0022274, 0.0023923, 0.0025657, 0.0026394, 0.002694, 0.0027347, 0.0027754, 0.0028673, 0.002938, 0.0030432, 0.003111, 0.0032407, 0.003911, 0.0039759, 0.0041493, 0.0044118, 0.0047192, 0.0051908, 0.0070439, 0.011136]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.526\n",
      "    win = [-0.018396, -0.0083938, -0.0031621, -0.0028001, -0.0019118, -0.0013917, -0.0012881, -0.00098669, -0.00089119, -0.00064104, -0.00060451, -0.00048943, -0.00041401, -0.00034177, -0.00025251, -0.00024118, -0.00021008, -0.00019283, -0.00018101, -0.00017323, -0.00015715, -0.00014227, -0.00011746, -0.00011135, -0.00010096, -9.2163e-05, -8.056e-05, -6.9199e-05, -6.3244e-05, -5.3844e-05, -4.8654e-05, -4.7321e-05, -4.4098e-05, -3.9908e-05, -3.7069e-05, -3.568e-05, -3.4676e-05, -3.0734e-05, -2.9474e-05, -2.82e-05, -2.5387e-05, -2.1294e-05, -1.9477e-05, -1.6794e-05, -1.5003e-05, -1.4149e-05, -1.3601e-05, -1.2817e-05, -1.2065e-05, -1.1533e-05, -1.0239e-05, -9.594e-06, -8.5351e-06, -7.9391e-06, -7.3057e-06, -6.463e-06, -5.7013e-06, -5.2817e-06, -4.8039e-06, -3.9378e-06, -3.5908e-06, -3.3745e-06, -2.9666e-06, -2.7882e-06, -2.5763e-06, -2.4093e-06, -1.9837e-06, -1.9506e-06, -1.6551e-06, -1.5008e-06, -1.3756e-06, -1.2776e-06, -1.169e-06, -9.6492e-07, -9.3351e-07, -8.5801e-07, -8.3023e-07, -6.8604e-07, -5.8244e-07, -5.6275e-07, -5.1484e-07, -4.4924e-07, -4.23e-07, -3.5988e-07, -3.1436e-07, -2.9705e-07, -2.8083e-07, -2.1817e-07, -1.8726e-07, -1.6576e-07, -1.4496e-07, -1.287e-07, -1.1849e-07, -9.7543e-08, -8.7239e-08, -7.5553e-08, -6.4189e-08, -5.5263e-08, -4.8674e-08, -4.0448e-08, -2.7284e-08, -1.9327e-08, -1.4575e-08, -1.2727e-08, -1.1844e-08, -1.3438e-09, 4.0173e-09, 1.2966e-08, 1.7635e-08, 2.3448e-08, 3.1293e-08, 7.3397e-08, 9.5321e-08, 1.014e-07, 1.4963e-07, 1.5981e-07, 2.2231e-07, 2.9581e-07, 3.1779e-07, 4.7504e-07, 5.559e-07, 6.3861e-07, 6.9976e-07, 9.1368e-07, 9.4215e-07, 1.1136e-06, 1.3743e-06, 1.4263e-06, 1.7868e-06, 1.8789e-06, 2.0767e-06, 2.3551e-06, 2.6634e-06, 2.7783e-06, 3.5799e-06, 3.9106e-06, 4.67e-06, 4.7969e-06, 5.238e-06, 6.1975e-06, 6.9022e-06, 7.8884e-06, 8.7691e-06, 1.0866e-05, 1.1513e-05, 1.3186e-05, 1.4965e-05, 1.6231e-05, 1.8253e-05, 1.8624e-05, 2.132e-05, 2.2977e-05, 2.3853e-05, 2.5804e-05, 3.0088e-05, 3.3425e-05, 3.8492e-05, 4.042e-05, 4.1762e-05, 4.4873e-05, 5.1456e-05, 5.4257e-05, 5.7414e-05, 6.3857e-05, 6.5741e-05, 7.7887e-05, 8.6721e-05, 9.7442e-05, 0.0001094, 0.00012177, 0.00012461, 0.00012921, 0.00013605, 0.00016373, 0.00016488, 0.00018909, 0.00021577, 0.00024065, 0.00025876, 0.00027418, 0.00030902, 0.00035388, 0.00037594, 0.00040206, 0.00043881, 0.00049899, 0.00052089, 0.00056157, 0.00067916, 0.00082169, 0.0008749, 0.00091211, 0.0012406, 0.0014376, 0.001656, 0.0018134, 0.002944, 0.0044604, 0.014576, 0.035502]\n",
      "    wout = [-0.0018178, -0.0016979, -0.0015976, -0.0014748, -0.0014209, -0.0013018, -0.0012028, -0.0011357, -0.0011058, -0.0010187, -0.00094847, -0.00093293, -0.000908, -0.00086911, -0.00084627, -0.0008259, -0.00078993, -0.00074006, -0.00071763, -0.00068865, -0.00063499, -0.0006305, -0.00060501, -0.00058352, -0.00056887, -0.00055298, -0.00053595, -0.00053372, -0.00052206, -0.00051225, -0.0005061, -0.00047388, -0.00046699, -0.00045915, -0.00045583, -0.00044084, -0.00042861, -0.00042379, -0.00040768, -0.00038836, -0.00036918, -0.000355, -0.00034867, -0.00034216, -0.00033486, -0.00031342, -0.00030543, -0.00030286, -0.00029039, -0.00028828, -0.00027908, -0.00027305, -0.00026532, -0.00025461, -0.00024832, -0.00024376, -0.00022909, -0.00021944, -0.00021306, -0.00020761, -0.00020355, -0.00019103, -0.00018107, -0.00016894, -0.00016605, -0.00015286, -0.00014304, -0.00013604, -0.0001339, -0.00012475, -0.00011201, -0.00010709, -9.8116e-05, -8.7627e-05, -8.5001e-05, -7.4801e-05, -7.0025e-05, -6.7618e-05, -6.5862e-05, -5.5347e-05, -4.7016e-05, -3.857e-05, -3.6858e-05, -2.8766e-05, -2.3764e-05, -9.5669e-06, -5.568e-06, -2.0095e-06, 5.7927e-07, 5.5628e-06, 9.8067e-06, 1.3894e-05, 2.4453e-05, 3.2011e-05, 3.8417e-05, 4.2797e-05, 5.158e-05, 5.2725e-05, 5.3943e-05, 5.7626e-05, 6.551e-05, 7.2497e-05, 8.1579e-05, 8.6622e-05, 8.931e-05, 9.3803e-05, 9.7734e-05, 0.00010057, 0.00011017, 0.00011564, 0.0001217, 0.00013173, 0.00013609, 0.0001549, 0.00015557, 0.00016034, 0.0001706, 0.00017353, 0.00017943, 0.00018231, 0.00019079, 0.00019593, 0.00020418, 0.00022068, 0.00022426, 0.00023365, 0.00024413, 0.0002515, 0.00026122, 0.00028088, 0.00029238, 0.00029896, 0.00030715, 0.00031407, 0.00033322, 0.00034, 0.00034209, 0.00036181, 0.00037034, 0.00039197, 0.00040326, 0.00041628, 0.00043626, 0.00045079, 0.00045319, 0.00045947, 0.00046508, 0.00047082, 0.00048084, 0.00049444, 0.0005071, 0.00052346, 0.00053691, 0.00054188, 0.00055669, 0.00057031, 0.00057049, 0.00058784, 0.00060167, 0.00061924, 0.00065152, 0.00065278, 0.0006717, 0.00069242, 0.00071176, 0.00071342, 0.00072783, 0.00074857, 0.00076253, 0.00077943, 0.00079297, 0.00080374, 0.00083696, 0.00087542, 0.00089643, 0.00093711, 0.00093995, 0.00095522, 0.0009651, 0.0010278, 0.001082, 0.0010926, 0.0011111, 0.0011554, 0.0011873, 0.0012263, 0.0012438, 0.0012926, 0.0013193, 0.0013299, 0.0014557, 0.001534, 0.0015956, 0.0016952, 0.0017467, 0.0019156, 0.0020218, 0.0035813, 0.0042103, 0.0050243]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 1, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.472\n",
      "    win = [-0.01989, -0.007881, -0.0054161, -0.0030887, -0.0026356, -0.0021151, -0.0015854, -0.0014807, -0.0013449, -0.0010285, -0.00093625, -0.00081406, -0.00064211, -0.00059735, -0.00053901, -0.00051627, -0.00046987, -0.00045066, -0.00039829, -0.00038002, -0.0003294, -0.00031954, -0.00028665, -0.00026622, -0.00024313, -0.00024041, -0.00020739, -0.00019205, -0.00017827, -0.00016557, -0.00015665, -0.00014617, -0.00013406, -0.00012426, -0.00012167, -0.00011832, -0.00010778, -9.6489e-05, -8.9579e-05, -8.5648e-05, -8.073e-05, -7.608e-05, -7.3222e-05, -6.9867e-05, -6.3515e-05, -5.5924e-05, -5.0215e-05, -5.0036e-05, -4.875e-05, -4.6013e-05, -4.08e-05, -3.6847e-05, -3.3927e-05, -3.1705e-05, -3.0007e-05, -2.8028e-05, -2.5674e-05, -2.4224e-05, -2.2966e-05, -1.9081e-05, -1.8238e-05, -1.6453e-05, -1.4639e-05, -1.4049e-05, -1.2248e-05, -1.1488e-05, -1.1212e-05, -1.0543e-05, -9.9432e-06, -9.0068e-06, -8.2411e-06, -6.2071e-06, -5.7576e-06, -5.3656e-06, -4.9272e-06, -4.3787e-06, -4.0366e-06, -3.7468e-06, -3.1882e-06, -3.0228e-06, -2.5788e-06, -2.0312e-06, -1.9889e-06, -1.8768e-06, -1.7792e-06, -1.1214e-06, -8.1213e-07, -6.7462e-07, -5.3227e-07, -4.8285e-07, -2.3195e-07, -1.764e-07, 5.1085e-08, 1.9123e-07, 2.3312e-07, 3.3151e-07, 4.5365e-07, 4.8689e-07, 6.2167e-07, 7.316e-07, 1.0223e-06, 1.0414e-06, 1.2041e-06, 1.2495e-06, 1.3894e-06, 1.5136e-06, 1.8525e-06, 2.1874e-06, 2.4921e-06, 2.724e-06, 3.062e-06, 3.2904e-06, 3.5186e-06, 4.4036e-06, 4.7274e-06, 5.6496e-06, 6.0759e-06, 6.7397e-06, 7.2252e-06, 7.6435e-06, 8.1537e-06, 9.0202e-06, 1.0523e-05, 1.1639e-05, 1.2807e-05, 1.3051e-05, 1.4457e-05, 1.5292e-05, 1.5928e-05, 1.676e-05, 1.8507e-05, 2.0611e-05, 2.0945e-05, 2.2403e-05, 2.4687e-05, 2.6516e-05, 3.212e-05, 3.3865e-05, 3.5171e-05, 3.6763e-05, 3.8774e-05, 4.2769e-05, 4.4791e-05, 4.8176e-05, 5.0055e-05, 5.3422e-05, 5.8061e-05, 6.2656e-05, 6.5428e-05, 6.8793e-05, 7.405e-05, 7.905e-05, 8.4182e-05, 9.0707e-05, 9.6948e-05, 0.00010542, 0.00010889, 0.00012023, 0.0001226, 0.00012667, 0.00013895, 0.00015734, 0.00016489, 0.00017357, 0.00019836, 0.00020492, 0.00021717, 0.00023326, 0.00024892, 0.00029189, 0.00031163, 0.00033448, 0.00035176, 0.00037111, 0.00040367, 0.00043278, 0.00045543, 0.00051774, 0.00054629, 0.00064422, 0.00065024, 0.00067841, 0.00077344, 0.00084734, 0.00095115, 0.0010173, 0.0010854, 0.0012144, 0.0013104, 0.0018129, 0.002199, 0.0023745, 0.0032538, 0.0037402, 0.0045818, 0.0048452, 0.0051672, 0.0099453, 0.025215, 0.07574]\n",
      "    wout = [-0.0028505, -0.0026151, -0.0017515, -0.0017073, -0.0015286, -0.0014269, -0.0013706, -0.0012895, -0.001245, -0.0011577, -0.0011111, -0.0010572, -0.0009921, -0.0009437, -0.00088864, -0.00085943, -0.00083151, -0.0008053, -0.00078138, -0.00076953, -0.00075128, -0.0007106, -0.00068495, -0.00067495, -0.00066721, -0.00062864, -0.00061618, -0.00061289, -0.00059992, -0.00058579, -0.00056308, -0.00055728, -0.00053829, -0.00050755, -0.00049017, -0.00045996, -0.00044133, -0.00043599, -0.00042389, -0.00041583, -0.00040752, -0.00035824, -0.0003501, -0.00033775, -0.0003178, -0.000314, -0.00030137, -0.00029681, -0.00029136, -0.00027629, -0.00026487, -0.00025027, -0.00024525, -0.00023765, -0.00023204, -0.00022666, -0.00022236, -0.00021562, -0.00020632, -0.00020149, -0.00019485, -0.00018754, -0.0001788, -0.00017738, -0.00016864, -0.00015234, -0.00015066, -0.00014202, -0.00013305, -0.00012324, -0.00010275, -0.00010054, -9.1753e-05, -8.3689e-05, -8.0666e-05, -7.1451e-05, -6.9849e-05, -6.7744e-05, -6.0874e-05, -5.1885e-05, -4.7075e-05, -3.6764e-05, -3.6185e-05, -2.8254e-05, -2.2262e-05, -1.4937e-05, -5.0392e-06, -2.1412e-06, 6.5932e-06, 1.1159e-05, 1.6666e-05, 1.8728e-05, 3.417e-05, 3.6806e-05, 4.49e-05, 5.3313e-05, 5.8495e-05, 6.4379e-05, 7.3841e-05, 8.1177e-05, 8.2437e-05, 8.8081e-05, 9.6649e-05, 0.00010018, 0.00010978, 0.0001119, 0.00011491, 0.00012749, 0.00012826, 0.00014088, 0.00014607, 0.00015338, 0.00015871, 0.00016297, 0.0001726, 0.00017859, 0.00018735, 0.00019251, 0.00020441, 0.00021022, 0.00021599, 0.00022039, 0.00022601, 0.00023128, 0.0002411, 0.0002465, 0.00025523, 0.00026438, 0.00027666, 0.0002785, 0.00028354, 0.0002895, 0.00029811, 0.00030783, 0.00031553, 0.00033225, 0.0003343, 0.00034335, 0.00035165, 0.00036203, 0.00037251, 0.00037699, 0.00038575, 0.00039365, 0.00040607, 0.00040935, 0.00041573, 0.00042675, 0.00044692, 0.00045137, 0.00045713, 0.00046535, 0.00046915, 0.00047935, 0.00050507, 0.00050975, 0.00052708, 0.00053531, 0.00054852, 0.00055885, 0.00056698, 0.00059636, 0.00061017, 0.00061441, 0.00062073, 0.00064552, 0.00065005, 0.00067345, 0.00068535, 0.00070066, 0.00071765, 0.00072575, 0.00074444, 0.00075723, 0.00076728, 0.0008108, 0.00082995, 0.00084905, 0.0008978, 0.00092819, 0.00095644, 0.00096515, 0.00099613, 0.0010472, 0.0010848, 0.0011133, 0.001135, 0.0011658, 0.0012452, 0.0012825, 0.0013121, 0.0014059, 0.0014372, 0.0014784, 0.0015755, 0.0017639, 0.001991, 0.0022099, 0.0024178, 0.0028263]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 2, Step: 100\n",
      "    cos_sim = -0.560\n",
      "    win = [-0.00087297, -0.00049855, -0.00038229, -0.00014189, -0.00012716, -9.292e-05, -7.0057e-05, -6.4312e-05, -4.7376e-05, -3.7987e-05, -2.897e-05, -2.3868e-05, -2.1397e-05, -1.9318e-05, -1.721e-05, -1.48e-05, -1.4207e-05, -1.1004e-05, -1.0644e-05, -1.0276e-05, -8.485e-06, -8.2351e-06, -6.7854e-06, -6.0139e-06, -5.5053e-06, -4.147e-06, -3.4585e-06, -3.3391e-06, -2.8231e-06, -2.3127e-06, -2.1176e-06, -1.8311e-06, -1.6454e-06, -1.5052e-06, -1.2698e-06, -1.1566e-06, -1.0403e-06, -8.9476e-07, -7.6877e-07, -7.0659e-07, -6.0322e-07, -5.6022e-07, -5.0202e-07, -4.4665e-07, -3.7638e-07, -3.3859e-07, -3.0488e-07, -2.3637e-07, -2.2413e-07, -1.8983e-07, -1.6936e-07, -1.4264e-07, -1.1169e-07, -9.9856e-08, -8.7502e-08, -7.2491e-08, -5.6264e-08, -4.9638e-08, -4.6965e-08, -4.0095e-08, -3.8095e-08, -3.1351e-08, -2.7355e-08, -2.2998e-08, -1.9138e-08, -1.3759e-08, -1.226e-08, -7.8728e-09, -7.6503e-09, -5.3858e-09, -4.131e-09, -3.6025e-09, -3.0858e-09, -1.935e-09, -1.4127e-09, -1.1415e-09, -5.8554e-10, -5.1994e-10, -2.8164e-10, -4.1849e-11, 2.0342e-10, 4.5172e-10, 5.2227e-10, 9.0319e-10, 1.0997e-09, 1.5278e-09, 2.1193e-09, 2.5201e-09, 3.4622e-09, 3.8672e-09, 4.2405e-09, 5.1821e-09, 6.3713e-09, 7.173e-09, 8.659e-09, 1.1501e-08, 1.2219e-08, 1.5077e-08, 1.6812e-08, 2.0192e-08, 2.2975e-08, 2.4047e-08, 2.7511e-08, 3.0031e-08, 3.3847e-08, 3.6955e-08, 3.9775e-08, 4.4144e-08, 4.5001e-08, 5.1648e-08, 6.0141e-08, 6.8513e-08, 7.3314e-08, 8.6544e-08, 9.6447e-08, 1.0658e-07, 1.0991e-07, 1.1874e-07, 1.2306e-07, 1.3597e-07, 1.5559e-07, 1.7143e-07, 1.8052e-07, 1.9337e-07, 2.2473e-07, 2.4001e-07, 2.5138e-07, 3.0707e-07, 3.1871e-07, 3.4791e-07, 3.7477e-07, 4.01e-07, 4.2383e-07, 4.6826e-07, 4.9391e-07, 4.9726e-07, 5.2896e-07, 5.8393e-07, 6.2605e-07, 6.84e-07, 7.012e-07, 8.3665e-07, 8.6547e-07, 9.6114e-07, 1.0992e-06, 1.1151e-06, 1.2662e-06, 1.3998e-06, 1.5236e-06, 1.7579e-06, 1.8559e-06, 1.9374e-06, 2.0076e-06, 2.1408e-06, 2.2945e-06, 2.5129e-06, 2.8441e-06, 3.0606e-06, 3.3575e-06, 3.8871e-06, 4.0798e-06, 4.7819e-06, 5.7252e-06, 6.5239e-06, 6.5995e-06, 7.097e-06, 8.6283e-06, 9.0283e-06, 9.633e-06, 1.0478e-05, 1.2024e-05, 1.3285e-05, 1.3481e-05, 1.4934e-05, 1.798e-05, 2.1056e-05, 2.2693e-05, 2.6147e-05, 2.9711e-05, 3.465e-05, 4.2869e-05, 5.4859e-05, 6.0204e-05, 6.5813e-05, 7.5669e-05, 8.6616e-05, 0.00010032, 0.00010667, 0.00014683, 0.00022685, 0.00026284, 0.00033082, 0.00053409, 0.00067799, 0.00095698, 0.0010641, 0.0032585, 0.0035998, 0.018719, 0.15094]\n",
      "    wout = [-0.012043, -0.011314, -0.010518, -0.0097962, -0.0094853, -0.0077539, -0.0071904, -0.0059014, -0.0054847, -0.0051479, -0.004905, -0.0047606, -0.0045956, -0.0044308, -0.004339, -0.004272, -0.0041018, -0.0036784, -0.0034462, -0.00339, -0.0032873, -0.0032561, -0.0032056, -0.0031478, -0.0030286, -0.0029773, -0.0029417, -0.002858, -0.0027858, -0.0027515, -0.0026649, -0.0026118, -0.0024176, -0.0023001, -0.0022581, -0.0021467, -0.002114, -0.0020286, -0.0019644, -0.0019497, -0.001829, -0.0017989, -0.0017264, -0.0016815, -0.0015767, -0.0014195, -0.001322, -0.0012002, -0.0011754, -0.0011417, -0.0010975, -0.0010066, -0.00098366, -0.00093651, -0.00092597, -0.00086376, -0.00084002, -0.00083192, -0.00076508, -0.00075344, -0.00072907, -0.00064103, -0.00063572, -0.0006305, -0.00061187, -0.00059661, -0.00055302, -0.00053658, -0.0005279, -0.00051564, -0.00049412, -0.00047567, -0.00043987, -0.00041838, -0.0004082, -0.00039068, -0.00038823, -0.0003678, -0.00032314, -0.00029845, -0.00029483, -0.00028355, -0.00027589, -0.00027079, -0.00026138, -0.00025721, -0.00024533, -0.00024209, -0.00022392, -0.00021701, -0.00019903, -0.00017946, -0.00017074, -0.00015966, -0.00014908, -0.00012158, -0.00011826, -0.00010615, -9.0102e-05, -8.2699e-05, -7.9548e-05, -7.3615e-05, -6.638e-05, -4.5844e-05, -2.9637e-05, -2.5637e-05, -1.884e-05, 1.0282e-05, 1.9581e-05, 3.7398e-05, 5.0074e-05, 5.6138e-05, 6.7808e-05, 0.000101, 0.00011491, 0.00012279, 0.00012536, 0.0001318, 0.00014425, 0.00015599, 0.00015743, 0.00017307, 0.00020874, 0.00021574, 0.0002288, 0.00023217, 0.00025094, 0.00026109, 0.00027512, 0.00028378, 0.00029324, 0.00035393, 0.00036389, 0.00036625, 0.0005006, 0.00056797, 0.00062939, 0.00068387, 0.00071065, 0.00075131, 0.00077642, 0.00094892, 0.0010974, 0.0011806, 0.0012224, 0.0012761, 0.0014537, 0.0015831, 0.0018312, 0.0019056, 0.0019311, 0.0019836, 0.0020062, 0.0020825, 0.0022582, 0.0022807, 0.0023169, 0.0023589, 0.0025996, 0.0026477, 0.0026612, 0.0027272, 0.0027493, 0.0028759, 0.0029288, 0.0030571, 0.0031389, 0.0032973, 0.0038346, 0.0040124, 0.0041736, 0.0043643, 0.0044443, 0.0045252, 0.0046253, 0.0047578, 0.0048983, 0.005041, 0.0053384, 0.0054897, 0.0057325, 0.0058536, 0.0060414, 0.0065772, 0.0066671, 0.0067058, 0.0073831, 0.0077719, 0.0079621, 0.0082603, 0.0095261, 0.010195, 0.010421, 0.011264, 0.011889, 0.014721, 0.015219, 0.016795, 0.019597, 0.029277]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 4, Step: 200\n",
      "    cos_sim = -0.515\n",
      "    win = [-0.066022, -0.019775, -0.0034001, -0.0016329, -0.00067465, -5.5287e-05, -1.0543e-05, -8.8819e-06, -7.428e-07, -4.2263e-07, -1.4319e-07, -7.6365e-08, -3.1934e-08, -9.5174e-09, -7.8789e-09, -3.2363e-09, -1.2226e-09, -1.128e-09, -1.0209e-09, -7.2702e-10, -7.1324e-10, -6.7678e-10, -6.6942e-10, -6.2329e-10, -6.0373e-10, -5.7981e-10, -5.764e-10, -5.6785e-10, -5.4952e-10, -5.3614e-10, -5.2795e-10, -5.1655e-10, -5.0241e-10, -4.8537e-10, -4.7385e-10, -4.6766e-10, -4.6523e-10, -4.4137e-10, -4.3591e-10, -4.3093e-10, -4.1922e-10, -4.0534e-10, -3.921e-10, -3.8838e-10, -3.7909e-10, -3.7682e-10, -3.6021e-10, -3.5183e-10, -3.5055e-10, -3.4299e-10, -3.3655e-10, -3.2489e-10, -3.1682e-10, -2.9903e-10, -2.9512e-10, -2.8238e-10, -2.8048e-10, -2.6915e-10, -2.6836e-10, -2.6145e-10, -2.5535e-10, -2.4606e-10, -2.3324e-10, -2.3279e-10, -2.1414e-10, -2.066e-10, -1.9582e-10, -1.9255e-10, -1.8965e-10, -1.8143e-10, -1.7265e-10, -1.6378e-10, -1.6135e-10, -1.522e-10, -1.4825e-10, -1.4162e-10, -1.3295e-10, -1.3122e-10, -1.1918e-10, -1.1697e-10, -1.0697e-10, -1.0197e-10, -9.9894e-11, -8.8082e-11, -7.8961e-11, -7.2815e-11, -6.8323e-11, -5.6835e-11, -5.5097e-11, -5.188e-11, -4.5365e-11, -4.24e-11, -3.7793e-11, -2.6398e-11, -2.2419e-11, -1.8764e-11, -1.8465e-11, -9.2422e-12, -4.3972e-12, -1.5979e-12, 6.9516e-12, 1.6773e-11, 1.9641e-11, 2.8103e-11, 3.3151e-11, 4.2451e-11, 4.513e-11, 5.2097e-11, 5.5721e-11, 5.8773e-11, 6.292e-11, 7.7516e-11, 8.7364e-11, 8.948e-11, 9.7633e-11, 1.037e-10, 1.0924e-10, 1.1494e-10, 1.1874e-10, 1.2416e-10, 1.3128e-10, 1.4023e-10, 1.4643e-10, 1.4848e-10, 1.5352e-10, 1.5771e-10, 1.6188e-10, 1.7512e-10, 1.8334e-10, 1.9054e-10, 1.965e-10, 2.0489e-10, 2.1463e-10, 2.2144e-10, 2.2626e-10, 2.3634e-10, 2.384e-10, 2.4336e-10, 2.6107e-10, 2.6647e-10, 2.735e-10, 2.7714e-10, 2.8356e-10, 2.9764e-10, 3.026e-10, 3.1488e-10, 3.1846e-10, 3.3199e-10, 3.3381e-10, 3.4263e-10, 3.4618e-10, 3.5395e-10, 3.7865e-10, 3.8768e-10, 3.9791e-10, 4.0705e-10, 4.1498e-10, 4.17e-10, 4.2769e-10, 4.4406e-10, 4.471e-10, 4.5312e-10, 4.7166e-10, 4.9025e-10, 5.0264e-10, 5.1186e-10, 5.31e-10, 5.4959e-10, 5.5904e-10, 5.6575e-10, 5.746e-10, 5.9194e-10, 6.4085e-10, 6.4578e-10, 6.7986e-10, 6.8973e-10, 7.1147e-10, 7.2772e-10, 7.6543e-10, 8.8739e-10, 9.9021e-10, 1.3904e-09, 2.0255e-09, 4.9382e-09, 6.6745e-09, 1.011e-08, 2.5239e-08, 4.5516e-08, 2.3765e-07, 6.3452e-07, 1.8913e-06, 5.2137e-06, 2.501e-05, 8.2362e-05, 0.00013047, 0.00032036, 0.00052589, 0.0018367, 0.012104, 0.019531]\n",
      "    wout = [-0.011554, -0.0095876, -0.0085625, -0.0065136, -0.0045771, -0.0045032, -0.0043819, -0.0041547, -0.0041329, -0.0038615, -0.0037529, -0.0036812, -0.0035465, -0.0034138, -0.0031937, -0.0030973, -0.0030347, -0.0029758, -0.0028882, -0.0026816, -0.0026407, -0.0024934, -0.0024493, -0.0024146, -0.0023441, -0.0022698, -0.0022149, -0.0021564, -0.0019453, -0.0018943, -0.0017037, -0.0016501, -0.0016092, -0.0015441, -0.001507, -0.0015001, -0.0014721, -0.0014449, -0.0014339, -0.0014027, -0.0013797, -0.00134, -0.0013086, -0.0012645, -0.0012416, -0.0011575, -0.00114, -0.0011297, -0.0011123, -0.0010983, -0.0010333, -0.00095429, -0.00091956, -0.00088956, -0.00088609, -0.00085035, -0.00084952, -0.00082715, -0.00078645, -0.00075426, -0.00075301, -0.00073711, -0.00072552, -0.00071951, -0.00069231, -0.00066538, -0.00063534, -0.00061499, -0.00059432, -0.00058818, -0.00056249, -0.00052909, -0.00050195, -0.00049362, -0.00048923, -0.00048452, -0.00046397, -0.0004254, -0.00041735, -0.00041283, -0.00039337, -0.00037006, -0.000325, -0.00028578, -0.00027919, -0.00027021, -0.00025474, -0.00022108, -0.00021459, -0.00020617, -0.00017275, -0.00016039, -0.0001507, -0.00014939, -0.00014105, -0.00013322, -0.00012895, -0.00012567, -0.00011958, -0.00011888, -0.00011708, -0.00010213, -0.00010085, -9.5222e-05, -8.6242e-05, -7.6343e-05, -6.9147e-05, -6.5787e-05, -6.0221e-05, -5.0969e-05, -4.5256e-05, -3.0992e-05, -2.7141e-05, -2.0357e-05, -1.7757e-05, -8.6146e-06, 1.1141e-05, 1.3683e-05, 1.6378e-05, 1.775e-05, 2.4047e-05, 5.3465e-05, 6.0816e-05, 7.9767e-05, 8.5927e-05, 9.2043e-05, 0.00011482, 0.00011607, 0.000126, 0.00014939, 0.00015129, 0.00016894, 0.00019473, 0.00019544, 0.00019868, 0.00021973, 0.00022554, 0.00030201, 0.00031412, 0.00032365, 0.00034647, 0.00035784, 0.00042359, 0.00046795, 0.0004794, 0.000511, 0.00052281, 0.00055822, 0.00057597, 0.00060963, 0.00062801, 0.00072496, 0.00075605, 0.00079235, 0.000826, 0.00084359, 0.00087959, 0.00090225, 0.00092367, 0.00098079, 0.0010046, 0.0010554, 0.0011287, 0.00117, 0.0012285, 0.0013081, 0.0013495, 0.0013605, 0.0013786, 0.001445, 0.0014616, 0.0015268, 0.0015851, 0.0016233, 0.0016968, 0.0017327, 0.0017868, 0.0018525, 0.0019187, 0.0020265, 0.002044, 0.0020756, 0.0025282, 0.002551, 0.0027365, 0.0027893, 0.0031416, 0.0032435, 0.0034305, 0.0035185, 0.0035471, 0.0037989, 0.0044592, 0.004822, 0.0053324, 0.0053975, 0.006214, 0.00723, 0.0078771, 0.0090422]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 7, Step: 300\n",
      "    cos_sim = -0.496\n",
      "    win = [-0.013843, -0.0030741, -0.0024297, -0.001486, -0.0011243, -0.0010794, -0.00085053, -0.00084311, -0.0005573, -0.00042731, -0.00032638, -0.00029505, -0.00026599, -0.00023725, -0.00020656, -0.0001763, -0.0001531, -0.00014473, -0.00013662, -0.00012191, -0.00010758, -0.00010021, -8.7931e-05, -8.2917e-05, -7.7973e-05, -7.2197e-05, -7.0413e-05, -5.8803e-05, -5.7062e-05, -5.5282e-05, -4.6197e-05, -4.476e-05, -4.2971e-05, -4.1239e-05, -3.8054e-05, -3.4515e-05, -3.397e-05, -2.908e-05, -2.7086e-05, -2.5581e-05, -2.3789e-05, -2.1156e-05, -1.9932e-05, -1.9347e-05, -1.7172e-05, -1.6586e-05, -1.5344e-05, -1.483e-05, -1.4165e-05, -1.3266e-05, -1.2495e-05, -1.1498e-05, -1.116e-05, -1.0744e-05, -1.0138e-05, -8.7651e-06, -8.3057e-06, -7.8995e-06, -7.6309e-06, -7.0737e-06, -6.2494e-06, -6.0665e-06, -5.5978e-06, -5.083e-06, -4.6924e-06, -4.3796e-06, -4.0918e-06, -3.9399e-06, -3.7039e-06, -3.5431e-06, -3.1687e-06, -3.077e-06, -2.6402e-06, -2.5887e-06, -2.3849e-06, -2.2899e-06, -1.8849e-06, -1.8753e-06, -1.7611e-06, -1.6781e-06, -1.4687e-06, -1.3469e-06, -1.2898e-06, -1.1243e-06, -1.0285e-06, -9.7894e-07, -8.6538e-07, -8.2485e-07, -7.6318e-07, -7.1778e-07, -6.1777e-07, -5.5107e-07, -4.6557e-07, -4.3278e-07, -3.9678e-07, -3.5526e-07, -3.3161e-07, -2.83e-07, -2.6374e-07, -1.9044e-07, -1.7854e-07, -1.364e-07, -1.3372e-07, -1.0484e-07, -8.5023e-08, -6.2984e-08, -5.6356e-08, -4.9742e-08, -3.9147e-08, -2.4367e-08, -5.9377e-09, 8.311e-09, 2.1407e-08, 4.9479e-08, 6.0631e-08, 6.9396e-08, 1.172e-07, 1.2194e-07, 1.788e-07, 1.8394e-07, 2.4172e-07, 2.6763e-07, 2.9657e-07, 3.6133e-07, 3.7496e-07, 4.1851e-07, 4.6373e-07, 5.28e-07, 5.6739e-07, 5.7898e-07, 8.0164e-07, 8.6843e-07, 9.3127e-07, 9.7626e-07, 1.0018e-06, 1.1736e-06, 1.4152e-06, 1.4365e-06, 1.6226e-06, 1.9382e-06, 2.1277e-06, 2.3858e-06, 2.5628e-06, 3.1321e-06, 3.4641e-06, 3.5464e-06, 3.997e-06, 4.1999e-06, 4.6676e-06, 4.8758e-06, 5.5698e-06, 6.2649e-06, 6.5832e-06, 7.1201e-06, 7.8703e-06, 8.8877e-06, 9.8209e-06, 1.0138e-05, 1.1113e-05, 1.2179e-05, 1.3443e-05, 1.4301e-05, 1.525e-05, 1.5925e-05, 1.763e-05, 1.7971e-05, 1.9935e-05, 2.2965e-05, 2.3579e-05, 2.5778e-05, 2.5965e-05, 3.118e-05, 3.3194e-05, 3.9525e-05, 4.2036e-05, 4.9907e-05, 5.2906e-05, 6.1161e-05, 6.742e-05, 7.7696e-05, 8.2961e-05, 9.3462e-05, 0.00010584, 0.00011462, 0.00012524, 0.00014013, 0.00016735, 0.00018744, 0.00024398, 0.00025709, 0.0002867, 0.00036603, 0.00044216, 0.00054527, 0.00067708, 0.001804, 0.0022882, 0.0025429, 0.0070404, 0.021664]\n",
      "    wout = [-0.002718, -0.0018894, -0.0018167, -0.0013752, -0.0012221, -0.00089804, -0.00086947, -0.00083638, -0.00082294, -0.00079294, -0.00077561, -0.00074557, -0.00072165, -0.00068461, -0.00066784, -0.00063528, -0.00062609, -0.00060638, -0.00058836, -0.00058409, -0.0005314, -0.00052397, -0.0005075, -0.00050534, -0.00050001, -0.00049443, -0.0004834, -0.00047708, -0.00046428, -0.00045635, -0.00045488, -0.00044782, -0.00043716, -0.00042963, -0.00042317, -0.0004197, -0.00040467, -0.00039754, -0.00039508, -0.00038354, -0.00037753, -0.00037143, -0.00035479, -0.00034273, -0.00033416, -0.0003266, -0.00032214, -0.00029897, -0.00027571, -0.00026595, -0.00025871, -0.00024577, -0.0002416, -0.00023609, -0.00022777, -0.00022709, -0.00022036, -0.00021531, -0.00020877, -0.00019877, -0.00019174, -0.00019001, -0.00018323, -0.00017632, -0.00017245, -0.00016747, -0.00015626, -0.00014702, -0.00013969, -0.00013854, -0.0001284, -0.00012417, -0.00011825, -0.00011589, -0.0001079, -9.6025e-05, -9.0455e-05, -8.4663e-05, -8.1505e-05, -7.8546e-05, -7.1699e-05, -6.2695e-05, -6.2368e-05, -5.737e-05, -5.5918e-05, -5.1297e-05, -4.6562e-05, -3.1567e-05, -2.9107e-05, -2.2452e-05, -2.0546e-05, -1.3841e-05, -9.2748e-06, -7.3341e-06, -3.2635e-06, -5.0633e-07, 3.9012e-06, 5.3978e-06, 1.2137e-05, 1.6406e-05, 2.1115e-05, 2.6722e-05, 3.0101e-05, 3.4447e-05, 4.2025e-05, 5.002e-05, 5.2319e-05, 5.5737e-05, 6.1009e-05, 7.0147e-05, 7.7416e-05, 0.000101, 0.00010503, 0.00010763, 0.00011475, 0.00012746, 0.00013262, 0.00013406, 0.00013601, 0.00014262, 0.00014849, 0.00015453, 0.00015633, 0.00016355, 0.00017043, 0.00017134, 0.00018006, 0.00018209, 0.00019462, 0.00020583, 0.00020675, 0.00021125, 0.00021681, 0.00023257, 0.00023572, 0.00024606, 0.00026212, 0.0002653, 0.00027559, 0.00027862, 0.00028975, 0.00030534, 0.00031835, 0.00032476, 0.00033015, 0.0003382, 0.00034709, 0.0003551, 0.00035749, 0.00036693, 0.00037911, 0.00039031, 0.00039263, 0.00040612, 0.00041408, 0.00042422, 0.00042798, 0.00044034, 0.00044843, 0.00046183, 0.00047798, 0.00049505, 0.00050402, 0.00052191, 0.00053005, 0.00053582, 0.00055578, 0.00057129, 0.00058778, 0.00060251, 0.00062797, 0.00065242, 0.00066969, 0.00070045, 0.00074496, 0.00076372, 0.00076772, 0.00078526, 0.00079312, 0.00085465, 0.00086492, 0.00087683, 0.00090236, 0.00093283, 0.00099252, 0.001059, 0.0011206, 0.001162, 0.0012965, 0.0013618, 0.0014161, 0.0014927, 0.0016529, 0.0016839, 0.0017194, 0.001863, 0.0019878, 0.0021429, 0.0023951, 0.0040237]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 9, Step: 400\n",
      "    cos_sim = -0.441\n",
      "    win = [-0.029472, -0.0075558, -0.0035666, -0.0021755, -0.00081373, -0.00051583, -0.0003576, -0.00025325, -8.4224e-05, -4.0195e-05, -2.995e-05, -1.9376e-05, -1.5886e-05, -2.4252e-06, -7.8898e-07, -5.4499e-08, -4.4826e-08, -3.7926e-08, -1.5354e-08, -9.0897e-09, -3.9266e-09, -1.721e-09, -1.4117e-09, -1.3144e-09, -1.2756e-09, -1.2471e-09, -1.2399e-09, -1.2048e-09, -1.1501e-09, -1.0905e-09, -1.0773e-09, -1.059e-09, -1.0166e-09, -1.0057e-09, -9.8455e-10, -9.6588e-10, -9.4139e-10, -9.2763e-10, -9.0142e-10, -8.7551e-10, -8.531e-10, -8.4125e-10, -8.2353e-10, -8.0746e-10, -7.8714e-10, -7.7066e-10, -7.5719e-10, -7.4069e-10, -7.0617e-10, -6.9559e-10, -6.867e-10, -6.6948e-10, -6.5941e-10, -6.3758e-10, -6.1956e-10, -6.0207e-10, -5.8906e-10, -5.547e-10, -5.4544e-10, -5.3815e-10, -5.2485e-10, -5.1378e-10, -5.0559e-10, -4.88e-10, -4.677e-10, -4.5649e-10, -4.4508e-10, -4.2638e-10, -4.0943e-10, -4.0239e-10, -3.7595e-10, -3.6572e-10, -3.568e-10, -3.4456e-10, -3.3609e-10, -3.2041e-10, -3.1348e-10, -3.0544e-10, -2.9708e-10, -2.8161e-10, -2.6461e-10, -2.5638e-10, -2.5113e-10, -2.3223e-10, -2.2739e-10, -2.2092e-10, -2.1065e-10, -1.8595e-10, -1.681e-10, -1.5969e-10, -1.4771e-10, -1.3766e-10, -1.2653e-10, -1.1249e-10, -9.4675e-11, -8.104e-11, -7.1623e-11, -6.4595e-11, -5.47e-11, -5.104e-11, -3.471e-11, -2.0191e-11, -9.5448e-12, -3.7635e-12, 3.945e-12, 1.3611e-11, 1.9818e-11, 3.6436e-11, 6.0694e-11, 8.0984e-11, 8.4787e-11, 8.7752e-11, 1.0764e-10, 1.2033e-10, 1.3379e-10, 1.3821e-10, 1.5259e-10, 1.6054e-10, 1.727e-10, 1.7963e-10, 1.9788e-10, 2.2547e-10, 2.2622e-10, 2.3718e-10, 2.4711e-10, 2.6183e-10, 2.817e-10, 2.9226e-10, 2.9958e-10, 3.1836e-10, 3.2875e-10, 3.496e-10, 3.5339e-10, 3.5995e-10, 3.7888e-10, 3.8585e-10, 4.0605e-10, 4.2962e-10, 4.4256e-10, 4.5581e-10, 4.6511e-10, 4.8484e-10, 4.8659e-10, 5.0085e-10, 5.132e-10, 5.2748e-10, 5.3935e-10, 5.6224e-10, 5.6917e-10, 5.7763e-10, 5.9888e-10, 6.1117e-10, 6.3133e-10, 6.4225e-10, 6.519e-10, 6.5952e-10, 6.8262e-10, 7.0545e-10, 7.2169e-10, 7.4777e-10, 7.5837e-10, 7.7894e-10, 7.9032e-10, 8.1186e-10, 8.3189e-10, 8.4794e-10, 8.6358e-10, 8.6601e-10, 8.8932e-10, 9.2302e-10, 9.373e-10, 9.8242e-10, 9.9747e-10, 1.0124e-09, 1.0482e-09, 1.0781e-09, 1.1133e-09, 1.1263e-09, 1.1812e-09, 1.209e-09, 1.234e-09, 1.2352e-09, 1.3056e-09, 1.3421e-09, 1.4023e-09, 4.9884e-09, 6.7461e-09, 1.08e-08, 1.8271e-08, 4.5268e-08, 1.0649e-07, 3.6694e-07, 6.4138e-07, 3.0384e-06, 0.00088878, 0.0011889, 0.0032677, 0.0059662, 0.01706, 0.083004]\n",
      "    wout = [-0.0064028, -0.0062824, -0.0059215, -0.0049952, -0.0046989, -0.0041453, -0.0035997, -0.0031266, -0.0028525, -0.0026712, -0.0025338, -0.0023165, -0.0022684, -0.0020955, -0.0018874, -0.0018075, -0.0017756, -0.0017245, -0.0016892, -0.0015634, -0.0015538, -0.0014849, -0.0014678, -0.0014372, -0.0014038, -0.001322, -0.0013024, -0.0012378, -0.0012171, -0.00118, -0.0011666, -0.0010955, -0.001069, -0.0010636, -0.001041, -0.0010365, -0.0010226, -0.00097957, -0.00095144, -0.00094735, -0.00090371, -0.00088307, -0.00084992, -0.00083337, -0.00081321, -0.00076304, -0.00075665, -0.00075074, -0.00072403, -0.00067778, -0.00064111, -0.00062454, -0.00058412, -0.00057422, -0.00056145, -0.0005387, -0.00051562, -0.00048333, -0.00046277, -0.00045582, -0.0004471, -0.00044233, -0.00043888, -0.00042889, -0.0004191, -0.00040671, -0.00037862, -0.00037448, -0.00034532, -0.00033202, -0.00032089, -0.00031634, -0.0003137, -0.0003057, -0.00029905, -0.00029133, -0.00028301, -0.00028127, -0.00025959, -0.00025682, -0.00022953, -0.00020982, -0.00017599, -0.00015657, -0.00015167, -0.00014517, -0.0001164, -8.8717e-05, -6.4124e-05, -5.7525e-05, -4.7614e-05, -4.0897e-05, -3.4152e-05, -2.3271e-05, -2.1591e-05, 3.6373e-06, 1.4227e-05, 2.2324e-05, 3.5422e-05, 5.0344e-05, 5.9681e-05, 6.5838e-05, 8.6824e-05, 0.00010995, 0.00011363, 0.00011622, 0.00012581, 0.00015436, 0.00016754, 0.0001684, 0.00018198, 0.0001895, 0.00022611, 0.00023721, 0.00026016, 0.0002805, 0.00029468, 0.00030142, 0.00034345, 0.00035889, 0.00036333, 0.00037726, 0.00038987, 0.00041389, 0.00043335, 0.00046588, 0.00047634, 0.00050005, 0.00052389, 0.00053146, 0.00055703, 0.00057143, 0.00058595, 0.0006021, 0.00061967, 0.00065129, 0.00067202, 0.00068379, 0.00071172, 0.00071822, 0.0007242, 0.00073963, 0.00074768, 0.00077962, 0.00079621, 0.00080435, 0.00082568, 0.00082921, 0.0008528, 0.00088322, 0.00089614, 0.00094722, 0.00098355, 0.0010435, 0.0010684, 0.0011175, 0.0011736, 0.001219, 0.0012239, 0.0013057, 0.0013274, 0.0013288, 0.0013963, 0.0014297, 0.0014443, 0.0014708, 0.0014873, 0.0015192, 0.0016022, 0.0016088, 0.0016573, 0.0016792, 0.0017163, 0.0017461, 0.0017664, 0.001805, 0.0018594, 0.0019485, 0.002004, 0.0020433, 0.00209, 0.0022053, 0.0022262, 0.0022586, 0.0023402, 0.0024378, 0.002512, 0.0027392, 0.0028281, 0.0028644, 0.0031994, 0.0036006, 0.0041358, 0.0043245, 0.0046757, 0.0060611, 0.0061855, 0.0068257, 0.0085639, 0.012449]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 12, Step: 500\n",
      "    cos_sim = -0.331\n",
      "    win = [-0.014055, -0.0056507, -0.0038313, -0.0026224, -0.001664, -0.0015072, -0.001165, -0.00093633, -0.00082192, -0.00065183, -0.00054672, -0.00045077, -0.00037903, -0.00033586, -0.00029168, -0.00027258, -0.00025368, -0.0002187, -0.0002047, -0.00018284, -0.00015605, -0.0001536, -0.00013659, -0.00013026, -0.00012378, -0.00011493, -0.00011291, -0.00010133, -8.9858e-05, -7.855e-05, -7.5738e-05, -7.1992e-05, -6.5274e-05, -5.9963e-05, -5.3657e-05, -5.3125e-05, -5.1769e-05, -4.73e-05, -4.0502e-05, -3.8848e-05, -3.5822e-05, -3.2575e-05, -3.1463e-05, -3.0479e-05, -2.7322e-05, -2.6564e-05, -2.4883e-05, -2.0693e-05, -1.9662e-05, -1.8556e-05, -1.783e-05, -1.7374e-05, -1.6098e-05, -1.4021e-05, -1.2557e-05, -1.122e-05, -1.0779e-05, -1.0042e-05, -9.2503e-06, -8.351e-06, -7.4478e-06, -7.2677e-06, -6.6338e-06, -6.4192e-06, -5.7366e-06, -5.3668e-06, -4.8561e-06, -4.5671e-06, -3.9942e-06, -3.7833e-06, -3.5445e-06, -3.3635e-06, -2.7758e-06, -2.6863e-06, -2.2921e-06, -2.0128e-06, -1.8945e-06, -1.5361e-06, -1.4462e-06, -1.2692e-06, -1.2517e-06, -1.146e-06, -1.0346e-06, -7.5187e-07, -6.7458e-07, -6.2609e-07, -4.99e-07, -4.8516e-07, -3.8024e-07, -2.7847e-07, -2.5599e-07, -2.0555e-07, -1.0012e-07, -4.243e-08, -1.2949e-08, -4.1232e-09, 5.9937e-08, 8.6299e-08, 1.6551e-07, 1.8093e-07, 2.8946e-07, 3.1588e-07, 4.1822e-07, 5.3395e-07, 5.7256e-07, 6.2801e-07, 7.3112e-07, 7.8767e-07, 8.8932e-07, 1.0453e-06, 1.0893e-06, 1.1479e-06, 1.3402e-06, 1.5475e-06, 1.6921e-06, 1.8072e-06, 2.1687e-06, 2.3026e-06, 2.3717e-06, 2.5706e-06, 3.0077e-06, 3.0564e-06, 3.1713e-06, 3.6381e-06, 3.9285e-06, 4.1687e-06, 4.6539e-06, 4.7474e-06, 5.4532e-06, 5.504e-06, 5.7993e-06, 6.0055e-06, 6.8012e-06, 6.9805e-06, 7.3879e-06, 8.3391e-06, 9.0076e-06, 1.0134e-05, 1.0386e-05, 1.124e-05, 1.2551e-05, 1.2794e-05, 1.3207e-05, 1.4315e-05, 1.5938e-05, 1.7816e-05, 1.8849e-05, 1.9414e-05, 2.0547e-05, 2.2326e-05, 2.3394e-05, 2.4983e-05, 2.69e-05, 2.8413e-05, 2.95e-05, 3.1494e-05, 3.3825e-05, 3.5992e-05, 3.8429e-05, 3.8785e-05, 4.3472e-05, 4.6657e-05, 4.8139e-05, 5.0535e-05, 5.5311e-05, 6.0716e-05, 6.4487e-05, 7.1951e-05, 7.6982e-05, 8.357e-05, 8.9556e-05, 9.402e-05, 0.00010034, 0.00010754, 0.00012283, 0.0001294, 0.00013473, 0.00013504, 0.00015244, 0.00016015, 0.00018432, 0.00020081, 0.00021603, 0.00024214, 0.00028467, 0.00029732, 0.00031157, 0.00039919, 0.00041335, 0.00056139, 0.00062935, 0.00074695, 0.00089745, 0.00095102, 0.0010947, 0.001309, 0.0019244, 0.0030976, 0.0051209, 0.013328]\n",
      "    wout = [-0.0018035, -0.0014734, -0.001448, -0.0012957, -0.0010945, -0.00088073, -0.00083002, -0.00081614, -0.00078676, -0.00068708, -0.00067613, -0.0006065, -0.00059329, -0.00058783, -0.00057961, -0.0005681, -0.00052798, -0.00051561, -0.00047574, -0.00047148, -0.00044602, -0.00043643, -0.00042339, -0.00041439, -0.00041144, -0.00040234, -0.00038673, -0.0003792, -0.00037158, -0.0003675, -0.0003637, -0.00035323, -0.00035226, -0.0003486, -0.00034265, -0.00033453, -0.00032766, -0.00032347, -0.0003219, -0.00031228, -0.00030042, -0.00029225, -0.00028465, -0.00027395, -0.00026117, -0.00025628, -0.0002515, -0.00024972, -0.00024526, -0.00023619, -0.00022709, -0.00022483, -0.00021818, -0.00021137, -0.00020176, -0.00018608, -0.00018424, -0.00017635, -0.00017042, -0.00016804, -0.0001532, -0.00014981, -0.0001476, -0.00013441, -0.0001334, -0.00012703, -0.00012446, -0.00012117, -0.00011984, -0.00011642, -0.00010475, -0.00010096, -9.2157e-05, -8.6793e-05, -8.5344e-05, -8.0711e-05, -7.6158e-05, -7.1695e-05, -7.0887e-05, -6.4449e-05, -6.1839e-05, -6.071e-05, -5.5866e-05, -5.2104e-05, -4.351e-05, -3.6194e-05, -2.8714e-05, -2.5782e-05, -2.2376e-05, -2.0463e-05, -1.3648e-05, -1.1924e-05, -1.1297e-05, -1.5199e-06, 8.6221e-07, 4.9938e-06, 7.6279e-06, 8.2338e-06, 1.0963e-05, 1.3866e-05, 1.9183e-05, 2.1282e-05, 2.5138e-05, 2.8382e-05, 3.6951e-05, 3.9201e-05, 4.0779e-05, 4.3243e-05, 5.1243e-05, 5.46e-05, 6.2952e-05, 6.8929e-05, 7.1578e-05, 7.7584e-05, 8.0435e-05, 8.9652e-05, 9.6477e-05, 0.0001003, 0.00010416, 0.00010672, 0.00010826, 0.00010955, 0.0001129, 0.00011783, 0.00012068, 0.00012325, 0.00013199, 0.00013535, 0.0001374, 0.00013963, 0.00014468, 0.00014811, 0.00015462, 0.00015527, 0.00015952, 0.00017311, 0.00017531, 0.00017926, 0.00018724, 0.00019817, 0.00020112, 0.00020193, 0.00020807, 0.0002145, 0.0002189, 0.00022283, 0.00022596, 0.00023044, 0.00023244, 0.00024347, 0.00025352, 0.00025619, 0.00027141, 0.00027865, 0.00028561, 0.00029729, 0.00030746, 0.00031814, 0.00032242, 0.00033458, 0.00034002, 0.00034649, 0.00035641, 0.00036116, 0.00036735, 0.00037381, 0.0003754, 0.00038381, 0.00039172, 0.00039753, 0.00041076, 0.00042315, 0.00044643, 0.00044983, 0.00046639, 0.00047192, 0.00047938, 0.00048274, 0.00049699, 0.00055408, 0.00059506, 0.00060522, 0.00062421, 0.00066745, 0.00067897, 0.00069768, 0.00071811, 0.00074491, 0.0007733, 0.00082405, 0.00084385, 0.00090694, 0.0010041, 0.0013049, 0.0013661, 0.0013806, 0.0018398, 0.0024108, 0.0030358, 0.0041605]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 1, Epoch: 14, Step: 600\n",
      "    cos_sim = -0.270\n",
      "    win = [-0.14182, -0.014584, -0.0080829, -0.0022998, -0.0020155, -0.0011931, -0.00095531, -0.00029677, -0.00011249, -5.1611e-05, -3.3254e-05, -2.5692e-05, -1.7156e-05, -8.7808e-06, -3.1475e-06, -2.4911e-06, -3.2495e-07, -1.4795e-07, -8.3576e-08, -4.6906e-08, -1.3314e-08, -4.045e-09, -3.4845e-09, -3.1064e-09, -2.8083e-09, -2.6456e-09, -2.554e-09, -2.4348e-09, -2.3944e-09, -2.3119e-09, -2.2163e-09, -2.21e-09, -2.1355e-09, -2.1067e-09, -2.0688e-09, -2.0139e-09, -1.9767e-09, -1.9198e-09, -1.8981e-09, -1.8667e-09, -1.8087e-09, -1.7888e-09, -1.7087e-09, -1.6718e-09, -1.6377e-09, -1.6122e-09, -1.572e-09, -1.5361e-09, -1.475e-09, -1.4504e-09, -1.4032e-09, -1.381e-09, -1.3532e-09, -1.3316e-09, -1.3101e-09, -1.2723e-09, -1.2471e-09, -1.2187e-09, -1.1999e-09, -1.1218e-09, -1.0982e-09, -1.0877e-09, -1.0639e-09, -1.0486e-09, -1.0246e-09, -9.6155e-10, -9.1371e-10, -8.9784e-10, -8.5715e-10, -8.4793e-10, -8.1848e-10, -7.9327e-10, -7.8033e-10, -7.4785e-10, -7.0106e-10, -6.7774e-10, -6.564e-10, -6.2518e-10, -6.1007e-10, -5.8487e-10, -5.6183e-10, -5.4957e-10, -5.2449e-10, -5.0713e-10, -4.8212e-10, -4.5586e-10, -4.1909e-10, -3.8939e-10, -3.8287e-10, -3.4452e-10, -3.2008e-10, -2.8861e-10, -2.794e-10, -2.6067e-10, -2.3517e-10, -2.0145e-10, -1.6155e-10, -1.3573e-10, -1.288e-10, -1.1028e-10, -9.1043e-11, -4.1694e-11, -2.5368e-11, -2.1014e-11, 2.4015e-11, 3.1561e-11, 5.084e-11, 8.4592e-11, 9.4744e-11, 1.3072e-10, 1.6261e-10, 1.9203e-10, 2.3848e-10, 2.4801e-10, 2.6942e-10, 2.7579e-10, 3.1311e-10, 3.2938e-10, 3.4607e-10, 3.5848e-10, 4.0637e-10, 4.2475e-10, 4.36e-10, 4.5371e-10, 5.2488e-10, 5.354e-10, 5.5159e-10, 5.7391e-10, 5.8172e-10, 5.9323e-10, 6.3428e-10, 7.0686e-10, 7.2783e-10, 7.3366e-10, 7.5279e-10, 8.0842e-10, 8.2677e-10, 8.473e-10, 8.9376e-10, 9.0924e-10, 9.1842e-10, 9.5641e-10, 9.7286e-10, 9.8326e-10, 1.0303e-09, 1.0448e-09, 1.0793e-09, 1.0904e-09, 1.1329e-09, 1.1483e-09, 1.1668e-09, 1.2027e-09, 1.2457e-09, 1.2635e-09, 1.3087e-09, 1.3256e-09, 1.3635e-09, 1.405e-09, 1.4708e-09, 1.5003e-09, 1.5374e-09, 1.5889e-09, 1.5946e-09, 1.6619e-09, 1.7052e-09, 1.7258e-09, 1.7604e-09, 1.7776e-09, 1.8118e-09, 1.8414e-09, 1.877e-09, 1.9183e-09, 1.9919e-09, 2.0594e-09, 2.1094e-09, 2.1441e-09, 2.203e-09, 2.3541e-09, 2.3936e-09, 2.5276e-09, 2.5976e-09, 2.735e-09, 2.9642e-09, 3.0339e-09, 3.2181e-09, 3.7633e-09, 4.9119e-09, 1.5529e-08, 2.7584e-07, 9.4168e-07, 4.6888e-06, 8.0633e-06, 6.1753e-05, 0.00068183, 0.00111, 0.0026363, 0.0032618, 0.0153, 0.031139, 0.13005]\n",
      "    wout = [-0.0056238, -0.0040072, -0.003937, -0.0035319, -0.0031214, -0.0028934, -0.0026563, -0.0025995, -0.0023143, -0.0021741, -0.0020667, -0.0019387, -0.0018105, -0.0017593, -0.0017366, -0.0015743, -0.0015167, -0.0014844, -0.0014555, -0.0013935, -0.0013563, -0.0012284, -0.0011857, -0.0010727, -0.0010288, -0.0010183, -0.0009971, -0.00097638, -0.00096541, -0.00094878, -0.00093066, -0.00087213, -0.00084833, -0.00082222, -0.00081198, -0.00078944, -0.0007354, -0.00072277, -0.00070178, -0.00067422, -0.00065197, -0.00063086, -0.00061479, -0.00060089, -0.00055831, -0.00054434, -0.00053576, -0.00053266, -0.00053244, -0.00050819, -0.00049756, -0.0004909, -0.00048962, -0.00048435, -0.00045476, -0.00045376, -0.00044736, -0.00043146, -0.00041312, -0.00040293, -0.00039479, -0.00039082, -0.00038569, -0.00038304, -0.00036787, -0.00036617, -0.0003612, -0.00032177, -0.00031114, -0.0003108, -0.0002853, -0.00027424, -0.00026293, -0.00025319, -0.00022937, -0.00021361, -0.00020063, -0.00019521, -0.00017118, -0.00015884, -0.00015242, -0.00013398, -0.00013132, -0.00011404, -0.00011033, -0.00010197, -8.1505e-05, -8.0823e-05, -7.9538e-05, -7.2226e-05, -6.3555e-05, -6.1164e-05, -5.5468e-05, -5.389e-05, -4.4428e-05, -3.6536e-05, -3.2833e-05, -2.9217e-05, -2.531e-05, -1.2988e-05, -9.3326e-06, -3.3702e-06, 2.6647e-06, 5.5154e-06, 2.1469e-05, 2.5269e-05, 3.0555e-05, 3.2799e-05, 3.8421e-05, 4.2863e-05, 4.8843e-05, 5.291e-05, 5.67e-05, 7.6009e-05, 8.5675e-05, 9.7648e-05, 0.00010194, 0.0001106, 0.00011308, 0.00013127, 0.00014186, 0.00014723, 0.00015757, 0.00016989, 0.00017675, 0.00019413, 0.00020276, 0.00021086, 0.00022059, 0.00023151, 0.00023492, 0.00024953, 0.00025283, 0.00026774, 0.00030209, 0.00031301, 0.00033009, 0.00033632, 0.00036449, 0.00038662, 0.00039976, 0.00042544, 0.00043132, 0.00043823, 0.00046283, 0.00047107, 0.00048052, 0.00050603, 0.00051955, 0.00053013, 0.00054882, 0.00056552, 0.0005717, 0.00057834, 0.00061367, 0.00064448, 0.0006601, 0.000666, 0.00070813, 0.00073689, 0.00076661, 0.00079806, 0.00082513, 0.00083571, 0.00084559, 0.00087283, 0.00087558, 0.00087778, 0.00091475, 0.00092872, 0.00098292, 0.001009, 0.0010145, 0.0010415, 0.0010595, 0.0011018, 0.0011049, 0.0011125, 0.0011575, 0.0011803, 0.0012464, 0.0013013, 0.0013226, 0.0013536, 0.0014228, 0.0017555, 0.0017988, 0.0019518, 0.0020841, 0.002127, 0.0024059, 0.0026016, 0.0032227, 0.0034011, 0.0035319, 0.0039102, 0.0041187, 0.004142, 0.0043115, 0.0052273]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.908\n",
      "    win = [-0.0041169, -0.0024151, -0.0012177, -0.0010696, -0.00063846, -0.0005668, -0.00042859, -0.00035256, -0.00027935, -0.00024564, -0.00020332, -0.00015171, -0.00013638, -0.00010791, -9.0793e-05, -7.5571e-05, -7.3603e-05, -5.5931e-05, -4.9942e-05, -4.3106e-05, -3.114e-05, -2.7769e-05, -2.5154e-05, -2.5039e-05, -1.9575e-05, -1.8444e-05, -1.6747e-05, -1.5003e-05, -1.1746e-05, -9.3168e-06, -8.7631e-06, -7.897e-06, -6.6157e-06, -6.3292e-06, -5.3777e-06, -5.2046e-06, -4.7577e-06, -4.1936e-06, -3.4958e-06, -3.1074e-06, -3.0905e-06, -2.9351e-06, -2.3922e-06, -2.0813e-06, -1.8857e-06, -1.6321e-06, -1.4161e-06, -1.3398e-06, -1.2682e-06, -1.0965e-06, -9.2426e-07, -7.4272e-07, -6.3354e-07, -5.5389e-07, -4.7243e-07, -4.0565e-07, -3.5077e-07, -2.7696e-07, -2.2935e-07, -2.0297e-07, -1.8181e-07, -1.3423e-07, -1.1343e-07, -8.4915e-08, -7.8387e-08, -5.8718e-08, -5.4084e-08, -3.6162e-08, -2.9534e-08, -2.2586e-08, -1.4911e-08, -1.1427e-08, -7.7526e-09, -6.7716e-09, -3.2754e-09, -3.0511e-09, -2.1627e-09, -1.9934e-09, -1.3187e-09, -5.9673e-10, -5.1511e-10, -4.9104e-10, -4.0689e-10, -3.486e-10, -2.7518e-10, -2.5942e-10, -2.3345e-10, -1.8886e-10, -1.7871e-10, -1.6387e-10, -1.3952e-10, -1.2972e-10, -1.1541e-10, -9.1309e-11, -8.503e-11, -5.0444e-11, -4.0863e-11, -1.8317e-11, -1.6829e-11, -6.4923e-12, 4.1823e-13, 3.2015e-11, 5.4472e-11, 6.6841e-11, 8.436e-11, 9.947e-11, 1.1384e-10, 1.2012e-10, 1.426e-10, 1.5902e-10, 2.0841e-10, 2.2303e-10, 2.4657e-10, 2.9098e-10, 3.4465e-10, 3.6074e-10, 4.8888e-10, 6.0377e-10, 1.1466e-09, 1.5438e-09, 2.0504e-09, 2.7893e-09, 4.3301e-09, 5.0506e-09, 9.5977e-09, 1.1236e-08, 1.3262e-08, 1.4021e-08, 1.9122e-08, 2.6929e-08, 3.1404e-08, 3.6522e-08, 4.3512e-08, 5.0511e-08, 5.142e-08, 8.2079e-08, 1.0079e-07, 1.0707e-07, 1.8237e-07, 2.2022e-07, 2.4536e-07, 2.9861e-07, 3.3743e-07, 4.3823e-07, 6.0304e-07, 7.3124e-07, 7.7576e-07, 9.6275e-07, 1.1176e-06, 1.3261e-06, 1.5846e-06, 1.8788e-06, 2.2048e-06, 2.5138e-06, 2.8171e-06, 3.3172e-06, 4.097e-06, 4.7625e-06, 5.5221e-06, 5.6679e-06, 6.2836e-06, 7.0658e-06, 7.6071e-06, 8.6741e-06, 1.0322e-05, 1.0572e-05, 1.2068e-05, 1.5247e-05, 1.6719e-05, 1.9235e-05, 2.1781e-05, 2.5898e-05, 3.0463e-05, 3.2036e-05, 3.4354e-05, 4.3607e-05, 4.6177e-05, 5.3873e-05, 5.7662e-05, 5.9736e-05, 7.1897e-05, 8.1108e-05, 8.5064e-05, 0.00010276, 0.00012118, 0.00013562, 0.00013719, 0.00017279, 0.00021517, 0.00036504, 0.00041924, 0.00054344, 0.00079794, 0.0013107, 0.0013488, 0.0020185, 0.0027727, 0.0041288, 0.0095051, 0.062083]\n",
      "    wout = [-0.0012915, -0.0011112, -0.0010116, -0.0009712, -0.00095595, -0.00091193, -0.00089405, -0.00084437, -0.00081493, -0.00077868, -0.00070622, -0.00068326, -0.00062865, -0.00058775, -0.00048283, -0.00045924, -0.00043743, -0.00040288, -0.0003815, -0.00036663, -0.00030898, -0.00029131, -0.00027629, -0.00025579, -0.0002457, -0.00023452, -0.00022226, -0.00021377, -0.00021274, -0.00019072, -0.00018188, -0.00017977, -0.00015748, -0.00015222, -0.00014863, -0.00013587, -0.00012605, -0.00011956, -0.0001065, -9.2799e-05, -8.4424e-05, -7.7725e-05, -7.7308e-05, -7.2324e-05, -7.0333e-05, -6.9587e-05, -6.5902e-05, -6.0179e-05, -5.7892e-05, -5.7522e-05, -5.3029e-05, -5.0158e-05, -4.9955e-05, -4.3278e-05, -3.8474e-05, -3.4439e-05, -2.9422e-05, -2.6568e-05, -2.5948e-05, -2.5547e-05, -2.0808e-05, -2.014e-05, -1.955e-05, -1.5911e-05, -1.3944e-05, -1.2641e-05, -1.2482e-05, -1.2184e-05, -1.1385e-05, -9.3432e-06, -7.2676e-06, -5.6043e-06, -4.1869e-06, -3.6974e-06, -3.458e-06, -3.4411e-06, -2.1365e-06, -1.4933e-06, -5.8122e-07, -3.3789e-07, -3.349e-07, -2.9506e-07, -2.8896e-07, -2.6617e-07, -2.2852e-07, -2.272e-07, -1.8915e-07, -4.4004e-08, -1.0205e-08, -8.116e-09, -2.0695e-09, -9.1607e-10, -3.552e-10, -2.949e-10, -6.6457e-11, -6.4106e-11, -3.9308e-11, -3.5555e-11, -2.0905e-11, -8.0726e-12, 5.8273e-12, 1.4551e-11, 1.604e-11, 1.7157e-11, 2.1388e-11, 2.8559e-11, 3.3737e-11, 4.1112e-11, 9.4739e-11, 1.055e-10, 1.8937e-10, 3.5688e-10, 6.4454e-10, 9.9152e-10, 1.1677e-09, 1.5291e-09, 2.1011e-09, 2.7414e-09, 6.7887e-09, 9.2733e-09, 9.9042e-09, 8.4461e-08, 1.0388e-07, 1.4374e-07, 2.2683e-07, 8.0828e-07, 8.8119e-07, 9.692e-07, 1.3675e-06, 1.6443e-06, 2.0784e-06, 2.8441e-06, 3.0969e-06, 3.4697e-06, 3.9713e-06, 6.5034e-06, 7.1292e-06, 8.813e-06, 9.1892e-06, 9.6058e-06, 1.01e-05, 1.1468e-05, 1.2878e-05, 1.3563e-05, 1.4101e-05, 1.593e-05, 1.7063e-05, 1.7907e-05, 1.8801e-05, 2.2802e-05, 2.5046e-05, 2.6078e-05, 2.6921e-05, 2.8039e-05, 3.0217e-05, 3.1481e-05, 3.2229e-05, 3.3301e-05, 3.6164e-05, 4.2981e-05, 4.7588e-05, 5.2638e-05, 5.8643e-05, 5.9223e-05, 6.0411e-05, 6.2486e-05, 6.4883e-05, 6.9354e-05, 7.0254e-05, 7.0436e-05, 7.6891e-05, 8.1124e-05, 8.4972e-05, 8.8441e-05, 8.9938e-05, 9.2512e-05, 0.00010214, 0.00010467, 0.00011303, 0.00012157, 0.00014763, 0.00015306, 0.000158, 0.00017484, 0.00019481, 0.00021413, 0.00023793, 0.00025378, 0.00027926, 0.00028189, 0.00030437, 0.00040308, 0.00041675, 0.00048507, 0.00052551, 0.00061465, 0.00066282, 0.00071839, 0.0011704, 0.0012698]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 2, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.932\n",
      "    win = [-0.0060711, -0.003842, -0.0026836, -0.0019769, -0.0012872, -0.00098043, -0.00069265, -0.00061603, -0.00054345, -0.00043792, -0.00024225, -0.00022738, -0.00019022, -0.00013595, -0.00011353, -0.00010305, -7.7378e-05, -6.06e-05, -5.8611e-05, -5.3728e-05, -3.8205e-05, -3.6671e-05, -3.2652e-05, -2.6099e-05, -2.472e-05, -1.8844e-05, -1.505e-05, -1.481e-05, -1.0435e-05, -8.6185e-06, -7.5747e-06, -6.5176e-06, -6.0373e-06, -4.6733e-06, -4.2535e-06, -3.6615e-06, -2.3969e-06, -2.2096e-06, -1.874e-06, -1.8036e-06, -1.5295e-06, -1.2976e-06, -9.9432e-07, -8.6736e-07, -7.9604e-07, -6.6708e-07, -5.472e-07, -5.3886e-07, -3.422e-07, -2.9114e-07, -1.8603e-07, -1.4575e-07, -1.162e-07, -8.179e-08, -7.4075e-08, -6.7605e-08, -4.9239e-08, -3.9743e-08, -3.6716e-08, -2.9086e-08, -2.3234e-08, -1.4786e-08, -1.0403e-08, -6.5469e-09, -5.6587e-09, -3.4363e-09, -1.6397e-09, -1.5101e-09, -1.0185e-09, -4.8738e-10, -3.8932e-10, -3.2693e-10, -2.8914e-10, -2.6588e-10, -2.439e-10, -2.0518e-10, -1.9236e-10, -1.8093e-10, -1.6357e-10, -1.5106e-10, -1.3367e-10, -1.2895e-10, -1.0078e-10, -9.5142e-11, -8.2929e-11, -7.3442e-11, -4.7467e-11, -4.5451e-11, -2.6042e-11, -1.6075e-11, -6.2247e-12, 5.606e-12, 1.2013e-11, 1.689e-11, 3.0253e-11, 3.3616e-11, 3.8207e-11, 6.382e-11, 6.9484e-11, 8.8283e-11, 9.7761e-11, 1.0146e-10, 1.2539e-10, 1.407e-10, 1.5244e-10, 1.6291e-10, 1.8419e-10, 1.9458e-10, 1.9995e-10, 2.1438e-10, 2.7709e-10, 2.917e-10, 3.22e-10, 3.4948e-10, 4.3555e-10, 5.281e-10, 7.9581e-10, 1.245e-09, 1.8159e-09, 2.5984e-09, 3.5304e-09, 4.2484e-09, 4.819e-09, 5.4271e-09, 6.9352e-09, 1.0663e-08, 1.2504e-08, 1.3692e-08, 1.8079e-08, 2.4053e-08, 2.9359e-08, 3.2959e-08, 4.0171e-08, 4.7211e-08, 6.4866e-08, 8.4533e-08, 9.3128e-08, 1.2804e-07, 1.6992e-07, 1.9447e-07, 2.5227e-07, 3.0697e-07, 3.9046e-07, 4.2009e-07, 4.5798e-07, 5.6e-07, 6.9733e-07, 8.4187e-07, 8.6183e-07, 1.0171e-06, 1.1206e-06, 1.3926e-06, 1.449e-06, 2.359e-06, 2.6399e-06, 2.7517e-06, 3.2822e-06, 3.4317e-06, 5.1595e-06, 5.4351e-06, 6.025e-06, 6.3571e-06, 7.0175e-06, 9.9257e-06, 1.0788e-05, 1.2497e-05, 1.4046e-05, 1.7669e-05, 1.81e-05, 2.113e-05, 2.4863e-05, 2.8318e-05, 3.2655e-05, 4.1089e-05, 4.3933e-05, 4.5355e-05, 4.9506e-05, 5.6195e-05, 6.2664e-05, 7.2225e-05, 7.6092e-05, 9.1744e-05, 0.00012074, 0.00014571, 0.00015212, 0.00020167, 0.00020526, 0.00029749, 0.00033728, 0.00044718, 0.00054817, 0.00061353, 0.00087383, 0.0011608, 0.0013421, 0.0017317, 0.0024277, 0.0046965, 0.0049129, 0.049404]\n",
      "    wout = [-0.00044803, -0.00042717, -0.00039149, -0.00026349, -0.00023551, -0.00022822, -0.00018832, -0.00017738, -0.00017663, -0.00016975, -0.0001394, -0.00012853, -0.00012527, -0.00012291, -0.00011805, -9.4067e-05, -8.1345e-05, -7.3678e-05, -7.2616e-05, -7.022e-05, -6.923e-05, -6.5324e-05, -6.2101e-05, -6.0726e-05, -5.1608e-05, -4.7397e-05, -4.5479e-05, -4.4921e-05, -3.9552e-05, -3.5935e-05, -3.4414e-05, -3.4152e-05, -3.2183e-05, -3.0779e-05, -3.0095e-05, -2.8255e-05, -2.7069e-05, -2.6377e-05, -2.5782e-05, -2.5394e-05, -2.3784e-05, -2.2782e-05, -2.2591e-05, -2.1119e-05, -1.9624e-05, -1.9206e-05, -1.8606e-05, -1.4858e-05, -1.4753e-05, -1.3791e-05, -1.3584e-05, -1.2342e-05, -1.1356e-05, -1.1008e-05, -1.0211e-05, -9.8405e-06, -9.1502e-06, -7.8625e-06, -7.6435e-06, -7.4869e-06, -7.064e-06, -6.8961e-06, -6.5368e-06, -5.8446e-06, -5.6883e-06, -5.528e-06, -5.3104e-06, -5.2305e-06, -5.1398e-06, -4.4518e-06, -4.2504e-06, -4.1149e-06, -3.8042e-06, -3.7365e-06, -3.7017e-06, -3.3495e-06, -3.2426e-06, -3.093e-06, -2.9592e-06, -2.5562e-06, -2.5335e-06, -2.4133e-06, -2.1923e-06, -1.7341e-06, -1.696e-06, -1.6635e-06, -1.5053e-06, -1.3612e-06, -1.1056e-06, -1.0379e-06, -1.0121e-06, -9.3117e-07, -8.3402e-07, -6.6783e-07, -5.9497e-07, -5.683e-07, -3.2052e-07, -3.1535e-07, -2.3818e-07, -1.7196e-07, -1.4964e-07, -1.1643e-07, -7.9389e-08, -7.2404e-08, -4.2224e-08, -3.6146e-08, -7.7226e-09, -1.2567e-09, -1.2555e-10, 2.4696e-10, 1.2615e-08, 2.015e-08, 8.4537e-08, 9.5762e-08, 1.2302e-07, 3.1495e-07, 3.9312e-07, 4.0263e-07, 4.8332e-07, 5.5373e-07, 6.6586e-07, 7.4916e-07, 7.5777e-07, 7.6877e-07, 7.7402e-07, 8.1789e-07, 9.8874e-07, 1.045e-06, 1.8811e-06, 2.1125e-06, 3.1542e-06, 3.4146e-06, 3.4363e-06, 3.8897e-06, 4.1314e-06, 4.4392e-06, 4.4968e-06, 4.5837e-06, 4.827e-06, 4.9613e-06, 5.2922e-06, 6.0337e-06, 7.2136e-06, 7.7108e-06, 8.0508e-06, 8.7142e-06, 9.5658e-06, 1.0105e-05, 1.0455e-05, 1.1047e-05, 1.1693e-05, 1.2459e-05, 1.404e-05, 1.4626e-05, 1.516e-05, 1.5781e-05, 1.6518e-05, 1.782e-05, 1.9277e-05, 2.0232e-05, 2.1503e-05, 2.1637e-05, 2.2215e-05, 2.5909e-05, 2.6831e-05, 2.7443e-05, 3.104e-05, 3.2462e-05, 3.3148e-05, 3.5486e-05, 3.9073e-05, 3.9357e-05, 4.4353e-05, 4.7123e-05, 5.3933e-05, 6.3384e-05, 6.5088e-05, 6.8189e-05, 7.1586e-05, 7.6233e-05, 7.9285e-05, 8.7156e-05, 9.1488e-05, 0.00010036, 0.00010977, 0.00011968, 0.00013109, 0.00013526, 0.00013653, 0.00014754, 0.00015573, 0.00021381, 0.00021644, 0.00024532, 0.00027938, 0.00030498, 0.00036895, 0.00037469, 0.00042857, 0.0014899]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 2, Step: 100\n",
      "    cos_sim = 0.602\n",
      "    win = [-0.042419, -0.0044072, -0.0024737, -0.0010417, -0.00072276, -0.00053967, -0.00049385, -0.00018067, -0.00015735, -0.00012798, -0.00010243, -9.3883e-05, -7.9682e-05, -7.278e-05, -5.2412e-05, -4.7824e-05, -4.6563e-05, -3.3828e-05, -3.0137e-05, -2.6521e-05, -2.4508e-05, -2.2199e-05, -1.9014e-05, -1.6123e-05, -1.3328e-05, -1.1953e-05, -1.1105e-05, -8.7221e-06, -8.0571e-06, -7.1374e-06, -6.9491e-06, -6.3765e-06, -5.0567e-06, -4.403e-06, -4.0733e-06, -3.7797e-06, -3.5373e-06, -3.2793e-06, -3.1222e-06, -2.822e-06, -2.4031e-06, -2.1169e-06, -1.7776e-06, -1.7199e-06, -1.6213e-06, -1.4342e-06, -1.1435e-06, -1.1369e-06, -1.0439e-06, -8.905e-07, -7.6242e-07, -7.0128e-07, -6.1922e-07, -5.2178e-07, -4.2422e-07, -3.9688e-07, -3.5628e-07, -2.9831e-07, -2.7938e-07, -2.4678e-07, -2.3005e-07, -2.0822e-07, -2.0082e-07, -1.8556e-07, -1.4054e-07, -1.3461e-07, -1.1082e-07, -1.0291e-07, -9.4669e-08, -8.693e-08, -6.5643e-08, -5.3648e-08, -5.2012e-08, -4.4673e-08, -4.0409e-08, -3.2822e-08, -2.5213e-08, -1.9492e-08, -1.6855e-08, -1.3556e-08, -1.2536e-08, -9.2188e-09, -6.8623e-09, -6.5857e-09, -5.8724e-09, -4.8403e-09, -3.6521e-09, -2.703e-09, -2.4289e-09, -1.6442e-09, -1.2006e-09, -1.0894e-09, -8.9826e-10, -6.1609e-10, -5.6558e-10, -5.3234e-10, -4.9571e-10, -3.8643e-10, -3.5933e-10, -3.1966e-10, -2.6924e-10, -2.1895e-10, -2.0667e-10, -1.7641e-10, -1.5415e-10, -1.3028e-10, -1.055e-10, -8.7934e-11, -7.0204e-11, -5.4403e-11, -2.9808e-11, -1.2994e-11, 3.0581e-13, 1.0167e-11, 3.8953e-11, 4.2808e-11, 7.0222e-11, 9.0853e-11, 9.6673e-11, 1.2139e-10, 1.2838e-10, 1.546e-10, 1.796e-10, 2.0091e-10, 2.2128e-10, 2.3903e-10, 2.7979e-10, 3.5623e-10, 3.7278e-10, 4.2706e-10, 5.1484e-10, 6.5386e-10, 8.4356e-10, 1.0249e-09, 1.286e-09, 1.631e-09, 2.174e-09, 2.8214e-09, 3.9526e-09, 6.8793e-09, 7.5034e-09, 1.0624e-08, 1.1153e-08, 1.4231e-08, 1.6981e-08, 2.1004e-08, 2.6161e-08, 4.0087e-08, 4.7841e-08, 5.442e-08, 5.9548e-08, 7.0891e-08, 8.4821e-08, 9.9e-08, 1.3605e-07, 1.4482e-07, 1.6458e-07, 1.737e-07, 2.0442e-07, 2.2321e-07, 2.3089e-07, 2.9029e-07, 3.169e-07, 3.8083e-07, 4.2479e-07, 4.4561e-07, 5.4124e-07, 5.5588e-07, 6.2681e-07, 8.6047e-07, 1.2044e-06, 1.2188e-06, 1.4104e-06, 1.5117e-06, 1.9614e-06, 2.7195e-06, 3.1308e-06, 3.8877e-06, 4.7307e-06, 5.5786e-06, 7.0124e-06, 1.0175e-05, 1.4513e-05, 1.6937e-05, 1.9013e-05, 2.05e-05, 2.8108e-05, 2.8339e-05, 3.7502e-05, 4.1692e-05, 4.4923e-05, 4.93e-05, 5.6e-05, 9.5634e-05, 0.00016885, 0.00019198, 0.00038321, 0.00091032, 0.0023783, 0.004089]\n",
      "    wout = [-0.00037168, -0.00029044, -0.00026347, -0.00025364, -0.00024269, -0.00023708, -0.00021385, -0.00016564, -0.0001575, -0.00015033, -0.0001483, -0.00013379, -0.00012, -0.00011608, -0.00011364, -0.00010969, -0.000103, -9.9313e-05, -9.8308e-05, -8.8581e-05, -8.1689e-05, -7.9816e-05, -7.9057e-05, -7.591e-05, -7.3522e-05, -7.1783e-05, -7.0358e-05, -6.8115e-05, -6.758e-05, -6.6929e-05, -6.5417e-05, -6.5204e-05, -6.4595e-05, -6.3354e-05, -6.2908e-05, -6.1484e-05, -5.9755e-05, -5.7293e-05, -5.3957e-05, -5.2994e-05, -5.1753e-05, -5.0882e-05, -4.9438e-05, -4.9165e-05, -4.5895e-05, -4.3977e-05, -4.1148e-05, -4.0632e-05, -3.993e-05, -3.9234e-05, -3.8041e-05, -3.7242e-05, -3.7066e-05, -3.4379e-05, -3.3218e-05, -3.2998e-05, -3.0908e-05, -2.9939e-05, -2.882e-05, -2.8571e-05, -2.7094e-05, -2.6497e-05, -2.6039e-05, -2.5358e-05, -2.3909e-05, -2.1908e-05, -2.0226e-05, -1.9527e-05, -1.9297e-05, -1.8511e-05, -1.8004e-05, -1.7023e-05, -1.6912e-05, -1.6045e-05, -1.4853e-05, -1.2711e-05, -1.1991e-05, -1.1887e-05, -1.1448e-05, -1.0925e-05, -1.0509e-05, -9.9057e-06, -8.9257e-06, -8.5128e-06, -7.3844e-06, -6.81e-06, -6.2908e-06, -5.4591e-06, -5.3367e-06, -4.5754e-06, -3.9559e-06, -2.8183e-06, -4.408e-07, -2.0819e-07, 8.7877e-07, 1.2765e-06, 1.7872e-06, 2.956e-06, 3.2961e-06, 4.3024e-06, 5.714e-06, 6.5966e-06, 6.9972e-06, 8.0973e-06, 8.7115e-06, 9.9459e-06, 1.0946e-05, 1.2016e-05, 1.2981e-05, 1.3349e-05, 1.3671e-05, 1.4595e-05, 1.6029e-05, 1.65e-05, 1.6654e-05, 1.6763e-05, 1.7414e-05, 1.8189e-05, 1.8394e-05, 1.9682e-05, 2.0689e-05, 2.1681e-05, 2.2651e-05, 2.3481e-05, 2.4156e-05, 2.5566e-05, 2.6409e-05, 2.7127e-05, 2.8628e-05, 2.9033e-05, 3.0471e-05, 3.1067e-05, 3.2224e-05, 3.3023e-05, 3.5073e-05, 3.6158e-05, 3.7693e-05, 3.8535e-05, 3.9215e-05, 4.1674e-05, 4.4083e-05, 4.4484e-05, 4.5661e-05, 4.6941e-05, 4.863e-05, 4.9904e-05, 5.2359e-05, 5.2673e-05, 5.4453e-05, 5.4617e-05, 5.5258e-05, 5.7437e-05, 5.9095e-05, 6.058e-05, 6.1276e-05, 6.3407e-05, 6.5779e-05, 6.8769e-05, 7.0289e-05, 7.1199e-05, 7.3351e-05, 7.4358e-05, 7.6611e-05, 7.7894e-05, 7.8428e-05, 7.9553e-05, 8.2441e-05, 8.3168e-05, 8.5335e-05, 8.873e-05, 9.3729e-05, 9.7277e-05, 9.8645e-05, 0.00010116, 0.00010876, 0.0001095, 0.00011237, 0.00011502, 0.00011795, 0.00012025, 0.00012641, 0.0001292, 0.00013886, 0.0001423, 0.00014425, 0.00015019, 0.0001584, 0.00016563, 0.0001757, 0.00017739, 0.00019117, 0.00020068, 0.00021492, 0.00025782, 0.00028199, 0.00029602, 0.00032447, 0.00040596, 0.00051927, 0.000648]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 4, Step: 200\n",
      "    cos_sim = 0.615\n",
      "    win = [-0.043353, -0.017881, -3.8983e-05, -6.7658e-06, -2.561e-06, -1.674e-06, -7.2022e-07, -5.3781e-08, -4.5873e-08, -1.1852e-08, -6.2396e-09, -5.8346e-09, -5.0933e-09, -4.5299e-09, -4.2633e-09, -4.077e-09, -3.9546e-09, -3.8685e-09, -3.5364e-09, -3.4425e-09, -3.3853e-09, -3.1876e-09, -3.1396e-09, -3.0848e-09, -2.9651e-09, -2.944e-09, -2.8652e-09, -2.8118e-09, -2.7581e-09, -2.6516e-09, -2.6123e-09, -2.5572e-09, -2.4842e-09, -2.4512e-09, -2.4067e-09, -2.3141e-09, -2.2457e-09, -2.1987e-09, -2.1224e-09, -2.089e-09, -2.0002e-09, -1.9759e-09, -1.9246e-09, -1.887e-09, -1.8461e-09, -1.787e-09, -1.7361e-09, -1.7083e-09, -1.6712e-09, -1.6336e-09, -1.6078e-09, -1.5872e-09, -1.4962e-09, -1.4868e-09, -1.4564e-09, -1.4362e-09, -1.4218e-09, -1.3675e-09, -1.3388e-09, -1.2559e-09, -1.2378e-09, -1.133e-09, -1.0945e-09, -1.0708e-09, -1.0483e-09, -1.0146e-09, -9.792e-10, -9.5778e-10, -9.4326e-10, -8.851e-10, -8.1177e-10, -7.8288e-10, -7.4252e-10, -7.1167e-10, -6.9144e-10, -6.6981e-10, -6.5638e-10, -6.0517e-10, -5.4524e-10, -5.1851e-10, -5.112e-10, -4.6447e-10, -4.3213e-10, -3.8502e-10, -3.2946e-10, -2.8833e-10, -2.7491e-10, -2.5141e-10, -1.932e-10, -1.8156e-10, -1.2567e-10, -1.1507e-10, -8.0448e-11, -6.0596e-11, -1.7312e-11, 3.5526e-11, 6.0608e-11, 9.8081e-11, 1.4741e-10, 1.8537e-10, 2.205e-10, 2.2747e-10, 3.1381e-10, 3.3219e-10, 3.5101e-10, 4.0093e-10, 4.4893e-10, 4.5159e-10, 5.3627e-10, 5.4618e-10, 5.7644e-10, 5.9882e-10, 6.2416e-10, 6.4505e-10, 7.0584e-10, 7.6793e-10, 8.1008e-10, 8.3339e-10, 8.5137e-10, 8.8147e-10, 9.0435e-10, 9.4138e-10, 9.5382e-10, 1.0247e-09, 1.072e-09, 1.0844e-09, 1.1094e-09, 1.1759e-09, 1.1803e-09, 1.2658e-09, 1.2967e-09, 1.3326e-09, 1.3579e-09, 1.3994e-09, 1.4318e-09, 1.4599e-09, 1.5303e-09, 1.5504e-09, 1.6303e-09, 1.7004e-09, 1.7093e-09, 1.7459e-09, 1.771e-09, 1.8389e-09, 1.8487e-09, 1.94e-09, 1.993e-09, 2.0351e-09, 2.0529e-09, 2.1051e-09, 2.1874e-09, 2.2349e-09, 2.2555e-09, 2.291e-09, 2.3654e-09, 2.4157e-09, 2.5063e-09, 2.61e-09, 2.6546e-09, 2.7099e-09, 2.7259e-09, 2.8013e-09, 2.862e-09, 2.9279e-09, 3.0767e-09, 3.1016e-09, 3.2368e-09, 3.3188e-09, 3.357e-09, 3.4538e-09, 3.5069e-09, 3.6404e-09, 3.7401e-09, 4.024e-09, 4.3882e-09, 4.598e-09, 6.3768e-09, 7.525e-09, 3.2426e-08, 3.4805e-08, 3.9892e-08, 1.0089e-07, 1.7159e-07, 1.9675e-07, 4.7028e-07, 6.4379e-07, 3.3176e-06, 6.4211e-06, 7.6847e-06, 2.8705e-05, 6.7068e-05, 0.00022685, 0.00029317, 0.00056293, 0.001168, 0.0021349, 0.0041457, 0.011332, 0.052842, 0.47006]\n",
      "    wout = [-0.010619, -0.005154, -0.0041724, -0.0041014, -0.0033852, -0.0029199, -0.0028902, -0.0025059, -0.0022981, -0.0022298, -0.001941, -0.0018508, -0.0016235, -0.0015024, -0.0014409, -0.0014001, -0.0013234, -0.0011425, -0.00097775, -0.00084573, -0.00080063, -0.0007959, -0.00071372, -0.00069073, -0.00041214, -0.00029649, -0.00021615, -0.00018844, -0.0001069, -9.6747e-05, -8.9978e-05, -8.2284e-05, -4.0553e-05, -3.5503e-05, -3.4188e-05, -2.7481e-05, -2.1739e-05, -1.6879e-05, -1.3897e-05, -9.9429e-06, -7.6972e-06, -7.3811e-06, -4.6991e-06, -4.651e-06, -4.2333e-06, -3.3525e-06, -2.0773e-06, -1.6233e-06, -1.5078e-06, -1.3416e-06, -1.2379e-06, -1.22e-06, -1.0725e-06, -9.1439e-07, -7.6024e-07, -7.0295e-07, -6.8817e-07, -6.1757e-07, -6.145e-07, -6.0379e-07, -5.2632e-07, -5.2243e-07, -5.1594e-07, -3.4241e-07, -3.2573e-07, -3.2525e-07, -3.0675e-07, -2.0748e-07, -1.8445e-07, -1.6659e-07, -1.5295e-07, -1.2304e-07, -1.0701e-07, -1.0406e-07, -7.1479e-08, -4.0923e-08, -3.2318e-08, -3.221e-08, -3.0648e-08, -3.0146e-08, -2.8415e-08, -2.2604e-08, -1.8152e-08, -1.4662e-08, -1.3555e-08, -1.1751e-08, -9.7819e-09, -8.5321e-09, -8.2559e-09, -7.989e-09, -7.2396e-09, -4.798e-09, -4.7348e-09, -4.1373e-09, -2.8994e-09, -2.7106e-09, -1.9786e-09, -9.465e-10, -1.6465e-11, 7.2854e-14, 7.2172e-11, 9.9183e-11, 6.8016e-09, 1.6294e-08, 1.9725e-08, 2.208e-08, 2.7055e-08, 2.7634e-08, 3.6926e-08, 3.7171e-08, 4.0295e-08, 4.9906e-08, 5.8547e-08, 6.5892e-08, 7.0867e-08, 7.9014e-08, 7.9781e-08, 2.8803e-07, 5.1901e-07, 5.5515e-07, 6.7228e-07, 8.4746e-07, 9.4357e-07, 1.0805e-06, 1.1395e-06, 1.1981e-06, 1.9314e-06, 3.9652e-06, 4.3686e-06, 4.5852e-06, 5.7094e-06, 7.3332e-06, 8.0084e-06, 8.4972e-06, 1.0505e-05, 1.1976e-05, 1.5654e-05, 1.792e-05, 1.826e-05, 2.1859e-05, 2.2246e-05, 2.4405e-05, 2.6069e-05, 2.8825e-05, 2.9389e-05, 5.0205e-05, 5.4969e-05, 5.8928e-05, 6.0476e-05, 6.4298e-05, 9.1992e-05, 9.5124e-05, 0.0001774, 0.00026699, 0.00030354, 0.00046486, 0.00056535, 0.00060004, 0.00076365, 0.00080554, 0.00082452, 0.00084231, 0.00086702, 0.00095665, 0.0010371, 0.0010643, 0.001102, 0.0012108, 0.0012684, 0.0016626, 0.0017016, 0.001741, 0.0017537, 0.0018892, 0.0019614, 0.0019862, 0.0020123, 0.0020445, 0.0020861, 0.0022163, 0.0022691, 0.0026285, 0.0026581, 0.0029318, 0.002988, 0.0030374, 0.0031911, 0.0033357, 0.0033646, 0.0035117, 0.0035961, 0.0038752, 0.0040942, 0.0041912, 0.0045498, 0.0048039, 0.0052824, 0.005896, 0.0064412, 0.010037]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 7, Step: 300\n",
      "    cos_sim = 0.691\n",
      "    win = [-0.0096282, -0.0033784, -0.0017215, -0.0010284, -0.0007607, -0.00053043, -0.00046201, -0.00041677, -0.00037575, -0.00032976, -0.00027351, -0.00021774, -0.00013546, -0.00011625, -9.8753e-05, -7.9696e-05, -7.2565e-05, -5.9923e-05, -5.3155e-05, -4.7147e-05, -4.6306e-05, -3.6259e-05, -3.1571e-05, -2.8951e-05, -2.4523e-05, -2.1557e-05, -1.6904e-05, -1.5912e-05, -1.3791e-05, -9.2574e-06, -8.4998e-06, -5.8668e-06, -5.331e-06, -4.9818e-06, -4.5953e-06, -3.9557e-06, -3.6059e-06, -3.1284e-06, -2.6382e-06, -2.4271e-06, -2.0089e-06, -1.7378e-06, -1.6164e-06, -1.2977e-06, -1.1791e-06, -8.3756e-07, -7.1821e-07, -6.9604e-07, -5.6326e-07, -4.634e-07, -3.7905e-07, -3.0942e-07, -2.8107e-07, -2.0457e-07, -1.913e-07, -1.6211e-07, -1.1969e-07, -8.535e-08, -6.9887e-08, -6.0855e-08, -5.3006e-08, -4.2911e-08, -3.7037e-08, -3.1791e-08, -2.3061e-08, -1.9807e-08, -1.7029e-08, -1.0418e-08, -9.6543e-09, -6.5944e-09, -3.2094e-09, -8.0461e-10, -5.0528e-10, -3.3533e-10, -2.3053e-10, -2.0354e-10, -1.7784e-10, -1.7032e-10, -1.6376e-10, -1.4979e-10, -1.3931e-10, -1.168e-10, -1.1408e-10, -1.0624e-10, -9.2571e-11, -8.6969e-11, -7.7435e-11, -7.1877e-11, -5.9033e-11, -5.176e-11, -4.8547e-11, -4.6348e-11, -3.9032e-11, -3.2235e-11, -2.8448e-11, -2.611e-11, -2.0232e-11, -6.979e-12, -3.4853e-12, -1.02e-12, 7.5674e-12, 1.2098e-11, 1.9623e-11, 2.1993e-11, 2.5249e-11, 3.7391e-11, 4.4706e-11, 5.4171e-11, 5.9644e-11, 6.4136e-11, 6.8016e-11, 7.1431e-11, 9.0434e-11, 1.0262e-10, 1.1302e-10, 1.2364e-10, 1.2535e-10, 1.3614e-10, 1.4563e-10, 1.5769e-10, 1.7309e-10, 1.8396e-10, 2.1635e-10, 2.2947e-10, 2.9396e-10, 3.5161e-10, 4.6912e-10, 8.2533e-10, 1.1661e-09, 1.2207e-09, 1.9853e-09, 2.9071e-09, 5.3269e-09, 7.8289e-09, 9.0226e-09, 1.7805e-08, 2.6481e-08, 4.1747e-08, 4.6548e-08, 7.1249e-08, 7.907e-08, 1.0976e-07, 1.2397e-07, 1.6044e-07, 2.2271e-07, 2.3213e-07, 2.8294e-07, 3.4501e-07, 3.9624e-07, 4.1486e-07, 5.7313e-07, 6.4599e-07, 7.2651e-07, 7.9334e-07, 9.3304e-07, 1.2634e-06, 1.3582e-06, 1.5002e-06, 1.8634e-06, 2.2218e-06, 2.6757e-06, 2.9511e-06, 3.3924e-06, 4.0206e-06, 4.1366e-06, 4.9077e-06, 5.7067e-06, 7.6086e-06, 8.2938e-06, 1.126e-05, 1.1721e-05, 1.5294e-05, 1.5993e-05, 2.029e-05, 2.3767e-05, 2.4189e-05, 2.8425e-05, 3.579e-05, 4.439e-05, 5.1402e-05, 5.6946e-05, 5.9116e-05, 6.5772e-05, 7.5567e-05, 8.2949e-05, 8.4821e-05, 0.00011349, 0.00011635, 0.00012895, 0.00017048, 0.00017292, 0.0002317, 0.0003755, 0.00060385, 0.00081973, 0.00092314, 0.0014221, 0.002299, 0.0090408, 0.01416]\n",
      "    wout = [-0.0011363, -0.00070037, -0.00061901, -0.00043634, -0.00040118, -0.00034334, -0.00030342, -0.00029193, -0.00027534, -0.00026543, -0.00023248, -0.00022497, -0.00021323, -0.00020364, -0.00019184, -0.00018722, -0.0001773, -0.0001693, -0.00016179, -0.00013745, -0.00013395, -0.00013083, -0.00012775, -0.00011443, -0.00011396, -0.00010276, -9.7164e-05, -8.9572e-05, -8.5093e-05, -8.1428e-05, -7.6818e-05, -6.481e-05, -5.6237e-05, -4.9476e-05, -4.6885e-05, -4.4321e-05, -4.2169e-05, -3.9826e-05, -3.689e-05, -2.3363e-05, -2.2506e-05, -2.1716e-05, -2.0192e-05, -1.7665e-05, -1.4228e-05, -1.2748e-05, -1.1331e-05, -1.0516e-05, -9.4401e-06, -5.5105e-06, -4.4487e-06, -4.2456e-06, -2.6791e-06, -2.1854e-06, -1.5574e-06, -1.0957e-06, -5.4092e-07, -3.4757e-07, -1.9979e-07, -7.6915e-08, -3.0786e-08, -2.9063e-08, -1.509e-08, -9.1279e-09, -7.8567e-09, -6.5868e-09, -5.6413e-09, -5.3487e-09, -4.7767e-09, -4.7539e-09, -4.7517e-09, -3.5431e-09, -3.218e-09, -1.1115e-09, -9.772e-10, -9.6745e-10, -6.8242e-10, -5.8162e-10, -5.7642e-10, -3.5624e-10, -3.2248e-10, -1.9663e-10, -6.0454e-11, -5.457e-11, 6.3526e-11, 1.0633e-10, 1.3966e-10, 1.8428e-10, 2.7854e-10, 5.1441e-10, 5.8856e-10, 2.3065e-09, 2.3248e-09, 2.6358e-09, 3.1281e-09, 3.3128e-09, 3.3268e-09, 4.0757e-09, 4.4923e-09, 5.2588e-09, 5.8469e-09, 6.0463e-09, 6.8817e-09, 1.3347e-08, 1.6859e-08, 1.7237e-08, 2.1186e-08, 2.2427e-08, 2.5243e-08, 2.9217e-08, 4.3697e-08, 4.472e-08, 4.8859e-08, 5.0657e-08, 5.3348e-08, 5.8591e-08, 6.0262e-08, 7.5376e-08, 7.5832e-08, 7.9516e-08, 8.234e-08, 9.8507e-08, 1.4298e-07, 1.5749e-07, 1.7086e-07, 2.149e-07, 2.2007e-07, 2.6131e-07, 3.2921e-07, 3.6411e-07, 4.2635e-07, 5.2228e-07, 6.2232e-07, 6.7777e-07, 7.0055e-07, 7.4114e-07, 7.9326e-07, 8.5725e-07, 9.9214e-07, 1.0797e-06, 1.5593e-06, 1.6274e-06, 2.2011e-06, 2.385e-06, 2.5822e-06, 3.6963e-06, 4.7151e-06, 4.8858e-06, 7.0276e-06, 8.674e-06, 1.3245e-05, 1.6886e-05, 2.6009e-05, 2.7769e-05, 3.6545e-05, 3.9488e-05, 4.2301e-05, 4.6469e-05, 5.2084e-05, 5.4354e-05, 5.7571e-05, 6.1294e-05, 6.3947e-05, 7.002e-05, 7.7821e-05, 8.5064e-05, 9.0246e-05, 9.0985e-05, 9.9792e-05, 0.00010057, 0.00010554, 0.00011821, 0.00012009, 0.00012767, 0.00013908, 0.00014643, 0.00015122, 0.00015411, 0.00015586, 0.00016892, 0.00018397, 0.00018814, 0.00020901, 0.00022277, 0.00023487, 0.00025468, 0.00026719, 0.00027303, 0.00028498, 0.00029852, 0.00031351, 0.00032898, 0.00036836, 0.00036998, 0.00044889, 0.00048211, 0.00058782, 0.00074445, 0.00091816, 0.0034097]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 9, Step: 400\n",
      "    cos_sim = 0.597\n",
      "    win = [-0.024446, -0.0010046, -2.3626e-05, -1.3303e-05, -8.3494e-06, -6.8024e-06, -1.0786e-06, -7.3574e-07, -2.6528e-08, -3.5129e-09, -2.9535e-09, -2.0525e-09, -1.6721e-09, -1.1861e-09, -1.055e-09, -8.956e-10, -8.3408e-10, -7.2834e-10, -6.7957e-10, -6.4194e-10, -6.3533e-10, -6.0282e-10, -5.8489e-10, -5.7447e-10, -5.5277e-10, -5.382e-10, -5.2396e-10, -5.2135e-10, -5.0592e-10, -4.9914e-10, -4.8118e-10, -4.7246e-10, -4.6235e-10, -4.5028e-10, -4.3267e-10, -4.1925e-10, -4.1159e-10, -3.9598e-10, -3.9288e-10, -3.8501e-10, -3.7563e-10, -3.6411e-10, -3.5271e-10, -3.4837e-10, -3.3682e-10, -3.2818e-10, -3.2586e-10, -3.2268e-10, -3.0489e-10, -3.0336e-10, -2.9645e-10, -2.8227e-10, -2.7731e-10, -2.7018e-10, -2.5979e-10, -2.5117e-10, -2.4688e-10, -2.4236e-10, -2.3567e-10, -2.2718e-10, -2.2021e-10, -2.1481e-10, -2.0802e-10, -2.0087e-10, -1.9832e-10, -1.9315e-10, -1.8881e-10, -1.8479e-10, -1.7558e-10, -1.6873e-10, -1.6272e-10, -1.5296e-10, -1.5154e-10, -1.476e-10, -1.452e-10, -1.4114e-10, -1.2581e-10, -1.2105e-10, -1.1709e-10, -1.1071e-10, -1.0787e-10, -1.0531e-10, -9.7547e-11, -9.0613e-11, -8.7933e-11, -7.7e-11, -6.7224e-11, -6.5278e-11, -5.5161e-11, -5.385e-11, -4.7205e-11, -4.3639e-11, -4.0334e-11, -3.5812e-11, -3.3063e-11, -2.77e-11, -2.0996e-11, -1.5761e-11, -1.0487e-11, -4.1959e-12, -1.332e-12, 3.8197e-12, 1.4104e-11, 1.6043e-11, 2.0845e-11, 3.0081e-11, 3.445e-11, 3.7285e-11, 4.0186e-11, 5.1031e-11, 5.2688e-11, 5.907e-11, 6.3583e-11, 6.7754e-11, 7.0439e-11, 7.7203e-11, 7.8857e-11, 8.5729e-11, 8.6903e-11, 9.3303e-11, 1.0824e-10, 1.1348e-10, 1.1938e-10, 1.2638e-10, 1.3061e-10, 1.3838e-10, 1.4091e-10, 1.4514e-10, 1.4759e-10, 1.5293e-10, 1.6086e-10, 1.6538e-10, 1.7172e-10, 1.7558e-10, 1.8074e-10, 1.9336e-10, 1.9569e-10, 1.9975e-10, 2.0792e-10, 2.1645e-10, 2.1804e-10, 2.2592e-10, 2.3773e-10, 2.5198e-10, 2.5668e-10, 2.5705e-10, 2.688e-10, 2.7045e-10, 2.8227e-10, 2.8639e-10, 2.8863e-10, 3.0659e-10, 3.1037e-10, 3.131e-10, 3.1464e-10, 3.268e-10, 3.3115e-10, 3.4418e-10, 3.4932e-10, 3.5569e-10, 3.6996e-10, 3.765e-10, 3.879e-10, 3.927e-10, 4.0653e-10, 4.1218e-10, 4.2181e-10, 4.2935e-10, 4.3859e-10, 4.4539e-10, 4.5388e-10, 4.7487e-10, 4.8496e-10, 4.985e-10, 5.3278e-10, 5.5344e-10, 5.6694e-10, 5.9398e-10, 6.2206e-10, 6.2915e-10, 6.6058e-10, 6.7795e-10, 7.1565e-10, 8.422e-10, 9.6858e-10, 1.0565e-09, 1.2191e-09, 2.5678e-09, 1.4493e-08, 1.4719e-08, 6.3854e-08, 1.6214e-07, 5.5456e-07, 9.5606e-06, 2.8248e-05, 0.00010779, 0.0011083, 0.0011624, 0.018351, 0.058298]\n",
      "    wout = [-0.0079988, -0.0069244, -0.0044982, -0.0035199, -0.0032461, -0.0030932, -0.0029686, -0.0026513, -0.0020004, -0.0015397, -0.0013841, -0.0013009, -0.0011172, -0.0010728, -0.00099878, -0.00079522, -0.00069395, -0.00068816, -0.00066677, -0.00064673, -0.00062543, -0.00057551, -0.00056998, -0.00055787, -0.00052556, -0.0005001, -0.00045047, -0.00042961, -0.00039568, -0.00037528, -0.00034813, -0.00034272, -0.00032979, -0.00030349, -0.00029281, -0.00028664, -0.00027978, -0.00025991, -0.00021825, -0.00019847, -0.00019142, -0.00018197, -0.00017602, -0.00015701, -0.00015072, -0.00014457, -0.00012166, -0.00010617, -0.00010306, -9.8322e-05, -9.7025e-05, -8.5e-05, -7.0412e-05, -6.481e-05, -5.4205e-05, -5.2326e-05, -5.0076e-05, -4.7898e-05, -4.5286e-05, -4.3976e-05, -4.2064e-05, -4.0183e-05, -3.8764e-05, -2.999e-05, -2.377e-05, -2.2164e-05, -1.7991e-05, -1.5188e-05, -1.3393e-05, -1.0767e-05, -8.7959e-06, -7.2392e-06, -5.7406e-06, -2.6485e-06, -2.3127e-06, -1.6521e-06, -5.1296e-07, -2.3805e-07, -2.0066e-07, -6.93e-08, 2.533e-09, 4.9446e-08, 5.3229e-08, 8.7649e-08, 9.401e-08, 1.3e-07, 2.4934e-07, 3.3355e-07, 4.8336e-07, 9.8037e-07, 1.2472e-06, 1.2988e-06, 1.3629e-06, 1.6651e-06, 2.7247e-06, 2.7378e-06, 2.8886e-06, 3.1116e-06, 3.1375e-06, 3.2777e-06, 3.4307e-06, 4.1582e-06, 4.6792e-06, 5.4621e-06, 5.5773e-06, 5.7973e-06, 7.0991e-06, 8.6359e-06, 9.1186e-06, 9.4368e-06, 9.6122e-06, 1.0824e-05, 1.2529e-05, 1.294e-05, 1.4875e-05, 1.7601e-05, 1.8086e-05, 1.8284e-05, 2.2367e-05, 2.2529e-05, 2.3313e-05, 2.3854e-05, 2.4036e-05, 2.5117e-05, 2.5616e-05, 2.641e-05, 2.7032e-05, 2.7208e-05, 2.7731e-05, 2.9912e-05, 3.473e-05, 3.7564e-05, 4.1052e-05, 4.4881e-05, 4.6779e-05, 5.8988e-05, 6.0815e-05, 6.3842e-05, 6.5095e-05, 7.3722e-05, 8.812e-05, 9.3082e-05, 9.4486e-05, 0.00010305, 0.00010548, 0.00010923, 0.00011326, 0.00011558, 0.0001318, 0.00013597, 0.00013696, 0.00014154, 0.00014594, 0.00015592, 0.00017125, 0.00020985, 0.00022417, 0.00024875, 0.00025571, 0.00026903, 0.00028112, 0.00029277, 0.00030082, 0.00031973, 0.00033932, 0.00034085, 0.00034483, 0.0003601, 0.00039085, 0.0004086, 0.0004381, 0.00047295, 0.00049448, 0.00050193, 0.00053532, 0.00055697, 0.00056628, 0.00061468, 0.00063659, 0.0006811, 0.00070477, 0.00071129, 0.00073828, 0.00085105, 0.00086614, 0.00089547, 0.00095428, 0.0012268, 0.0014161, 0.00144, 0.0014434, 0.0015934, 0.0019654, 0.0020203, 0.0024283, 0.0027293, 0.0028259, 0.0046214, 0.0072777, 0.0090535]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 12, Step: 500\n",
      "    cos_sim = 0.523\n",
      "    win = [-0.005845, -0.0017706, -0.0014085, -0.00097697, -0.00083982, -0.00080323, -0.00054031, -0.00046761, -0.00037744, -0.00030747, -0.0002698, -0.00025293, -0.00019902, -0.00017016, -0.00016581, -0.0001432, -0.00013219, -0.00010176, -8.8356e-05, -7.9218e-05, -7.2177e-05, -6.1897e-05, -5.9661e-05, -5.4605e-05, -5.0508e-05, -4.4021e-05, -4.3369e-05, -4.1932e-05, -3.9573e-05, -3.4347e-05, -3.1714e-05, -2.9587e-05, -2.6478e-05, -2.4921e-05, -2.2365e-05, -2.1599e-05, -2.0712e-05, -1.9316e-05, -1.8026e-05, -1.6867e-05, -1.5749e-05, -1.4285e-05, -1.2818e-05, -1.1839e-05, -1.1311e-05, -9.6606e-06, -9.431e-06, -8.4989e-06, -8.035e-06, -7.6303e-06, -5.9387e-06, -5.7437e-06, -5.3948e-06, -5.0482e-06, -4.55e-06, -4.1117e-06, -3.7175e-06, -3.1095e-06, -2.9242e-06, -2.6422e-06, -2.2575e-06, -2.2108e-06, -2.0569e-06, -1.8458e-06, -1.6852e-06, -1.4529e-06, -1.3184e-06, -1.1938e-06, -1.1312e-06, -9.9613e-07, -8.2031e-07, -7.6319e-07, -7.0742e-07, -6.0904e-07, -5.8799e-07, -5.1613e-07, -4.4072e-07, -3.8198e-07, -3.4922e-07, -2.8704e-07, -2.5205e-07, -2.0647e-07, -1.7283e-07, -1.2532e-07, -1.1067e-07, -9.1307e-08, -8.0528e-08, -6.6518e-08, -4.9439e-08, -4.4741e-08, -4.168e-08, -2.8948e-08, -2.4324e-08, -1.7719e-08, -9.6334e-09, -6.0188e-09, -4.5413e-10, 3.0872e-09, 5.8305e-09, 1.2407e-08, 1.4681e-08, 3.3756e-08, 4.3964e-08, 5.1031e-08, 5.9037e-08, 7.1424e-08, 8.9326e-08, 1.0532e-07, 1.2169e-07, 1.53e-07, 1.8065e-07, 2.16e-07, 2.451e-07, 2.9941e-07, 3.5826e-07, 3.826e-07, 4.1247e-07, 4.8374e-07, 5.3728e-07, 6.0165e-07, 6.818e-07, 7.512e-07, 7.629e-07, 8.2628e-07, 8.4904e-07, 1.0503e-06, 1.2103e-06, 1.3391e-06, 1.354e-06, 1.3899e-06, 1.6156e-06, 1.6951e-06, 1.7264e-06, 2.1069e-06, 2.2317e-06, 2.2903e-06, 2.5047e-06, 2.7958e-06, 3.037e-06, 3.2664e-06, 3.5063e-06, 3.8174e-06, 4.2527e-06, 4.4498e-06, 5.2098e-06, 5.3118e-06, 5.7481e-06, 6.137e-06, 6.7824e-06, 7.1446e-06, 7.6934e-06, 8.1917e-06, 8.9762e-06, 9.41e-06, 1.033e-05, 1.1321e-05, 1.1522e-05, 1.2757e-05, 1.4138e-05, 1.4235e-05, 1.5633e-05, 1.7197e-05, 1.9659e-05, 2.2501e-05, 2.4993e-05, 2.6133e-05, 2.8679e-05, 3.1321e-05, 3.5249e-05, 3.5426e-05, 4.2487e-05, 4.6067e-05, 5.3211e-05, 5.7366e-05, 6.0585e-05, 6.5192e-05, 7.2725e-05, 8.3454e-05, 9.2194e-05, 0.00011645, 0.00012075, 0.00015322, 0.00016625, 0.00018792, 0.00024268, 0.00028748, 0.00031251, 0.00036121, 0.00045269, 0.00050041, 0.00056257, 0.00075195, 0.00083303, 0.00095694, 0.001189, 0.0018498, 0.0020994, 0.0029908, 0.014693, 0.021548]\n",
      "    wout = [-0.0035886, -0.0022791, -0.0021467, -0.0018423, -0.0013315, -0.0010921, -0.00096028, -0.00090239, -0.00084588, -0.00073683, -0.00070869, -0.00066096, -0.00063224, -0.00058879, -0.00056052, -0.0004708, -0.00044105, -0.00042345, -0.00039319, -0.00034095, -0.00033296, -0.00031367, -0.00030998, -0.00028834, -0.00026669, -0.0002643, -0.00025583, -0.00023332, -0.00022987, -0.00021131, -0.00019904, -0.00017539, -0.0001608, -0.00013643, -0.00012852, -0.00011949, -0.00011449, -0.00011164, -9.5407e-05, -7.7097e-05, -7.4827e-05, -7.1111e-05, -6.744e-05, -6.693e-05, -6.5982e-05, -6.4226e-05, -6.2296e-05, -6.0166e-05, -5.9233e-05, -5.5413e-05, -5.5226e-05, -5.3086e-05, -5.216e-05, -5.1369e-05, -4.6811e-05, -4.2796e-05, -4.2447e-05, -4.0483e-05, -3.8495e-05, -3.6624e-05, -3.617e-05, -3.5587e-05, -3.1979e-05, -2.8337e-05, -2.7708e-05, -2.63e-05, -2.406e-05, -2.3373e-05, -2.2286e-05, -2.2273e-05, -2.0509e-05, -1.9174e-05, -1.7775e-05, -1.6919e-05, -1.5645e-05, -1.5212e-05, -1.4512e-05, -1.3763e-05, -1.3476e-05, -1.067e-05, -1.0326e-05, -8.9269e-06, -7.9653e-06, -7.486e-06, -5.6467e-06, -5.1338e-06, -3.0168e-06, -2.3089e-06, -2.1256e-06, 3.4967e-07, 6.0719e-07, 7.7338e-07, 3.8775e-06, 4.3058e-06, 4.5608e-06, 5.4546e-06, 5.5685e-06, 6.352e-06, 6.8678e-06, 6.973e-06, 8.4827e-06, 9.0398e-06, 9.9588e-06, 1.033e-05, 1.1336e-05, 1.3903e-05, 1.5578e-05, 1.5953e-05, 1.6021e-05, 1.7005e-05, 1.7663e-05, 1.9665e-05, 2.1085e-05, 2.2248e-05, 2.2413e-05, 2.4777e-05, 2.6061e-05, 2.8327e-05, 3.0671e-05, 3.2942e-05, 3.9968e-05, 4.0689e-05, 4.4126e-05, 4.4457e-05, 4.5198e-05, 4.647e-05, 4.8783e-05, 5.3665e-05, 5.9747e-05, 6.0305e-05, 6.561e-05, 7.0715e-05, 7.222e-05, 7.282e-05, 7.3554e-05, 7.5287e-05, 7.5972e-05, 7.6978e-05, 8.0151e-05, 8.7697e-05, 8.8921e-05, 9.1328e-05, 9.1917e-05, 9.7219e-05, 0.00010233, 0.00010441, 0.00010539, 0.00010729, 0.00011111, 0.00011441, 0.00012257, 0.00012474, 0.00013301, 0.00015106, 0.00015658, 0.00017277, 0.00018226, 0.00019817, 0.00022643, 0.00023202, 0.00026138, 0.00030479, 0.00031473, 0.00032131, 0.00034011, 0.00035992, 0.00036187, 0.00038753, 0.00041095, 0.00043693, 0.00047886, 0.00048937, 0.00052024, 0.00055268, 0.00058311, 0.00061038, 0.00071926, 0.00073679, 0.00075116, 0.00076003, 0.0008204, 0.00086621, 0.00089345, 0.00092779, 0.00095859, 0.0010066, 0.001151, 0.0011542, 0.001274, 0.0013775, 0.0015014, 0.0016071, 0.0017107, 0.0019653, 0.0020651, 0.0022506, 0.0023595, 0.0028175, 0.0030811, 0.003457]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 1, Epoch: 14, Step: 600\n",
      "    cos_sim = 0.333\n",
      "    win = [-0.18825, -0.019761, -0.0059279, -0.00064683, -0.00043912, -0.00012035, -2.9241e-06, -1.7289e-07, -1.3571e-07, -3.8981e-08, -3.134e-08, -6.5167e-09, -5.774e-09, -4.6935e-09, -3.0983e-09, -2.7139e-09, -2.4412e-09, -2.2993e-09, -2.0927e-09, -2.0569e-09, -1.9996e-09, -1.911e-09, -1.8302e-09, -1.8087e-09, -1.7662e-09, -1.73e-09, -1.659e-09, -1.6028e-09, -1.5809e-09, -1.5605e-09, -1.4386e-09, -1.4327e-09, -1.3957e-09, -1.3875e-09, -1.3714e-09, -1.3191e-09, -1.3082e-09, -1.2485e-09, -1.226e-09, -1.1659e-09, -1.1467e-09, -1.1244e-09, -1.1e-09, -1.0496e-09, -1.0281e-09, -1.024e-09, -9.8551e-10, -9.6626e-10, -9.4196e-10, -9.0372e-10, -8.8758e-10, -8.6761e-10, -8.6179e-10, -8.4714e-10, -8.2898e-10, -8.0729e-10, -7.9244e-10, -7.7156e-10, -7.4183e-10, -7.3027e-10, -6.7908e-10, -6.4497e-10, -6.3895e-10, -6.233e-10, -5.9896e-10, -5.9046e-10, -5.7741e-10, -5.6272e-10, -5.2658e-10, -5.1665e-10, -5.022e-10, -4.9138e-10, -4.8022e-10, -4.5585e-10, -4.2601e-10, -3.9521e-10, -3.7009e-10, -3.6261e-10, -3.3882e-10, -3.2132e-10, -2.8553e-10, -2.7391e-10, -2.593e-10, -2.4344e-10, -2.2527e-10, -2.0265e-10, -1.9669e-10, -1.6076e-10, -1.4327e-10, -1.2573e-10, -9.7523e-11, -7.8101e-11, -6.3236e-11, -4.8979e-11, -4.366e-11, -1.3728e-11, 3.5483e-12, 1.788e-11, 2.8399e-11, 6.2237e-11, 7.6967e-11, 8.886e-11, 1.028e-10, 1.1894e-10, 1.3369e-10, 1.6466e-10, 1.936e-10, 2.0847e-10, 2.1343e-10, 2.3361e-10, 2.4764e-10, 2.8539e-10, 2.9564e-10, 3.0284e-10, 3.2443e-10, 3.5171e-10, 3.7344e-10, 3.7594e-10, 3.9865e-10, 4.0851e-10, 4.2499e-10, 4.4722e-10, 4.8724e-10, 5.0682e-10, 5.1904e-10, 5.2493e-10, 5.5367e-10, 5.8682e-10, 5.9315e-10, 6.1598e-10, 6.3406e-10, 6.6107e-10, 6.7297e-10, 7.0594e-10, 7.2846e-10, 7.4354e-10, 7.4829e-10, 7.9813e-10, 8.0319e-10, 8.2089e-10, 8.3462e-10, 8.6659e-10, 8.7803e-10, 8.945e-10, 9.2332e-10, 9.5419e-10, 9.7414e-10, 1.0172e-09, 1.0619e-09, 1.0798e-09, 1.0874e-09, 1.1227e-09, 1.158e-09, 1.2061e-09, 1.2315e-09, 1.262e-09, 1.2891e-09, 1.302e-09, 1.3356e-09, 1.3558e-09, 1.4242e-09, 1.4312e-09, 1.4699e-09, 1.5007e-09, 1.5361e-09, 1.5907e-09, 1.6302e-09, 1.6381e-09, 1.7431e-09, 1.7903e-09, 1.8295e-09, 1.8516e-09, 1.8844e-09, 1.9892e-09, 2.1079e-09, 2.1973e-09, 2.3145e-09, 2.4716e-09, 2.9185e-09, 3.3639e-09, 3.7714e-09, 5.1288e-09, 9.0664e-09, 3.1472e-08, 4.2412e-08, 5.7608e-08, 1.7592e-07, 2.0988e-07, 2.7271e-07, 1.1376e-06, 7.9767e-06, 1.624e-05, 6.5545e-05, 0.00016054, 0.0011287, 0.0015902, 0.0047406, 0.014406, 0.025491, 0.040551]\n",
      "    wout = [-0.013497, -0.010876, -0.0097611, -0.0093558, -0.0085201, -0.0075358, -0.0066368, -0.0061292, -0.0055883, -0.0055236, -0.0053214, -0.0051423, -0.004778, -0.0044406, -0.0040669, -0.0036149, -0.0035494, -0.0035072, -0.0034173, -0.0031278, -0.0029932, -0.0029821, -0.0029463, -0.0028395, -0.0027919, -0.0027354, -0.0026578, -0.0026089, -0.0025434, -0.0025266, -0.0024635, -0.0024274, -0.002408, -0.0022659, -0.0021888, -0.0020226, -0.0019986, -0.001919, -0.0018493, -0.0018251, -0.0017769, -0.001744, -0.0017304, -0.0017031, -0.0016001, -0.0014756, -0.0013942, -0.0013393, -0.0013187, -0.0012577, -0.0012253, -0.0011912, -0.0011247, -0.0010951, -0.0010834, -0.0010477, -0.0010133, -0.00095895, -0.00090587, -0.00086619, -0.00085744, -0.00083746, -0.00076656, -0.00069623, -0.00067833, -0.00067674, -0.00066387, -0.00063113, -0.00060385, -0.00055131, -0.00045631, -0.00043846, -0.00041396, -0.00037308, -0.00034223, -0.00031561, -0.00027362, -0.00025261, -0.00022181, -0.00020373, -0.00019996, -0.00018054, -0.0001533, -0.00012928, -0.00011849, -0.00011289, -0.00010866, -9.7756e-05, -9.0622e-05, -7.0954e-05, -6.1218e-05, -5.6398e-05, -4.6163e-05, -2.9974e-05, -1.8171e-05, -1.3375e-05, -3.5616e-06, -8.502e-07, 2.8056e-06, 1.468e-05, 1.8824e-05, 2.5138e-05, 2.7105e-05, 3.2137e-05, 4.4479e-05, 4.9432e-05, 6.4048e-05, 6.9296e-05, 9.2615e-05, 0.00010617, 0.00010888, 0.00012764, 0.00013961, 0.00015828, 0.00018148, 0.00019129, 0.00019754, 0.00021618, 0.00022181, 0.00022558, 0.00029005, 0.00031744, 0.00032317, 0.00037833, 0.00043866, 0.00044485, 0.00044897, 0.00046445, 0.00047185, 0.00047946, 0.00052599, 0.00057537, 0.00060052, 0.00062092, 0.0006499, 0.00067308, 0.00072337, 0.0007338, 0.00075771, 0.00077562, 0.00080414, 0.00081744, 0.00083691, 0.00084808, 0.00090156, 0.00091656, 0.00096289, 0.00099559, 0.0010343, 0.0010374, 0.0011179, 0.0011649, 0.0011728, 0.0012306, 0.0012601, 0.0013304, 0.0013352, 0.0013645, 0.0014525, 0.0015605, 0.0015941, 0.0016664, 0.0016684, 0.0016792, 0.0017013, 0.0017516, 0.001762, 0.0018279, 0.0018964, 0.0019683, 0.0020559, 0.002067, 0.0020813, 0.002118, 0.002136, 0.0021458, 0.0022453, 0.0023614, 0.0024779, 0.0025418, 0.0025961, 0.0026247, 0.0026783, 0.0029436, 0.00313, 0.0031776, 0.0033217, 0.0034109, 0.0036117, 0.003946, 0.0040459, 0.0048177, 0.0054555, 0.0058053, 0.0061816, 0.0069657, 0.0072068, 0.009095, 0.0096646, 0.010106]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.923\n",
      "    win = [-0.07978, -0.0091269, -0.0073339, -0.0037204, -0.0019919, -0.0012869, -0.0010711, -0.00079427, -0.00055492, -0.00044865, -0.00035431, -0.00029043, -0.00026885, -0.00021411, -0.0002021, -0.00018951, -0.00015467, -0.00013304, -0.00011588, -0.00010508, -9.2557e-05, -8.2994e-05, -7.003e-05, -6.5704e-05, -5.9252e-05, -5.367e-05, -4.213e-05, -4.0939e-05, -3.6715e-05, -3.4081e-05, -2.9161e-05, -2.5092e-05, -2.3344e-05, -2.0452e-05, -1.8513e-05, -1.6632e-05, -1.4068e-05, -1.364e-05, -1.2017e-05, -1.0608e-05, -8.7405e-06, -8.0522e-06, -6.9616e-06, -6.0945e-06, -5.4784e-06, -4.4015e-06, -3.7753e-06, -3.4819e-06, -3.0744e-06, -2.7163e-06, -2.4653e-06, -1.9262e-06, -1.6503e-06, -1.4934e-06, -1.2246e-06, -1.21e-06, -9.6374e-07, -9.0425e-07, -8.0971e-07, -7.5308e-07, -6.1937e-07, -3.8978e-07, -2.9964e-07, -2.4436e-07, -2.1552e-07, -1.5813e-07, -1.4458e-07, -1.1694e-07, -1.0162e-07, -9.4824e-08, -6.4225e-08, -5.3936e-08, -5.2644e-08, -4.0154e-08, -2.9718e-08, -2.2458e-08, -2.0525e-08, -1.6858e-08, -1.1125e-08, -1.0333e-08, -8.4779e-09, -5.4871e-09, -5.173e-09, -3.5725e-09, -1.7387e-09, -1.4167e-09, -1.0812e-09, -7.0695e-10, -5.4634e-10, -3.9543e-10, -3.264e-10, -3.1602e-10, -2.9922e-10, -2.3256e-10, -2.1225e-10, -1.7103e-10, -1.4826e-10, -1.2689e-10, -1.0333e-10, -8.3554e-11, -6.9538e-11, -4.0785e-11, -1.6989e-11, 9.7216e-14, 1.3607e-11, 6.8113e-11, 8.304e-11, 1.096e-10, 1.4135e-10, 1.7592e-10, 1.9481e-10, 2.1482e-10, 2.7355e-10, 2.8639e-10, 3.1854e-10, 3.4811e-10, 3.6593e-10, 5.7118e-10, 8.1425e-10, 9.5676e-10, 1.4199e-09, 1.8213e-09, 2.6663e-09, 3.2625e-09, 3.9714e-09, 4.8439e-09, 5.3435e-09, 7.3776e-09, 9.2897e-09, 1.2329e-08, 1.316e-08, 1.7897e-08, 1.9887e-08, 2.682e-08, 2.7911e-08, 3.4383e-08, 3.8668e-08, 5.1473e-08, 5.5023e-08, 8.483e-08, 9.6474e-08, 1.1069e-07, 1.4407e-07, 1.705e-07, 1.977e-07, 2.1186e-07, 2.566e-07, 3.107e-07, 3.3947e-07, 4.0049e-07, 5.0856e-07, 7.5585e-07, 7.8466e-07, 8.1325e-07, 1.0564e-06, 1.375e-06, 1.487e-06, 1.6752e-06, 2.0522e-06, 2.25e-06, 2.7874e-06, 3.2091e-06, 4.2362e-06, 4.6743e-06, 4.8737e-06, 5.4855e-06, 6.4972e-06, 7.3707e-06, 7.7398e-06, 8.4706e-06, 1.1781e-05, 1.6846e-05, 1.7427e-05, 1.9326e-05, 2.476e-05, 2.7514e-05, 2.9529e-05, 3.6666e-05, 3.791e-05, 4.7071e-05, 5.1627e-05, 5.6001e-05, 7.1557e-05, 8.7929e-05, 0.00013509, 0.00015472, 0.00019338, 0.00020774, 0.00025133, 0.00040547, 0.00047732, 0.00062859, 0.00078453, 0.00095759, 0.0013817, 0.0018285, 0.0020064, 0.0024703, 0.0040693, 0.0084846]\n",
      "    wout = [-0.0005407, -0.00025141, -0.00016777, -0.00013992, -0.00012848, -0.00011633, -9.1331e-05, -8.8547e-05, -8.3557e-05, -8.0304e-05, -7.545e-05, -6.6782e-05, -5.9494e-05, -5.7025e-05, -4.9935e-05, -4.9014e-05, -4.7183e-05, -4.0252e-05, -3.6889e-05, -3.4811e-05, -3.4623e-05, -3.1642e-05, -3.142e-05, -2.9891e-05, -2.9074e-05, -2.8109e-05, -2.7663e-05, -2.729e-05, -2.6406e-05, -2.6048e-05, -2.5837e-05, -2.5023e-05, -2.4012e-05, -2.3546e-05, -2.2811e-05, -2.1698e-05, -2.1504e-05, -2.0505e-05, -2.0243e-05, -1.9662e-05, -1.9478e-05, -1.9208e-05, -1.8318e-05, -1.7837e-05, -1.7716e-05, -1.6296e-05, -1.5746e-05, -1.4002e-05, -1.329e-05, -1.2284e-05, -1.2211e-05, -1.0923e-05, -1.0508e-05, -1.0249e-05, -9.9322e-06, -9.3013e-06, -8.8437e-06, -8.5216e-06, -8.0355e-06, -6.8963e-06, -6.4023e-06, -6.0697e-06, -5.7224e-06, -5.6422e-06, -5.0414e-06, -4.7296e-06, -4.3154e-06, -3.6016e-06, -2.9887e-06, -2.8513e-06, -2.7715e-06, -2.5388e-06, -2.4946e-06, -2.2155e-06, -1.482e-06, -1.4107e-06, -1.0979e-06, -7.9574e-07, -7.2594e-07, -7.2335e-07, -6.8816e-07, -5.8739e-07, -4.2209e-07, -3.5633e-07, -3.1546e-07, -2.9816e-07, -2.5169e-07, -1.4175e-07, -1.0518e-09, -4.3862e-11, 6.1256e-10, 5.8541e-09, 7.3338e-09, 2.1214e-08, 2.186e-08, 3.6413e-08, 4.7143e-08, 1.2644e-07, 1.6155e-07, 2.3539e-07, 2.3605e-07, 2.5328e-07, 2.6252e-07, 2.7875e-07, 3.1023e-07, 3.5299e-07, 4.1025e-07, 4.3851e-07, 4.6722e-07, 5.3159e-07, 6.2182e-07, 7.2583e-07, 7.798e-07, 9.9949e-07, 1.0668e-06, 1.2763e-06, 1.312e-06, 1.455e-06, 1.506e-06, 1.5139e-06, 1.5718e-06, 1.5942e-06, 1.7958e-06, 1.9486e-06, 2.2423e-06, 2.5916e-06, 2.6633e-06, 2.9368e-06, 3.0921e-06, 3.24e-06, 3.405e-06, 3.5154e-06, 4.3199e-06, 4.5341e-06, 4.976e-06, 5.5555e-06, 5.674e-06, 6.5627e-06, 6.6277e-06, 6.9e-06, 7.1426e-06, 7.4238e-06, 7.5889e-06, 7.7114e-06, 8.1071e-06, 8.7367e-06, 1.0023e-05, 1.0068e-05, 1.0472e-05, 1.0756e-05, 1.1397e-05, 1.2527e-05, 1.2927e-05, 1.301e-05, 1.4317e-05, 1.513e-05, 1.6185e-05, 1.6482e-05, 1.705e-05, 1.7369e-05, 1.7732e-05, 1.7766e-05, 1.8116e-05, 1.9079e-05, 1.9929e-05, 2.1796e-05, 2.4762e-05, 2.5469e-05, 2.6212e-05, 2.6222e-05, 2.7247e-05, 2.8134e-05, 3.0309e-05, 3.1577e-05, 3.2012e-05, 3.2976e-05, 3.3689e-05, 3.4986e-05, 3.8579e-05, 3.9552e-05, 4.2934e-05, 4.6896e-05, 4.7646e-05, 4.9286e-05, 5.3427e-05, 5.6706e-05, 5.8184e-05, 6.0542e-05, 6.3966e-05, 6.8317e-05, 7.0529e-05, 8.082e-05, 8.723e-05, 9.9796e-05, 0.00011258, 0.00013058, 0.00014344, 0.0001794, 0.00020706, 0.00025632]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: longer, Seed: 3, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.913\n",
      "    win = [-0.010239, -0.0051095, -0.0031797, -0.0018889, -0.0015304, -0.0011382, -0.0010522, -0.000852, -0.00063773, -0.00061642, -0.00054035, -0.00034809, -0.00020406, -0.00015463, -0.00014377, -0.00011097, -8.65e-05, -7.675e-05, -7.0135e-05, -6.7291e-05, -5.6376e-05, -4.5343e-05, -4.1546e-05, -3.7387e-05, -3.2228e-05, -3.1419e-05, -2.8585e-05, -2.4948e-05, -2.3047e-05, -2.0476e-05, -1.898e-05, -1.7792e-05, -1.6612e-05, -1.5424e-05, -1.2202e-05, -1.1802e-05, -1.0799e-05, -9.8646e-06, -8.7893e-06, -7.8184e-06, -7.4525e-06, -6.3361e-06, -5.9873e-06, -5.2764e-06, -4.949e-06, -4.7158e-06, -4.083e-06, -3.3193e-06, -2.8787e-06, -2.2555e-06, -2.1351e-06, -2.0477e-06, -1.6818e-06, -1.3531e-06, -1.1399e-06, -1.0006e-06, -8.126e-07, -7.7283e-07, -6.1691e-07, -5.0726e-07, -4.8269e-07, -4.6426e-07, -4.0457e-07, -3.6009e-07, -3.2631e-07, -3.006e-07, -2.5485e-07, -2.2391e-07, -2.1361e-07, -1.9531e-07, -1.4156e-07, -1.1967e-07, -8.8697e-08, -7.7758e-08, -6.7424e-08, -5.1081e-08, -4.6774e-08, -4.1768e-08, -3.519e-08, -3.2036e-08, -2.073e-08, -1.4659e-08, -1.1944e-08, -9.4543e-09, -7.9995e-09, -6.8988e-09, -6.2201e-09, -4.9557e-09, -3.8008e-09, -3.5132e-09, -3.0125e-09, -2.213e-09, -1.5721e-09, -1.0795e-09, -8.4887e-10, -6.7759e-10, -5.9082e-10, -4.3407e-10, -2.1842e-10, -1.7918e-10, -1.2843e-10, -6.0252e-11, -4.1265e-11, -1.0648e-11, 2.194e-11, 9.0879e-11, 1.2531e-10, 1.7562e-10, 3.4176e-10, 4.6213e-10, 7.1813e-10, 9.0061e-10, 1.3577e-09, 1.6734e-09, 2.4884e-09, 3.0393e-09, 3.4119e-09, 6.2087e-09, 6.8648e-09, 8.7297e-09, 9.1435e-09, 1.4007e-08, 1.7874e-08, 2.1112e-08, 2.333e-08, 3.3965e-08, 3.8132e-08, 4.7141e-08, 5.349e-08, 5.796e-08, 6.8113e-08, 7.7969e-08, 8.4274e-08, 9.1526e-08, 1.0237e-07, 1.4527e-07, 1.647e-07, 2.6288e-07, 2.8117e-07, 3.6322e-07, 4.1904e-07, 4.6871e-07, 6.2446e-07, 6.6698e-07, 7.554e-07, 9.8092e-07, 1.3501e-06, 1.4683e-06, 1.5951e-06, 1.9363e-06, 2.186e-06, 2.559e-06, 3.6548e-06, 3.9463e-06, 4.1638e-06, 4.7486e-06, 5.5305e-06, 5.8673e-06, 7.861e-06, 9.4806e-06, 1.0986e-05, 1.3421e-05, 1.5653e-05, 1.645e-05, 1.9581e-05, 2.0051e-05, 2.3266e-05, 2.8011e-05, 3.6346e-05, 3.9686e-05, 4.1129e-05, 4.346e-05, 4.5967e-05, 5.3147e-05, 5.8883e-05, 6.257e-05, 8.7591e-05, 9.4342e-05, 0.00010533, 0.00011442, 0.00012419, 0.00016665, 0.00018418, 0.00018971, 0.0002237, 0.0002882, 0.00034783, 0.00037812, 0.00046968, 0.00052921, 0.00055852, 0.00084235, 0.00091877, 0.0010597, 0.0013715, 0.0014972, 0.0025631, 0.0037019, 0.0086029, 0.026663]\n",
      "    wout = [-0.0012236, -0.00077022, -0.00065888, -0.00043043, -0.00042538, -0.00041754, -0.00036764, -0.00036228, -0.00030932, -0.00030185, -0.00029545, -0.00027749, -0.00026512, -0.00023378, -0.000223, -0.00021175, -0.00019482, -0.00018606, -0.00017054, -0.00016749, -0.00016427, -0.00016104, -0.00014629, -0.00012561, -0.00012522, -0.00011935, -0.00011481, -0.00011348, -0.00011001, -9.2053e-05, -8.9415e-05, -8.4337e-05, -8.2244e-05, -8.1087e-05, -7.9351e-05, -7.4242e-05, -6.8603e-05, -6.317e-05, -6.213e-05, -5.5816e-05, -5.4297e-05, -5.2492e-05, -4.7476e-05, -4.6651e-05, -4.5182e-05, -4.2891e-05, -4.061e-05, -3.9823e-05, -3.8728e-05, -3.8097e-05, -3.6685e-05, -3.5167e-05, -3.4946e-05, -3.2307e-05, -3.1681e-05, -3.1019e-05, -3.0206e-05, -2.9526e-05, -2.6232e-05, -2.3046e-05, -2.1639e-05, -2.1137e-05, -2.0229e-05, -1.8184e-05, -1.7893e-05, -1.4863e-05, -1.3399e-05, -1.0867e-05, -1.0199e-05, -9.9444e-06, -9.6619e-06, -9.2625e-06, -8.4152e-06, -5.6701e-06, -4.3876e-06, -4.2436e-06, -3.9222e-06, -3.6273e-06, -3.3747e-06, -2.1987e-06, -1.9741e-06, -1.885e-06, -1.8819e-06, -1.8224e-06, -1.4675e-06, -1.3086e-06, -1.1561e-06, -8.3592e-07, -6.0018e-07, -4.6796e-07, -2.5376e-07, -2.0231e-07, -1.256e-07, -5.4301e-08, 6.0789e-09, 1.7261e-07, 2.7682e-07, 3.4107e-07, 9.8224e-07, 1.1158e-06, 1.2839e-06, 1.7902e-06, 2.1158e-06, 2.2757e-06, 3.1094e-06, 3.9778e-06, 4.2124e-06, 4.5034e-06, 5.6563e-06, 6.0054e-06, 6.2819e-06, 6.9883e-06, 8.5205e-06, 9.9742e-06, 1.1584e-05, 1.1975e-05, 1.2456e-05, 1.2911e-05, 1.5149e-05, 1.5298e-05, 1.5471e-05, 1.6547e-05, 1.7382e-05, 1.8225e-05, 2.0441e-05, 2.0814e-05, 2.2002e-05, 2.4149e-05, 2.4388e-05, 2.5997e-05, 2.8279e-05, 2.8997e-05, 3.0198e-05, 3.1657e-05, 3.2401e-05, 3.3024e-05, 3.5481e-05, 3.6038e-05, 3.737e-05, 3.7937e-05, 3.831e-05, 4.1136e-05, 4.1785e-05, 4.3092e-05, 4.5479e-05, 4.8075e-05, 4.858e-05, 5.0178e-05, 5.0504e-05, 5.1189e-05, 5.2581e-05, 5.3958e-05, 5.6142e-05, 5.7366e-05, 5.9172e-05, 6.0436e-05, 6.1574e-05, 6.477e-05, 6.6714e-05, 7.2875e-05, 7.5081e-05, 7.5523e-05, 7.9376e-05, 8.2711e-05, 8.7834e-05, 8.9908e-05, 9.7637e-05, 9.8705e-05, 0.00010343, 0.00011311, 0.00011377, 0.00012689, 0.00013518, 0.00015085, 0.00016317, 0.00016817, 0.00017389, 0.00019195, 0.0002133, 0.00023083, 0.00023858, 0.00025076, 0.00025608, 0.00026246, 0.00028259, 0.00029093, 0.00029516, 0.00033361, 0.00033771, 0.00034733, 0.00037185, 0.00040264, 0.00044408, 0.00048809, 0.00051611, 0.00062808, 0.00068459, 0.00073356, 0.00093583, 0.0014333]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 1, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.655\n",
      "    win = [-0.0063395, -0.0010356, -0.00049996, -0.00044066, -0.00034224, -0.00022165, -0.00011455, -8.5081e-05, -6.1155e-05, -5.9094e-05, -4.1129e-05, -3.8978e-05, -3.7218e-05, -3.3906e-05, -2.8974e-05, -2.5416e-05, -2.28e-05, -2.1662e-05, -1.6131e-05, -1.2228e-05, -1.0611e-05, -9.693e-06, -8.0185e-06, -6.8752e-06, -6.5663e-06, -4.9179e-06, -4.4192e-06, -3.7371e-06, -3.4602e-06, -3.2127e-06, -2.7898e-06, -2.4899e-06, -2.2435e-06, -1.8401e-06, -1.7716e-06, -1.7132e-06, -1.4024e-06, -1.312e-06, -1.1833e-06, -1.1591e-06, -1.004e-06, -9.5207e-07, -9.0001e-07, -7.4919e-07, -7.0891e-07, -6.6907e-07, -5.5443e-07, -5.0296e-07, -4.3765e-07, -4.137e-07, -3.488e-07, -3.3042e-07, -3.0724e-07, -2.671e-07, -2.6296e-07, -2.3706e-07, -2.1147e-07, -1.7895e-07, -1.7134e-07, -1.6398e-07, -1.3288e-07, -1.2314e-07, -1.0376e-07, -9.7869e-08, -9.0689e-08, -8.1225e-08, -7.1353e-08, -7.0607e-08, -5.6367e-08, -5.215e-08, -4.4117e-08, -4.2468e-08, -3.7142e-08, -3.245e-08, -3.0787e-08, -2.5417e-08, -2.1461e-08, -1.9044e-08, -1.6204e-08, -1.5195e-08, -1.2882e-08, -1.0645e-08, -9.3839e-09, -8.3366e-09, -6.6798e-09, -5.1631e-09, -3.8966e-09, -2.9877e-09, -2.6283e-09, -1.8365e-09, -1.4864e-09, -1.0667e-09, -7.0105e-10, -4.5862e-10, -3.6472e-10, -1.6114e-10, -5.7581e-11, 9.0142e-11, 1.9128e-10, 4.1013e-10, 5.6797e-10, 7.6892e-10, 1.2397e-09, 1.7011e-09, 2.1857e-09, 2.325e-09, 2.8142e-09, 3.5058e-09, 4.6618e-09, 5.1403e-09, 7.0835e-09, 7.9431e-09, 8.7423e-09, 9.7357e-09, 1.1782e-08, 1.3558e-08, 1.4003e-08, 1.7069e-08, 1.8006e-08, 2.3492e-08, 2.7787e-08, 3.1291e-08, 3.8639e-08, 4.516e-08, 4.8354e-08, 6.2438e-08, 6.8176e-08, 7.437e-08, 7.5143e-08, 8.4872e-08, 8.9285e-08, 9.8253e-08, 1.2393e-07, 1.3162e-07, 1.5174e-07, 1.6475e-07, 1.7528e-07, 2.0225e-07, 2.2388e-07, 2.6559e-07, 2.7943e-07, 3.1627e-07, 3.7563e-07, 4.1973e-07, 4.839e-07, 5.0654e-07, 5.8029e-07, 5.9361e-07, 7.3898e-07, 8.4322e-07, 8.9647e-07, 1.0829e-06, 1.1247e-06, 1.3423e-06, 1.3796e-06, 1.6133e-06, 1.7116e-06, 2.2079e-06, 2.3013e-06, 2.484e-06, 2.845e-06, 3.123e-06, 3.3718e-06, 3.8419e-06, 4.4013e-06, 4.6725e-06, 4.8953e-06, 6.8235e-06, 7.5486e-06, 7.9998e-06, 8.0887e-06, 9.2363e-06, 1.0172e-05, 1.2394e-05, 1.3104e-05, 1.4516e-05, 1.6942e-05, 1.8522e-05, 2.2496e-05, 2.3355e-05, 2.597e-05, 2.869e-05, 3.0666e-05, 3.9069e-05, 4.2227e-05, 4.6536e-05, 5.8138e-05, 6.1763e-05, 7.6124e-05, 8.6587e-05, 0.00012287, 0.00015674, 0.00018159, 0.000248, 0.00026589, 0.00046261, 0.00075895, 0.0018878, 0.002612, 0.015964]\n",
      "    wout = [-0.00052568, -0.00042119, -0.00028396, -0.00027856, -0.00023707, -0.00021087, -0.00018214, -0.00017422, -0.00016101, -0.00015248, -0.00014568, -0.00013794, -0.0001227, -0.00011875, -0.00011434, -0.0001115, -0.00010412, -9.7515e-05, -9.5252e-05, -9.3189e-05, -8.4432e-05, -8.1524e-05, -7.8931e-05, -7.6843e-05, -7.6212e-05, -7.5338e-05, -7.3176e-05, -6.6758e-05, -6.4295e-05, -5.8917e-05, -5.637e-05, -5.5323e-05, -4.9877e-05, -4.8806e-05, -4.5698e-05, -4.4766e-05, -4.2851e-05, -4.1357e-05, -4.0101e-05, -3.6792e-05, -3.1353e-05, -2.4393e-05, -2.2826e-05, -2.2429e-05, -1.9747e-05, -1.8771e-05, -1.7221e-05, -1.7042e-05, -1.674e-05, -1.603e-05, -1.3096e-05, -1.2141e-05, -1.0248e-05, -9.8289e-06, -8.5781e-06, -8.3145e-06, -7.906e-06, -6.913e-06, -6.702e-06, -6.634e-06, -6.2298e-06, -6.0019e-06, -5.3203e-06, -4.8315e-06, -4.3995e-06, -4.3248e-06, -4.1492e-06, -3.8578e-06, -3.7682e-06, -3.5394e-06, -3.4264e-06, -3.3713e-06, -3.2301e-06, -2.9627e-06, -2.7305e-06, -2.2579e-06, -2.0814e-06, -1.962e-06, -1.749e-06, -1.6823e-06, -1.4947e-06, -1.4466e-06, -1.4077e-06, -1.3656e-06, -1.1653e-06, -1.0305e-06, -1.0164e-06, -7.2926e-07, -6.977e-07, -4.278e-07, -2.9693e-07, -1.471e-07, -6.8666e-09, 1.4766e-07, 2.4065e-07, 3.2961e-07, 3.9445e-07, 5.2423e-07, 6.1067e-07, 7.4269e-07, 1.1029e-06, 1.2106e-06, 1.3249e-06, 1.3675e-06, 1.524e-06, 1.7956e-06, 2.0364e-06, 2.4422e-06, 2.7295e-06, 2.8421e-06, 2.8718e-06, 2.9256e-06, 2.9525e-06, 3.0852e-06, 3.1809e-06, 3.2662e-06, 3.3033e-06, 3.4177e-06, 3.6891e-06, 3.8135e-06, 4.4602e-06, 4.5282e-06, 4.89e-06, 4.9793e-06, 5.3882e-06, 5.5031e-06, 5.8186e-06, 5.9759e-06, 6.0452e-06, 6.1936e-06, 6.8342e-06, 7.79e-06, 8.7394e-06, 9.4011e-06, 1.0045e-05, 1.0255e-05, 1.1395e-05, 1.2363e-05, 1.246e-05, 1.899e-05, 2.0939e-05, 2.1337e-05, 2.2002e-05, 2.2456e-05, 2.3247e-05, 2.4349e-05, 2.6273e-05, 2.6713e-05, 2.7528e-05, 2.7764e-05, 2.9663e-05, 3.048e-05, 3.1125e-05, 3.1582e-05, 3.447e-05, 3.6286e-05, 4.1427e-05, 4.4418e-05, 4.563e-05, 4.7678e-05, 4.9343e-05, 5.1371e-05, 5.4465e-05, 5.5963e-05, 6.0671e-05, 6.4032e-05, 6.4693e-05, 6.6253e-05, 6.7723e-05, 6.9174e-05, 7.3366e-05, 7.9987e-05, 8.0501e-05, 8.0907e-05, 9.0103e-05, 9.3138e-05, 9.6648e-05, 9.953e-05, 0.00010062, 0.0001018, 0.00010211, 0.00010693, 0.000111, 0.00011829, 0.00011922, 0.00012208, 0.00012286, 0.00012694, 0.00012755, 0.00012909, 0.00014305, 0.00018018, 0.00021444, 0.00022846, 0.00024725, 0.00026849, 0.00027534, 0.00031634, 0.00045769, 0.00052078]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 1, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.377\n",
      "    win = [-0.0072145, -0.0032735, -0.0025433, -0.0013208, -0.00079739, -0.00040323, -0.00030897, -0.00025094, -0.00021776, -0.00019134, -0.00015383, -0.00012014, -0.00011358, -9.493e-05, -8.6469e-05, -7.4125e-05, -6.1935e-05, -5.831e-05, -5.1311e-05, -4.4296e-05, -3.7248e-05, -3.3269e-05, -3.0761e-05, -2.7259e-05, -2.4804e-05, -2.0011e-05, -1.8513e-05, -1.7877e-05, -1.5668e-05, -1.3539e-05, -1.2142e-05, -1.109e-05, -1.0746e-05, -8.6985e-06, -8.355e-06, -7.4969e-06, -6.8661e-06, -6.2013e-06, -5.1682e-06, -4.2624e-06, -3.9758e-06, -3.7311e-06, -3.404e-06, -2.9783e-06, -2.7056e-06, -2.6504e-06, -2.346e-06, -2.1808e-06, -1.8474e-06, -1.5581e-06, -1.4033e-06, -1.3186e-06, -1.0429e-06, -9.6165e-07, -8.331e-07, -7.394e-07, -6.2521e-07, -5.7028e-07, -5.1201e-07, -4.4349e-07, -3.8794e-07, -3.5431e-07, -3.1154e-07, -2.7574e-07, -2.5195e-07, -2.1136e-07, -1.9916e-07, -1.8557e-07, -1.4013e-07, -1.2482e-07, -1.1571e-07, -1.025e-07, -7.6392e-08, -6.3981e-08, -5.9266e-08, -5.5182e-08, -5.0007e-08, -4.3873e-08, -2.8077e-08, -2.5396e-08, -2.1407e-08, -1.7749e-08, -1.5917e-08, -1.3682e-08, -1.2072e-08, -8.7851e-09, -5.5961e-09, -5.3301e-09, -3.1483e-09, -2.5238e-09, -1.9817e-09, -4.898e-10, -1.9832e-10, -1.2243e-10, 1.8446e-10, 7.8087e-10, 1.2952e-09, 2.1282e-09, 2.6078e-09, 4.1946e-09, 4.7876e-09, 7.148e-09, 8.1325e-09, 1.1043e-08, 1.3133e-08, 1.6897e-08, 1.9085e-08, 2.1641e-08, 2.3263e-08, 2.9861e-08, 3.1616e-08, 4.1609e-08, 4.2984e-08, 4.8571e-08, 4.951e-08, 5.626e-08, 6.7475e-08, 7.2493e-08, 8.093e-08, 8.8929e-08, 9.4399e-08, 1.0234e-07, 1.3568e-07, 1.4709e-07, 1.6015e-07, 1.751e-07, 1.8411e-07, 2.1412e-07, 2.1649e-07, 2.5947e-07, 2.7207e-07, 2.8961e-07, 3.2328e-07, 3.8412e-07, 4.4009e-07, 4.7952e-07, 5.0011e-07, 5.769e-07, 6.1261e-07, 6.5776e-07, 7.6216e-07, 9.0686e-07, 9.9022e-07, 1.1325e-06, 1.1896e-06, 1.3313e-06, 1.4335e-06, 1.6192e-06, 1.7324e-06, 1.813e-06, 1.9515e-06, 2.1319e-06, 2.2887e-06, 2.6297e-06, 2.787e-06, 3.0923e-06, 3.307e-06, 3.7098e-06, 4.0131e-06, 4.1999e-06, 4.7511e-06, 5.0466e-06, 5.5495e-06, 6.0699e-06, 6.9674e-06, 7.2942e-06, 7.8514e-06, 8.4687e-06, 9.3254e-06, 1.0031e-05, 1.0347e-05, 1.1006e-05, 1.2728e-05, 1.4316e-05, 1.5399e-05, 1.6487e-05, 1.8917e-05, 1.9991e-05, 2.1565e-05, 2.3977e-05, 3.1489e-05, 3.5138e-05, 3.9539e-05, 4.2644e-05, 5.2276e-05, 5.8669e-05, 7.0132e-05, 7.3618e-05, 8.5591e-05, 0.00010302, 0.00011833, 0.00014187, 0.00019882, 0.00021166, 0.00030385, 0.00051702, 0.00089658, 0.0015434, 0.0088581, 0.021677]\n",
      "    wout = [-0.0012519, -0.0011937, -0.0010089, -0.00093027, -0.00088287, -0.00083882, -0.00078744, -0.00073104, -0.00071908, -0.00064863, -0.00057095, -0.00055021, -0.00054338, -0.00052228, -0.00051086, -0.00049425, -0.00046835, -0.00044968, -0.00043957, -0.00042756, -0.00041344, -0.00040636, -0.00038973, -0.00036787, -0.00036063, -0.0003467, -0.000337, -0.00033233, -0.00030662, -0.00029577, -0.00028056, -0.00027339, -0.00027089, -0.00025767, -0.00025019, -0.00024044, -0.00022851, -0.00021671, -0.00020822, -0.0002003, -0.00019661, -0.00015937, -0.0001559, -0.00014331, -0.00012987, -0.00012087, -0.00011139, -0.00010672, -0.00010048, -8.6976e-05, -8.2792e-05, -7.872e-05, -6.3397e-05, -5.2446e-05, -5.1547e-05, -4.6956e-05, -4.4561e-05, -4.3432e-05, -3.9785e-05, -3.6077e-05, -3.4509e-05, -3.3599e-05, -3.3283e-05, -3.1901e-05, -2.7948e-05, -2.7597e-05, -2.6453e-05, -2.3141e-05, -2.2689e-05, -2.0387e-05, -2.0283e-05, -1.9159e-05, -1.8936e-05, -1.7941e-05, -1.7322e-05, -1.5323e-05, -1.2681e-05, -1.2345e-05, -9.7043e-06, -8.9157e-06, -8.1475e-06, -6.9182e-06, -6.5337e-06, -6.4242e-06, -6.0054e-06, -5.8143e-06, -5.5125e-06, -5.3249e-06, -4.7745e-06, -4.4879e-06, -4.4695e-06, -4.3302e-06, -4.1441e-06, -4.0837e-06, -4.0353e-06, -3.9984e-06, -3.1986e-06, -2.7617e-06, -2.5196e-06, -2.2935e-06, -2.2231e-06, -2.1762e-06, -2.0794e-06, -1.813e-06, -1.4928e-06, -1.3478e-06, -5.0293e-07, -4.3849e-07, -4.2907e-07, -2.4702e-07, -1.5736e-07, -1.1429e-07, 4.9318e-08, 2.8128e-07, 4.3563e-07, 9.6073e-07, 1.5817e-06, 2.2086e-06, 2.8437e-06, 4.2846e-06, 5.36e-06, 7.2209e-06, 9.548e-06, 1.0595e-05, 1.1693e-05, 1.2787e-05, 1.3722e-05, 1.3949e-05, 1.4281e-05, 1.4406e-05, 1.596e-05, 1.8021e-05, 1.9759e-05, 2.1016e-05, 2.2135e-05, 2.2541e-05, 2.3007e-05, 2.3775e-05, 2.3872e-05, 2.5663e-05, 2.7198e-05, 3.0134e-05, 3.3055e-05, 3.3162e-05, 3.4126e-05, 3.6413e-05, 3.6516e-05, 3.8738e-05, 3.9873e-05, 4.3766e-05, 4.5505e-05, 4.639e-05, 4.7431e-05, 5.0227e-05, 5.0367e-05, 5.2965e-05, 5.3936e-05, 5.5929e-05, 5.763e-05, 6.2474e-05, 7.0483e-05, 7.195e-05, 7.5503e-05, 7.7977e-05, 7.8928e-05, 8.5201e-05, 9.8698e-05, 0.00010324, 0.00010489, 0.00012095, 0.00012743, 0.00013047, 0.0001384, 0.00014534, 0.00016543, 0.00017287, 0.00017678, 0.00018678, 0.00018842, 0.00019554, 0.00020302, 0.00021186, 0.0002206, 0.00023336, 0.00024623, 0.0002714, 0.00028894, 0.00030329, 0.00042856, 0.00046687, 0.00056878, 0.000657, 0.00068756, 0.00072341, 0.00078198, 0.00094088, 0.001017, 0.001131, 0.0013002, 0.0032883]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 2, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.708\n",
      "    win = [-0.060712, -0.0092974, -0.00412, -0.0038116, -0.0016619, -0.00074493, -0.00043386, -0.00042045, -0.00033256, -0.00030369, -0.00024745, -0.00022404, -0.00019218, -0.00017466, -0.00014335, -0.00013484, -0.0001142, -0.00010413, -8.454e-05, -8.0312e-05, -7.414e-05, -6.0548e-05, -5.258e-05, -4.7088e-05, -4.2015e-05, -3.7081e-05, -3.451e-05, -3.0571e-05, -2.6268e-05, -2.502e-05, -2.3558e-05, -2.0227e-05, -1.9318e-05, -1.71e-05, -1.5095e-05, -1.3975e-05, -1.236e-05, -1.2176e-05, -1.1364e-05, -9.3803e-06, -8.6645e-06, -7.6467e-06, -6.9029e-06, -5.9068e-06, -5.4026e-06, -4.9701e-06, -4.2287e-06, -3.7905e-06, -3.4947e-06, -3.0334e-06, -2.9218e-06, -2.7816e-06, -2.479e-06, -2.1619e-06, -2.1224e-06, -1.8892e-06, -1.7137e-06, -1.6184e-06, -1.4923e-06, -1.3396e-06, -1.265e-06, -1.1214e-06, -9.6584e-07, -8.7876e-07, -7.7429e-07, -7.2549e-07, -6.1112e-07, -5.7604e-07, -5.3374e-07, -4.305e-07, -3.6429e-07, -3.4224e-07, -3.3151e-07, -3.0262e-07, -2.5558e-07, -2.123e-07, -1.9905e-07, -1.7399e-07, -1.3752e-07, -1.1419e-07, -1.1165e-07, -7.8217e-08, -6.7659e-08, -5.6072e-08, -4.3112e-08, -4.2238e-08, -3.8964e-08, -3.2711e-08, -2.6579e-08, -2.4946e-08, -2.0547e-08, -1.5566e-08, -1.2661e-08, -1.0229e-08, -7.2231e-09, -3.3274e-09, -2.1804e-09, -1.4029e-09, -6.5336e-10, -3.5318e-10, -1.2003e-10, 1.9416e-10, 3.7474e-10, 1.216e-09, 2.7816e-09, 4.537e-09, 7.4544e-09, 9.9578e-09, 1.2793e-08, 1.3949e-08, 2.0114e-08, 2.3034e-08, 3.017e-08, 4.6845e-08, 5.2449e-08, 6.1789e-08, 6.6134e-08, 6.9201e-08, 9.1582e-08, 9.7282e-08, 1.2081e-07, 1.3589e-07, 1.5862e-07, 2.0836e-07, 2.1488e-07, 2.9505e-07, 3.0325e-07, 3.4507e-07, 3.6988e-07, 3.951e-07, 4.4164e-07, 4.8495e-07, 5.5098e-07, 6.0406e-07, 6.2826e-07, 7.897e-07, 9.0707e-07, 1.0885e-06, 1.1258e-06, 1.3228e-06, 1.5214e-06, 1.6482e-06, 1.7008e-06, 1.8926e-06, 2.02e-06, 2.1917e-06, 2.3706e-06, 2.5057e-06, 2.7444e-06, 2.8524e-06, 2.9394e-06, 3.4805e-06, 3.676e-06, 4.2088e-06, 4.4508e-06, 5.7051e-06, 6.5645e-06, 6.7426e-06, 7.135e-06, 7.6732e-06, 8.6774e-06, 9.6224e-06, 1.0277e-05, 1.08e-05, 1.2245e-05, 1.3589e-05, 1.4471e-05, 1.6215e-05, 1.7985e-05, 2.2844e-05, 2.433e-05, 2.7715e-05, 3.0812e-05, 3.1438e-05, 3.233e-05, 3.8788e-05, 3.9721e-05, 4.935e-05, 5.4188e-05, 6.1575e-05, 6.4504e-05, 7.1188e-05, 7.3837e-05, 9.4013e-05, 0.0001063, 0.00011893, 0.00013377, 0.00016479, 0.00017882, 0.00022574, 0.00027034, 0.00032104, 0.00034735, 0.00042015, 0.0005759, 0.00095699, 0.0013021, 0.0013585, 0.003096, 0.0069167]\n",
      "    wout = [-0.0030325, -0.0024106, -0.0022819, -0.0017547, -0.001687, -0.0014785, -0.0013226, -0.0012648, -0.001257, -0.0011999, -0.0011415, -0.0011245, -0.0010567, -0.0010108, -0.00094205, -0.00084309, -0.00072126, -0.00048619, -0.00046666, -0.0004583, -0.00044207, -0.00039527, -0.00035992, -0.00034013, -0.00030495, -0.00027077, -0.00026027, -0.00024236, -0.00020679, -0.00016815, -0.00015223, -0.0001488, -0.00014351, -0.00012332, -0.00011939, -0.00010061, -8.4908e-05, -7.6557e-05, -7.1978e-05, -7.0312e-05, -6.8664e-05, -6.5046e-05, -5.8283e-05, -5.4219e-05, -4.5785e-05, -4.5165e-05, -3.3288e-05, -3.242e-05, -3.1235e-05, -3.0015e-05, -2.7983e-05, -2.7039e-05, -2.5504e-05, -2.46e-05, -2.4366e-05, -2.2331e-05, -2.2108e-05, -2.1529e-05, -1.8207e-05, -1.6609e-05, -1.6444e-05, -1.5218e-05, -1.4645e-05, -1.4214e-05, -1.3826e-05, -1.3239e-05, -1.2963e-05, -1.2302e-05, -1.1847e-05, -1.0845e-05, -9.38e-06, -9.2437e-06, -8.5606e-06, -6.7587e-06, -6.415e-06, -6.0318e-06, -5.3802e-06, -4.8363e-06, -3.8683e-06, -3.6921e-06, -3.2424e-06, -2.6545e-06, -2.1946e-06, -2.1361e-06, -1.9392e-06, -1.903e-06, -1.7991e-06, -1.3101e-06, -1.1829e-06, -9.9767e-07, -9.1705e-07, -6.3073e-07, -5.3094e-07, -4.0032e-07, -3.4623e-07, -2.9939e-07, -4.0801e-08, -3.1419e-08, -1.0382e-08, -6.1922e-09, -4.3477e-09, 2.0581e-09, 1.5189e-08, 1.7228e-08, 3.5727e-08, 7.0181e-08, 7.7645e-08, 1.7333e-07, 1.8589e-07, 2.2003e-07, 2.2915e-07, 2.493e-07, 2.7924e-07, 4.1747e-07, 4.8227e-07, 5.4451e-07, 8.2463e-07, 1.1443e-06, 1.1597e-06, 1.3166e-06, 1.3467e-06, 1.4032e-06, 1.4775e-06, 1.495e-06, 1.7505e-06, 1.9083e-06, 2.0195e-06, 2.0679e-06, 2.3008e-06, 2.5999e-06, 2.6629e-06, 2.7854e-06, 3.3522e-06, 3.5139e-06, 3.772e-06, 3.8513e-06, 4.1611e-06, 4.2906e-06, 5.2394e-06, 5.3587e-06, 5.7661e-06, 6.1126e-06, 6.4155e-06, 6.7706e-06, 7.0512e-06, 7.1396e-06, 7.4595e-06, 8.1692e-06, 8.2741e-06, 8.9503e-06, 9.4925e-06, 1.0331e-05, 1.1613e-05, 1.1751e-05, 1.2333e-05, 1.2466e-05, 1.3032e-05, 1.3243e-05, 1.3856e-05, 1.6482e-05, 1.7208e-05, 1.7877e-05, 1.8233e-05, 2.0223e-05, 2.159e-05, 2.3712e-05, 2.5173e-05, 2.6251e-05, 2.743e-05, 3.2775e-05, 4.2743e-05, 5.9007e-05, 7.5718e-05, 0.00010255, 0.00010306, 0.00011418, 0.00013195, 0.00015503, 0.00015579, 0.00015881, 0.00016905, 0.00020379, 0.00023442, 0.00025072, 0.00028162, 0.00030915, 0.00033685, 0.00035174, 0.00036105, 0.00044049, 0.00046669, 0.0005484, 0.0005992, 0.00067272, 0.0009547, 0.00098856, 0.001426, 0.001657, 0.0029476, 0.0067381]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 2, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.789\n",
      "    win = [-0.032932, -0.0082847, -0.0046648, -0.0026111, -0.0017248, -0.0012708, -0.00090506, -0.000675, -0.00057232, -0.00047686, -0.00037249, -0.00032427, -0.0002953, -0.00025593, -0.00022625, -0.00020143, -0.00018904, -0.00016858, -0.00015936, -0.00013261, -0.00011124, -0.00010664, -9.0798e-05, -8.4879e-05, -8.0606e-05, -7.4342e-05, -7.0036e-05, -6.2681e-05, -5.796e-05, -5.2847e-05, -5.0711e-05, -4.8576e-05, -4.0137e-05, -3.4279e-05, -3.3304e-05, -2.8944e-05, -2.7749e-05, -2.494e-05, -2.4408e-05, -2.1749e-05, -1.9503e-05, -1.8853e-05, -1.6764e-05, -1.5249e-05, -1.4144e-05, -1.2716e-05, -1.1812e-05, -1.1697e-05, -9.8164e-06, -9.216e-06, -8.3953e-06, -7.9828e-06, -7.5315e-06, -6.9606e-06, -6.2474e-06, -5.9276e-06, -4.7833e-06, -4.4733e-06, -4.2538e-06, -3.6478e-06, -3.2784e-06, -2.8782e-06, -2.6431e-06, -2.379e-06, -2.1406e-06, -1.8764e-06, -1.4369e-06, -1.3536e-06, -1.3164e-06, -9.8806e-07, -9.063e-07, -8.474e-07, -7.6146e-07, -5.8886e-07, -5.32e-07, -4.0977e-07, -3.3802e-07, -3.0596e-07, -2.7904e-07, -2.6356e-07, -2.1588e-07, -1.8755e-07, -1.7323e-07, -1.4105e-07, -1.0707e-07, -8.9914e-08, -7.5617e-08, -6.8826e-08, -5.101e-08, -4.2336e-08, -3.4348e-08, -2.9459e-08, -2.5386e-08, -2.0314e-08, -1.6976e-08, -1.4242e-08, -8.3399e-09, -6.3352e-09, -4.7258e-09, -3.0485e-09, -1.9422e-09, -1.6954e-09, -1.47e-09, -8.5028e-10, -4.8707e-10, -3.4482e-10, -1.5677e-10, 3.8653e-11, 9.1802e-11, 1.6198e-10, 3.2159e-10, 7.4047e-10, 8.403e-10, 1.0309e-09, 1.9863e-09, 2.6346e-09, 3.3584e-09, 4.1679e-09, 5.435e-09, 8.526e-09, 1.0261e-08, 1.1806e-08, 1.2881e-08, 2.1126e-08, 2.383e-08, 2.8703e-08, 2.926e-08, 4.6272e-08, 5.2822e-08, 6.1978e-08, 8.3128e-08, 1.0467e-07, 1.261e-07, 1.3822e-07, 1.6816e-07, 2.1592e-07, 3.1563e-07, 3.5628e-07, 4.0711e-07, 4.7155e-07, 5.9441e-07, 7.2241e-07, 7.4904e-07, 1.1084e-06, 1.2353e-06, 1.4046e-06, 1.5961e-06, 1.8245e-06, 1.9487e-06, 2.4061e-06, 2.7471e-06, 3.2921e-06, 3.3293e-06, 3.9304e-06, 4.5222e-06, 4.9062e-06, 5.4439e-06, 6.5966e-06, 7.0309e-06, 7.3969e-06, 8.1675e-06, 8.4851e-06, 1.0524e-05, 1.1803e-05, 1.3356e-05, 1.3844e-05, 1.4575e-05, 1.5709e-05, 1.7481e-05, 2.047e-05, 2.1963e-05, 2.294e-05, 2.7718e-05, 3.1695e-05, 3.2895e-05, 3.8824e-05, 4.1637e-05, 4.339e-05, 5.4567e-05, 5.7948e-05, 6.7204e-05, 7.3076e-05, 8.1496e-05, 9.8645e-05, 0.00010606, 0.00012976, 0.00014469, 0.00015259, 0.00016698, 0.00016932, 0.00020911, 0.00024443, 0.00030973, 0.00034876, 0.00037083, 0.00049862, 0.00072211, 0.0009458, 0.0019217, 0.013085]\n",
      "    wout = [-0.0015689, -0.0012348, -0.0011908, -0.0011788, -0.0010992, -0.0010523, -0.0010275, -0.00095066, -0.00085304, -0.00077751, -0.00076108, -0.00072565, -0.00071101, -0.00063776, -0.0006096, -0.0005945, -0.00048717, -0.00045225, -0.00044901, -0.00043045, -0.00037457, -0.00035406, -0.00033567, -0.00031871, -0.00029162, -0.00025779, -0.00022742, -0.00021323, -0.00019496, -0.00017419, -0.00016558, -0.00015095, -0.00014331, -0.00013466, -0.00010874, -0.0001035, -8.8861e-05, -7.2984e-05, -7.0099e-05, -5.4318e-05, -5.3425e-05, -4.8732e-05, -4.5807e-05, -4.1831e-05, -4.1365e-05, -4.111e-05, -3.7928e-05, -3.6941e-05, -3.5463e-05, -3.505e-05, -3.1562e-05, -2.8956e-05, -2.4782e-05, -2.4282e-05, -2.1046e-05, -1.7306e-05, -1.6844e-05, -1.5264e-05, -1.4348e-05, -1.3338e-05, -1.2595e-05, -1.2581e-05, -1.224e-05, -1.2109e-05, -1.0006e-05, -9.5809e-06, -8.7288e-06, -6.4904e-06, -6.1429e-06, -5.9585e-06, -5.685e-06, -5.0685e-06, -5.0138e-06, -3.782e-06, -2.644e-06, -2.5587e-06, -2.4382e-06, -1.9641e-06, -1.6987e-06, -1.3756e-06, -1.0772e-06, -9.91e-07, -9.1489e-07, -8.1903e-07, -7.8586e-07, -7.5802e-07, -7.3283e-07, -4.7924e-07, -4.7324e-07, -1.666e-07, -1.1922e-07, -9.6143e-08, 2.5681e-09, 3.4457e-09, 3.321e-08, 6.0064e-08, 6.8606e-08, 1.0829e-07, 1.4701e-07, 1.7109e-07, 1.727e-07, 2.6056e-07, 6.1448e-07, 6.3743e-07, 7.4837e-07, 9.4211e-07, 1.169e-06, 1.3694e-06, 1.817e-06, 2.0098e-06, 2.0172e-06, 2.2493e-06, 2.2917e-06, 2.3179e-06, 2.6855e-06, 3.1916e-06, 3.2923e-06, 3.7383e-06, 3.8258e-06, 3.8315e-06, 4.2102e-06, 4.4692e-06, 4.9527e-06, 6.6373e-06, 7.0404e-06, 7.3089e-06, 7.4879e-06, 8.0485e-06, 9.8979e-06, 1.0997e-05, 1.2513e-05, 1.2757e-05, 1.2839e-05, 1.313e-05, 1.3682e-05, 1.4479e-05, 1.4836e-05, 1.4939e-05, 1.5578e-05, 1.5602e-05, 1.6285e-05, 1.844e-05, 1.9853e-05, 2.3751e-05, 2.4054e-05, 2.4719e-05, 2.5149e-05, 2.7516e-05, 2.8908e-05, 2.9736e-05, 3.0458e-05, 3.0924e-05, 3.2086e-05, 3.254e-05, 3.3501e-05, 3.4062e-05, 3.4716e-05, 3.5182e-05, 3.5852e-05, 3.8365e-05, 4.1016e-05, 4.3702e-05, 4.6675e-05, 4.812e-05, 5.0255e-05, 5.1463e-05, 5.3812e-05, 5.4435e-05, 5.6472e-05, 5.7629e-05, 6.2252e-05, 7.4101e-05, 7.6036e-05, 8.34e-05, 9.3473e-05, 9.4277e-05, 0.00010458, 0.00011842, 0.00012163, 0.0001235, 0.0001357, 0.0001814, 0.00023322, 0.00024478, 0.00031405, 0.00034287, 0.00043949, 0.00049486, 0.00053911, 0.00059425, 0.00066238, 0.00075714, 0.0008046, 0.00083253, 0.00092911, 0.0010723, 0.0014658, 0.0017283, 0.0019265, 0.0028253]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 3, Phase 2, Epoch: 1, Step: 100\n",
      "    cos_sim = 0.895\n",
      "    win = [-0.0088402, -0.0021673, -0.00072804, -0.00031121, -0.00023758, -0.00012637, -8.7806e-05, -8.1555e-05, -4.0039e-05, -3.7413e-05, -2.5701e-05, -2.2322e-05, -1.9935e-05, -1.7044e-05, -1.4574e-05, -1.0664e-05, -9.4086e-06, -8.9971e-06, -7.6324e-06, -7.5762e-06, -6.5591e-06, -5.8779e-06, -4.9345e-06, -3.607e-06, -3.3645e-06, -3.0685e-06, -3.0103e-06, -2.6135e-06, -2.0509e-06, -2.0298e-06, -1.669e-06, -1.5305e-06, -1.3733e-06, -1.194e-06, -1.1473e-06, -8.6205e-07, -7.6529e-07, -7.0195e-07, -6.4069e-07, -6.2163e-07, -5.4821e-07, -5.1068e-07, -4.4166e-07, -3.7585e-07, -3.5839e-07, -3.3859e-07, -2.731e-07, -2.2847e-07, -1.8936e-07, -1.8129e-07, -1.4784e-07, -1.3157e-07, -1.2201e-07, -1.042e-07, -9.2266e-08, -8.8683e-08, -7.2038e-08, -6.4661e-08, -6.088e-08, -5.8179e-08, -4.8422e-08, -3.8484e-08, -3.2991e-08, -2.7883e-08, -2.3783e-08, -2.0351e-08, -1.9927e-08, -1.603e-08, -1.2826e-08, -1.1892e-08, -1.1596e-08, -9.3586e-09, -7.9835e-09, -7.2465e-09, -5.8025e-09, -5.4967e-09, -4.4252e-09, -4.2553e-09, -3.6177e-09, -2.9088e-09, -2.5582e-09, -2.1972e-09, -1.9539e-09, -1.4699e-09, -1.4361e-09, -1.0915e-09, -9.98e-10, -7.5939e-10, -6.4639e-10, -5.5773e-10, -4.9381e-10, -4.4394e-10, -4.0776e-10, -2.6907e-10, -2.0486e-10, -1.6899e-10, -1.6291e-10, -1.0059e-10, -9.6639e-11, -7.939e-11, -4.8688e-11, -3.3691e-11, -2.2802e-11, -1.0194e-11, -3.2299e-12, -8.0975e-13, 1.4822e-11, 1.9979e-11, 3.3445e-11, 4.0668e-11, 5.6745e-11, 6.7443e-11, 1.0244e-10, 1.3227e-10, 1.9008e-10, 2.839e-10, 3.3342e-10, 4.1607e-10, 5.1102e-10, 6.1172e-10, 8.1587e-10, 1.1266e-09, 1.243e-09, 1.4876e-09, 1.6427e-09, 2.1728e-09, 2.5369e-09, 3.1231e-09, 3.6039e-09, 3.9319e-09, 4.4208e-09, 5.0201e-09, 6.0042e-09, 6.6526e-09, 7.8357e-09, 9.4618e-09, 9.8385e-09, 1.131e-08, 1.5037e-08, 1.7321e-08, 2.0566e-08, 2.1166e-08, 2.243e-08, 2.5149e-08, 2.9547e-08, 3.0574e-08, 3.4326e-08, 3.5058e-08, 3.9533e-08, 4.6739e-08, 5.4032e-08, 5.976e-08, 7.069e-08, 8.8346e-08, 1.0901e-07, 1.2062e-07, 1.3763e-07, 1.5377e-07, 1.672e-07, 1.7905e-07, 2.0829e-07, 2.3098e-07, 2.6965e-07, 2.918e-07, 3.2233e-07, 3.5547e-07, 4.4747e-07, 4.8643e-07, 5.2158e-07, 7.1772e-07, 7.6512e-07, 8.4116e-07, 1.0334e-06, 1.172e-06, 1.4648e-06, 1.5348e-06, 1.8874e-06, 1.9501e-06, 2.1845e-06, 2.4393e-06, 2.7146e-06, 3.4988e-06, 3.7298e-06, 4.6425e-06, 5.002e-06, 6.8807e-06, 7.79e-06, 9.1389e-06, 1.0867e-05, 1.6415e-05, 1.9188e-05, 2.6116e-05, 2.8653e-05, 2.9499e-05, 4.4039e-05, 6.3718e-05, 6.7306e-05, 8.4749e-05, 0.00017195, 0.00070067]\n",
      "    wout = [-5.1653e-05, -1.797e-05, -1.4892e-05, -1.4215e-05, -1.2384e-05, -1.1466e-05, -8.0837e-06, -7.8736e-06, -7.2721e-06, -6.1849e-06, -5.9134e-06, -5.5335e-06, -5.4481e-06, -5.108e-06, -4.889e-06, -3.9195e-06, -3.8141e-06, -3.3537e-06, -2.8345e-06, -2.3555e-06, -2.3276e-06, -2.2036e-06, -1.7861e-06, -1.6499e-06, -1.306e-06, -1.188e-06, -1.1858e-06, -1.1135e-06, -9.3138e-07, -9.0736e-07, -7.2649e-07, -7.0382e-07, -6.6341e-07, -6.3165e-07, -5.1235e-07, -4.9956e-07, -4.4343e-07, -4.3607e-07, -2.675e-07, -2.226e-07, -2.0547e-07, -1.9625e-07, -1.8745e-07, -1.7986e-07, -1.7558e-07, -1.7395e-07, -1.7161e-07, -1.6456e-07, -1.6319e-07, -1.5835e-07, -1.5744e-07, -1.498e-07, -1.4385e-07, -1.4183e-07, -1.343e-07, -1.3215e-07, -1.3043e-07, -1.2999e-07, -1.2863e-07, -1.2523e-07, -1.1903e-07, -1.0847e-07, -9.5585e-08, -9.3658e-08, -8.7717e-08, -8.5209e-08, -8.4296e-08, -8.1532e-08, -6.9668e-08, -6.8283e-08, -6.6579e-08, -6.0785e-08, -5.8303e-08, -5.3503e-08, -5.0455e-08, -4.4112e-08, -4.2932e-08, -3.6959e-08, -3.4904e-08, -3.1695e-08, -3.0282e-08, -2.8617e-08, -2.652e-08, -2.3068e-08, -2.2705e-08, -1.9587e-08, -1.9471e-08, -1.9203e-08, -1.897e-08, -1.8469e-08, -1.7781e-08, -1.7188e-08, -1.6264e-08, -1.5146e-08, -1.4333e-08, -1.4055e-08, -1.2877e-08, -1.2461e-08, -9.3348e-09, -6.6301e-09, -5.8953e-09, -4.8738e-09, -3.6742e-09, -3.3292e-09, -2.5025e-09, -1.7464e-09, -1.177e-09, -1.0726e-09, -6.9908e-11, 1.3618e-10, 1.7838e-10, 1.1648e-09, 2.5285e-09, 3.6563e-09, 4.1504e-09, 4.8642e-09, 8.3167e-09, 9.1818e-09, 1.099e-08, 1.3248e-08, 1.4211e-08, 1.6449e-08, 1.7444e-08, 2.2005e-08, 2.4095e-08, 2.4403e-08, 2.6894e-08, 2.8293e-08, 3.1505e-08, 3.5101e-08, 4.0324e-08, 4.1842e-08, 4.6391e-08, 4.7949e-08, 5.1256e-08, 5.1685e-08, 5.2012e-08, 5.591e-08, 5.6909e-08, 5.9105e-08, 6.1888e-08, 6.3991e-08, 6.4895e-08, 6.628e-08, 6.8699e-08, 6.9864e-08, 7.1701e-08, 7.4118e-08, 8.654e-08, 8.9966e-08, 1.0194e-07, 1.0488e-07, 1.0706e-07, 1.1247e-07, 1.1525e-07, 1.2163e-07, 1.2768e-07, 1.3136e-07, 1.4032e-07, 1.6165e-07, 1.6885e-07, 1.8875e-07, 1.894e-07, 2.9667e-07, 3.2674e-07, 3.8167e-07, 3.8836e-07, 5.1845e-07, 5.4873e-07, 5.6616e-07, 6.5697e-07, 7.4924e-07, 1.0124e-06, 1.0796e-06, 1.0862e-06, 1.72e-06, 1.8823e-06, 2.6962e-06, 3.0936e-06, 3.3817e-06, 3.9862e-06, 4.3588e-06, 5.3508e-06, 6.1369e-06, 7.183e-06, 7.4348e-06, 1.0811e-05, 1.1776e-05, 1.2467e-05, 1.493e-05, 1.6798e-05, 1.7765e-05, 1.786e-05, 2.1298e-05, 2.8142e-05, 3.1894e-05, 4.7405e-05, 6.3927e-05, 0.0001066, 0.00018514]\n",
      "--------------------------------------------------------------------------------\n",
      "Training: shorter, Seed: 3, Phase 2, Epoch: 3, Step: 200\n",
      "    cos_sim = 0.873\n",
      "    win = [-0.015482, -0.0045966, -0.0013634, -0.00097618, -0.00067319, -0.00062448, -0.0003644, -0.00030117, -0.00027602, -0.00023979, -0.00018405, -0.00016413, -0.0001116, -0.00010325, -8.6547e-05, -8.4236e-05, -7.991e-05, -7.4606e-05, -6.5644e-05, -5.4416e-05, -5.1129e-05, -4.8526e-05, -4.2952e-05, -3.8269e-05, -3.2143e-05, -2.9544e-05, -2.7858e-05, -2.3388e-05, -2.1902e-05, -1.9417e-05, -1.6103e-05, -1.5089e-05, -1.4222e-05, -1.2828e-05, -1.1331e-05, -1.0259e-05, -9.3366e-06, -8.6612e-06, -8.0244e-06, -7.7774e-06, -6.604e-06, -6.2001e-06, -5.5608e-06, -4.9069e-06, -4.4617e-06, -4.0219e-06, -3.7296e-06, -3.6076e-06, -3.1566e-06, -2.6455e-06, -2.4618e-06, -1.893e-06, -1.7653e-06, -1.6771e-06, -1.3839e-06, -1.2208e-06, -9.8395e-07, -9.0368e-07, -8.5511e-07, -7.3603e-07, -5.905e-07, -5.533e-07, -4.4265e-07, -3.9524e-07, -3.3152e-07, -3.1197e-07, -2.7571e-07, -2.5498e-07, -1.9062e-07, -1.6903e-07, -1.4978e-07, -1.2287e-07, -1.1394e-07, -8.6046e-08, -6.7351e-08, -6.3463e-08, -6.043e-08, -5.0297e-08, -4.2253e-08, -3.6041e-08, -3.122e-08, -2.5993e-08, -2.5228e-08, -2.055e-08, -1.8348e-08, -1.536e-08, -1.2352e-08, -8.6705e-09, -6.8577e-09, -5.9317e-09, -5.4586e-09, -3.5475e-09, -2.4715e-09, -1.3627e-09, -1.117e-09, -9.6013e-10, -8.5474e-10, -4.8209e-10, -4.1265e-10, -3.5535e-10, -3.2419e-10, -2.6525e-10, -2.2241e-10, -1.5058e-10, -1.3556e-10, -1.1195e-10, -6.8039e-11, -1.8891e-11, -3.6474e-13, 7.8728e-11, 9.5125e-11, 1.4523e-10, 1.6739e-10, 2.1477e-10, 2.508e-10, 3.0427e-10, 3.4583e-10, 3.679e-10, 5.2509e-10, 7.2908e-10, 8.307e-10, 8.7553e-10, 1.3173e-09, 1.7944e-09, 2.0133e-09, 3.0474e-09, 3.5042e-09, 3.8151e-09, 6.7512e-09, 7.3255e-09, 8.2343e-09, 1.0429e-08, 1.1877e-08, 2.1161e-08, 2.255e-08, 3.5996e-08, 4.0016e-08, 4.5964e-08, 5.7003e-08, 8.6445e-08, 1.1299e-07, 1.5927e-07, 2.0402e-07, 2.4727e-07, 2.8773e-07, 3.2809e-07, 3.7157e-07, 3.901e-07, 4.106e-07, 5.1062e-07, 5.8202e-07, 6.4557e-07, 6.6214e-07, 7.9491e-07, 9.4284e-07, 1.1522e-06, 1.3828e-06, 1.4812e-06, 1.7526e-06, 1.9246e-06, 2.0738e-06, 2.3751e-06, 2.846e-06, 3.1174e-06, 3.2414e-06, 3.8635e-06, 4.8008e-06, 6.2345e-06, 6.6623e-06, 6.9219e-06, 8.6034e-06, 8.981e-06, 1.0967e-05, 1.2269e-05, 1.4382e-05, 1.5584e-05, 1.7978e-05, 2.4684e-05, 2.7135e-05, 3.3819e-05, 3.9802e-05, 5.6237e-05, 5.9454e-05, 6.4712e-05, 8.1619e-05, 8.3785e-05, 8.8616e-05, 0.00010176, 0.00013074, 0.00014136, 0.00018055, 0.00021504, 0.00030182, 0.00036381, 0.00050838, 0.00056041, 0.00084543, 0.0018997, 0.0071468, 0.024579]\n",
      "    wout = [-7.3699e-05, -6.7536e-05, -6.6852e-05, -6.2666e-05, -5.9405e-05, -5.3404e-05, -5.2339e-05, -4.6471e-05, -4.5158e-05, -4.1259e-05, -4.0415e-05, -3.7626e-05, -3.6893e-05, -3.2715e-05, -3.1968e-05, -3.0131e-05, -2.9528e-05, -2.8431e-05, -2.7321e-05, -2.6397e-05, -2.509e-05, -2.4939e-05, -2.4831e-05, -2.4642e-05, -2.2849e-05, -2.2544e-05, -2.2272e-05, -2.183e-05, -2.096e-05, -2.0438e-05, -1.9715e-05, -1.9554e-05, -1.9377e-05, -1.9131e-05, -1.9056e-05, -1.8741e-05, -1.8538e-05, -1.8293e-05, -1.8143e-05, -1.781e-05, -1.7459e-05, -1.7268e-05, -1.6915e-05, -1.6414e-05, -1.613e-05, -1.591e-05, -1.5551e-05, -1.5372e-05, -1.4789e-05, -1.4458e-05, -1.4388e-05, -1.4201e-05, -1.3625e-05, -1.319e-05, -1.2995e-05, -1.2667e-05, -1.2089e-05, -1.1916e-05, -1.1804e-05, -1.1629e-05, -1.1071e-05, -1.0712e-05, -1.0491e-05, -1.0328e-05, -1.0239e-05, -1.0127e-05, -9.6256e-06, -9.2547e-06, -8.5285e-06, -8.5193e-06, -8.3594e-06, -7.9394e-06, -7.6809e-06, -7.4907e-06, -7.1644e-06, -7.061e-06, -6.4671e-06, -5.8449e-06, -5.7038e-06, -5.6899e-06, -5.532e-06, -5.4314e-06, -5.3746e-06, -5.1229e-06, -4.6958e-06, -4.416e-06, -4.1617e-06, -4.0765e-06, -3.5897e-06, -3.483e-06, -2.9124e-06, -2.8682e-06, -2.7539e-06, -2.5667e-06, -2.5012e-06, -2.2304e-06, -2.088e-06, -2.0596e-06, -1.9746e-06, -1.9539e-06, -1.866e-06, -1.6774e-06, -1.5461e-06, -1.4004e-06, -1.2618e-06, -1.128e-06, -1.011e-06, -8.7704e-07, -7.4691e-07, -4.8649e-07, -4.7339e-07, -3.0994e-07, -2.2312e-07, -1.7357e-07, -1.0076e-07, 2.6068e-08, 1.7815e-07, 3.4947e-07, 5.3576e-07, 5.8975e-07, 7.0317e-07, 7.4446e-07, 8.0678e-07, 8.4705e-07, 1.023e-06, 1.1358e-06, 1.187e-06, 1.4355e-06, 1.9866e-06, 2.1712e-06, 2.259e-06, 2.8699e-06, 3.1297e-06, 3.5599e-06, 4.0159e-06, 4.5284e-06, 4.8294e-06, 5.1731e-06, 5.6779e-06, 5.7294e-06, 6.1117e-06, 6.6413e-06, 7.3783e-06, 7.8237e-06, 8.0282e-06, 8.3797e-06, 8.3934e-06, 1.0399e-05, 1.0634e-05, 1.1689e-05, 1.219e-05, 1.2545e-05, 1.4397e-05, 1.4829e-05, 1.4934e-05, 1.5524e-05, 1.5901e-05, 1.6975e-05, 1.734e-05, 1.8354e-05, 1.8885e-05, 1.908e-05, 1.9708e-05, 2.0713e-05, 2.0941e-05, 2.1305e-05, 2.2064e-05, 2.264e-05, 2.3363e-05, 2.3687e-05, 2.5791e-05, 2.5888e-05, 2.6552e-05, 2.7343e-05, 2.8082e-05, 2.8166e-05, 2.8846e-05, 2.9258e-05, 2.9567e-05, 3.208e-05, 3.3066e-05, 3.3872e-05, 3.5836e-05, 3.7229e-05, 3.8358e-05, 3.9605e-05, 4.0261e-05, 4.0503e-05, 4.2652e-05, 4.2945e-05, 4.4827e-05, 4.5923e-05, 4.6942e-05, 5.1814e-05, 5.549e-05, 5.9027e-05, 6.196e-05, 6.8457e-05, 8.2486e-05, 0.00012461]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_coupling_events(log_text):\n",
    "    # Split the log into sections by training type (\"longer trainig\" and \"shorter trainig\")\n",
    "    # (Note: the logs contain the spelling \"trainig\" so we use that string.)\n",
    "    sections = re.split(r'={5,}\\s*(longer|shorter)\\s+trainig\\s*={5,}', log_text, flags=re.IGNORECASE)\n",
    "    # The split list will contain header parts and captured training type strings.\n",
    "    results = []\n",
    "    # Iterate over the sections. Every odd index is a training type, followed by the corresponding section text.\n",
    "    for i in range(1, len(sections), 2):\n",
    "        training_type = sections[i].strip().lower()  # \"longer\" or \"shorter\"\n",
    "        section_text = sections[i+1]\n",
    "        # Each experiment run starts with the line \"experimental config =\"\n",
    "        runs = re.split(r'(experimental config =)', section_text)\n",
    "        run_texts = []\n",
    "        # Reconstruct each run (by joining the marker with the following text)\n",
    "        for j in range(1, len(runs), 2):\n",
    "            run_text = runs[j] + runs[j+1]\n",
    "            run_texts.append(run_text)\n",
    "        \n",
    "        # For seed numbering, assume the order in the section gives seed 1,2,3,...\n",
    "        for seed_index, run in enumerate(run_texts, start=1):\n",
    "            # Define a regex to capture each coupling event block\n",
    "            # It finds the block that starts with \"####################epoch\" then win and wout lines.\n",
    "            event_pattern = re.compile(\n",
    "                r'#{5,}\\s*epoch\\s+(\\d+),\\s*step\\s+(\\d+),\\s*cos_sim\\s*=\\s*([-\\d\\.Ee]+).*?'\n",
    "                r'win\\s*=\\s*tensor\\((.*?)\\).*?'\n",
    "                r'wout\\s*=\\s*tensor\\((.*?)\\)',\n",
    "                flags=re.DOTALL | re.IGNORECASE\n",
    "            )\n",
    "            # Determine the location of the coupling marker for phase change.\n",
    "            coupling_marker = \"===== Applying unit coupling =====\"\n",
    "            coupling_index = run.find(coupling_marker)\n",
    "            \n",
    "            for match in event_pattern.finditer(run):\n",
    "                epoch = int(match.group(1))\n",
    "                step  = int(match.group(2))\n",
    "                cos_sim = match.group(3).strip()\n",
    "                win_str = match.group(4).strip()\n",
    "                wout_str = match.group(5).strip()\n",
    "                \n",
    "                # Determine phase: if the coupling event occurs AFTER the marker, it's phase 2.\n",
    "                if coupling_index != -1 and match.start() > coupling_index:\n",
    "                    phase = \"Phase 2\"\n",
    "                else:\n",
    "                    phase = \"Phase 1\"\n",
    "                \n",
    "                # Save the extracted event information.\n",
    "                results.append({\n",
    "                    \"training_type\": training_type,  # \"longer\" or \"shorter\"\n",
    "                    \"seed\": seed_index,\n",
    "                    \"phase\": phase,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": step,\n",
    "                    \"cos_sim\": cos_sim,\n",
    "                    \"win\": eval(win_str),\n",
    "                    \"wout\": eval(wout_str)\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    # Read your log text from a file (or define it as a multiline string)\n",
    "    with open(\"log.txt\", \"r\") as f:\n",
    "        log_text = f.read()\n",
    "    \n",
    "    events = extract_coupling_events(log_text)\n",
    "    \n",
    "    # Print out the extracted events:\n",
    "    for ev in events:\n",
    "        print(f\"Training: {ev['training_type']}, Seed: {ev['seed']}, {ev['phase']}, Epoch: {ev['epoch']}, Step: {ev['step']}\")\n",
    "        print(f\"    cos_sim = {ev['cos_sim']}\")\n",
    "        print(f\"    win = {ev['win']}\")\n",
    "        print(f\"    wout = {ev['wout']}\")\n",
    "        print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c2e5c3d-5def-4860-9541-1853778ead45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= longer trainig =================\n",
      "Cosine similarity: 0.9910 (after coupling) -> 0.5049 (final)\n",
      "Cosine similarity: 0.9916 (after coupling) -> 0.8850 (final)\n",
      "Cosine similarity: 0.9905 (after coupling) -> 0.8488 (final)\n",
      "================= shorter trainig =================\n",
      "Cosine similarity: 0.9901 (after coupling) -> 0.6765 (final)\n",
      "Cosine similarity: 0.9898 (after coupling) -> 0.4225 (final)\n",
      "Cosine similarity: 0.9886 (after coupling) -> 0.7354 (final)\n"
     ]
    }
   ],
   "source": [
    "b = False\n",
    "for w in s.split('\\n'):\n",
    "    if 'longer trainig' in w or 'shorter trainig' in w:\n",
    "        print(w)\n",
    "    if b:\n",
    "        print(w)\n",
    "        b = False\n",
    "    if 'Final Results' in w:\n",
    "        b = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3859551-0c9e-4d96-ac51-24d0431cc88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,\n",
       "          0.,   0.,   1.,   1.,   2., 180.,   9.,   2.,   0.,   0.,   2.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([-1.00570000e-02, -9.46087667e-03, -8.86475333e-03, -8.26863000e-03,\n",
       "        -7.67250667e-03, -7.07638333e-03, -6.48026000e-03, -5.88413667e-03,\n",
       "        -5.28801333e-03, -4.69189000e-03, -4.09576667e-03, -3.49964333e-03,\n",
       "        -2.90352000e-03, -2.30739667e-03, -1.71127333e-03, -1.11515000e-03,\n",
       "        -5.19026667e-04,  7.70966667e-05,  6.73220000e-04,  1.26934333e-03,\n",
       "         1.86546667e-03,  2.46159000e-03,  3.05771333e-03,  3.65383667e-03,\n",
       "         4.24996000e-03,  4.84608333e-03,  5.44220667e-03,  6.03833000e-03,\n",
       "         6.63445333e-03,  7.23057667e-03,  7.82670000e-03]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnLUlEQVR4nO3df1RU553H8c8EYUQWpiKBYSIitZq0Yk3URGN/CKZBp/7YxjbRxJPFE+OprdpY9KSy2Rw0mxWbniT2xE3SZC3RaqrbTbTumtZiItos2lWIrT+yKUb8FUFWVxkwZiDy7B9dZjMOIOjMzgO+X+fcc7jPfe69z5eHHx/u3Ms4jDFGAAAAFrkp2gMAAAC4EgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdXtEewLVoaWnR6dOnlZiYKIfDEe3hAACATjDGqKGhQR6PRzfd1PE1km4ZUE6fPq2MjIxoDwMAAFyDkydPqn///h326ZYBJTExUdJfCkxKSoryaAAAQGf4fD5lZGQEfo93pFsGlNaXdZKSkggoAAB0M525PYObZAEAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACs0yvaAwCAcBq4ZOs173tsxaQwjgTA9eAKCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA63Q5oOzatUtTpkyRx+ORw+HQ5s2bg7Y7HI42l5/85CeBPjk5OSHbZ8yYcd3FAACAnqHLAeXixYsaPny4Vq1a1eb2mpqaoOXnP/+5HA6Hvv3tbwf1mzNnTlC/n/3sZ9dWAQAA6HG6/H9QvF6vvF5vu9vdbnfQ+q9//Wvl5ubq85//fFB7nz59QvoCAABIEb4H5cyZM9q6datmz54dsm39+vVKSUnR0KFDtXjxYjU0NLR7HL/fL5/PF7QAAICeK6L/SXbNmjVKTEzUtGnTgtpnzpyprKwsud1uHTx4UIWFhfrjH/+o0tLSNo9TXFysZcuWRXKoAADAIhENKD//+c81c+ZM9e7dO6h9zpw5gY+zs7M1ePBgjRo1SpWVlRoxYkTIcQoLC1VQUBBY9/l8ysjIiNzAAQBAVEUsoPz+97/XBx98oI0bN16174gRIxQbG6uqqqo2A4rT6ZTT6YzEMAEAgIUidg/K6tWrNXLkSA0fPvyqfQ8dOqTm5malp6dHajgAAKAb6fIVlMbGRh05ciSwXl1drf379ys5OVkDBgyQ9JeXYH71q1/p2WefDdn/ww8/1Pr16/XNb35TKSkpOnz4sBYtWqQ77rhDX/nKV66jFAAA0FN0OaDs27dPubm5gfXWe0Py8/P12muvSZI2bNggY4wefPDBkP3j4uL09ttv66c//akaGxuVkZGhSZMmqaioSDExMddYBgAA6EkcxhgT7UF0lc/nk8vlUn19vZKSkqI9HAAWGbhk6zXve2zFpDCOBMCVuvL7m/fiAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOlwPKrl27NGXKFHk8HjkcDm3evDlo+6xZs+RwOIKWMWPGBPXx+/1asGCBUlJSlJCQoKlTp+rUqVPXVQgAAOg5uhxQLl68qOHDh2vVqlXt9pk4caJqamoCy1tvvRW0feHChdq0aZM2bNigd999V42NjZo8ebIuX77c9QoAAECP06urO3i9Xnm93g77OJ1Oud3uNrfV19dr9erV+sUvfqFvfOMbkqR169YpIyND27dv14QJE7o6JAAA0MNE5B6UsrIypaamasiQIZozZ47q6uoC2yoqKtTc3Ky8vLxAm8fjUXZ2tsrLy9s8nt/vl8/nC1oAAEDPFfaA4vV6tX79er3zzjt69tlntXfvXo0fP15+v1+SVFtbq7i4OPXt2zdov7S0NNXW1rZ5zOLiYrlcrsCSkZER7mEDAACLdPklnquZPn164OPs7GyNGjVKmZmZ2rp1q6ZNm9bufsYYORyONrcVFhaqoKAgsO7z+QgpAAD0YBF/zDg9PV2ZmZmqqqqSJLndbjU1Nen8+fNB/erq6pSWltbmMZxOp5KSkoIWAADQc0U8oJw7d04nT55Uenq6JGnkyJGKjY1VaWlpoE9NTY0OHjyosWPHRno4AACgG+jySzyNjY06cuRIYL26ulr79+9XcnKykpOTtXTpUn37299Wenq6jh07pr/9279VSkqK7rvvPkmSy+XS7NmztWjRIvXr10/JyclavHixhg0bFniqBwAA3Ni6HFD27dun3NzcwHrrvSH5+fl66aWXdODAAa1du1YXLlxQenq6cnNztXHjRiUmJgb2ef7559WrVy898MADunTpku655x699tpriomJCUNJAACgu3MYY0y0B9FVPp9PLpdL9fX13I8CIMjAJVuved9jKyaFcSQArtSV39+8Fw8AALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdbocUHbt2qUpU6bI4/HI4XBo8+bNgW3Nzc360Y9+pGHDhikhIUEej0d/8zd/o9OnTwcdIycnRw6HI2iZMWPGdRcDAAB6hi4HlIsXL2r48OFatWpVyLaPP/5YlZWVevLJJ1VZWak333xTf/7znzV16tSQvnPmzFFNTU1g+dnPfnZtFQAAgB6nV1d38Hq98nq9bW5zuVwqLS0NanvhhRd011136cSJExowYECgvU+fPnK73V09PQAAuAFE/B6U+vp6ORwOfe5znwtqX79+vVJSUjR06FAtXrxYDQ0N7R7D7/fL5/MFLQAAoOfq8hWUrvjkk0+0ZMkSPfTQQ0pKSgq0z5w5U1lZWXK73Tp48KAKCwv1xz/+MeTqS6vi4mItW7YskkMFAAAWiVhAaW5u1owZM9TS0qIXX3wxaNucOXMCH2dnZ2vw4MEaNWqUKisrNWLEiJBjFRYWqqCgILDu8/mUkZERqaEDAIAoi0hAaW5u1gMPPKDq6mq98847QVdP2jJixAjFxsaqqqqqzYDidDrldDojMVQAAGChsAeU1nBSVVWlHTt2qF+/flfd59ChQ2publZ6enq4hwMAALqhLgeUxsZGHTlyJLBeXV2t/fv3Kzk5WR6PR9/5zndUWVmpf/u3f9Ply5dVW1srSUpOTlZcXJw+/PBDrV+/Xt/85jeVkpKiw4cPa9GiRbrjjjv0la98JXyVAQCAbqvLAWXfvn3Kzc0NrLfeG5Kfn6+lS5dqy5YtkqTbb789aL8dO3YoJydHcXFxevvtt/XTn/5UjY2NysjI0KRJk1RUVKSYmJjrKAUAAPQUXQ4oOTk5Msa0u72jbZKUkZGhnTt3dvW0AADgBsJ78QAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYp8sBZdeuXZoyZYo8Ho8cDoc2b94ctN0Yo6VLl8rj8Sg+Pl45OTk6dOhQUB+/368FCxYoJSVFCQkJmjp1qk6dOnVdhQAAgJ6jywHl4sWLGj58uFatWtXm9meeeUbPPfecVq1apb1798rtduvee+9VQ0NDoM/ChQu1adMmbdiwQe+++64aGxs1efJkXb58+dorAQAAPUavru7g9Xrl9Xrb3GaM0cqVK/XEE09o2rRpkqQ1a9YoLS1Nr7/+ur773e+qvr5eq1ev1i9+8Qt94xvfkCStW7dOGRkZ2r59uyZMmHAd5QAAgJ4grPegVFdXq7a2Vnl5eYE2p9OpcePGqby8XJJUUVGh5ubmoD4ej0fZ2dmBPlfy+/3y+XxBCwAA6LnCGlBqa2slSWlpaUHtaWlpgW21tbWKi4tT37592+1zpeLiYrlcrsCSkZERzmEDAADLROQpHofDEbRujAlpu1JHfQoLC1VfXx9YTp48GbaxAgAA+4Q1oLjdbkkKuRJSV1cXuKridrvV1NSk8+fPt9vnSk6nU0lJSUELAADoucIaULKysuR2u1VaWhpoa2pq0s6dOzV27FhJ0siRIxUbGxvUp6amRgcPHgz0AQAAN7YuP8XT2NioI0eOBNarq6u1f/9+JScna8CAAVq4cKGWL1+uwYMHa/DgwVq+fLn69Omjhx56SJLkcrk0e/ZsLVq0SP369VNycrIWL16sYcOGBZ7qAQAAN7YuB5R9+/YpNzc3sF5QUCBJys/P12uvvabHH39cly5d0ve//32dP39eo0eP1u9+9zslJiYG9nn++efVq1cvPfDAA7p06ZLuuecevfbaa4qJiQlDSQAAoLtzGGNMtAfRVT6fTy6XS/X19dyPAiDIwCVbr3nfYysmhXEkAK7Uld/fvBcPAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOuEPaAMHDhQDocjZJk3b54kadasWSHbxowZE+5hAACAbqxXuA+4d+9eXb58ObB+8OBB3Xvvvbr//vsDbRMnTlRJSUlgPS4uLtzDAAAA3VjYA8rNN98ctL5ixQoNGjRI48aNC7Q5nU653e5wnxoAAPQQEb0HpampSevWrdMjjzwih8MRaC8rK1NqaqqGDBmiOXPmqK6uLpLDAAAA3UzYr6B81ubNm3XhwgXNmjUr0Ob1enX//fcrMzNT1dXVevLJJzV+/HhVVFTI6XS2eRy/3y+/3x9Y9/l8kRw2AACIsogGlNWrV8vr9crj8QTapk+fHvg4Oztbo0aNUmZmprZu3app06a1eZzi4mItW7YskkMFAAAWidhLPMePH9f27dv16KOPdtgvPT1dmZmZqqqqardPYWGh6uvrA8vJkyfDPVwAAGCRiF1BKSkpUWpqqiZNmtRhv3PnzunkyZNKT09vt4/T6Wz35R8AANDzROQKSktLi0pKSpSfn69evf4vAzU2Nmrx4sXavXu3jh07prKyMk2ZMkUpKSm67777IjEUAADQDUXkCsr27dt14sQJPfLII0HtMTExOnDggNauXasLFy4oPT1dubm52rhxoxITEyMxFAAA0A1FJKDk5eXJGBPSHh8fr23btkXilAAAoAfhvXgAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBP2gLJ06VI5HI6gxe12B7YbY7R06VJ5PB7Fx8crJydHhw4dCvcwAABANxaRKyhDhw5VTU1NYDlw4EBg2zPPPKPnnntOq1at0t69e+V2u3XvvfeqoaEhEkMBAADdUEQCSq9eveR2uwPLzTffLOkvV09WrlypJ554QtOmTVN2drbWrFmjjz/+WK+//nokhgIAALqhiASUqqoqeTweZWVlacaMGTp69Kgkqbq6WrW1tcrLywv0dTqdGjdunMrLyyMxFAAA0A31CvcBR48erbVr12rIkCE6c+aMnn76aY0dO1aHDh1SbW2tJCktLS1on7S0NB0/frzdY/r9fvn9/sC6z+cL97ABAIBFwh5QvF5v4ONhw4bp7rvv1qBBg7RmzRqNGTNGkuRwOIL2McaEtH1WcXGxli1bFu6hAgAAS0X8MeOEhAQNGzZMVVVVgad5Wq+ktKqrqwu5qvJZhYWFqq+vDywnT56M6JgBAEB0RTyg+P1+vf/++0pPT1dWVpbcbrdKS0sD25uamrRz506NHTu23WM4nU4lJSUFLQAAoOcK+0s8ixcv1pQpUzRgwADV1dXp6aefls/nU35+vhwOhxYuXKjly5dr8ODBGjx4sJYvX64+ffrooYceCvdQAABANxX2gHLq1Ck9+OCDOnv2rG6++WaNGTNGe/bsUWZmpiTp8ccf16VLl/T9739f58+f1+jRo/W73/1OiYmJ4R4KAADophzGGBPtQXSVz+eTy+VSfX09L/cACDJwydZr3vfYiklhHAmAK3Xl9zfvxQMAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgnbAHlOLiYt15551KTExUamqqvvWtb+mDDz4I6jNr1iw5HI6gZcyYMeEeCgAA6KbCHlB27typefPmac+ePSotLdWnn36qvLw8Xbx4MajfxIkTVVNTE1jeeuutcA8FAAB0U73CfcDf/va3QeslJSVKTU1VRUWFvv71rwfanU6n3G53uE8PAAB6gIjfg1JfXy9JSk5ODmovKytTamqqhgwZojlz5qiurq7dY/j9fvl8vqAFAAD0XBENKMYYFRQU6Ktf/aqys7MD7V6vV+vXr9c777yjZ599Vnv37tX48ePl9/vbPE5xcbFcLldgycjIiOSwAQBAlDmMMSZSB583b562bt2qd999V/3792+3X01NjTIzM7VhwwZNmzYtZLvf7w8KLz6fTxkZGaqvr1dSUlJExg6gexq4ZOs173tsxaQwjgTAlXw+n1wuV6d+f4f9HpRWCxYs0JYtW7Rr164Ow4kkpaenKzMzU1VVVW1udzqdcjqdkRgmAACwUNgDijFGCxYs0KZNm1RWVqasrKyr7nPu3DmdPHlS6enp4R4OAADohsJ+D8q8efO0bt06vf7660pMTFRtba1qa2t16dIlSVJjY6MWL16s3bt369ixYyorK9OUKVOUkpKi++67L9zDAQAA3VDYr6C89NJLkqScnJyg9pKSEs2aNUsxMTE6cOCA1q5dqwsXLig9PV25ubnauHGjEhMTwz0cAADQDUXkJZ6OxMfHa9u2beE+LQAA6EF4Lx4AAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdXtEeAADYYuCSrde877EVk8I4EgBcQQEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOlF9s8AXX3xRP/nJT1RTU6OhQ4dq5cqV+trXvhbNIQGwwPW8aV+0dNc3Guyu40bPF7UrKBs3btTChQv1xBNP6L333tPXvvY1eb1enThxIlpDAgAAlojaFZTnnntOs2fP1qOPPipJWrlypbZt26aXXnpJxcXF0RqWJP6i6OmY387rjlcyAPyf7vzzLioBpampSRUVFVqyZElQe15ensrLy0P6+/1++f3+wHp9fb0kyefzRWR8Lf6Pr3nfSI0J4cP8dt71fK7QedH8uuL7oWezbX5bj2mMuWrfqASUs2fP6vLly0pLSwtqT0tLU21tbUj/4uJiLVu2LKQ9IyMjYmO8Vq6V0R4BIon5RSR016+r7jpudE4k57ehoUEul6vDPlG9SdbhcAStG2NC2iSpsLBQBQUFgfWWlhb993//t/r169dm/+vl8/mUkZGhkydPKikpKezHtxm1Uzu13zhu5NqlG7v+aNVujFFDQ4M8Hs9V+0YloKSkpCgmJibkakldXV3IVRVJcjqdcjqdQW2f+9znIjlESVJSUtIN90Xbitqp/UZD7Tdm7dKNXX80ar/alZNWUXmKJy4uTiNHjlRpaWlQe2lpqcaOHRuNIQEAAItE7SWegoICPfzwwxo1apTuvvtuvfLKKzpx4oTmzp0brSEBAABLRC2gTJ8+XefOndNTTz2lmpoaZWdn66233lJmZma0hhTgdDpVVFQU8rLSjYDaqf1GQ+03Zu3SjV1/d6jdYTrzrA8AAMD/I96LBwAAWIeAAgAArENAAQAA1iGgAAAA6/TIgHL+/Hk9/PDDcrlccrlcevjhh3XhwoUO9zHGaOnSpfJ4PIqPj1dOTo4OHToU1OeVV15RTk6OkpKS5HA42jxmZ8594sQJTZkyRQkJCUpJSdEPfvADNTU1XWfVnT//lTpTu9/v14IFC5SSkqKEhARNnTpVp06dCmwvKyuTw+Foc9m7d2+gX1vbX3755W5duyQNHDgwpK4r32uqJ877sWPHNHv2bGVlZSk+Pl6DBg1SUVFRSF3hnPcXX3xRWVlZ6t27t0aOHKnf//73HfbfuXOnRo4cqd69e+vzn/98m+d944039KUvfUlOp1Nf+tKXtGnTpi6ftzOfz+sVjdqLi4t15513KjExUampqfrWt76lDz74IKjPrFmzQuZ3zJgx11/wZ0Sj9qVLl4bU5Xa7g/r01Hlv62eaw+HQvHnzAn0iPu+mB5o4caLJzs425eXlpry83GRnZ5vJkyd3uM+KFStMYmKieeONN8yBAwfM9OnTTXp6uvH5fIE+zz//vCkuLjbFxcVGkjl//nyXz/3pp5+a7Oxsk5ubayorK01paanxeDxm/vz5Vtc+d+5cc8stt5jS0lJTWVlpcnNzzfDhw82nn35qjDHG7/ebmpqaoOXRRx81AwcONC0tLYHjSDIlJSVB/T7++ONuXbsxxmRmZpqnnnoqqK6GhobA9p4677/5zW/MrFmzzLZt28yHH35ofv3rX5vU1FSzaNGioHOFa943bNhgYmNjzauvvmoOHz5sHnvsMZOQkGCOHz/eZv+jR4+aPn36mMcee8wcPnzYvPrqqyY2Ntb8y7/8S6BPeXm5iYmJMcuXLzfvv/++Wb58uenVq5fZs2dPl87bmc/n9YhW7RMmTDAlJSXm4MGDZv/+/WbSpElmwIABprGxMdAnPz/fTJw4MWh+z507F5a6o1l7UVGRGTp0aFBddXV1QefqqfNeV1cXVHdpaamRZHbs2BHoE+l573EB5fDhw0ZS0Cd69+7dRpL5z//8zzb3aWlpMW6326xYsSLQ9sknnxiXy2VefvnlkP47duxoM6B05txvvfWWuemmm8xHH30U6PPLX/7SOJ1OU19ff001d+X8V+pM7RcuXDCxsbFmw4YNgT4fffSRuemmm8xvf/vbNo/b1NRkUlNTzVNPPRXULsls2rTpWktsV7Rrz8zMNM8//3y747tR5t0YY5555hmTlZUV1Baueb/rrrvM3Llzg9puu+02s2TJkjb7P/744+a2224Lavvud79rxowZE1h/4IEHzMSJE4P6TJgwwcyYMaPT5+3qz5BrEa3ar1RXV2ckmZ07dwba8vPzzV//9V93tpQui1btRUVFZvjw4e2O60aa98cee8wMGjQo6A/OSM97j3uJZ/fu3XK5XBo9enSgbcyYMXK5XCovL29zn+rqatXW1iovLy/Q5nQ6NW7cuHb3udZz7969W9nZ2UFvlDRhwgT5/X5VVFR0+lzXev4rdab2iooKNTc3B/XxeDzKzs5u97hbtmzR2bNnNWvWrJBt8+fPV0pKiu688069/PLLamlpuZZyg9hQ+49//GP169dPt99+u/7hH/4h6GWOG2XeJam+vl7Jyckh7dc7701NTaqoqAgajyTl5eW1O57du3eH9J8wYYL27dun5ubmDvu0HrMz5w3Xz5D2RKv2ttTX10tSyByXlZUpNTVVQ4YM0Zw5c1RXV9e54q4i2rVXVVXJ4/EoKytLM2bM0NGjRwPbbpR5b2pq0rp16/TII4+EvEFvpOZdivK7GUdCbW2tUlNTQ9pTU1ND3pzws/tICnmjwrS0NB0/fjys566trQ05T9++fRUXF9fu+MJ5/rb2kTquvba2VnFxcerbt29In/aOu3r1ak2YMEEZGRlB7X//93+ve+65R/Hx8Xr77be1aNEinT17Vn/3d3/XuSLbEe3aH3vsMY0YMUJ9+/bVf/zHf6iwsFDV1dX6p3/6p8BxboR5//DDD/XCCy/o2WefDWoPx7yfPXtWly9fbnPMHdXZVv9PP/1UZ8+eVXp6ert9Wo/ZmfOG62dIe6JV+5WMMSooKNBXv/pVZWdnB9q9Xq/uv/9+ZWZmqrq6Wk8++aTGjx+vioqK6/5PpdGsffTo0Vq7dq2GDBmiM2fO6Omnn9bYsWN16NAh9evX74aZ982bN+vChQshf3BGct6lbhRQli5dqmXLlnXYp/VmzCsTnvSXb6y22j/ryu2d2edqx2jrOF0dn621t9fn1KlT2rZtm/75n/85ZNtnfyHdfvvtkqSnnnqq3V9U3aX2H/7wh4GPv/zlL6tv3776zne+E7iqci3j6y61tzp9+rQmTpyo+++/X48++mjQtq7OezjH3Fb/K9s7c8xw9bke0aq91fz58/WnP/1J7777blD79OnTAx9nZ2dr1KhRyszM1NatWzVt2rQOKuq8aNTu9XoDHw8bNkx33323Bg0apDVr1qigoOCax9ZV0Z731atXy+v1Bl0BliI/790moMyfP18zZszosM/AgQP1pz/9SWfOnAnZ9l//9V8hibFV613ZtbW1Sk9PD7TX1dW1u097x7naud1ut/7whz8EbT9//ryam5vbPVe0a3e73WpqatL58+eD/pquq6tr892nS0pK1K9fP02dOrXDMUt/eSnC5/PpzJkzbY6xu9X+2bok6ciRI+rXr1+Pn/fTp08rNzc38MafV3O1eW9LSkqKYmJiQv7K6+j71O12t9m/V69egeDYXp/WY3bmvOH6GdKeaNX+WQsWLNCWLVu0a9cu9e/fv8PxpqenKzMzU1VVVVet7WpsqL1VQkKChg0bFqjrRpj348ePa/v27XrzzTevOt5wzruknvcUT+sNg3/4wx8CbXv27OnUDYM//vGPA21+v/+ab5Lt6NytN0uePn060GfDhg1hvVky3LW33iy5cePGQJ/Tp0+3ebNkS0uLycrKCnmKoz0vvPCC6d27t/nkk086XWdbbKj9s/71X//VSArcad+T5/3UqVNm8ODBZsaMGUFPNnXkWuf9rrvuMt/73veC2r74xS92eMPgF7/4xaC2uXPnhtww6PV6g/pMnDgx5CbZjs7b1Z8h1yJatbe0tJh58+YZj8dj/vznP3dqrGfPnjVOp9OsWbOmU/2vJlq1X+mTTz4xt9xyi1m2bJkxpmfPe6uioiLjdrtNc3PzVcca7nnvcQHFmL98or/85S+b3bt3m927d5thw4aFPHJ56623mjfffDOwvmLFCuNyucybb75pDhw4YB588MGQR8VqamrMe++9Z1599VUjyezatcu89957QY9VXe3crY+b3nPPPaaystJs377d9O/fP6yPm0ai9rlz55r+/fub7du3m8rKSjN+/PiQR22NMWb79u1Gkjl8+HDI2LZs2WJeeeUVc+DAAXPkyBHz6quvmqSkJPODH/ygW9deXl5unnvuOfPee++Zo0ePmo0bNxqPx2OmTp0aOEZPnfePPvrIfOELXzDjx483p06dCnrcsFU45731kcvVq1ebw4cPm4ULF5qEhARz7NgxY4wxS5YsMQ8//HCgf+sjlz/84Q/N4cOHzerVq0Meufz3f/93ExMTY1asWGHef/99s2LFinYfM27vvJ39fF6PaNX+ve99z7hcLlNWVtbmY+INDQ1m0aJFpry83FRXV5sdO3aYu+++29xyyy3dvvZFixaZsrIyc/ToUbNnzx4zefJkk5iYeEPMuzHGXL582QwYMMD86Ec/ChnX/8e898iAcu7cOTNz5kyTmJhoEhMTzcyZM0Ouduh//y9Dq5aWlkBSdDqd5utf/7o5cOBA0D5FRUVGUsjy2eN05tzHjx83kyZNMvHx8SY5OdnMnz//uq8gRLr2S5cumfnz55vk5GQTHx9vJk+ebE6cOBFy/gcffNCMHTu2zbH95je/Mbfffrv5q7/6K9OnTx+TnZ1tVq5c2alk3hnRqr2iosKMHj3auFwu07t3b3PrrbeaoqIic/HixaDj9MR5LykpafN74rMXZ8M97//4j/9oMjMzTVxcnBkxYkTI467jxo0L6l9WVmbuuOMOExcXZwYOHGheeumlkGP+6le/MrfeequJjY01t912m3njjTe6dF5jOvf5vF7RqL29+W39Wvr4449NXl6eufnmm01sbKwZMGCAyc/Pb/PnQ3ervfV/msTGxhqPx2OmTZtmDh06FNSnp867McZs27bNSDIffPBByLb/j3l3GPO/d88AAABYosf9HxQAAND9EVAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ3/AVtwWWh9s0RiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.hist(events[0]['win'],30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72cb72a7-3170-4cb1-bf6e-9ab86b15dcd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win 1 shorter, 0.377 min win = -0.00015570549999999997, max win = 0.00010378549999999973\n",
      "win 2 shorter, 0.708 min win = -0.00025026199999999996, max win = 0.00022796999999999924\n",
      "win 3 shorter, 0.873 min win = -0.00018683699999999995, max win = 0.00014331949999999932\n",
      "win 2 longer, 0.908 min win = -0.00020543599999999995, max win = 0.0003677499999999991\n",
      "win 3 longer, 0.913 min win = -0.0005441534999999999, max win = 0.0005306754999999995\n",
      "win 2 longer, 0.932 min win = -0.0002520334999999998, max win = 0.0004522294999999983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd1b6b70fd0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGdCAYAAAD60sxaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdeXyM1/7A8c/MZN8m+yIiiTWCWCKILdGiqktUW1RrKaWu3url11a1rqWluqq2LlVSqnSl1ipCTSyxS2KLRDZCVmTfM/P8/hgZItskQoLzfr3mlck85znPeSIy3znne86RSZIkIQiCIAiC8AiRN3YDBEEQBEEQ7jcRAAmCIAiC8MgRAZAgCIIgCI8cEQAJgiAIgvDIEQGQIAiCIAiPHBEACYIgCILwyBEBkCAIgiAIjxwRAAmCIAiC8MgxaOwGNEUajYbk5GQsLS2RyWSN3RxBEARBEPQgSRK5ubk0a9YMubzmPh4RAFUhOTkZNze3xm6GIAiCIAj1kJSURPPmzWssIwKgKlhaWgLaH6CVlVUjt0YQBEEQBH3k5OTg5uamex+viQiAqlA+7GVlZSUCIEEQBEF4wOiTviKSoAVBEARBeOSIAEgQBEEQhEeOCIAEQRAEQXjkiBygepIkibKyMtRqdWM3RRDuOUNDQxQKRWM3QxAEocGIAKgeSkpKSElJoaCgoLGbIgj3hUwmo3nz5lhYWDR2UwRBEBqECIDqSKPRkJCQgEKhoFmzZhgZGYnFEoWHmiRJZGRkcOXKFdq0aSN6ggRBeCiIAKiOSkpK0Gg0uLm5YWZm1tjNEYT7wsHBgcTEREpLS0UAJAjCQ0EkQddTbUtsC8LDRPRyCoLwsBHv4oIgCIIgPHJEACQIgiAIwiNHBEBCldasWYO1tXVjN6MSmUzG5s2bG7sZgiAIwgNOBEBClUaOHElMTMxd1VFUVMT48ePp1KkTBgYGDBs2rGEad480RnAVGhqKr68vJiYmtGzZku+++67G8tevX2fIkCE0a9YMY2Nj3Nzc+Pe//01OTo6uzLx585DJZJUe5ubmujLjx4+vskyHDh3u2b0KgiA0JY0eAC1btgxPT09MTEzw9fXlwIED1ZZNSUlh9OjRtGvXDrlczn/+858qy23cuBFvb2+MjY3x9vZm06ZN96j1Dy9TU1McHR3vqg61Wo2pqSnTpk1j4MCBDdSyhldSUtKg9ZWWlupVLiEhgaFDh9KvXz/Cw8N5//33mTZtGhs3bqz2HLlcTlBQEFu3biUmJoY1a9awZ88epkyZoivz9ttvk5KSUuHh7e3Niy++qCvz9ddfVzielJSEra1thTKCIAgPNakR/frrr5KhoaG0cuVK6fz589Jbb70lmZubS5cuXaqyfEJCgjRt2jTpxx9/lLp06SK99dZblcqEhYVJCoVC+vjjj6WoqCjp448/lgwMDKQjR47o3a7s7GwJkLKzsysdKywslM6fPy8VFhbqXtNoNFJ+cWmjPDQajV73tHXrVkmpVEpqtVqSJEkKDw+XAOntt9/WlZk8ebI0atQoSZIkafXq1ZJSqdQdmzt3rtS5c2dp7dq1kru7u2RlZSWNHDlSysnJ0ev648aNk4KCgmotV1xcLL3xxhuSs7OzZGxsLLm7u0sff/yx7jggrVy5Uho2bJhkamoqtW7dWtqyZUuFOlQqleTn5ycZGRlJzs7O0syZM6XS0lLd8YCAAOmNN96Qpk+fLtnZ2Un9+/eX3N3dJUD3cHd3r/Cz69atm2RsbCx5enpK8+bNq1AfIC1fvlx69tlnJTMzM2nOnDl6/UzeffddycvLq8Jrr7/+utSrVy+9zi/39ddfS82bN6/2eEREhARI+/fvr7bMpk2bJJlMJiUmJlZ5vKrfe6GBaDSSlHRckvbMl6T1IyXph6GS9OsrkqT6TJLSohq7dYLwQKnp/ftOjboO0OLFi5k4cSKvvfYaAEuWLGHXrl0sX76cRYsWVSrv4eHB119/DcAPP/xQZZ1Llixh0KBBzJo1C4BZs2YRGhrKkiVL+OWXX+7JfRSWqvGes+ue1F2b8x8+gZlR7f+M/fv3Jzc3l/DwcHx9fQkNDcXe3p7Q0FBdGZVKxfTp06utIy4ujs2bN7N9+3YyMzMZMWIEn3zyCQsXLmyQewH45ptv2Lp1K7///jstWrQgKSmJpKSkCmXmz5/PZ599xueff863337Lyy+/zKVLl7C1teXq1asMHTqU8ePHs3btWi5cuMCkSZMwMTFh3rx5ujp+/PFH/vWvf3Ho0CEkScLOzg5HR0dWr17NkCFDdGvd7Nq1i1deeYVvvvmGfv36ERcXx+TJkwGYO3eurr65c+eyaNEivvrqK73XyTl8+DCDBw+u8NoTTzxBcHAwpaWlGBoa1lpHcnIyf/75JwEBAdWWWbVqFW3btqVfv37VlgkODmbgwIG4u7vr1XahgVw5CbtmQdLRyseitsK+BeDRDwYvgGZd7nvzBOFh1mhDYCUlJZw8ebLSG8DgwYMJCwurd73VvanUVGdxcTE5OTkVHg8bpVJJly5dUKlUwK1gJzIyktzcXFJTU4mJiSEwMLDaOjQaDWvWrKFjx47069ePMWPGsHfv3gZt5+XLl2nTpg19+/bF3d2dvn378tJLL1UoM378eF566SVat27Nxx9/TH5+PseOHQO0Q6pubm4sXboULy8vhg0bxvz58/nyyy/RaDS6Olq3bs1nn31Gu3bt8PLywsHBAQBra2ucnZ113y9cuJD33nuPcePG0bJlSwYNGsRHH33EihUrKrRp9OjRTJgwgZYtW+odRKSmpuLk5FThNScnJ8rKyrh27VqN57700kuYmZnh6uqKlZUVq1atqrJccXEx69evZ+LEidXWlZKSwt9//637ICLcB5IEB7+CVY9rgx8DU+gwHJ76Ep4PhkEfQdsnQSaHxAOwcgDsng1q/YZXBUGoXaP1AF27dg21Wl3lG0Bqamq9663uTaWmOhctWsT8+fPrfU1TQwXnP3yi3uffDVND/VflDQwMRKVSMWPGDA4cOMCCBQvYuHEjBw8eJCsrCycnJ7y8vKo938PDA0tLS933Li4upKen31X77zR+/HgGDRpEu3btGDJkCE8//XSlgNbHx0f33NzcHEtLS107oqKi8Pf3r7BwX58+fcjLy+PKlSu0aNECgO7du+vVnpMnT3L8+PEKvVxqtZqioiIKCgp0q4HrW9+d7lxgUJKkKl+/01dffcXcuXOJjo7m/fffZ8aMGSxbtqxSuT///JPc3FzGjh1bbV3lM/6aepL6Q0OjgW1vQvg67fedRsCg+WDVrGK5PtMg+wqEzIWzGyDsW0iJhBd/BDPb+99uQXjINPpWGFW9AdztqrN1rXPWrFnMmDFD931OTg5ubm51up4+w1CNLTAwkODgYCIjI5HL5Xh7exMQEEBoaCiZmZk1DqMAlYZkZDJZhV6VhtCtWzcSEhL4+++/2bNnDyNGjGDgwIFs2LBBr3ZU9W9dVVBx+4yommg0GubPn8/w4cMrHTMxMalzfbdzdnauFJinp6djYGCAnZ1drec6Ozvj5eWFnZ0d/fr147///S8uLi4Vyq1atYqnn34aZ2fnKuuRJIkffviBMWPGYGRkVOd7EOpIkmDX+9rgR6aAJz+FHpOqL69sDi8EQ4fnYNPrkLAf1jwF47aBuf39a7cgPIQa7V3b3t4ehUJR5RvAnT04dVHdm0pNdRobG2NsbFzvaz4oyvOAlixZQkBAADKZjICAABYtWkRmZiZvvfVWYzcRACsrK0aOHMnIkSN54YUXGDJkCDdu3MDWtvZPvd7e3mzcuLFCIBQWFoalpSWurq41nmtoaIhara7wWrdu3YiOjqZ169b1v6Fq+Pv7s23btgqv7d69m+7du+uV/1OuPMArLi6u8HpCQgL79u1j69at1Z4bGhpKbGxsjUNkQgM6shyOLtc+f+478Bmh33ntnwbb3fDTcEg/D2uDtEGQ6AkShHprtBwgIyMjfH19CQkJqfB6SEgIvXv3rne9/v7+lercvXv3XdX5sCjPA1q3bp0u16d///6cOnWq1vyf+jp//jwRERHcuHGD7OxsIiIiiIiIqLb8V199xa+//sqFCxeIiYnhjz/+wNnZWe9FGadOnUpSUhJvvvkmFy5cYMuWLcydO5cZM2bUun+bh4cHe/fuJTU1lczMTADmzJnD2rVrmTdvHufOnSMqKorffvuN2bNn6/sjqNaUKVO4dOkSM2bMICoqih9++IHg4GDefvttXZlNmzZVGJbcsWMHq1ev5uzZsyQmJrJjxw7+9a9/0adPHzw8PCrU/8MPP+Di4sKTTz5ZbRuCg4Pp2bMnHTt2vOv7EWpx9RSE/Ff7fPAC8BmBRiNxKPYa//d7JE9/ewD/RXsZ9r9DzN1ylpi03IrnO3WA8dvB3BHSzsLvY0VOkCDchUYdt5kxYwZjxoyhe/fu+Pv78/3333P58mXdmiazZs3i6tWrrF27VndO+ZtnXl4eGRkZREREYGRkhLe3NwBvvfUW/fv359NPPyUoKIgtW7awZ88eDh48eN/vrykaMGAAp06d0gU7NjY2eHt7k5ycTPv27Rv8ekOHDuXSpUu677t27Qrc6rW4k4WFBZ9++ikXL15EoVDg5+fHjh079N581tXVlR07dvDOO+/QuXNnbG1tmThxol4By5dffsmMGTNYuXIlrq6uJCYm8sQTT7B9+3Y+/PBDPvvsMwwNDfHy8tIrYdjDw4Px48dXmH12O09PT3bs2MH06dP53//+R7Nmzfjmm294/vnndWWys7OJjo7WfW9qasrKlSuZPn06xcXFuLm5MXz4cN57770KdZcnrI8fP77aWWnZ2dls3LhRN7NSuIeKc2HjRNCUgXcQ+P+b8MuZvL/pLFEpFSddpGQXEZGUxY+HLzG8qyvzgzpgaXKzR9C+DYzdDMGDtcnRf78LT391/+9HEB4CMqm6d6L7ZNmyZXz22WekpKTQsWNHvvrqK/r37w9oE2ITExN1M5eg6uRQd3d3EhMTdd9v2LCB2bNnEx8fT6tWrVi4cGGVORzVycnJQalUkp2djZWVVYVjRUVFJCQk6BZvFISqFBYWYmtry44dOxgwYEBjN+euid/7u7RzFhxZBlbN0bx+kK/DMvjmn4tIElgYG/BcV1f6trHH0dKYq1mFbI9MYff5VDQSuNuZsXZCD9ztbsszi/4bfnkJkGD4Sv2H0gThIVfT+/edGj0AaopEACTcrZ07d/Ltt9/y119/NXZTGoT4vb8Laefgu34gqSl56Q+mn7DnrzMpAAzv6srsp72xNa+cgH7y0g2m/RLB1axCXK1N+X2KP67WprcK7FsEoZ+AkSVMOQC2nvfrjgShyapLANToW2EIwsNoyJAhD03wI9wFSYK/3gZJjdrrGf512Ia/zqRgqJDx2Qs+LB7ZpcrgB8DX3ZZNb/Smpb05V7MKGRt8lPzislsF+r8Dbr2gJFc7Q6yBZ2QKwsNOBECCIAj3SvTfcDkMycCU2QWj2XshHWMDOavH92BE99qX2nC0NGHdaz1xtjIhLiOfuVvP3TqoMIDnV2p7gJKOwonge3gjgvDwEQGQIAjCvaDRwD7tApqH7F/klxgJIwM5weP86NtG/zV8mlmbsmRUF+Qy2HDyCtsik28dtG4Bj8/RPt8zH7KvNuQdCMJDTQRAgiAI98L5TZB2lhIDC95I7AvAly92rlPwU65XSzv+PUC7FtWH28+TW3Tb9He/idDcTzsUtvvul2cQhEeFCIAEQRAamkYDoZ8BsLx4CNlY8PbgtjzTuVktJ1bvjcda42lvTkZuMd/svXjrgFxxcyq8DM79CZeP3GXjBeHRIAIgQRCEhhYbAhkXyMeUVaVDeKKDE28MuLvVxI0NFMx5Rrve2epDiSRcy7910LkTdLu539vO90RCtCDoQQRAgiAIDUwK+waAdWWPYW/vwBcvdr7rPQ4BBrRzJLCdA2UaiaX/xFY8+NhsbUJ0cri2J0gQhBqJAEioUvkO4U2NTCZj8+bNjd0MQaje1VPIEg9SKilYp3mSr0d1ubWScwOYPrAtAJsjrnLp+m29QBaO2h3kAVSfgLqsirMFQSgnAiChSiNHjiQmJuau6lCpVAQFBeHi4oK5uTldunRh/fr1DdTChtcYwVVoaCi+vr6YmJjQsmVLvvvuuxrLr1mzBplMVuUjPT1dV+7MmTMEBARgamqKq6srH374YaXtR9avX0/nzp0xMzPDxcWFV199levXr9+T+3yU5O//HwDbNb0YNcgfn+bWDVp/ZzdrAts5oK6qF6jnFDC1hesX4czvDXpdQXjYiABIqJKpqSmOjo53VUdYWBg+Pj5s3LiR06dPM2HCBMaOHVtpB/TGVlJS0qD1lZbqt0FlQkICQ4cOpV+/foSHh/P+++8zbdo0Nm7cWO05I0eOJCUlpcLjiSeeICAgQPfvlZOTw6BBg2jWrBnHjx/n22+/5YsvvmDx4sW6eg4ePMjYsWOZOHEi586d448//uD48eN67XEmVK8s9xqG0VsAOO7wIlMCWt2T60x7vA2g7QVKzy26dcDECvr+R/tc9AIJQs0koZLs7GwJkLKzsysdKywslM6fPy8VFhY2Qsvqb+vWrZJSqZTUarUkSZIUHh4uAdLbb7+tKzN58mRp1KhRkiRJ0urVqyWlUqk7NnfuXKlz587S2rVrJXd3d8nKykoaOXKklJOTU6d2DB06VHr11VerPV5cXCy98cYbkrOzs2RsbCy5u7tLH3/8se44IK1cuVIaNmyYZGpqKrVu3VrasmVLhTpUKpXk5+cnGRkZSc7OztLMmTOl0tJS3fGAgADpjTfekKZPny7Z2dlJ/fv3l9zd3SVA93B3d6/ws+vWrZtkbGwseXp6SvPmzatQHyAtX75cevbZZyUzMzNpzpw5ev0s3n33XcnLy6vCa6+//rrUq1cvvc6XJElKT0+XDA0NpbVr1+peW7ZsmaRUKqWioiLda4sWLZKaNWsmaTQaSZIk6fPPP5datmxZoa5vvvlGat68eZXXeVB/7++3g2vnSNJcK+ncnM5S0vW8e3qt5/53UHKfuV1aEhJT8UBxviR96ilJc60kKfL3e9oGQWhqanr/vpPoAWoIkgQl+Y3z0HMrt/79+5Obm0t4eDigHXqxt7cnNDRUV0alUhEQEFBtHXFxcWzevJnt27ezfft2QkND+eSTT+r0o8rOzsbW1rba49988w1bt27l999/Jzo6mnXr1uHh4VGhzPz58xkxYgSnT59m6NChvPzyy9y4cQOAq1evMnToUPz8/IiMjGT58uUEBwezYMGCCnX8+OOPGBgYcOjQIVasWMHx48cBWL16NSkpKbrvd+3axSuvvMK0adM4f/48K1asYM2aNSxcuLBCfXPnziUoKIgzZ84wYcIEvX4Whw8fZvDgwRVee+KJJzhx4oTevUhr167FzMyMF154oUK9AQEBGBsbV6g3OTlZt2lw7969uXLlCjt27ECSJNLS0tiwYQNPPfWUXtcVKotNy8E19lcACjuPo7mteS1n3J1xvT0AWH/0EiVlt836MjKDnv/SPj+0RO+/EYLwqDFo7AY8FEoL4OP6r+9xV95PBqPa/9AqlUq6dOmCSqXC19cXlUrF9OnTmT9/Prm5ueTn5xMTE0NgYGC1dWg0GtasWYOlpSUAY8aMYe/evZWCgeps2LCB48ePs2LFimrLXL58mTZt2tC3b19kMhnu7u6VyowfP56XXnoJgI8//phvv/2WY8eOMWTIEJYtW4abmxtLly5FJpPh5eVFcnIyM2fOZM6cOcjl2pi/devWfPbZZ5Xqtra2xtnZWff9woULee+99xg3bhwALVu25KOPPuLdd99l7ty5unKjR4/WO/Apl5qaipOTU4XXnJycKCsr49q1a7i4uNRaxw8//MDo0aMxNb21SWZqamqloLH8OqmpqXh6etK7d2/Wr1/PyJEjKSoqoqysjGeffZZvv/22TvcgaKk1Ej/9up75slQKZGZ0e+reDyU+2dGFhZZRpOcW8/fZFIK6uN462OM1bfCTdhZi90CbQfe8PYLwoBE9QI+QwMBAVCoVkiRx4MABgoKC6NixIwcPHmTfvn04OTnh5eVV7fkeHh664AfAxcWlQuJtTVQqFePHj2flypV06NCh2nLjx48nIiKCdu3aMW3aNHbv3l2pjI+Pj+65ubk5lpaWunZERUXh7+9fYcpxnz59yMvL48qVK7rXunfvrle7T548yYcffoiFhYXuMWnSJFJSUigoKKhzfXe6c2q0dPPTuj5Tpg8fPsz58+eZOHFines9f/4806ZNY86cOZw8eZKdO3eSkJDAlClT6nUfj7q1hxPxuabd/Fbq8DwyY8tazrh7RgZyXurRAoDfTyRVPGhqA77jtc9vTskXBKEi0QPUEAzNtD0xjXVtPQUGBhIcHExkZCRyuRxvb28CAgIIDQ0lMzOzxuEvAEPDilN5ZTIZGj0WXAsNDeWZZ55h8eLFjB07tsay3bp1IyEhgb///ps9e/YwYsQIBg4cyIYNG/RqhyRJegUV5ub6DU9oNBrmz5/P8OHDKx0zMTGpc323c3Z2JjU1tcJr6enpGBgYYGdnV+v5q1atokuXLvj6+upVL9zqCVq0aBF9+vThnXfeAbRBpbm5Of369WPBggV69T4JWpevF7B0ZwQH5McAMO9Z8+94Q3rBtzlf771IWNx1rmQW0Nzmtr8HPafAkWWQsB/So8Cx/X1rlyA8CEQPUEOQybTDUI3xqMPiauV5QEuWLCEgIACZTEZAQAAqlarW/J/6UqlUPPXUU3zyySdMnjxZr3OsrKwYOXIkK1eu5LfffmPjxo26HJ/aeHt7ExYWVmHKd1hYGJaWlri6utZwpjawUqvVFV7r1q0b0dHRtG7dutKjfDitvvz9/QkJCanw2u7du+nevXulIO9OeXl5/P7771X2/vj7+7N///4Ks9t2795Ns2bNdENjBQUFldqvUCgAKk2XF6onSRIfbD7DAM1hzGTFSHattfty3Sdutmb0bmWHJMHGk3dshGrtBl43c7qOVj/sLAiPKhEAPULK84DWrVuny/Xp378/p06dqjX/pz7Kg59p06bx/PPPk5qaSmpqao3BzFdffcWvv/7KhQsXiImJ4Y8//sDZ2VnvRRmnTp1KUlISb775JhcuXGDLli3MnTuXGTNm1BqweHh4sHfvXlJTU8nMzARgzpw5rF27lnnz5nHu3DmioqL47bffmD377jednDJlCpcuXWLGjBlERUXxww8/EBwczNtvv60rs2nTpiqHJX/77TfKysp4+eWXKx0bPXo0xsbGjB8/nrNnz7Jp0yY+/vhjZsyYoesFe+aZZ/jzzz9Zvnw58fHxHDp0iGnTptGjRw+aNWukfLYH0NbIZA5cvMaLBvsBkHV+qU4fShrCi92bA7DhVBIazR3Ba8+bQ5qRv0Jh5n1tlyA0dSIAesQMGDAAtVqtC3ZsbGzw9vbGwcGB9u0btot8zZo1FBQUsGjRIlxcXHSPqoaTyllYWPDpp5/SvXt3/Pz8SExMZMeOHXr3tri6urJjxw6OHTtG586dmTJlChMnTtQrYPnyyy8JCQnBzc2Nrl27AtrZU9u3byckJAQ/Pz969erF4sWLq0zOvpOHhwfz5s2r9rinpyc7duxApVLRpUsXPvroI7755huef/55XZns7Gyio6MrnRscHMzw4cOxsbGpdEypVBISEsKVK1fo3r07U6dOZcaMGcyYMUNXZvz48SxevJilS5fSsWNHXnzxRdq1a8eff4otFPSVXVDKR9vP48QNesqitC/6jLzv7RjSwQULYwOSbhRy6vIdQY57H3DqCGWFEPHzfW+bIDRlMkn0d1eSk5ODUqkkOzsbKyurCseKiopISEjA09OzQg6IINyusLAQW1tbduzYwYABAxq7OXdN/N5X9v6mM/x89DJvK/fx7+KV4NYLJu5qlLZM/y2CTeFXGd/bg3nP3jHJ4Pgq+Ov/wMELph657z1UgnA/1fT+fSfRAyQI90BoaCiPPfbYQxH8CJWdvHSDn49eBmCc1Sntix2GNVp7numsTVr/60wK6juHwTq9CAamkHEBko41QusEoWkSAZAg3ANDhgzhr7/+auxmCPdAqVrD+3+eBWBSZyMsM05qD3gHNVqb+rZ2QGlqSEZuMccS7sixM1FCh+e0z0+tvf+NE4QmSgRAgiAIdRB8MIHotFxszY2Y3uxm7k8Lf7BqvORxIwM5T3TQLnGw/XQVS3L4ahfy5NyfUJRzH1smCE2XCIAEQRD0lHSjgCV7YgD4YGh7zC7e3Ni3vIelET3low3Adp9PqzwbzK0n2LXRrlof1bQ2IxaExiICIEEQBD1IksScLWcpKtXg39KO4a0kuHIMkEH7Zxu7efi3tMPS2ICM3GIirmRVPCiT3ZqhdvrX+942QWiKRAAkCIKghx1nUtkXnYGRQs6C5zoii9qqPeDeG6waf+VsIwM5gV6OAOw+l1a5gM8I7deEA5B9tfJxQXjEiABIEAShFrlFpczfdg6AfwW2opWDBZzbpD3oPazxGnaHQd7aPKCQ86mVD9q4Q4vegARn/ri/DROEJkgEQIIgCLX4es9F0nOL8bQ351+BrSArCa4cB2Tg3fjDX+UC2zlgqJARl5FPXEZe5QKdbw6DiQBIEEQAJAiCUJOLabmsCUsEYN6zHTAxVMCFm0scuPcGS+fGa9wdrEwM6dVSu5Hu3qgqhsHaPwtyA0g7C9cu3ufWCULTIgIgoUpr1qzRe/+t+0kmk7F58+bGbobwiJAkiXnbzlGmkRjs7URAWwftgegd2q/thjZe46rx2M08IFV0RuWDZrbQ8ubinOc2379GCUITJAIgoUojR44kJibmruqIjo5mwIABODk5YWJiQsuWLZk9ezalpaUN1MqG1RjBVWhoKL6+vrqfz3fffVfrOcePH+fxxx/H2toaGxsbBg8eTEREhO74vHnzkMlklR7m5ua6MgcPHqRPnz7Y2dlhamqKl5cXX3311b24xQfa32dTORR7HWMDOf992lv7YlE2XDqkfd7uycZrXDUC22kDoOOJN8grLqtcoHzKfnkOkyA8okQAJFTJ1NQUR0fHu6rD0NCQsWPHsnv3bqKjo1myZAkrV65k7ty5DdTKhlFSUtKg9ekb4CUkJDB06FD69etHeHg477//PtOmTWPjxo3VnpObm8sTTzxBixYtOHr0KAcPHsTKyoonnnhCd923336blJSUCg9vb29efPFFXT3m5ub8+9//Zv/+/URFRTF79mxmz57N999/f3c3/xApKCljwfbzAEwJaIWbrZn2QOwe0JSBfVuwa9WILayap705HnZmlKolDsVeq1zAayjIDSH9HGRU3mhXEB4VIgB6RGzbtg1ra2s0Gg0AERERyGQy3nnnHV2Z119/nZdeegmoPAQ2b948unTpwk8//YSHhwdKpZJRo0aRm5tb7TVbtmzJq6++SufOnXF3d+fZZ5/l5Zdf5sCBA9WeU1JSwr///W9cXFwwMTHBw8ODRYsWVShz7do1nnvuOczMzGjTpg1bt26tcDw0NJQePXpgbGyMi4sL7733HmVltz4JBwYG8u9//5sZM2Zgb2/PoEGD8PDwAOC5555DJpPpvi//2d3eSzN//vwK9clkMr777juCgoIwNzdnwYIF1d7f7b777jtatGjBkiVLaN++Pa+99hoTJkzgiy++qPac6OhoMjMz+fDDD2nXrh0dOnRg7ty5pKenc/mydm8qCwsLnJ2ddY+0tDTOnz/PxIkTdfV07dqVl156iQ4dOuDh4cErr7zCE088UeO/zaNm2b44krOLaG5jqk18Lhe9U/u17ZDGaZgeynuBVNHplQ+a2kCrx7TPz2+5j60ShKZFBEANQJIkCkoLGuUhSVLtDQT69+9Pbm4u4eHhgDZIsLe3JzQ0VFdGpVIREBBQbR1xcXFs3ryZ7du3s337dkJDQ/nkk0/0/jnFxsayc+fOGq/xzTffsHXrVn7//Xeio6NZt25dhWAEYP78+YwYMYLTp08zdOhQXn75ZW7c0O5/dPXqVYYOHYqfnx+RkZEsX76c4ODgSkHJjz/+iIGBAYcOHWLFihUcP34cgNWrV5OSkqL7fteuXbzyyitMmzaN8+fPs2LFCtasWcPChQsr1Dd37lyCgoI4c+YMEyZM0OvncfjwYQYPHlzhtSeeeIITJ05U24vUrl077O3tCQ4OpqSkhMLCQoKDg+nQoQPu7u5VnrNq1Sratm1Lv379qm1LeHg4YWFhNf7bPEoSr+Xz/f54AGY/5a1NfAZQl8HF3drnTXD4q1xgO22ukio6o+q/EeUz1y5sv4+tEoSmxaCxG/AwKCwrpOfPPRvl2kdHH8XM0KzWckqlki5duqBSqfD19UWlUjF9+nTmz59Pbm4u+fn5xMTEEBgYWG0dGo2GNWvWYGlpCcCYMWPYu3dvpWDgTr179+bUqVMUFxczefJkPvzww2rLXr58mTZt2tC3b19kMlmVb+rjx4/X9VR9/PHHfPvttxw7dowhQ4awbNky3NzcWLp0KTKZDC8vL5KTk5k5cyZz5sxBLtfG/K1bt+azzz6rVLe1tTXOzrdm9SxcuJD33nuPceO0eym1bNmSjz76iHfffbfCUN7o0aP1DnzKpaam4uTkVOE1JycnysrKuHbtGi4ulRfXs7S0RKVSERQUxEcffQRA27Zt2bVrFwYGlf87FxcXs379et57770q29C8eXMyMjIoKytj3rx5vPbaa3W6h4fVR9vPU6LW0K+NvW6PLQCSjkBRFpjaQvMejda+2vRqaYeRgZyU7CLir+Vr1y26XdshIJNDSqR2Sr+1W+M0VBAakegBeoQEBgaiUqmQJIkDBw4QFBREx44dOXjwIPv27cPJyQkvL69qz/fw8NAFPwAuLi6kp1fRxX6H3377jVOnTvHzzz/z119/1TjEM378eCIiImjXrh3Tpk1j9+7dlcr4+Pjonpubm2NpaalrR1RUFP7+/shkMl2ZPn36kJeXx5UrV3Svde/evdZ2A5w8eZIPP/wQCwsL3WPSpEmkpKRQUFBQ5/rudHs7Ad2n9TtfL1dYWMiECRPo06cPR44c4dChQ3To0IGhQ4dSWFhYqfyff/5Jbm4uY8eOrbK+AwcOcOLECb777juWLFnCL7/8Uq/7eJgcuJjB3gvpGMhlzH2mQ8V/i+i/tV/bDAZF0/38aGKooLu7DQBhVeUBmduDWy/t8/IZbYLwiGm6/4MfIKYGphwdfbTRrq2vwMBAgoODiYyMRC6X4+3tTUBAAKGhoWRmZtY6/GFoaFjhe5lMpsspqombm/bTpbe3N2q1msmTJ/N///d/KBSKSmW7detGQkICf//9N3v27GHEiBEMHDiQDRs26NUOSZL0CipunxFVE41Gw/z58xk+fHilYyYmJnWu73bOzs6kplZcsTc9PR0DAwPs7OyqPOfnn38mMTGRw4cP63qzfv75Z2xsbNiyZQujRo2qUH7VqlU8/fTTFXq1bufp6QlAp06dSEtLY968ebretUeRWiOx8C/tDu9j/N1p7XhHz0nMzfyfdk03/6dc71Z2hMVdJyzuOmP8PSoX8HoKLodph8F6vn7f2ycIjU0EQA1AJpPpNQzV2MrzgJYsWUJAQAAymYyAgAAWLVpEZmYmb7311j1vgyRJlJaW1pi7ZGVlxciRIxk5ciQvvPACQ4YM4caNG9ja2tZav7e3Nxs3bqwQCIWFhWFpaYmrq2uN5xoaGqJWqyu81q1bN6Kjo2ndurUed1c3/v7+bNtWcWfu3bt3071790pBXrmCggLkcnmFYK78+zuD0YSEBPbt21cpSbw6kiRRXFxcx7t4uPx56goXUnOxNDFg2mNtKh68dhGux2pnULV6vHEaWAe9W9vD7hgOx19Ho5GQy+/oVfQaCrs/gMRDUHBDu0aQIDxCxBDYI6Q8D2jdunW6XJ/+/ftz6tSpWvN/6mP9+vX8/vvvREVFER8fzx9//MGsWbMYOXJklfkqAF999RW//vorFy5cICYmhj/++ANnZ2e9F2WcOnUqSUlJvPnmm1y4cIEtW7Ywd+5cZsyYoesxqY6Hhwd79+4lNTWVzMxMAObMmcPatWuZN28e586dIyoqit9++43Zs2fX6WdRlSlTpnDp0iVmzJhBVFQUP/zwA8HBwbz99tu6Mps2baowLDlo0CAyMzN54403iIqK4ty5c7z66qsYGBgwYMCACvX/8MMPuLi48OSTlZN1//e//7Ft2zYuXrzIxYsXWb16NV988QWvvPLKXd/Xg6qwRM2Xu7VrX/17QGtszI0qFigf/vLoCyZW97l1defjqsTC2ICsglLOp+RULmDbEhzag6SG+H33v4GC0MhEAPSIGTBgAGq1Whfs2NjY4O3tjYODA+3bt2/QaxkYGPDpp5/So0cPfHx8mDdvHm+88QarVq2q9hwLCws+/fRTunfvjp+fH4mJiezYsaPW4KWcq6srO3bs4NixY3Tu3JkpU6YwceJEvQKWL7/8kpCQENzc3OjatSugnZW1fft2QkJC8PPzo1evXixevLjaGVe38/DwYN68edUe9/T0ZMeOHahUKrp06cJHH33EN998w/PPP68rk52dTXT0rbVavLy82LZtG6dPn8bf359+/fqRnJzMzp07KyRNlyesjx8/vsqhRo1Gw6xZs+jSpQvdu3fn22+/5ZNPPqkxQf1hF3wwntScIlytTRnX26NygZhd2q9NePbX7QwUcnp4ant1Dsddr7pQm4Har7F771OrBKHpkEn6zqN+hOTk5KBUKsnOzsbKquInvaKiIhISEvD09KyQAyIItyssLMTW1pYdO3ZU6pl5ED3sv/fX8ooJ/FxFXnEZX4/qQlCXO4ZLi/PgUw/QlMKbp5rkAohVWXUgngV/RRHYzoE1r1Yxay0+FNY+CxZOMOMC6PlBQxCaqprev+8kftsF4R4IDQ3lscceeyiCn0fBclUcecVldHJV8oxPs8oFLh3SBj/W7tqhowdE71b2ABxLuEGpuooJCy16gaE55KVpN0gVhEeICIAE4R4YMmQIf/31V2M3Q9BDWk4R645cAuCdJ9pVThYGiPtH+7XVY1DNEgVNkZezJbbmRhSUqIlMyqpcwMAYPPtrn8fuua9tE4TGJgIgQRAeactVcRSXaejubkO/NvZVF7o9AHqAyOUy/Ftql1QIqy4PqPXNGW0iABIeMSIAEgThkZWSXcjPR7V7qM0Y1LbqBSizkuBajHbl5PLekgeIf6vyAKiKBREBWt9MhE46qt3pXhAeESIAEgThkbVsXxwlag09PW11gUIl5VPEXbuDqfV9a1tD6X3zvk5dyqKwRF25gK0n2LXW7nCfsP8+t04QGo8IgARBeCRdzSrk1+Pa3p/p1fX+wG3DXw9mQrunvTkuShNK1BpOXsqsulB5L5AYBhMeISIAEgThkbRsXyylaok+re3o1bKa3h+NGuJV2ucPWP5POZlMplsP6MSlG1UXaj1I+/XiHhArowiPCBEACYLwyLmWV8wfJ7Wb475555YXt0uJgMJMMLYCV9/707h7oLvHzQAosZoeII8+YGACOVcgI7rqMoLwkBEBkCAIj5y1YYmUlGno7GZNT88a9sAqH/7y7A+KqvdnexD4eWh3hj91OZOyqtYDMjQF9z7a57Eh97FlgtB4RAAkVGnNmjV677/VkBITE5HJZERERNz3awuPhoKSMtbeXPfn9f4tq8/9AYi7mQD9gOb/lGvraImliQEFJWoupOZWXah8OrxIhBYeESIAEqo0cuRIYmJi7qoOlUpFUFAQLi4umJub06VLF9avX99ALXywZGZmMmbMGJRKJUqlkjFjxpCVlVXjOWlpaYwfP55mzZphZmbGkCFDuHjxYoUyxcXFvPnmm9jb22Nubs6zzz7LlStXKpSJiYkhKCgIe3t7rKys6NOnD/v2PbqbX/5x4gpZBaW425nxRAfn6gsW52qnhsMDm/9TTi6X4euu7QU6nlhNHpBHX+3XS4dBXXafWiYIjUcEQEKVTE1NcXR0vKs6wsLC8PHxYePGjZw+fZoJEyYwduxYtm3b1kCtbFwlJSV6lx09ejQRERHs3LmTnTt3EhERwZgxY6otL0kSw4YNIz4+ni1bthAeHo67uzsDBw4kPz9fV+4///kPmzZt4tdff+XgwYPk5eXx9NNPo1bfmu781FNPUVZWxj///MPJkyfp0qULTz/9NKmpqfW78QdYmVrDqoPxALzW1xNFVas+l0s8pJ0abuPxQG1/UR2/2vKAnDqCiRJKciE18j62TBAaiSRUkp2dLQFSdnZ2pWOFhYXS+fPnpcLCwkZoWf1t3bpVUiqVklqtliRJksLDwyVAevvtt3VlJk+eLI0aNUqSJElavXq1pFQqdcfmzp0rde7cWVq7dq3k7u4uWVlZSSNHjpRycnLq1I6hQ4dKr776arXHExISJEAKDw/XvaZSqSQ/Pz/JyMhIcnZ2lmbOnCmVlpbqjgcEBEhvvvmm9M4770g2NjaSk5OTNHfu3Ar1RkVFSX369JGMjY2l9u3bSyEhIRIgbdq0SVfmypUr0ogRIyRra2vJ1tZWevbZZ6WEhATd8XHjxklBQUHSxx9/LLm4uEju7u563fP58+clQDpy5IjutcOHD0uAdOHChSrPiY6OlgDp7NmzutfKysokW1tbaeXKlZIkSVJWVpZkaGgo/frrr7oyV69eleRyubRz505JkiQpIyNDAqT9+/fryuTk5EiAtGfPHr3aL0kP7u/9nbZGXJXcZ26Xun64WyooLqu58F/vSNJcK0na9p/707h77EjcNcl95napx8IQSaPRVF3o51Haez645P42ThAaSE3v33cSPUANQJIkNAUFjfKQ9Jyy2r9/f3JzcwkPDwe0m3Xa29sTGhqqK6NSqQgICKi2jri4ODZv3sz27dvZvn07oaGhfPLJJ3X6WWVnZ2NrW0PS6R2uXr3K0KFD8fPzIzIykuXLlxMcHMyCBQsqlPvxxx8xNzfn6NGjfPbZZ3z44YeEhGiTOTUaDcOGDcPMzIyjR4/y/fff88EHH1Q4v6CggAEDBmBhYcH+/fs5ePAgFhYWDBkypEJPz969e4mKiiIkJITt27frdQ+HDx9GqVTSs2dP3Wu9evVCqVQSFhZW5TnFxcUAFXZeVygUGBkZcfDgQQBOnjxJaWkpgwcP1pVp1qwZHTt21NVrZ2dH+/btWbt2Lfn5+ZSVlbFixQqcnJzw9X1wZzXV16oD2t6fsf7umBopai78gG5/UZ3ObtYYKmSk5RRzJbOw6kLlw2CJh+5fwwShkRg0dgMeBlJhIdHdGufNpN2pk8jMzGotp1Qq6dKlCyqVCl9fX1QqFdOnT2f+/Pnk5uaSn59PTEwMgYGB1dah0WhYs2YNlpaWAIwZM4a9e/eycOFCvdq6YcMGjh8/zooVK/QqD7Bs2TLc3NxYunQpMpkMLy8vkpOTmTlzJnPmzEEu18bwPj4+zJ07F4A2bdqwdOlS9u7dy6BBg9i9ezdxcXGoVCqcnbU5HwsXLmTQoEG66/z666/I5XJWrVqlS4pdvXo11tbWqFQqXZBhbm7OqlWrMDIy0vseUlNTqxxOdHR0rHYYysvLC3d3d2bNmsWKFSswNzdn8eLFpKamkpKSoqvXyMgIGxubCuc6OTnp6pXJZISEhBAUFISlpSVyuRwnJyd27tzZKEnujSkyKYvIK9kYGcgZ08u95sJZl+H6RZApwKPf/WngPWZiqKCjq5Lwy1kcT7yBm20VfzfKA6DLN/OAFOItQnh4iR6gR0hgYCAqlQpJkjhw4ABBQUF07NiRgwcPsm/fPpycnPDy8qr2fA8PD13wA+Di4kJ6erpe11apVIwfP56VK1fSoUMHvdscFRWFv79/hZk6ffr0IS8vr0Kyr4+PT4Xzbm9bdHQ0bm5uuuAHoEePHhXKnzx5ktjYWCwtLbGwsMDCwgJbW1uKioqIi4vTlevUqVOdgp9yVc00kiSp2hlIhoaGbNy4kZiYGGxtbTEzM0OlUvHkk0+iUNTcc3F7vZIkMXXqVBwdHTlw4ADHjh0jKCiIp59+WhdIPSrWH9XO/Hqqkwt2FsY1Fy6f/eXq+0Buf1EdXR5QdStCl+cBFedA6un72DJBuP9EeN8AZKamtDt1stGura/AwECCg4OJjIxELpfj7e1NQEAAoaGhZGZm1jj8Bdo35QrXlsnQaKpYU+QOoaGhPPPMMyxevJixY8fq3V6oOkgoH/a7/fWa2lZToFFOo9Hg6+tb5Sw1BwcH3XNzc/M6tR/A2dmZtLS0Sq9nZGTg5ORU7Xm+vr5ERESQnZ1NSUkJDg4O9OzZk+7du+vqLSkpITMzs0IvUHp6Or179wbgn3/+Yfv27WRmZmJlZQVoe9VCQkL48ccfee+99+p8Pw+i7IJStkYmA/Byzxa1n/CQDX+V6+5uw/fAiepmgskV0KI3xPwNiQfBtdt9bZ8g3E+iB6gByGQy5GZmjfKo7Y39duV5QEuWLCEgIACZTEZAQAAqlarW/J/6UqlUPPXUU3zyySdMnjy5zud7e3sTFhZWIdcpLCwMS0tLXF1d9arDy8uLy5cvVwhCjh8/XqFMt27duHjxIo6OjrRu3brCQ6lU1rndt/P39yc7O5tjx47pXjt69CjZ2dm6QKUmSqUSBwcHLl68yIkTJwgKCgK0AZKhoaEu1wkgJSWFs2fP6uotKCgA0A0VlpPL5XoFrw+LjaeuUFSqwcvZUjcdvFoPwfYX1Sm/95i0PLIKqpnFqMsDOnifWiUIjUMEQI+Q8jygdevW6XJ9+vfvz6lTp2rN/6mP8uBn2rRpPP/886SmppKamsqNG9V8+qzC1KlTSUpK4s033+TChQts2bKFuXPnMmPGjEpv6tUZNGgQrVq1Yty4cZw+fZpDhw7pkqDLA8iXX34Ze3t7goKCOHDgAAkJCYSGhvLWW29VWlenrtq3b8+QIUOYNGkSR44c4ciRI0yaNImnn36adu3a6cp5eXmxadMm3fd//PEHKpVKNxV+0KBBDBs2TJePpFQqmThxIv/3f//H3r17CQ8P55VXXqFTp04MHKjd3NLf3x8bGxvGjRtHZGQkMTExvPPOOyQkJPDUU0/d1X09KCRJ0g1/vdzLvfYPDckRUJT1wG9/URU7C2NaOmh7MavdGPXOPCBBeEg1egC0bNkyPD09MTExwdfXlwMHDtRYPjQ0FF9fX0xMTGjZsiXfffddpTJLliyhXbt2mJqa4ubmxvTp0ykqKrpXt/BAGTBgAGq1Whfs2NjY4O3tjYODA+3bt2/Qa61Zs4aCggIWLVqEi4uL7jF8+HC963B1dWXHjh0cO3aMzp07M2XKFCZOnMjs2bP1rkOhULB582by8vLw8/Pjtdde051fPsvKzMyM/fv306JFC4YPH0779u2ZMGEChYWFuqGj6sybNw8PD48ay6xfv55OnToxePBgBg8ejI+PDz/99FOFMtHR0WRnZ+u+T0lJYcyYMXh5eTFt2jTGjBnDL7/8UuGcr776imHDhjFixAj69OmDmZkZ27Zt0+UJ2dvbs3PnTvLy8njsscfo3r07Bw8eZMuWLXTu3Fmvn9+D7nD8deIy8jEzUjCsS7PaT6iw/cXDlyXg515LHpBzJzAWeUDCI+BezcXXx6+//ioZGhpKK1eulM6fPy+99dZbkrm5uXTp0qUqy8fHx0tmZmbSW2+9JZ0/f15auXKlZGhoKG3YsEFXZt26dZKxsbG0fv16KSEhQdq1a5fk4uIi/ec/+q/l8TCuAyRUdPDgQQmQYmNj77qucePGSePGjbv7RjVhD/Lv/dR1JyX3mdulWX+e1u+E4CHatXCOrbq3DWskvxy9JLnP3C6NWnG4+kLrR95cD+jr+9cwQWgAdVkHqFE/3ixevJiJEyfy2muvAdqem127drF8+XIWLVpUqfx3331HixYtWLJkCaAdWjhx4gRffPEFzz//PKBdc6VPnz6MHj0a0M5ceumllyrkXwiPnk2bNmFhYUGbNm2IjY3lrbfeok+fPrRq1equ6w4NDWX/frF/UlOUkVvMrnPaJQFe6VnL1HfQbn9x5ebfiocs/6dclxbWAJy+koVaI1W9GrZHX20i9KVD0Gfa/W2gINwnjTYEVlJSwsmTJyss4gYwePDgaheHO3z4cKXyTzzxBCdOnKC0tBSAvn37cvLkSV3AEx8fz44dO2rMdyguLiYnJ6fCQ3i45ObmMnXqVLy8vBg/fjx+fn5s2bKlQepOSEjAzc2tQeoSGtbWyGTKNBKdmyvxblbzUCagTfzVlIGNJ9h63vsGNoI2jpaYGynIL1ETm55XdSHdvmBh2qRwQXgINVoP0LVr11Cr1ZWmAd++iNudUlNTqyxfVlbGtWvXcHFxYdSoUWRkZNC3b18kSaKsrIx//etfNU73XbRoEfPnz7/7mxKarLFjx9Z5Cr7w4PvzlDaB/Xnf5vqd8JBOf7+dQi6jU3MlR+JvEJGUSTtny8qFnDuBkYU2DyjjAjjpv3aXIDwoGj3Dr6o1XmqapVHbmjAqlYqFCxeybNkyevbsqRvucHFx4b///W+Vdc6aNYsZM2bovs/JyRGf6AXhARedmsu55BwM5DKe9tEj+RmaRACkkTScSjtFZEYkiTmJZBVlYWZoho2JDV0du9LDuQc2JrVM5a9FFzebmwFQFiP9qlgXSa7QrgGUsB+SjooASHgoNVoAZG9vj0KhqNTbk56eXu3icM7OzlWWNzAwwM7ODoD//ve/jBkzRpdX1KlTJ/Lz85k8eTIffPBBlVOnjY2NMTauZWVYQRAeKH+Ga3t/Bng5Ymuux+rdmZfgeqx2+wvP+7/9RXZxNj+e+5EtcVtIL6h6hfX1UeuRy+QMbDGQyT6TaWfbrspyteniZg1A+OWs6gu59bwZAB2H7hPqdR1BaMoaLQAyMjLC19eXkJAQnnvuOd3r5fsWVcXf359t27ZVeG337t10795dtxJwQUFBpSBHoVAgSZLeG4cKgvBgU2sktoRrV34e3lW/BTOJv7n9RfPu2u0g7pMyTRlrz69l1elV5JbmAmBpaElv1960UrbC3syewtJCkvOTOZpylNisWHZf2s3uS7sJahXErJ6zMDes2wrlXW8mQsek5ZJfXIa5cRVvBc1vbheTdPRubk8QmqxGHQKbMWMGY8aMoXv37vj7+/P9999z+fJlpkyZAmiHpq5evcratWsBmDJlCkuXLmXGjBlMmjSJw4cPExwcXGFtlPItF7p27aobAvvvf//Ls88+W+seSoIgPBwOx10nNacIKxMDHmtfeSPaKjXC8FdSbhLvHXiP0xna9Xba2LRhis8UAt0CMVJU3WsVkxnDqtOr2Jm4ky1xWziZdpJP+3+Kj4NPleWr4mRlgrOVCak5RZy5mk2vlnaVCzXXbrnCjTjIvw7mVZQRhAdYowZAI0eO5Pr163z44YekpKTQsWNHduzYgbu7drpqSkoKly9f1pX39PRkx44dTJ8+nf/97380a9aMb775RjcFHmD27NnIZDJmz57N1atXcXBw4JlnntF7x3JBEB585cNfT3duhrGBHh98GmH7iwNXDvDO/nfIL83H0tCSd/ze4dlWz6KQ19zetjZt+SzgM0Z6jeT9A+9zJe8KE3dN5KsBX9HXta/e1+/iZs3Oc6lEJmVVHQCZ2YJ9W7gWo10aoN2Tdb1FQWjSZJIYF6okJycHpVJJdnZ2pVWAi4qKSEhI0K1eLQiPggfp976gpIzuC/ZQUKJm47/88b258nGNrpyAVY9rV0B+N/6erwC96eIm5h+ej1pS09WxK5/0+4RmFnomat8mtySXmftncuDqAQzkBnza71MGewyu/UTgu9A4Pvn7Ak92dGb5K9Vs+bHlDQhfB31nwMC5dW6fINxvNb1/36nRt8IQmqY1a9ZgbW1936+bmJiITCYjIiLivl9beDjsPpdGQYkadzszurXQc7ZU+fBXy3u//cXac2uZEzYHtaTm2VbPEvxEcL2CHwBLI0u+HvA1QzyGUKYpY+b+mRxL0W/R1/JE6IikrOoLlecBXTlefRlBeECJAEio0siRI4mJibmrOqKjoxkwYABOTk66vdtmz56tW7TyUZKZmcmYMWNQKpUolUrGjBlDVlZWjeekpaUxfvx4mjVrhpmZGUOGDOHixYsVynz//fcEBgZiZWWFTCarss5nn32WFi1aYGJigouLC2PGjCE5ObkB765p+etMCgBBnZvVvvFpufuU//N79O98fuJzAF7r9BoL+izAUG54V3UaKgz5pN8n2iBIKuM/qv8Qnx1f63mdXJXIZZCSXURaTjV7JbrdDICunhQbowoPHREACVUyNTXF0VHP5NFqGBoaMnbsWHbv3k10dDRLlixh5cqVzJ37cHSll5SU6F129OjRREREsHPnTnbu3ElERARjxoyptrwkSQwbNky3E3x4eDju7u4MHDiQ/Px8XbmCggKGDBnC+++/X21dAwYM4Pfffyc6OpqNGzcSFxfHCy+8oHfbHyR5xWWExmQAMNTHRb+TinJu9XDcwwBoR/wOFhxZAMDEjhOZ1nWa/gFaFTRqDYlnrrFn9Xn++Pgkbf58ioknP+XJY2+w/NvNXDx7FUlTfYaDubEBbZ20iyBWOx3evp12WLC0ANLO1rutgtAUiQDoEbFt2zasra3RaDQAREREIJPJeOedd3RlXn/9dV566SWg8hDYvHnz6NKlCz/99BMeHh4olUpGjRpFbm5utdds2bIlr776Kp07d8bd3Z1nn32Wl19+mQMHDtSp7aGhofTo0QNjY2NcXFx47733KCu79Wk0MDCQadOm8e6772Jra4uzszPz5s2rUMeFCxfo27cvJiYmeHt7s2fPHmQyGZs3b9aVuXr1KiNHjsTGxgY7OzuCgoJITEzUHR8/fjzDhg1j0aJFNGvWjLZt2+rV/qioKHbu3MmqVavw9/fH39+flStXsn37dqKjo6s85+LFixw5coTly5fj5+dHu3btWLZsGXl5eRVmPf7nP//hvffeo1evXtVef/r06fTq1Qt3d3d69+7Ne++9x5EjRx7Knrh/LqRTUqahpb057ZyqWOG4KuXbX9i2BBuPe9KuiPQIZh+ajYTES14v8Va3t+od/EgaiQuHU1j7wWH++t9poo+mcv1KHqXFGgxLTLAtdMHjShd2L41mw6cnSE3Irrau8unw1Q6DyeW3ZoMlif0UhYeLCIAagCRJlBarG+Whbw57//79yc3NJTw8HNAGFfb29oSGhurKqFQqAgICqq0jLi6OzZs3s337drZv305oaCiffPKJ3j+n2NhYdu7cWeM17nT16lWGDh2Kn58fkZGRLF++nODgYBYsWFCh3I8//oi5uTlHjx7ls88+48MPPyQkJAQAjUbDsGHDMDMz4+jRo3z//fd88MEHFc4vKChgwIABWFhYsH//fg4ePIiFhQVDhgyp0NOzd+9eoqKiCAkJYfv27Xrdw+HDh1EqlfTs2VP3Wq9evVAqldXue1dcXAxQIeFYoVBgZGTEwYMH9bpuVW7cuMH69evp3bu3bu2sh8nfN4e/nuzk3GSGv5Lzknlr31uUakp5zO0x3uvxXr2Dn6z0AjZ8dpK9P0aRn1WMqaUhPgOa89RUH0bP68mo//ag/WgLLjgeoUReRPqlXDZ+epIjm+Oq7A26lQeUWf1F3W7+3l4RAZDwcGn0rTAeBmUlGr5/K7T2gvfA5K8DMDSufZqvUqmkS5cuqFQqfH19UalUTJ8+nfnz55Obm0t+fj4xMTEEBgZWW4dGo2HNmjVYWmo/WY8ZM4a9e/fWusRA7969OXXqFMXFxUyePJkPP/xQ7/tbtmwZbm5uLF26FJlMhpeXF8nJycycOZM5c+boFr308fHRDa21adOGpUuXsnfvXgYNGsTu3buJi4tDpVLh7OwMwMKFCxk0aJDuOr/++ityuZxVq1bp3pxWr16NtbU1KpVKtwmvubk5q1atwshIj5WFb0pNTa1yONHR0bHafe+8vLxwd3dn1qxZrFixAnNzcxYvXkxqaiopKSl6X7vczJkzWbp0KQUFBfTq1Uvv4O1BUlBSxr5o7QrKT3bUc/gLbkuAHtDgbSpRl/Cfff/hRtEN2tm0Y1G/Rchl9fvcGXsynX9+iqK0SI2hiYLuQz3oPMANhWHF+h5z7UG08iSrjy2g75XhtErrxsmdl8hKL2Tg+PYYGN36e+HT3BqAs1dz0Ggk5FXtDO/mp/0qFkQUHjKiB+gREhgYiEqlQpIkDhw4QFBQEB07duTgwYPs27cPJycnvLy8qj3fw8NDF/wAuLi4kJ5e9ZL9t/vtt984deoUP//8M3/99RdffPGF3m2OiorC39+/wifmPn36kJeXx5UrV3Sv+fhUXATu9rZFR0fj5uamC34AevToUaH8yZMniY2NxdLSEgsLCywsLLC1taWoqIi4uDhduU6dOtUp+ClX1Sf+mva9MzQ0ZOPGjcTExGBra4uZmRkqlYonn3yyXgt6vvPOO4SHh7N7924UCgVjx4596FZGV0VnUFSqoYWtGR302fkdIDNRu9DfPdr+4osTXxB1IwprY2u+fexbzAzN6lyHJEmc2nWJXSvPUlqkxqW1ktFze9JtsHul4KfcpE6TaOvakpCWP5LeMwK5QkbcqXR2rTqHRq3RlWvjaIGxgZy84jISr+dXWReu3QEZZF2G3KoDdkF4EIkeoAZgYCRn8tf6D+s09LX1FRgYSHBwMJGRkcjlcry9vQkICCA0NJTMzMxah6buHDKRyWS6nKKalG8s6+3tjVqtZvLkyfzf//2fXm/kVQUJd26AW1vbattgF7S9W76+vqxfv77SMQcHB91zc/O6bTkA2j3s0tLSKr2ekZFR7b53AL6+vkRERJCdnU1JSQkODg707NmT7t2717kN9vb22Nvb07ZtW9q3b4+bmxtHjhzB39+/znU1VTvqNfxVvv2FX4Nvf7E7cTe/XNDmay3suxAXizr0St0kSRKHNsYSuScJgM4D3ej9XCvkipr/3xvIDZjjP4cR20bwp3w1n4/uTuKvGhJPX0P1czQDXvFCJpNhoJDj3cyK8MtZnLmaTUsHi8qVmViBozekn9PmAXk/W+f7EISmSPQANQCZTIahsaJRHnXJJSjPA1qyZAkBAQHIZDICAgJQqVS15v80FEmSKC0t1bv3wdvbm7CwsArlw8LCsLS0xNVVvz2evLy8uHz5coUg5PjxiuuadOvWjYsXL+Lo6Ejr1q0rPJTKu3tj9Pf3Jzs7m2PHbuVQHD16lOzsbHr37l3r+UqlEgcHBy5evMiJEyeq3StPX+U/y/I8o4dBUamafy5oe/yG1mf4q4Hzf9Ly05h3eB4Ar3Z8lf7N+9e5DkmSCLst+OnzQmv6vtCm1uCnXFubtrzc/mUAvkn7hAGvtkUmg6hDKUTcrBPAx1X7+33mSvXJ0rpE6ORTdb4PQWiqRAD0CCnPA1q3bp0u16d///6cOnWq1vyf+li/fj2///47UVFRxMfH88cffzBr1ixGjhyJgYF+nY9Tp04lKSmJN998kwsXLrBlyxbmzp3LjBkzKm16W51BgwbRqlUrxo0bx+nTpzl06JAuCbo8gHz55Zext7cnKCiIAwcOkJCQQGhoKG+99VaFobb6aN++PUOGDGHSpEkcOXKEI0eOMGnSJJ5++mnatbu1m7eXlxebNm3Sff/HH3+gUql0U+EHDRrEsGHDdPlIoM0vioiIIDY2FoAzZ84QERHBjRs3ADh27BhLly4lIiKCS5cusW/fPkaPHk2rVq0eqt6f0JgMCkrUuFqb4tNcz4BVXQYJN3P3GjAAkiSJuWFzyS3JpaNdR97s+ma96jm2PUEXqAS+3I4uA1vUuY6pXabiaOpIUm4SBw3/pu8I7czFI5vjSEvMAaDjzQDo9NUaAqBmXbVfr4oASHh4iADoETNgwADUarUu2LGxscHb2xsHBwfat2/foNcyMDDg008/pUePHvj4+DBv3jzeeOMNVq1apXcdrq6u7Nixg2PHjtG5c2emTJnCxIkTmT17tt51KBQKNm/eTF5eHn5+frz22mu688tnWZmZmbF//35atGjB8OHDad++PRMmTKCwsLDW5dTnzZuHh4dHjWXWr19Pp06dGDx4MIMHD8bHx4effvqpQpno6Giys2+9CaWkpDBmzBi8vLyYNm0aY8aMqTAFHuC7776ja9euTJo0CdAGtF27dmXr1q2Adj2nP//8k8cff5x27doxYcIEOnbsSGhoKMbGxrX/8B4QutlfHesw/JUcDkXZ2qGv8jf4BvBHzB8cSj6EscKYhf0W1muhw6iwFE78lQhAv5Ft6dBPzx3t72BuaM60btMACD4bTAt/C1p1dUCjlti96iwlhWW6ROjzydpE6Cq5dtN+TY6Ahyx3THh0ib3AqiD2Anv4HTp0iL59+xIbG0urVq3uqq7x48cD2rWTHlZN+fe+TK3Bd8EesgtL+WOKP34eeuz9BaD6FFQfQ/tnYeRPtZfXQ3JeMsO2DKOwrJB3/d5ljHf1i11WW8fFTLYsiUCjlvB90p1eQXf3+6nWqHlh2wvEZsXyWqfXeL3dVH5beIy8G8V0GtCc3i+0puO8XRSVatj7fwG0qioPSF0KH7uCuhjePAV2d9cmQbhXxF5ggnCHTZs2ERISQmJiInv27GHy5Mn06dPnroMf0K6p9NFHHzVAK4X6OHkpk+zCUqzNDPXf+wsg/mYCdAMNf0mSxMKjCyksK6SbYzdd/k1dZGcUsOO7M2jUEq26OdLzmZZ33S6FXMG0rtpeoHXn15Ery+Kxsdre3jOqK9y4kkeHZrXkASkMwbmT9nly+F23SRCaAhEACY+E3Nxcpk6dipeXF+PHj8fPz48tW7Y0SN0JCQm6mW7C/Vee/DygnSOKqtaxqUpRzq2VjVs1zPo/uy/tZv+V/RjIDZjrP7fO6/0UF5Ty1/9OU5xfhqO7JY+Pb49M3/upRaBbIJ0dOlOkLuKHsz/g5mVLGz8nkEC1PppON5cNOFNTHlD5MJjIAxIeEiIAEh4JY8eO5eLFixQVFXHlyhXWrFmDnZ1dYzdLaAB7orSz+x5vX4e96xIPgKQG21YNsv1FbkkunxzTroo+qdMkWlrXredG0kjsDj5HZmoBFjbGDJ3qg6FR3dd7qo5MJmNq56kAbLy4kcyiTPq+2AYjUwMyLufSKk9brsaZYOV5UqIHSHhIiABIEIQHVuK1fOIy8jGQy+jXxqH2E8rppr83TO/PisgVXCu8hoeVB691eq3O55/afYnL526gMJQz9F8+mCsbPkHdv5k/7W3bU1hWyM8XfsbMyogeT3sCUByRiYEE55KzUVeXCN3sZg9QSiRo1A3ePkG430QAJAjCA2vvzeEvPw9blKZ1mG3VgOv/JGYnsj5Ku4Dmu37vYqSo20rhybFZHN2aAED/UW1xaFH1Jq5SWRmaoiKkkpJ6reItk8l0wdnPUT+TX5pPx/6uWNqaUJxbSs8yQ/JL1CRcy6u6Avs2YGgOpflwLabO1xeEpkYEQIIgPLD+uVCP4a8bCXAjXrv9hcfdb3/xxYkvKJPK6Ofaj37N61ZfUV4pIcHnkDQSbXs40b53xUUci2NjSfvscxJHjiK6azeiu3Tlgk9nYnr2ImnqG9z4aR3q3Fy9r/d4i8fxsPIgpySHDTEbUBjK6RmkHa7zKzTARFNDHpBcAc26aJ+LPCDhISACIEEQHki5RaUcjdcu+Ph4++q3FKmkfPaXWw/tNg934dDVQ4ReCcVAZsA7fu/U6VxJI7Hnx/PkZRZj7WRGwOh2ujWMCs+e4/LrrxP/9DPc+OEHCiMjkUpLdedqcnLI++cf0hYuJPbxgWQs/R/qvGr28rqNQq5gfIfxAPxy4RfUGjVt/Zywc7XAUAPdig04LfKAhEeECIAEQXgg7Y+5RplGoqWDOZ72ddijrYGGv0o1pXx2/DMAXmr/Ep5KzzqdH77nMpfOXEdhIOeJSR0wMjFAU1JC+pIlJI4cSX7ofpDJsBw0kGaffkKrXTtpe+IEbY8dxeOP33F8+/8watUKTU4O15YuJWH4cArPnav1uk+1fAqlsZKreVc5cPUAMrmM7kM9APAtMSDqclb1J+sCINEDJDz4RAAkCMIDaW/58JdXHYa/1GUQv1/7/C4DoN+jfyc+Ox4bYxumdJ5Sp3NT47M5sjkegL4j2mDf3JLStHQujXqJ69+tALUaq6FP0mrn3zT/9luUQUEYubujsDBHYWWFaadO2L32Gi23bsF18ZcYNHOh9PJlLo16icxff6vx2iYGJgxvPRzQ5gIBtOzqgLm9CSaSDEV8fg2J0DcDoNSzUFZSp3sWhKZGBEBCldasWYO1tfV9v25iYiIymYyIiIj7fm3hwaHWSKiiMwB4zKsOw1/Jp6D47re/yCzK5H8R/wPgzW5vYmWk/1BaUX4pu1adRdJItOnuSId+zSi+eJHEUaMoOn8ehY0Nrl9/jevixRi5u9dYl0yhwGroUFr++ScWAx9HKi0ldd48rgcH13jeSK+RyJBxOOUw8dnxyOUyegzV9mB1LlAQm5xT9Ym2LcHEWrsidPp5ve9ZEJoiEQAJVRo5ciQxMXc300OlUhEUFISLiwvm5uZ06dKF9evXN1ALHyyZmZmMGTMGpVKJUqlkzJgxZGVl1XhOWloa48ePp1mzZpiZmTFkyBAuXrxYoczrr79Oq1atMDU1xcHBgaCgIC5cuKA7npiYyMSJE/H09MTU1JRWrVoxd+5cSkoe7E/v55KzuZFfgoWxAd096rD6c/nwV8tAbVJvPX1/+ntyS3JpZ9NO15uiD0mS2PtjFHk3ilE6mBL4shdF58+T+PIrlKWkYOTpiccfv2P1xODaK7uNwtqa5t9+i/3UfwGQ/vkXXFvxfbXlXS1cCXALAODXC78C0K6nE4WGYC7JOLG/mg2AZTIxDCY8NEQAJFTJ1NQUR8c6DC1UISwsDB8fHzZu3Mjp06eZMGECY8eOZdu2bQ3UysZVlyBi9OjRREREsHPnTnbu3ElERARjxlS/T5QkSQwbNky3E3x4eDju7u4MHDiQ/Pxbya6+vr6sXr2aqKgodu3ahSRJDB48GLVau07LhQsX0Gg0rFixgnPnzvHVV1/x3Xff8f7779f/xpuAAxevAdC7lR2Gijr8GYu7++0vkvOS+S1aO8w0o/sMFHUIpCL3JpF4+hpyAxlPTOqIlJJE0qTJaHJyMO3SBfef12PUvHm92iWTyXCYNg37adrd5zO++oqsPzdVW/4lr5cA2B63naKyIhQKOaWttLlU18OvVz/VXiRCCw8LSagkOztbAqTs7OxKxwoLC6Xz589LhYWFjdCy+tu6daukVColtVotSZIkhYeHS4D09ttv68pMnjxZGjVqlCRJkrR69WpJqVTqjs2dO1fq3LmztHbtWsnd3V2ysrKSRo4cKeXk5NSpHUOHDpVeffXVao8nJCRIgBQeHq57TaVSSX5+fpKRkZHk7OwszZw5UyotLdUdDwgIkN58803pnXfekWxsbCQnJydp7ty5FeqNioqS+vTpIxkbG0vt27eXQkJCJEDatGmTrsyVK1ekESNGSNbW1pKtra307LPPSgkJCbrj48aNk4KCgqSPP/5YcnFxkdzd3fW65/Pnz0uAdOTIEd1rhw8flgDpwoULVZ4THR0tAdLZs2d1r5WVlUm2trbSypUrq71WZGSkBEixsbHVlvnss88kT09Pvdperqn93r/4XZjkPnO7tPZwov4nFWZJ0jwbSZprJUk36nDeHWYfnC11XNNRmrBzgqTRaPQ+LzU+W1o29R9p6et7pdP7kqSSlBQpJnCAdL6dlxT/3HCpLDe33m26U9pXX0nn23lJUR07SfmnTlVZRq1RS4P/GCx1XNNR2ha3TZIkSVp3IF5a/Poeaenre6VL565VXfn5rdqf4fI+DdZeQWgoNb1/30n0ADUASZIoLSpqlIek54Jo/fv3Jzc3l/Bw7ae20NBQ7O3tCQ0N1ZVRqVQEBARUW0dcXBybN29m+/btbN++ndDQUD755JM6/ayys7OxtdVzt27g6tWrDB06FD8/PyIjI1m+fDnBwcEsWLCgQrkff/wRc3Nzjh49ymeffcaHH35ISEgIABqNhmHDhmFmZsbRo0f5/vvv+eCDDyqcX1BQwIABA7CwsGD//v0cPHgQCwsLhgwZUqGnZ+/evURFRRESEsL27dv1uofDhw+jVCrp2bOn7rVevXqhVCoJCwur8pzi4mKACjuvKxQKjIyMOHjwYJXn5Ofns3r1ajw9PWvcm6yu/wZNTV5xGacuZQLQv429/ifGh962/UXNuTXVVpEVz9a4rQC81e0t3bT12hQXaPN+tJucOuDd044rb/xbN+zltmolCosqdmGvJ4dp07AcNBCptJQrb06jNC2tUhm5TM6w1sMA2HxxMwAd3W04Y6TtPYzcm1R15c4+2q/pF0QitPBAM2jsBjwMyoqL+WbcC41y7Wk/bsDwtjfJ6iiVSrp06YJKpcLX1xeVSsX06dOZP38+ubm55OfnExMTQ2BgYLV1aDQa1qxZg6WldqXaMWPGsHfvXhYuXKhXWzds2MDx48dZsWKFXuUBli1bhpubG0uXLkUmk+Hl5UVycjIzZ85kzpw5yOXaGN7Hx4e5c+cC0KZNG5YuXcrevXsZNGgQu3fvJi4uDpVKhbOzMwALFy5k0KBBuuv8+uuvyOVyVq1apXtTW716NdbW1qhUKgYP1uZkmJubs2rVKoyM9F/tNzU1tcrhREdHR1JTU6s8x8vLC3d3d2bNmsWKFSswNzdn8eLFpKamkpKSUuln9O6775Kfn4+XlxchISHVti8uLo5vv/2WL7/8Uu/2NzVH4q5TppFoYWuGu10dpr/H7tF+bTOo5nI1WBqxFI2k4TG3x/Bx8NHrHEmS+GftBXKvF2Flb0LgK16kzZ9D0blzKKytcVu5EoMGDkhlcjnNPvmExEujKY6JIWXWLNxWrUImr/iZN6h1EMsjl3M09ShJuUm0c25GhKmabiUKLp+7QVZaAdZOZhUrt26hTYQuyoKMKHDp3KBtF4T7RfQAPUICAwNRqVRIksSBAwcICgqiY8eOHDx4kH379uHk5ISXl1e153t4eOiCHwAXFxfS09P1urZKpWL8+PGsXLmSDh066N3mqKgo/P39K3zS7tOnD3l5eVy5citR08en4pvR7W2Ljo7Gzc1NF/wA9OjRo0L5kydPEhsbi6WlJRYWFlhYWGBra0tRURFxcXG6cp06dapT8FOuqp4CSZKq7UEwNDRk48aNxMTEYGtri5mZGSqViieffBKFomLOycsvv0x4eDihoaG0adOGESNGUFRUVKnO5ORkhgwZwosvvshrr9V9v6qmYv9F7eyv/m3r0PsjSbet//N4va579tpZQi6FIEPGm13f1Pu80/9cIT4iQ5f3U7Dpd7I3bwa5HNclX2HU3LVe7amN3Nwc1yVLkJmYkB92mMyff6lUpplFM3q59AJgc+xmTAwV2DubEW+gAeDcweTKFctk4NxJ+zzl9D1puyDcD6IHqAEYGBsz7ccNjXZtfQUGBhIcHExkZCRyuRxvb28CAgIIDQ0lMzOzxuEv0L4p304mk6HRaGq9bmhoKM888wyLFy9m7NixercXqg4Syof9bn+9prbVFGiU02g0+Pr6VjlLzcHh1iab5uZ16HG4ydnZmbQqhiAyMjJwcqp+Crevry8RERFkZ2dTUlKCg4MDPXv2pHv37hXKlc8sa9OmDb169cLGxoZNmzbx0ksv6cokJyczYMAA/P39+f776mcHPQjKE6DrtPnptRjITgKFMXj0qdd1vz71NQDPtHqG1jat9TonNT6bsI2xAPR5vjVWRakkfqpdPNHx7bcx79WrXm3Rl3FLTxzffpu0BQtI/+ILzHv3xrhlxQUbn2vzHIdTDrM1bitvdHmDDs2URF5JoVWZggthKfR81hMDwzsSvV06Q+IBSBUBkPDgEj1ADUAmk2FoYtIoD31zEOBWHtCSJUsICAhAJpMREBCASqWqNf+nvlQqFU899RSffPIJkydPrvP53t7ehIWFVch1CgsLw9LSEldX/T45e3l5cfny5QpByPHjxyuU6datGxcvXsTR0ZHWrVtXeCiVyjq3+3b+/v5kZ2dz7Ngx3WtHjx4lOzub3r1713q+UqnEwcGBixcvcuLECYKCgmosL0mSLocItHlUgYGBdOvWjdWrV+uGDR9ESTcKSLiWj0Iuw7+Vnf4nxu7VfnX3B6O6B7FHU45yJOUIBnIDpnaZqtc5RXml7Fp5Fo1GolU3Rzr4O5D8zjtIJSVYBARg++r4OrejPmxGv4R5b3+koiJS/vvfSnmDj7V4DAtDC1LzUzmVdgpvFyviDTSUGMsoyi8l7lRG5UrL84BED5DwAHtw/xIKdVaeB7Ru3Tpdrk///v05depUrfk/9VEe/EybNo3nn3+e1NRUUlNTuXHjht51TJ06laSkJN58800uXLjAli1bmDt3LjNmzND7jXzQoEG0atWKcePGcfr0aQ4dOqRLgi4PIF9++WXs7e0JCgriwIEDJCQkEBoayltvvVVhqK0+2rdvz5AhQ5g0aRJHjhzhyJEjTJo0iaeffpp27drpynl5ebFp061py3/88QcqlUo3FX7QoEEMGzZMl48UHx/PokWLOHnyJJcvX+bw4cOMGDECU1NThg4dCmh7fgIDA3Fzc+OLL74gIyND9+/wICof/urWwhorkzrs/l6e/9N6YJ2vKUkS34Z/C8CItiNwtag98JY0EiGrtft8KR1NeWyMF9eWfE1xTAwKW1tcFi6o04eXuyGTy3FZsACZqSmFJ0+Sc8cyFMYKYwa6a38ufyX8RYdmVkgyiDbTBkrnDlytXKnLzQAo7Szo0QssCE2RCIAeMQMGDECtVuuCHRsbG7y9vXFwcKB9+/YNeq01a9ZQUFDAokWLcHFx0T2GD9d/4ThXV1d27NjBsWPH6Ny5M1OmTGHixInMnj1b7zoUCgWbN28mLy8PPz8/XnvtNd355bOszMzM2L9/Py1atGD48OG0b9+eCRMmUFhYiJVVzav8zps3Dw8PjxrLrF+/nk6dOjF48GAGDx6Mj48PP/30U4Uy0dHRZGff2ogyJSWFMWPG4OXlxbRp0xgzZgy//HIrj8PExIQDBw4wdOhQWrduzYgRIzA3NycsLEyXdL17925iY2P5559/aN68eYV/hwfRgZh6DH+VFsKlQ9rn9cj/OZx8mMiMSIwVxkzymaTXOSd3XuLyuesoDOUMmdyJsqgz3PjxRwBcFi7AwL4O+UsNwLBZM+xffx2AtM8/R52XV+H4Uy2fAmB34m7aOJsCcEBdiEwOKbHZZKUVVKzQrg0YmEBJHtyIv/c3IAj3gEzSdx71IyQnJwelUkl2dnalN7+ioiISEhLw9PSsMEVZeLAcOnSIvn37EhsbS6tWre6qrvHjxwPagO9h1RR+78vUGrp+FEJuURmbpvamaws9V4CO3QPrngfLZjDjvDaJV0+SJDH277FEZETwSvtXmNljZq3nXInOZOuScCQJHhvbnnbd7Uh4bjglcXEon3uOZos+1vv6DUlTUkL8M89QeukytuPG4TTrPd0xtUbNoA2DyCjM4JsB3zD7Z4mrWYUssHUkOz4X3yfd6RV0x/+TlY/B1ZPwwg/Q8fn7fDeCULWa3r/vJHqAhEfCpk2bCAkJITExkT179jB58mT69Olz18EPaJO8P/roowZopVCTyCvZ5BaVoTQ1xKe5tf4nxt6c/dX6sToFPwBHUo4QkRGBscKYCR0n1Fo+P7uY3cHnkCRo39uF9r1duP79Skri4lDY2eE08906Xb8hyY2McL459Htj/XpKkm6t86OQKxjiOQTQDoO1d9G+cRS6aoPd6KOpSHdukCrygIQHnAiAhEdCbm4uU6dOxcvLi/Hjx+Pn58eWLVsapO6EhIQaFx4UGsbhOO3wl39LOxTyOgQy9cz/kSSJ5ZHLAXih7Qs4mNU87KZRa9i96hyFOSXYuVrQf1RbiuPjuX5z3SvnD95H0QgbDN/Oon9/zHv3hrIyMr79tsKx8mEwVZKKNs7a/KoYhRojUwPybhRz9WJWxcrK84DETDDhASUCIOGRMHbsWC5evEhRURFXrlxhzZo12NnVYRaR0OgOx18HoHfrOvy7ZSXBtWiQybUboNbB0dSjhKeHYyQ30qv359DGWJIvZmFoomDI5I4oDOWkLViIVFqKeUB/LJ98sk7Xv1ccpk8HIGfbdoqib2147G3rTQvLFhSri8FMu6HuubRcWnfX5pNFH664ACfONxdATDmtXWdJEB4wIgASBKHJKypVcyJRu/1F77pMf4+7Of3dtTuY6r9rvCRJLI+41fvjaFbzxsAXDqdw+h/tbMGB47yxdjIjd88e8sPCkBka4vzBB/dt1ldtTDt1xHLIEJAkMr7+Wve6TCZjkLt2lezEwsMAXEzPpbWfdq2q2PAMSorKblXk5A0yBRRcg9w7giNBeACIAEgQhCYv/HIWxWUaHCyNaeVQhz2z6jn8dTz1OKfST2EoN6y19yctIQfV+mgA/J7yoGVXBzSFhaQv0u6TZztxAkYtWtTp+veaw7RpoFCQ988/FJ6+NYQ1yEMbAJ3IOISlqYZStUS2uQylgyllxWriI25bE8jQFOzbap+LPCDhASQCoHrSZwVkQXhYNPZk0fLhL/+Wdvr3pKhLtRugArSu2/T38tyf59s8j5N59at152cX8/d3p1GXafDsbI/fU9pVlq8H/0BpcjIGLi7Y12MB0HvNuKUnymeeAeDabSuDe9t642rhSlFZEW6ulwE4n5JLu17abWSij9yxfpTIAxIeYGIrjDoyMjJCLpeTnJyMg4MDRkZGTaZrWxDuBUmSyMjI0K54bliHxQcbUHkCdJ2Gv66cgOIc7dBXs656n3Y89Tgn0k5gKDdkYqeJ1ZYrK1Wzc8UZ8rNLsHExZ+Cr3sjkMkrT0rkeHAyA0ztvIzczq7aOxmQ3eRLZW7aQt2cvxRcvYtymjW4YbM25NcjMzwAenE/OYUjvlhzblsCV6ExybxRhaXtzKQRnHzj9mwiAhAeSCIDqSC6X4+npSUpKCsnJVWwUKAgPIZlMRvPmzSttxHo/FJSUEZGUBVC37S/K839aDgC5/u0u7/0Z3mY4zubOVZaRNBJ7VkeRGp+DsZkBQ//VCSMT7Z/TjG+/QSosxLRLlyaT+FwV45YtsRw8mNxdu7i2ciWun2n3KCsPgFLLToFsCOeTc7CyN8W1rTVXY7KIPppK9yc9tJW4iKnwwoNLBED1YGRkRIsWLSgrK0OtVjd2cwThnjM0NGyU4AfgRGImpWoJV2tTWtjWoTelHvk/J1JPcDz1OAZyA17r9Fq15cI2xRF3Kh25QsaTr3fC2lHbrqLoaLI3/gmA48x3m3zvsN3kSeTu2kXOXztwePNNjNzc6GTfCSczJ9IK0lCYx3E+xRSNRqJdLxdtAHQkFd8h7tp7K98VPusSFGaBqXVj3o4g1IkIgOqpfDigsYYEBOFRERZ3M/+nVR3yf/KvQXKE9nmrx/S+1neR3wEwvHX1vT+n910hIkSbH/P4uPa4trs1uyz98y9AkrAcMgSzrvoPuzUW0w4dMO/Xj/wDB7ix5kec/zsbmUxGoFsgv0X/hrFVFHnJXiRlFtCqmwOhP0eTlVbA9at52De31A4vWreArMuQegY8+zX2LQmC3kQStCAITdrtCyDqLW4fIIFTR7DSb9+zU2mnOJp6tMben/iIDA7+rl07p9ewlrTtcStIyj92jPyDB8HAAMcZ0/VvayOzm/AqAFmbNqHOyQHgMTdt0GhkGQVoOJ+cg5GJAe4dtf8GF0+k36rAqaP2a/r5+9ZmQWgIIgASBKHJyikq5cxV7Qax9cr/qcPsrxWntSs2D2s9DBeLykFTWkIOITe3ufDu14xuT7jrjkmSRMYS7Zo61i++0OSmvdfErFcvjNu0QSooIOvm8F135+6YG5qjlucgN7nKuWRtYFS+KGLsyfRbMwPLA6C0s/e97YJwN0QAJAhCk3Us/gYaCTztzWlmbarfSRoNxN4MgPTc/T0yI5Kw5DAUMgUTO1ae+ZWdUchfyyIpK9Xg3tGOgFFtKwzH5e/fT+GpU8iMjbGf8i/92tlEyGQybMaOASDzp5+QysowUhjR17UvAAaW5zmfog2APDrZY2AoJyejkIzLudoKnDpov6aKAEh4sIgASBCEJuvIzfV/etVl+CvtLOSng6E5tOil1ykrIrW9P8+0eobmls0rHCvKK2X70kgKc0txaGHJ4Nc6IFfc+tMpaTSk3+z9sXnlZQydal41uilSPvMMCmtrSpOTyf1Hu3lsoFsgAAYW5zmXrO2FMzRW4N7JHoDY8mEw3RBYFGjEpBDhwSECIEEQmqxjiTcA6NXSVv+Tymd/efYDA+Nai5+7fo4DVw8gl8mZ1GlShWNlJWp2LD9NVloBFrbGPPWGj266e7nc3bspjopCbm6O3WvVzxxryuQmJliPGglA5k/rAOjn2g+FTIHCJI30wmRu5JcA0ObOYTBbTzAwhbJCuJHQODcgCPUgAiBBEJqkvOIyzt7M//HzqEsAVJ7/o9/09/Len6GeQ2lhdSt3R9JI7FlznpS4bIxMDXjm310wV1YMqKSyMjK+/gYA21dfxcBG//3GmhqbUaNALqfg+HGK4+NRGivxdfIFwMAimgs3h8HcO9phaKwg90YRaQk52jWWHNtrKxF5QMIDRARAgiA0SScvZaKRoLmNqf75P8W5kHRE+1yP6e/RN6LZl7QPGTIm+VTs/Qn7M5a4UxnIFTKGTumEbTPzSudnb9lKSUICCmtrbMeP06+NTZShszMWgYEAZP32G8CtPCDzGKJStTk/BkYKPHzuHAa7mQeUdu7+NVgQ7pIIgARBaJKOJ2iHv3p41qH3J2E/aMrAxhPsWtVavHzm1xMeT9BS2VL3+ul9SUTsSQIqr/VTTlNSQsb/lgJgN3kyCos6bNLaRNncHAbL2rQZTVERfVz7AKAwj+Nc8jVduda+N4fBTqUjaaRbCyKKHiDhASICIEEQmqRjNwOgnnUJgGL1n/4emxlLyKUQACb73NqwND4igwO/XwQqr/Vzu6wNGyhLTsHA0RGb0S/p38YmzLxPHwxdXdHk5JDz907aWLfBytAOmbyUyGsRunLuHewwMlGQn1VMSnz2bT1AIgASHhwiABIEockpKlXr9v/SO/9Hkuq0/cX3Z7S7oA9yH0QbmzYApCZkExJ8DqpY6+d2mpISrn+/EtD2/shNTPRrYxMnUyiwfvFFALJ+/RWZTEZPp94AJJdEUKbWAKAwlOPZxQG4OQzm6K2tIOsyFGXf/4YLQj2IAEgQhCYnMimLErUGewtjPO0r595U6Ua8dk8quSF41LwlQ3x2PDsTdgK3en+yMwrYsex0tWv93C77zz8pS03FwNER6xdf0P/GHgDWzw8HAwMKIyMpjo1lcMsAAGSm0SRez9eVKx8Giw9PRzKxAStX7YH0qPveZkGoDxEACYLQ5BxPvDX8pff+X+W9Py16gXHN+TirTq9CQiLQLRAvWy8K80rY9m31a/3cTiop4dr32t4ju9deQ25c+1T7B4mBgwMW/fsDkPXnJvyb9QJJhsIkjcOX4nTl3LxsMTRRkJ9dQlpijhgGEx44IgASBKHJOVqfBGg9p79fzrnMjoQdAEzxmaJd62fZGbLTC7G0NalyrZ/bZW3aTFlyCgoHe6xHvKh/+x4g1s8PByB761as5GbYKLRDhPuTDurKKAzleNxcFDE+POPWgohiRWjhASF2gxcEoUkpU2s4eSkTqEP+T1kxJB7QPq8lAXrVmVWoJTV9XfvibevNrlVnSY3PxtjMgKf/3bnSWj+3k0pKuL5CO3PM/rXX7lvujyRJZKWlcOX8Wa5fuURWWioFOdloytTIFXIsbOywcnDApY0Xru28sbCtw8rZVbDo3x+FnR3qa9fIO3AAbxs/Dl2PITrnRIVyLbs4cPF4GnERGfi/2AEZiKnwwgNDBECCIDQp55JzKChRY2ViQDtnS/1OunwYSgvAwulWT0QVruZdZVvcNgBe93mdsE1x2rV+DGQ8Wc1aP7fL2rKF0uRkFPb2WI8cqfc91de1y4mcP6giOuwAORlpepyxBYBmbdvj3f8x2vcLxMhEzzWUbiMzNET5zDPcWLOGrD//ZMDklzh0fT2Z0jlKNaUYyg0BaNHBFsXNvcGu0w570O4Kr9GAXAwwCE2bCIAEQWhSyvN//DxsUcjrmP/T6nGoIWco+EwwZVIZ/i7+GMU4cijkAgCPj22Pa9uaV3GWSku5/p2298du4sR71vsjSRKXIk9xfNtGLp89rXtdrjDApU07nFu1wdq5GebW1igMDFGXlpKXeZ0byVdJjo4i41ICyTFRJMdEcei3n+j+zHC6PvE0hnVsr3L4c9xYs4Y8VSgDZ33Ah2XmyA3yOZx0iv7uPQEwMjGghbctCZHXiE80xV5hBCV52mR0W88G/bkIQkNr9ABo2bJlfP7556SkpNChQweWLFlCv37Vz+AIDQ1lxowZnDt3jmbNmvHuu+8yZcqUCmWysrL44IMP+PPPP8nMzMTT05Mvv/ySoUOH3uvbEQThLpXn//jVKf9Hu4FnTcNfqfmpbIrdBMAoy4mE/hStvc7TntWu9XO77K1bKb16FYWdnW7BwIaWHHOB0HU/kBx9HgCZXE7Lbj3w7heIZ9fuGBrXHsTkZd4g6qCK03v+Jis1hQM/ryEy5G8GTf43Hj5d9W6LSdu2mHTsSNHZs8j2/YNRqRdlBif5K06lC4AAWnZ10AZAkdfp4eAFqae1w2AiABKauEYNgH777Tf+85//sGzZMvr06cOKFSt48sknOX/+PC1atKhUPiEhgaFDhzJp0iTWrVvHoUOHmDp1Kg4ODjz//PMAlJSUMGjQIBwdHdmwYQPNmzcnKSkJS0s9u9IFQWg0Go2k6wHSOwE6JxnSzwGyGre/CD4TTJmmjL7mjxP3ezEajUQbPyf8nvKo9RJSaSnXynt/JkxAblr3YaWaFORkE/pTMOf3awM5AyNjfAYOwXdoEFYOddtd3sLGFr9nhuM7NIiogyoO/b6OnIw0Ni78Lz6PD2HA+MkYGBnpVZfymacpOnuWnO3bafFkL+I5yYn0wxXKeHSyRyaXcf1qPlmevbDmZgDU/uk6tVsQ7rdGDYAWL17MxIkTee3mDspLlixh165dLF++nEWLFlUq/91339GiRQuWLFkCQPv27Tlx4gRffPGFLgD64YcfuHHjBmFhYRgaasep3d2rXsxMEISm5WJ6HlkFpZgaKujYTKnfSeWzv1y7gVnVQVNKXgobL27EpNQc35NBFBeW4dxSyWNjvfSaZp+9bTulSUkobG2xeWmUvrejl+jDB9kbvIzC3ByQyegYOJDeI17G0tb+ruqVKxR0CHicNj17c/CXtYTv2s7pvTtJT4zjmRnvY2XvUGsdlk8+Sdonn1IYEUHAU6OJ10B6cRzXCq9hb6ptn4m5Ia5trblyIZP4wm50A0g7c1dtF4T7odGy1EpKSjh58iSDBw+u8PrgwYMJCwur8pzDhw9XKv/EE09w4sQJSktLAdi6dSv+/v688cYbODk50bFjRz7++GPUanW1bSkuLiYnJ6fCQxCE++/Yzd6fbu7WGBno+ecprvbp7ytOr0BdpuGFhP9QnClhZW/C0H91wsBQUWv1UlkZ11Z8B4DdhFeRm5np165alBQVsnP5ErYv+YTC3BzsW3gw+qMveGLKW3cd/NzOyMSUx159neff/xATC0tS4y6y/v3pZFxKqPVcQ0dHzHpph7t6xkWjLtQudng4uWIvUKuu2mAqPtlJ+4KYCSY8ABotALp27RpqtRonJ6cKrzs5OZGamlrlOampqVWWLysr49o17UZ98fHxbNiwAbVazY4dO5g9ezZffvklCxcurLYtixYtQqlU6h5ubm53eXeCINRH+f5fek9/16ghbp/2eauq83+ScpPYcnELAXGjsLjuiJGJgqemdsbUUr9hoJy//qL00mUUNjbYvNQwe35dv5rE+lnTOafaAzIZPZ8bySuLvsKlTbsGqb8qHj5deWXRV9i38KAgO4vf5r9HcsyFWs9TPq0dyrI78g9lea0BOHj1UIUynl0cQAZpKZCntoUbCVCc1/A3IQgNqNHnKd7Z/SxJUo1d0lWVv/11jUaDo6Mj33//Pb6+vowaNYoPPviA5cuXV1vnrFmzyM7O1j2SkpLqezuCINSTJEkcS7gO1CH/5+opKMoCEyW4+lZZ5LvI7+hwtT/trvVAJpcxZHLt0911bVKrubZc2/tj++qryM313JajBrEnjvLzBzO4kXwFC1s7Rs5ZRN9RY1AYGN513bVROjozcu4nuLT1ojg/nw0LZpNyMbrGcywHDUJmaIiUmIBnqnZ9oSPJR3V/ewHMlcY4e2qHLOOlQYAEGbUHV4LQmOoVACUk1N51Wht7e3sUCkWl3p709PRKvTzlnJ2dqyxvYGCAnZ32P6aLiwtt27ZFobjVtd2+fXtSU1MpKSmpsl5jY2OsrKwqPARBuL8u3yggLacYQ4WMrm41T0nXKZ/+3jIQFJVTGhOyEwg/FUPPy88A0G9EG9y89Z9dlrNjByWJiSiUSmxGj9b7vOqE79zGli8WUFJYSPP2HXll0RKae1e/btG9YGJhwYsfLKBFRx9Ki4v485N5XL9yudryCisrLAK1+4E9m5yJpDHgRvE1EnMSK5RreXMYLKG0j/YFsSWG0MTVKwBq3bo1AwYMYN26dRQVFdXrwkZGRvj6+hISElLh9ZCQEHr37l3lOf7+/pXK7969m+7du+sSnvv06UNsbCwajUZXJiYmBhcXF4z0nPkgCML9Vz781clVialR7bk5QK35P9+HreaxmDHIkePl70zHAFe92yOp1Vxbpu05tn31VRQW9e/9kSSJA7/8yD+rV4Ak4TNwCC/MXoC5tZ6BXgMzNDEh6J3/4tK6HUV5uWxY+F9yr1+rtrzV09oAskfcKTQF2hSBYynHKpTx9NHmLSXnuFKsMRNbYghNXr0CoMjISLp27cr//d//4ezszOuvv86xY8dqP/EOM2bMYNWqVfzwww9ERUUxffp0Ll++rFvXZ9asWYwdO1ZXfsqUKVy6dIkZM2YQFRXFDz/8QHBwMG+//bauzL/+9S+uX7/OW2+9RUxMDH/99Rcff/wxb7zxRn1uVRCE+6TO218U3ICrJ7XPq5j+fiE9GuM9rTAts8DS1ZCA0e3031gVyNnxNyUJCciVSmxeeVnv8+6kLitj1/IlHNv8BwB9Ro5h4GtvoDBo3GXYjExMee69udg1b0Hejets+WIhpSXFVZa1COiP3MICi+zrtI3X/vscTT1aoYy1kxk2zmZoJDmXiruJRGihyatXANSxY0cWL17M1atXWb16NampqfTt25cOHTqwePFiMjIy9Kpn5MiRLFmyhA8//JAuXbqwf/9+duzYoZu2npKSwuXLt7pmPT092bFjByqVii5duvDRRx/xzTff6KbAA7i5ubF7926OHz+Oj48P06ZN46233uK9996rz60KgnCfnLgZAPm669krEq8CSQMOXqBsXuGQJEls+uEIDvluqI2Lee4NP71mfOnOV6u5djNv0O7V8Sgsat5dvjqlxUVs/vwjzoXuRSaXM3jKNHoNH1mnQOxeMrW04rmZczCxtCIt/iIhK76tkNtTTm5iguWgQQD0jc4H4HjqcTSSpkI5z843h8GKe2gDoCrqEoSmQiZV9dteR8XFxSxbtoxZs2ZRUlKCoaEhI0eO5NNPP8XFxaUh2nlf5eTkoFQqyc7OFvlAgnAfZBWU0OVD7fD2ydkDsbOofkNSnS1vQPg66PUGDPm4wqFdfx0jdlseGjT4veZEr+6d6tSe7O1/kfz228iVSlrv3VOvAKi0pJjNn37I5bORGBgb88x/3qNlN78613M/XD57mg0LZyNpNAwY/zrdnnymUpm8Q4dImvgaOYZmvP6WhNqwlA3PbKCd7a2Za6nx2Wz87CRGsgImOI5DMT0CrMWsWuH+qcv7913NAjtx4gRTp07FxcWFxYsX8/bbbxMXF8c///zD1atXCQoKupvqBUF4RJQPf7W0N9cv+JGk27a/qDj8lRybycXt2rW8crvF1Tn4ub33x3bc2HoHP1s+X8Dls5EYmpjywvsfNdngB6BFRx8CXpkIwP51waQnxlcqY96zJwp7e6xKC+h4QdvTczSl4jCYk4cVplZGlEhmJJd0EInQQpNWrwBo8eLFdOrUid69e5OcnMzatWu5dOkSCxYswNPTU7etxalTpxq6vYIgPIRO1nX4K+MC5CaDgQm499G9XJBTwrbvwpFJcuLtIhj3Ut23Y8jdtYuSuDjkVlbYjhlT5/PLSkrY8vkCLp0Ox9DYhOGz5uHq5V3neu63bkOfpaVvD9RlZWz/+jNK75jgIjMwwOrJJwHoc167sOyx1Iq5nzK5DM9O2hm5umEwQWii6hUALV++nNGjR3P58mU2b97M008/jVxesaoWLVoQHBzcII0UBOHhVuf8n/LtL9x7g6F2Xy5JIxGy+hxleXDDNAXnpyRcLfWf9aWtQ0PGsmXAzd6fOu4hWFZSwpYvF3LpdDgGxsYMf28ezb061KmOxiKTyXhiyltY2NiSmXwF1U+rKpWxGqoNgHpczsCwTOJE2gnKNGUVyujygIr8kNLO3/uGC0I91SsACgkJYebMmTg7V9xBWZIkXdKykZER48aNu/sWCoLwUCsp0xCZlAVAdw89A6Dy6e+3rf58avclrkRlUiov4ZD3b0zqNrHObcndtYuS2DjklpZ17v3RaNT89c3nJEacvBX83Oc1fu6WmZWSJ//9fwCc3rOTy2cjKxw37dyZUlt7zMpK6BxvRH5pPuevVwxymnvZYGAgkadx4NqlG/et7YJQV/UKgFq1aqXbeuJ2N27cwNPT864bJQjCo+NccjbFZRqszQxpaa9Hvk1pIVy6uV9ga20AlBKXzdGt2ryVQx4beaHXs1ibWNepHZJGwzVd7884FHWYACFJEntXLSf2+GEUBgYMe+e/uHnXLfeoqWjRsTOdBw0FYNd331BSVKg7JpPLMQrU5lz1OGcCVB4GMzBS4NZO23OWkOoMZVVPrReExlavAKi6iWN5eXmYmJjcVYMEQXi06PJ/Wtggl+sxPfzSISgrAstm4OBFUX4pu4PPImngot1JrrvH8XL7uq/bk7s7hOKLsdren7F16/0J+2M9p/fuBJmModPewb1Tlzpfvynp//J4rBwcyclI4+Avaysca/asNjjqHp+HQi1VSoQG8OymXZYgocgXMmreakMQGkudVuKaMWMGoB0rnjNnDma37YqsVqs5evQoXbp0adAGCoLwcNMFQHoPf93c/LT1Y0jAP2ujyLtRTK7JDfa3/I1ZXWdiamBapzZIGg3X/vc/AGzHjKlT70/4zm0c2fgrAAMnTqVtzz61nNH0GZmaMWjym2xc+F8idv1Fh4DHcWqp3QjV0q87uaaWWBbm0vGSnHCjcErUJRgpbq207+FjjwwN18pakRt3AUsXn8a6FUGoVp16gMLDwwkPD0eSJM6cOaP7Pjw8nAsXLtC5c2fWrFlzj5oqCMLDRpKkWwnQLeqYAN3qcc6orpIQeQ1JrmFXm2BcbV14rvVzdW5Hbsgeii9eRG5hge24sbWfcFP04YP8s+Z7AHq/+DKdBz1Z52s3VR4+XWnXuz+SpGFv8HKkm9sLyRQKrnToCUCfaEOK1cVEZtyRK2RphLNtFgAJZ67f13YLgr7q1AO0b5/2k9err77K119/LRYJFAThriTdKCQjV7sBamc369pPyL4KGVEgk5Nh1INDGy8CcNR9K9csrjDf738YyOu2xUSF3p+xY1AolXqdlxIbzc7/LQZJossTT9Hr+VF1uu6DIHDMRBLCj5MSG82Zf3bjM3AIAOp+gXBiD74xGuRPSBxLPYafc8V1jjxay0g5BomXTBH9P0JTVK8coNWrV4vgRxCEu3bi5iyhDs2UmOizVUWcdvHDUqee7F5/BU2ZRF6zVCKc9tG7WW/6ufarcxtyd+6kOCZG2/szVr/en9zr19jy+QLKSkto2c2PAeMnN5ntLRqSha0dvV98BYCDv66lKD8PgGYBfcgxNMOyoJT2SVXnAbW8mQd0NasZxYVllY4LQmPT+6PS8OHDWbNmDVZWVgwfPrzGsn/++eddN0wQhIdfef5Pd33X/7k5/T0sdyxZaQUYWsr4o9nXKOQK3vV7t85BiFRWRsY33wJgO+FVFNbWtZ5TWqTd3ys/KxN7N3eemvYOcrn++4w9aLoOeZrTe3dy42oSRzf9TsArE/BqbsM6lw4Mvnycnhck1nqeoaisCBODW5NgrNt7Y6PYRKa6OZdPJtKmb+tGvAtBqEzvHiClUqn746JUKmt8CIIg6EMXAOmTAK1RQ9w+Eou7cTZOuwbZsfabKTYsYES7EbSyblXn62dv3UZJYiIKa2u9en8kjYady74iPSEOUyslw96dg5GpWa3nPcjkCgUBYyYAEP73VrJSU7CzMOZ0a18AesXIUKtLOXPtTMUTjS3xsNbOAEs4mXRf2ywI+tC7B2j16tVVPhcEQaiP7MJSotNyAeimTw9QcjiFBWr+yZ4GgGnXQo4q/sHKyIqpnafW+fpSSYku98du0mt67fkVtuEXYo4eQq4w4Nn/ex+lo1Odr/sg8uzSHXefrlw6Hc7+n1fz7Iz30XTpTv5+E6zzimh7VcGJ1BOV8oA83fIJvw6XYstQqzUoFHe1/aQgNKh6/TYWFhZSUFCg+/7SpUssWbKE3bt3N1jDBEF4uIVfzkSSoIWtGY6Wta8fJl3ciyp7KoUaJUpnE36y+BKAqV2m1nnRQ4CsjRspvXoVhYM9NqNH11r+Qth+jmz8BYBBk//9wGxx0RBkMhmBYyYik8m5eDSMlNho2ja35Yizdo+znhc0nEw7Wek8p9YOmMqzKClVkByTdZ9bLQg1q1cAFBQUxNq12sWxsrKy6NGjB19++SVBQUEsv7mLsiAIQk3qmv9z4Vg68cW9kMslUnqe5FpZBp5KT0a0G1Hna2uKiri2/DsA7F+fgty05nWDUmNj2LVsiba9zwynY+DAOl/zQWffwgPv/gMAOPjLWrxcLDnUTDu/q2e0RGR6BKXq0grnyF288TA+AUDimcq7BwhCY6pXAHTq1Cn69dPOttiwYQPOzs5cunSJtWvX8s033zRoAwVBeDjVZQHEnCtpHEjUvvl69JOzLkO7Ued7fu9hKDes87Uzf/mVsvR0DJq5YD3ixRrL5t64xuYvbs346jf60d3j0P+F0cgVBlw+G4lDzmVOOrWjyMAIhxxwvVLEuet37P7u1PFWAHT6WrW7CAhCY6hXAFRQUIDlzV2Sd+/ezfDhw5HL5fTq1YtLly41aAMFQXj4lKk1RJRvgOpuW2NZjUZizw+RlEpmOJslsM4yGI2kYYjHEHq79q7ztdV5+Vz/Xrt4ocPUqciNjKotW1pcxJbPF5CfeQO75i0Y+ubDPeOrNkpHJ91ijwk7/0AyNOS4oxcA3S9WMQxm25LmZheQU0rOtSIyUwvurFIQGk29AqDWrVuzefNmkpKS2LVrF4MHDwYgPT1drA8kCEKtolJyKShRY2liQBvHmpOPI0Iuk5JsgKGskJJuRzl9PRJzQ3Pe8XunXte+8eMa1JmZGLq3QBkUVG057YyvJaTFx2JqacVzM+dgbPZwz/jSR8/nRmBgbEx6/EV6GmVw2EWbC9X9osSJtBMVC8sVGDl50txIO0NMDIMJTUm9AqA5c+bw9ttv4+HhQc+ePfH39we0vUFdu3Zt0AYKgvDwKV8AsVstG6Bev5rH0W3aXd67KoP5lggA3uz6Jo5mjnW+btm1a9wI/gEAh2nTkBlWP3x2eOMvxBw5qJ3x9fYHKB2d63y9/2fvvsOjqtIHjn/v9PTee6Ek9N6bIAooWLCsve667v50dV1XdHWtq666uq6irt1VERs2OkjvhE6oIb33mUwy/f7+uEkgpMCEFCDn8zx5EmfO3HuOKbxzyvtejLz8AxqrxacWb2NHaF9cKhVxpZB7JA2ny9n0BWH9iKtfBssWZTGE80i7AqC5c+eSk5PDzp07WbZsWePjU6dO5fXXX++wzgmCcHE6mw3QTqeL1Z8ewuWQidfv4IfofZicdaQEpnBDnxvadd+y+fNx1dZi6N8f3xmt1+06smUDW76tP/F17x961ImvszH8iqtRa7XoK3Lxc1WRH9MHgH7pZo5Unlb9Pawf8XplaawwoxqL2X765QShW7Q7KUN4eDhDhgxBpTp5iZEjR9K3b98O6ZggCBevs9kAvXt5DqU5JvQ6J/4hH/GTjwcSEk+OftLtel8AtqwsKr/+BoDQRx5BUrX856/o+FGWva28kRt2xdX0n3Kp2/e62HkHBDLgEmXrw4iqNLaesgzWbB9QWD98NSUE6ouQXTI56WIWSDg/tCsAMpvNPPnkk4wdO5bk5GQSExObfAiCILQmv6qOwmoLapXE4FYKoJbn17BjcSYAo+NW81KYskx2Xe/rGBAyoF33LXn9DXA48Jo0Ea/Ro1psc/qJr4k339Gue/UEI2Zfi6RWE2PJZ6tBCWRTcmX2Z2xp2jBUCY7iNZsByNonAiDh/OD+2yjgnnvuYd26ddx6661ERERclEUABUHoHDuzlP0/qRG+eOqa/wlqXPpyysQPCGSF6xuydJ6E6P15YOgD7bpn3Z49mJYvB0ki9OE/t9hGnPhyj29wKP0mTuXAmhUkWjKoi43CIycfefMOXFe4UEn176+9Q8ArlHjbDnaZryHnYDkupwuVyAotdLN2BUBLly5l8eLFjBs3rqP7IwjCRa5x+auV/T+7lmUrS1+eGsKGpvP4ESVJ4VNjn8VP736tQVmWKX71VQD8rr4aQ5/eLbZZ9s6/G098XfWoOPF1NkZeNZf9a1aSUJdNfspQknPySU03c6LqBMkBpxQ/DUslrGY9er0Tay0UnagmstdZFsAVhE7SrhA8ICCAwMC2c3cIgiC0pK0CqGV5NexckgXA2OsTeT7zbVySxCx1EJNjp7TrfjVr1lK3Mw1Jryfkgf9rsc2WbxdwdMuGxhpf/mHixNfZCAiPxJU4GICjjjoABp+Q2ZW7tWnDsP6oJBdxIcWAWAYTzg/tCoCee+45nnrqqSb1wARBEM6kxurgUKERaD4DpCx9peNyyiQMCmaV9nsy7EaCHE4e633mWl0tke12Sl5TaoYF3nYb2vDmgc2RLRvZ8u2XAEy7936iU/q36149VdyUK5UvyjIpD/HFYIfCDSubNgpVaobFG3YBIh+QcH5o1xLYa6+9RkZGBmFhYcTHx6M9LZfGrl27OqRzgiBcXPbkVOGSIcrfgwi/pvW30pZmU5Zbg8FLS8RMeHydkq/nb+UV+Pdu/ch6Wyq/WogtIwN1QABB997T7PniE8dZNr/+xNesqxgwZXq77tOTDRrYl3WeCSTVZpLbO56g0n14bDmAfL98cn9omLIROta2DEl1JZVFtVSX1uIXIpYZhe7TrgDoqquu6uBuCILQEzQkQDx99qc010Ra/dLXmOsT+NveB3DKTi6rMTPNEAH+MW7fy1FZSel//gNAyIMPoj4tS31NRTk//PNZHDYrCUOGM/GWO9sxIiEx2Jv9AYNJqs2kqNaCVa2i3+FacqqzifOPVxqF9AFJhd6WT2SCB/kZtWTtL2fQJSIAErpPuwKgv//97x3dD0EQeoCW9v84HS5Wf3IIl0smcUgIP/MFxyqPESDpeLw8D4a2Xq6iLWX/+Q8uoxF93774Xze3yXN2q4UfXnmemvoTX7MeeFSc+GonnUaFd2wviktDCLOVkhnmT9+CCtI3/UTcrPpTe1oPCEqGsqPERZvJz5DI3l/GoEvcD2wFoaO0+xxiVVUVH3zwAfPmzaOiQnlXt2vXLvLz8zusc4IgXDycLpndOVVA0xmgtKVZlOfXYPDWYphSxeeHPwfguVqJQJcLEia5fS/LkaNUfrUQgLDH5yGpTwY3siyz/J1/U3ziGAZx4qtDpET4ssdvIAA5If44JTD/uqZpo/p9QAn+xwDIP1qFrc7Rpf0UhFO1KwDat28fvXv35uWXX+bVV1+lqqoKgEWLFjFv3ryO7J8gCBeJI0UmaqwOvHRq+oYry1Hl+TWkLcsGYOg1ETyz+0kAbkycw6SiY4AECRPduo8syxT/4x/gcuFz+eV4jRzZ5Pmt333FkfoTX3MeFie+OkJKhC/HvZJwGHxxIFHo70PgzhNNG4Upm8v9LXvwC/XA5ZTJPVTRDb0VBEW7AqCHH36YO+64g2PHjmEwGBofnzFjBuvXr++wzgmCcPFIq9//MyQ2ALVKwuV08etnhxpPfb1v/hfllnKS/ZP5s0+K8qKIQeDpXsoN06pV1G7bhqTXE/rII02eO7RpHZu/+QKAaffcT3SqOPHVEfpG+OCS1BwLHgRAZogf4cU2io/tO9koTJkBovgA8QOCAXEaTOhe7doDtGPHDt57771mj0dFRVFUVHTOnRIEoSlZlqnIz+X4jq2UZJ3A6bAjSSrCk3oRndKfyN59W61tdb7YeVoCxL2r8yjJNqHz0FAxPJ0N6RvQqXS8NOElDOv/rbwo0b3lL5fVSsnL/wQg6O670EVHNT6Xfzid5fNP1vhqqGUlnLuUCGVGb6M6mQG67ZiACi8DVT8vIOxhZWms4SQYpUeIn+jP3tW5ZB8oR3bJSCpRTUDoeu0KgAwGA0ajsdnjR44cISQk5Jw7JQjCScUnjrP6o3coPHak2XPHdyh1lwKjYhh11XX0HTcJlfr83My7M0sJgEbEB1JVXMu2n5UlkqQZ3vzl8CsAPDz8YfoE9IYT65QXJU526x7lH3yAPS8PTVgYQfecPPZeVVTIj68+j9PhIHnEaFHjq4MFe+sJ9tZTVgNhw8aTv2U1mSH+xK/fAg/XN/KLBZ032GqI8C9DZ1BTZ7JTnG0kPMH9DN+CcK7a9ZZxzpw5PPvss9jtdgAkSSInJ4fHHnuMa6+9tkM7KAg9lcvpZO1nH/DF4w9TeOwIao2GhCHDmXTr3Vz62z8y+bZ76D1qHDoPTyryc1n69r/44omHKcvN7u6uN1NYXUd+VR0qCQZF+7Hm88M47S7Ce/vwivEJbC4bE6ImcFPfm6A8A4x5oNZD7JizvoctK4vy9/4LQOijf0FVv7HZUlPD9y8/Q53JSFhiMjP/+Ig48dUJUiJ8ALD3HQ9Aia8n+swKHJVK4ItK1bgRWl2RTkxqEADZ+0VWaKF7tCsAevXVVyktLSU0NJS6ujomTZpEcnIyPj4+vPDCCx3dR0HocRw2Gz+//hJpi39All30HTeJe976iGsee5rhV1zNwKmXM2zWVVz58Dx+O/8TJtx0BwYvb0oyM/h83p/YvexnZFnu7mE0apj9SYnwJXtHCQXHqtDoVKxPWkieOY8o7yhenPCikjjvRP3poZiRyvHpsyDLMkXPPotss+E1bhy+M2cC4HTY+em1F6gsyMMnKISrHn0K7Sn7FoWO07AMdszqQdSggSBJ5AT5Ur56xclGjfuADpIwUAmAMveJfUBC92jXEpivry8bN25kzZo1pKWl4XK5GDp0KNOmTevo/glCj+Ow2Vj08tPkHNiHWqNh5v89Qu/R41ttr/f0ZOScuaROvIQV771J5u6d/Prxe1QU5DHl9t+eF0tiDfl/RoX5sfn74wA4RxTya9Vy9Go9r09+/WSh00z3l7+MvyzGvHkLkl5P+N+fQpIkZFlm5X/fJjd9PzoPD67+61N4B4gahp2lYQbocKGJu664ju/27iM/0If8ZT8SNvcGpVH9STCK04mdHQQSlOfVYKqw4BMoAlOha7kdALlcLj755BO+//57srKykCSJhIQEwsPDkeVTUp8LguA2WZZZ+f5b5BzYh87DgzmPPEls/4Fn9VrvgECu/uvf2fnz96z/4mP2LF+MsbSEKx+ah0an6+Set21ndgXIEJVRh8XixDMaXne+BBI8OfpJUoLqT325nJBZf5L0LAMgZ3U1xS+9BEDw7+9DFxsLwPYfvuHgulVIkoorHvwrIXEJHT0s4RQNqQ0OFRmJ7TcS2UuDwwzFRzNxWa2o9PrGJTCKD+LhrSM8wY+iE9VkHyin/8SoNq4uCB3PrSUwWZaZPXs299xzD/n5+QwYMIB+/fqRnZ3NHXfcwdVXX91Z/RSEHmHXkp9IX/8rkkrF7D8/cdbBTwNJkhgx+1pmP/w4Gq2OE7t28ONrL+Cw2Tqpx2dWY3WQXmAk1a7GkmNGpZFYGPEGsuTi+t7XMyf5lEzPhXvBUg16P4gYfFbXL/nX6zjLy9ElJRF0110AHNqwho1ffQbAJXf+joQhwzt6WMJpkkK80aolTBYHBUYrgeMHA5Dn5415a311+IYlsOocsFQTX78MJo7DC93BrQDok08+Yf369axevZrdu3ezYMECvvrqK/bu3cuqVav49ddf+eyzzzqrr4JwUcs/coh1n38IwKRb7iZuwOB2X6vXqLFc/djTaHR6svak1Z+AsndQT92zJ6cKgxOmWZRZqMMJG8jXZDIweCB/HfnXpo1PrFU+J0wA9ZknqGt376ZqoZLxOeLpvyPpdGTt3cWyd94AYNisOQy+bFZHDUVog06jIinEG4BDhSbGXHYt4MLoqSdz8c9KI48A8K2f6Sk51JgPKO9wJXabsxt6LfRkbgVACxYs4PHHH2fKlCnNnrvkkkt47LHH+OKLLzqsc4LQUzhsNla8+29kl7LheejM2ed8zdj+A7lm3tNo9Hqy9u5i+TvK9bvazuwKptZp0bvA7FfO2sDvCPMM41+T/4VOfdrSXGMAdOb8Py6bjaKnlLqEftdcg+eIERRlHOOn1/6By+mkz9iJTLrl7g4ejdCW1PqN0IcLjfSN7I/RX5l5TD+09+Sm/IZ8QMUHCYz0wjtQj9PuIv9wZXd0WejB3AqA9u3bx+WXX97q8zNmzGDv3r3n3ClB6Gm2fr+QioI8vPwDmHrX7ztsL11M6gDm/PkJVGo1hzauZcOCTzvkuu44saeUvnYNLsnFkrgP8dJ78c60dwjzCmva0F4HOfVLJWex/6ds/nysx46hDggg9C+PUFlUwPcvPY3daiG2/yAuv/+h8z455MWmb/1G6ENFRiRJwjJe2XdVaNBStXOH0uiUfUCSJJFQPwuUKZbBhC7m1l+HiooKwsLCWn0+LCyMykoRxQuCO0qzM9nx07cATL3r9xi8vTv0+vGDhjL9d0pV7h0/fceupT916PXbYjZZiT5hAWBP5CqqfYp5Y8ob9Aro1bxx7jZwWsEnEoJbeP4Udfv3U/7+BwCEP/00Vgm++8dT1BmrCY1PYvafn0Cj1Xb4eIS2pTTOAJmU/x4yDrXLikulYu93XyuNGk6ClaQDEDdQCYCy95WdV6kbhIufWwGQ0+lEo2l9XV6tVuNwiOq+guCOdZ9/hMvpJHnEGHqNGtsp9+g3aSrjb7wNgDWfvs+RLRs75T6nW/r5YbxcEpUeRaRFL+f5cc8zMmJky40blr8SJ0EbM2Auq5WCx+aB04nvrFlox43huxeepLq4CL/QMK6Z97So7t5NGk6CZZabqbU5GB4+nOIgpWrA4ROHlSXYxlxA6SDLRPX2R6NTYa62UZZb011dF3ogt47By7LMHXfcgV6vb/F5q9XaIZ0ShJ4ia99usvftRqXWMPm2zt2vMvKq66ipLGfP8sUsfetVvPz9iU7pvGKguYcqKN5bjoyLtUkL+OPwPzArsY0NyWdZ/qL0zTexZWSgDgnG/88P8/0/nqI0Jwsv/wCufeI5vPwDOm4QgltCfBpKYlg5WlxDv6g+pA1yMWu1E7NaTcb6NSSPnwAqLViroToPjX8MMSmBZO4tI2t/GSGxPt09DKGHcGsG6Pbbbyc0NBQ/P78WP0JDQ7nttts6q6+CcFGRXS42fPEJAIOnz8QvNLxT7ydJElPu+C3JI8bgdDj48dUXqCjI75R72W1OFn+SBsCB8A2ERgzk7v5tBHh1lVCwW/m6jQ3Qtbt2U/HRxwCE/O1v/PzevynKOIbBx5e5f3uegPDIDhuD0D4NCREPFRrRqrTEJg3Cy6Ysie3+4VvQ6CC4t9K4+CDAKdXhRVkMoeu4NQP08ccfd1Y/BKHHObx5PSVZGeg8PBl1zQ1dck+VSs3M//sz3zz7BIXHj7Dopaf5zfOv4unbscUo//fpUpzVnph0lWz0KeO9QU+3vbE7cwMgQ3Af8I1osYmrro7CefNAlvGefSWrd24k//BBdB6ezH38WYJj4jp0DEL7pET4suFYGYcLlaWvYaHDyAnbiZ/Rn9zCXGoqK/AO6wclB5WPPpcTN0DJB1SSZcRcbcXLr+VVBkHoSOKIhCB0A9nlYtsiZVPoiCuv6fAApC1avYGrHn0Sv9AwqooL+eGV57DbOmb5WpZl3v/1f5jTlH/A1oYcoK58JkNiz7AsdRblL4pfehlbdjZSaCg7/PRk79uNRq/nmnnPEJaY3CH9F85d3/CGGSBl1mdo2FC293UQYK5DBg6sXNqkJhiAl5++cekr56CYBRK6hgiABKEbZKRtpzwvB52HJ0NmXNnl9/f08+fqx57G4OVN4dHDLHvrX+ecI8jhcvCPLf8gb7ETFWqMkeUcNg+jX6QfnrozTDafugG6BcYlS6hauBCnSsXeUQPJ3LcbjVbHVX95kqg+KefUb6FjNZwEO1RkRJZlBgQPoDhYg2+dEhDtX7kEOfRkLqAG8QMaskKLAEjoGiIAEoQuJssy23/4BlD2/ug9vbqlH0FRMcx+5AlUag1Ht21i/ZeftPtaJpuJB9c8SPq6IkLNsaB3Ye3bG5AYHneGAqTVeVB+HCQVxDcv+mrLzqbwyadwqCT2jBtKXk4mGr2eqx/7+zllyxY6x6klMfIq6/DUepISnEpWhBmN04XRWE1edX0CzLJjSv4nIK5+H1BuegVOR9cn7BR6HhEACUIXy0vfT+HxI6i1WobOnHPmF3SimNQBXPb7BwHY+fP37F25xO1rHK88zk2Lb2L38QOMyFVOeU25LpVt9XtAhsefYfmr4fRX1DAwNF0KdNls5D30ENa6OtIG9qLYWInOw4NrH3+W2P6D3O6r0Pl0GhW9w5TlrIMFys/A0NCh7OwFEVXKLNC+LTvAKwRkZ2M+oNBYHzx8dditTgqOVXVL34WeRQRAgtDFdv6yCID+ky89L45sp06YwtjrbwZg9YfvcmLXjrN6nSzLLDq2iJuW3ERWdRZTc25B69IR2cuf6KHBHCmqD4DizhQArVU+t3D6q+Tlf2I8epTtvWMol53ovbyY+7fnie7b76zHJ3S9fpHKMlh6QTWg7AM6HCMRWqPk+Tm2dROWwPoUDIX7AJBUEnH9lWWwbLEMJnQBt06BCYJwbqqKCjmxeyegFOo8Fy6rFcv+/dTu3o3LVIOkUaP2D8Bj6FAMffsgtZG09HSjr7mR6uJiDq5bxc//epFrH3+W6NTWcwSV15Xz3NbnWJ2zGoDLndcTXp6EWqNi8s192JNXjUuG2EBPQn0Nrd9YllvdAG1cvoKcb79mZ68o6nQa5aj7E88RlpB01uMSuke/SD8gr3EGaEjoEJxqiaORVnzqrJg84JApmiEARfsbXxc/IIjDmwvJ2l/G+OvbzgYuCOdKBECC0IX2rFwCskz84GEERES16xqO8nLK3/+Ayq++QrZYWmyj8vHB76qrCLz5JnTx8We8piRJXPrbP1JnqubErh0s+uczzP3b80Qk92nSTpZlfjj+A6+lvUa1tRqNSsMfUx6Er5KwYGf4zDgCwr3YuS8POIvZn9LDUFMMGg+IOZkh2pqRwf7nn2FnrygcajX+4RFc89jT7f5/JnSthhmghgAowBBAol8iu5KPM2ezkfSoEPZnmBkcCNIpAVBMSiAqtUR1aR1VxbX4h4mM3kLnEUtggtBF7FYLB9esBGDw9DYyIrdClmUq/vc5xy+dTsUnnyBbLKiDg/GZPp2A224l4Kab8Jo0EZW3Ny6Ticr//Y+My2eQ/+dHsBcWnvH6ao2GKx56jJh+A7HV1fHdC09RcPRQ4/MHyg5w+7LbeWrzU1Rbq+kT0IcvZ35JfPooLDV2AiO9GDJdycWzM1upCTjsjPt/1iqf48aARjk676yuZuOf/sC2CH8cajVRvVP4zXOviuDnApIS4YskQZHRQlmNkmJhaNhQ9iRKhBvNqFwuSksqKbZ4Q/EBcDkB0Bk0RPbyByBLFEcVOpmYARKELnJ483os5hp8Q8JIGDLMrde6rFaKnn6G6kXK/iFD//6EPPgAXuPHN0swKDudmLdspfLzz6lZtw7j4sWYVq8m+L77CLrn7jaXxrQ6PVc9+iTfv/g0+YcP8u3zTzLyD/fwveVXlmYtBcBD48HvB/2eW1JvofioicOb94AEU27pi1qjwuF0sSe3CoAR8Wc4AXZa+QuHpY7F99/DcU+lj72Hj2bGg4+i0enc+v8ldC8vvYaEIC9OlJk5WGBkUu8QhoYO5VuPbymMNRBebaYgwIf9xmjCPQ5DxYnGArjxA4LJO1xJ1v5yBk+L7eaRCBczMQMkCF1k7wolgBg8fSYqlfqsX+eqqyPn7ruV4EelIvSxvxL/zdd4T5jQYnZlSa3Ge/w4Yt59h4TvvsVj2DBki4XSN94g66absZ7IbPN+OoMH1857hpCU3titFta/8R+ObFoPwOyk2fx81c/c2f9OJKeKtV8eAaD/xCjCE5UTXIcKTdTanPgaNCSHtFHZ3mmHrPqirAmTqCou5PPf3c5xh3IsevikS7nikSdE8HOBSm1cBlM2Qg8LU4L+DfEWoiuU02CHq4Oxu1RQuLfxdQ1ZoQuPVWGtE8W1hc4jAiBB6AKl2ZkUnziGSq2h3+RpZ/062eEg/8+PULczDZWPD7EfvE/QHXe0XVbiFIbUVOI+/x+RL7+EyscHy759ZF5zDVXffd/qa45UHOGvWx/nX7EryQo3o3ZJTNwbzHPOO3luzLOEeYUBsGt5DtUldXj66Rhz1cmNyTuyKgAYFheAStVGP/N3gc2EbAhg/6EiPn3oPsottWicTi67/Com3f/gWY9TOP8oG6FP7gOK8IogzDOMHckyQTV1eNjs2JwSx0xBTTZC+4d64h/micslk5te0S19F3oGEQAJQhc4sHYVAEnDR5512QtZlil69jlqfv0VSa8n5t138Bo71u17S5KE35w5JP78E15jxyBbLBQ+8QQF8x7HVVfX2O5o5VEeWvMQc3+ey8rslbjUErqrhpB8+VQAMpb/ytfPzsNYWkJVSS27lmUDMP66Xug8Ti6rpdXv/xl+puWvzHWUWz34Nm8wK/77Fg6nk4CaOq6eMZf+d97j9jiF88vJo/BKACRJEkPDhlIcIFEbHdg4C3SwOqxJAAQnZ4GyxT4goRN1ewA0f/58EhISMBgMDBs2jA0bNrTZft26dQwbNgyDwUBiYiLvvvtuq22/+uorJEniqquu6uBeC8LZczrspG9YA0D/KZee9euqv/+eqq+/BpWKyFdfwXOYe/uGTqcNDyfmgw8I+dODoFJRvWgRWddfz/E9a/nLur8w96e5rMpZhYTE5fGX8/3s73ltyr+Yc+dDzPy/R9B5eJB/OJ1P//IHfnztIxx2OzGpgSQPC228hyzL7MxW3rW3dQKspqKcX39Zx2cnhpJT6kDlctG3oJxZ02cTe3cbVeOFC0ZDAJRZZqbGqixlDQtVfoYP9PUgqlIJgHLM/hiz05WUCPUaqsNnHyxHdskIQmfo1gBo4cKF/OlPf+KJJ55g9+7dTJgwgRkzZpCTk9Ni+8zMTGbOnMmECRPYvXs3jz/+OA888ADfffdds7bZ2dk88sgjTJgwobOHIQhtykjbjsVkxDsgkPiBQ8/qNbacHIpe+AcAIQ8+iO+lZx84tUVSqQi+7z5iP/4YKSgQ67HjmG79PeZfliAjMz1uOovmLOKVSa+QHHCywGjK+Mnc+vJ/iOydgq2ujrKsZdhMnxISnYXDerKQal5lHcVGK1q1xKAY/2b3Lz5xnJX/fYsPHriH3TkSLlSEGs1MPJLL8FlXEfrAAx0yTqH7BXnrCa/PAXWoPiv40DDl5395TCWeNgeBdTZAIr1Io6RDqBeR7IfOoKbOZKc429jlfRd6hm49Bfavf/2Lu+++m3vuUaa733jjDZYvX84777zDiy++2Kz9u+++S2xsLG+88QYAKSkp7Ny5k1dffZVrr722sZ3T6eTmm2/mmWeeYcOGDVRVVXXFcAShRQfqj76nTrwElfrMm59lh4OCR/+KXFuL5/DhBN3TsTMidpedhV4H+PI2K7/9XmJAtsz//ezi/6Sp9LnhBVSGlhMX+oeFc81jL/DxXz7AXLEW2VnF5oUfkPbzAuIHDyN+0FD2mj3QOy0MjA5CslspLy6joiCPvEMHydq7i4r83MbrhVJDfIaR4Jo6Am6+mdC/Pir2/Fxk+kX6UmS0cDC/mhHxgST5J+Gr8+VgaDWyvy/RpVVUxIaSXh3KqIK9SH3CAVCrVcSkBpGxq4Ts/eWEJ5zdsrEguKPbAiCbzUZaWhqPPfZYk8enT5/O5s2bW3zNli1bmD59epPHLrvsMj788EPsdjtarRaAZ599lpCQEO6+++4zLqkBWK1WrKe8izUaxTsOoWOYqyrJ2rMLgH6Tz24Wp+LTz6jbsweVtzeRL7+EdBZB09k6UX2CeRvmkV6eDgZY/MAw+h1IQPXJt/DTSrKO/oao1/+FPiGhxdfvWJyN09WXkKRUUkZXsW/lYqqKCzmyeT1HNisnxX4LkANvtfCrp9ZoSB45lth969FsKkYCAu+6i9C/PCKCn4tQvyg/Vh8uadwIrZJUDAkdwrq8dZQNjSN87QHSCaHS5knBno1E9bms8bXxA5UAKGt/GaNmJ3bXEISLWLcFQGVlZTidTsLCwpo8HhYWRlFRUYuvKSoqarG9w+GgrKyMiIgINm3axIcffsiePXvOui8vvvgizzzzjNtjEIQzObJlA7LsIjy5N4GRZ07k5ygtpWz+fADC5j2GNqrjkv8tOraIf2z7BxanBT+9H48Mf4TZSbNRzVJRM/4yCv7yKNbDh8m6di4Rzz+H78yZTV5fmmti3xplBmfyzQOITQ1i+KzZ5B89RPbeXeSm7ycjIxuD3dz4Gp2HJwERUYTGJxA/aCiR0fFUPPUU5s0ZAITechlBj/6lw8YonF9OzwgNyjLYurx1pCWruPxXmYhaK7meBtJ3pxN1w8nXxvULAgnKcmuoqbTiHaDv6u4LF7luT4TYLImbLLf5TrCl9g2Pm0wmbrnlFt5//32Cg4PPug/z5s3j4Ycfbvxvo9FITEzMWb9eEFpzaONaQNlDczZKXn8Dl9mMYeBA/K6+ukP64HA5eG3na3x+6HMARkeM5vlxzzceZwfwHjeOhEWLKPjzn6nduZP8h/+MceVKwh9/HE1ICLJLZt2XR5BlSB4WSmyqckpHUqmI7tuP6L79KKq28NiLq9HKDrbNm4KPpx6NVtf4O2veupWC227DUVCIpHYROaoK3z890SFjFM5PDQHQsRITNocLnUbF0FBlH9CSoHxm6HREFpSTmxzF4Zw6JtusaHVKoOPhoyMs3pfiTCPZB8roN0FkAhc6VrcFQMHBwajV6mazPSUlJc1meRqEh4e32F6j0RAUFMTBgwfJysriyiuvbHze5XIBoNFoOHLkCElJzQsp6vV69Hrx7kLoWJVFBRQdP4okqegz5syb8ev276f6eyU/T/jj85BU535Gweq08sjaR1ibtxaA+wfdz+8G/Q6V1Pza2rBQYj/5mNK336b8v+9jWroM86bNhPzhfgpip1CcaURrUDP+upaLVG7LVCp4940OItDft/FxR2Ulpa+/oZxoA7ThQUQPPoShT1/wDjnnMQrnryh/D/w8tFTX2TlabKJ/lB/9gvqhV+spclYhDR9M4OadeMlWzE49xzevIWXy5Y2vjx8QRHGmkaz95SIAEjpct50C0+l0DBs2jJUrVzZ5fOXKlYxtJdfJmDFjmrVfsWIFw4cPR6vV0rdvX/bv38+ePXsaP2bPns2UKVPYs2ePmNURutThjUqZh9gBg/Dyb7smlizLlLz8TwD85szGY/Dgc76/xWHhwTUPsjZvLXq1nlcnvcrvB/++xeCngaTREPrggyR88zWG1FRcRiO5r77Fpi+VPC0jLovCy7/lNwvbM5Xj7yMTlPw/zqoqSt/8DxnTLm0Mfvx/cyMJ9w/E4O9oVv1duPhIkkT/KCUYPpCvZITWqrUMCB4AQN6gCCQgxqwsm6avXtLk9XH1x+HzDlfgsDu7qNdCT9GtS2APP/wwt956K8OHD2fMmDH897//JScnh/vuuw9Qlqby8/P57LPPALjvvvt46623ePjhh7n33nvZsmULH374IQsWLADAYDDQv3//Jvfw9/cHaPa4IHQmWZbdWv6q3bad2p07kbRaQh566Jzvb3faeXDNg2wu2IyHxoO3LnmLkREjz/zCeobUVOK/XkjV99+z9vs8HGoPvE256J/4KwVrp+I1fhyew4ahCQtr3KS940QZUaYSpmSXk/vH+dSsWw92OwD6Pn0Ie+JxvEaMgH8PVG6SMOmcxymc/wZE+bPpeDn78qu5sf6xoWFD2Vm8ky3xdq4FwnJNHE4JJPvYCUwVZfgEKoFPcLQ3Xv56zFVW8o9WKfuCBKGDdGsAdMMNN1BeXs6zzz5LYWEh/fv3Z8mSJcTFKRWlCwsLm+QESkhIYMmSJTz00EO8/fbbREZG8uabbzY5Ai8I54PS7EwqC/PRaHX0GjnmjO3L3n4bAP/rrkMbHn5O95ZlmWe3PtsY/MyfOp/h4cPdvo6k0VA35FIKft0NyPQ3r0eqraH6xx+p/vFHpZFajSYwEKfNzqsmE1qXE1ZDTf019CkpBP/ud/hMv1RZ0ivPgKocUGkhftw5jVO4MAyMVo6w78uranysISHiRschbk5NhfR0wiQTxbIP6evXMOqq6wBlBiluQBDpGwrI3lcmAiChQ3X7Juj777+f+++/v8XnPvnkk2aPTZo0iV27dp319Vu6hiB0tmPbNgEQP3gYOg/PNtuat22ndscOJK2WoN/ee873/mD/B/xw/AdUkopXJ73aruAHwOV0sf6rowD0mxDF0Js+oC4tjZp16zBv2ozlyBFwOnGUlgKgBWxqLX4D+uE5Yji+V1yJoU/vphc9sVb5HDMKdF7tHKFwIWkIgI4UmbDYnRi0agaFDkIlqcivyYcJt0B6OrEmI8XePqSvW83IOXMbN8/HDwgmfUMBWQfKmXCGQzKC4I5uD4AE4WIjyzJHtioBUO/RZ57lODn7M/ecZ3825G3gzd1vAjBv5DwmRk9s97UObiigPN+M3kvD6DlJSJKE5/DheA4fDn/+M7LTiaOsDEdpGe9uyWXB/lJmTB7IM9cMav2iJ5SSIGL/T88R5e9BoJeOCrONw0UmBsf446X1ok9AHw5VHOJE/0AigYAcC5p+TioK8ig6fpSIXn0AiO4TgFqjwlRuoaLQTFCkd/cOSLhodHstMEG42JTn5VBZkIdaoyFxaNv7bur276d2+3bQagm699xmf0pqS3hio3Ks/IY+N3Bj3xvP8Io2+lVjY9tPJwAYPTsRg7e2WRtJrUYbFoZH/36sqvOixDOQEcltnOpyOSFTSZZI0pR29024sEiSxIAoZRZo/6nLYGHKMtgW7yI04eFo7JCgUQrpHly3urGdVq8mqo9yiCB7f3kX9VroCUQAJAgd7Ng2JZN53MAh6D3bXv6q+ORTAHxnXI42IqLd93S6nMzbMI9KayV9A/vylxHnllxw20+ZWGsdBEV7k3qG48fVdXbS62s9jWyrAnzBbrBUg94PIoecU/+EC8ug+mWwvXnVjY811AXbVbob7ymTAYipz8J/ePM6HDZbY9v4+urwWaI6vNCBRAAkCB3s6LaG5a/xbbazFxVhXL4cgMDbbz+ney44vIDtRdvx0HjwysRX0Kvbn9eqNMfEwQ35AEy8oTcqVdt7LtKyK5BlSAj2ItS35TpiwMnlr4QJoOq48h7C+W9AtD8A+08JgIaEKkHw8crjqMYrM6VeuQ58PFRYzWYy0rY1to3rrwRARRnVWMz2Luq1cLETAZAgdKCKgnzKcrJQqdUkDRvVZtvKL74AhwPPESPw6Nev3fcsqClo3PfzyPBHiPeLb/e1ZFlmw8KjIEOvEWFE9vI/42u21ef/GZXQxuwPQMZa5bNY/upxGjZCHysxUWtzABDsEUycbxwyMkcSdEgGPc46Nb0MyjJY+vpfG1/vG+xBYKQXsgw56WIZTOgYIgAShA7UcPortv8gDN6tb9Z01dZS+fU3AATe0f7ZH1mWeW7rc9Q56hgaOpS5vee2+1oAx3YUU5hRjUanYuw1zbOmt2TbiaYJEFtkrYHc+nf0iSIA6mnCfA2E+epxyafVBasvi5FWuQ/vsUq6iJjSAgAy96RhrqpsbNu4DLZPBEBCxxABkCB0oIblr16j2j79Vb14Ma7qarQxMXhPntzu+63IXsHG/I1oVVqeHvt0m1mez8RmcbD5u+MADJsRj3dAG8tZ9cxWR2OG3zYDoJwt4LKDfywEisrePdGAKH8A9rW0D6h4F97TpgOgKoCImAhkl4vDm9Y3tm3ICp2TXo7L6eqiXgsXMxEACUIHqSouoiQzA0lSkTxidNttFyqlIQJuvKExk7K7rE4rr6e9DsC9A+4lwS+hXddpkLY0G3O1Dd9gA4OnnV3ZmN05VThcMlH+HkQHtLHhO+OU4+8ij0uPNKiNhIgHyg+gHT8KJLBW6ugTpgOaLoOFJ/ii99RgNTsoyjw5iyQI7SUCIEHoIMe2K6e/Yvr1x9PXr9V2dQcPYjlwALTac6r4/nn65+TX5BPqGcod/e9o93UAqopr2bNaybo+/vreaLRnF5Q1FEA94/6fxvw/YvmrpxoQ3XAU/uQMULRPNCEeIThcDg65CvBIUvJgReYfR6XWUJKVQWlOFgAqtYrY+kzQ4ji80BFEACQIHeTY1oblr7ZPf1XV7/3xvXQamsAzBA6tKK8r5/397wPwp6F/wkPj0a7rNNj07TFcDpnYfoGNey3OxrbMs9j/YyqCknRAEvW/erCB9SfBTpSZqa5TTnJJktR4GmxXyS68JyiFsO0Hc0gcOgJoOgskjsMLHUkEQILQAYxlpRQePwKS1GbtL2eNGePPPwPgf/0N7b7ff/f9F7PdTGpQKrMSZ7X7OqD8Y5K1vxyVSmL8db3OutSAxe5kT24VcIYA6MQ65XPEQPAStZx6qkAvHdEBSqB+ML/lfUA+V14PQG2+TMogpWL8oQ1rcDmVSvCx/YKQJKgoMGMsr+vK7gsXIREACUIHOL5jKwBRfVLw8g9otZ1xyWJctbXo4uLwHHX21dlPVWwu5tuj3wLw0LCHzmnjs9PhYuM3xwAYODWGgPCzr8+1K6cSm8NFmK+ehOA2XieWv4R6A1tIiNiQEXp3yW5UfVLR+krILomQvMMYfHwxV1WSs38PAAYvLeFJyjXEMphwrkQAJAgdoCFpW/Lwtjc/Vy/6AVDqfrW3qOMH+z/A5rIxNHQoo8LbzjV0JvvW5FFdUoenr44RM+Pdeu2WDOUfoLFJwa2PRZZPboAW+X96vEH1y2B7ck8eb+8d0BtfnS+1jloOVRzCZ4CSEb1u3Xr6jlVq2R1ssgymnAbLEgGQcI5EACQI58haayYvfT8AScNbD0hsWVnU7d4NKhW+s2e3615F5iK+O/YdAPcPvv+cKmPX1djYuSQLgFFzEtF5uFcbeXN9ADQmqY1lrdIjUFMEGgPEtB0cChe/IbHK7OiunCpkWQZAJakYHjYcgO1F2/GZoPycmPZkkVofAB3fsRVrbS0AcfX7gPKPVGK3Oru0/8LFRQRAgnCOMvek4XI6CYyKISCi9bpZVT/+CIDXuHFoQ0Pbda+PDnyE3WVnWNgwRoa3bwmtwY5fsrDVOQiO8abvGPfqkNVYHeyt3/8ztq0AqGH5K3YMaM+cV0i4uA2M9kOjkig1WcmrPLmHZ2SE8rO8vXA7HlPmoNY7cVmc+JRXEhgZjcNm5ei2jQAERnjhE2jA6XCRd6SyxfsIwtkQAZAgnKOMncryV1uzP7LLRXV9AOR31Zx23afSUsmiY4sA+N3A353T7E9FoZkD65V6X+Pm9jpjva/T7ciqwOGSiQ30PLv8P2L5SwAMWjX9In0BZQ9Zg4Zgfk/pHhzh/fCJVk6J1fzyPakTLwFOngaTJKnxNFi2OA0mnAMRAAnCOXA6HGTu2QnQZu2v2u3bcRQUovLxwWfq1Hbda+GRhVicFlICUxgdcW7LSZu/P47skkkYFEx0n9Y3bbemYf/PmMQ2Zn8cVshS3rWLDdBCg8ZlsOyTAVCyfzKBhkDqHHXsrzqCz+BoAExrN9B33CSQJPLSD1BdUgyczAqdfaC8cSlNENwlAiBBOAf5h9Oxms14+PoR0at3q+2qf1Bmf3xnzEBlcH8pyOq0suDwAgDu6HfHOc3+5KZXkF1/7H3sNcntusbmDOWd99jkNgKg7M1gN4N3OIQPaNd9hIvP0LiT+4AaSJLUZB+Q59jxqDQuHJUmdIVFxPY7eSQeIKqPPxqdippKK+X5NV07AOGiIQIgQTgHGTuV4++JQ0egUrWcPdllsWBasQJo//LXTxk/UWGpIMIrgkvjL21fZwGXS2bTd8qx9/6To/APa2P5qhVVtbbGgpZtzgAdX6V8Tp4myl8IjYbG+gNwqNBIne3kJuZREcoM6vai7agSxuAdaQHAtHIlqROVWdP0Db8iyzIarZrovkruKVEcVWgvEQAJQjvJstx4/L2t/T81a9fiqq1FGxWFx5Ah7brP/9L/B8AtKbegVWnb12Hg8OZCyvPN6D01jJjVvtphW09UIMuQHOpNqG8bs1nHViqfe01r132Ei1OUvwdhvnocLrlJXbAR4Urm570le7FGDsYnuj4AWrGC5JFj0Oj1VBYWUHjsCCCyQgvnTgRAgtBO5bnZVJcUo9HqiB/QemBT/fMvAPjOmtWupattRdvIrM7EU+PJNb2uaXd/bRYHW386AcCIWQkYvNoXSG1pWP5q6/RXVQ6UHQFJpRRAFYR6kiQxNLb5Mli8bzwhHiHYXDb21hXh1TcESSVjy8mFvHx6j1TKZKSvXw1AXH9lH1BxlpE6k61rByFcFEQAJAjtlJG2HYDYAYPQtrKvx1ldTc369QD4XtG+khVfHf4KgCuTrsRb592uawDsWpZNndGGX6gH/Se1flz/TDY3JkBsIwBqmP2JHgke7m+yFi5uDQFQ2ikboSVJOnkcvmg76qTReIVbATCesgx2ePN6HHY73gF6gmO8QYbsg2IZTHCfCIAEoZ3O5vi7ccUKsNvR9+6NoXfrm6RbU2QuYk2usvHzxj43tq+jgLG8jj2rcgEYe00yak37fvVLTBaOldQgSTAq4Sz2/4jlL6EFDRuhd+dUNjnF1XAcfnvRdogeiU+0kivItGoVMf0H4B0YhNVs5sQu5c1HQ1ZoURZDaA8RAAlCO5irKpXip0Di0NYTEhp/WQyA7xVXtOs+Xx/5GpfsYkT4CJID2ndiC2DrDydwOlxE9fYnYVBwu6+z+bjyD01KuC8BXrqWGzmsJwugJrd/w7Zw8eof5YtOraLcbCOnorbx8YZ9QPvL9lMbORDvKCtIMtb0QzgKCkmZoKRTaMgJFNdfCcJzDpbjdLq6eBTChU4EQILQDg3LX+HJvfEOaLkSuqO0lNrtSjvfmTPdvofdaW8se3Eusz9FmdUc21EMkpL08FyO0K8/VgrAhN5tBFE5W5Tj716hED6w3fcSLl56jZp+Uc0TIkZ7RxPhFYHD5WAPNjTeBjxDlP09plWr6FefFDFz905qjdWExvvi4aPFZnFSdLy6+Y0EoQ0iABKEdmg4/t5W8kPjypUgyxgGDUQX7f6em3V566iwVBDsEcyU2PYlEpRlmU3fHAeg7+hwQmJ92nWdhmttPKZsgJ7YK6T1hg37f5KngUr8iRFa1uo+oIZlsJJdEDn05GmwVasIio4lLLEXLqeTw5vWo1JJxPUTp8GE9hF/nQTBTXaLhZz9e4G29/+Yli0HwPeyy9t1n4bZnzlJc9p99P3E7lKKTlSj0akYPSepXddocLS4hhKTFYNWxbC4NjY2i/0/wllo+BlKy65q8vipG6GJGYlPlLIPqC5tF47S0malMU7NCi0I7hABkCC4KXv/Hhx2G74hYQTHxLXYxlFWRu1OpUSGz/Tpbt+jyFzEpvxNAFzd6+p29dPpdLHlhwwABk+Lxctf367rNNhQv/w1KiEIg7blpI9U5ULp4frj76L8hdC64fUB0OEiI9W19sbHG2aADpYfpCZiIFovF4YwFcgyxpUr6TtuIiq1muITxyjPyyEmNRCVSqKyqJaqktoW7yUILREBkCC46WTyw5Gt7qcxrVoFLheGAQPatfy16PgiZGSGhw0nzrflIOtMDm0soLqkDg8fLUOmx7brGqdaX7/8NaFXG/t/ji5TPkePBM+W90YJAkCor4HEEC9kGbZnVTQ+Hu4VTqxPLC7ZxQ6dEmj7RijLZKZly/H09SNhiFI24+D6X9F7aIjo5QeI02CCe0QAJAhucLmcjRugk4e3XpDU2Lj85f7sj0t28cOxHwDanfjQZnGwfXEWAMNnJqAzaNp1nQYWu5NtJ5R/XCb2bmP/z2Hl1Bt9ZpzT/YSeYXR9KZWtJ5oGLmMixwCwqXwfBPfBN0bZB1S7c2eTZbBDG9bgcjlPHoc/IPYBCWdPBECC4Iai40epM1aj9/Qiqm+/Fts4KioaT3/5XHaZ2/fYWbSTAnMBPlofLo1r3zHyPatyqTPa8A3xoN+EyHZdo0mfsiqxOlyE+erpFdpKMkZL9cnq733bl/RR6FlaC4DGRY4DYHPBZkiYgNbLiSHWH1wujCtXkjh0JAYvb2oqysk9sL/xOHz+0SpsFkeXjkG4cIkASBDccLw++WHCkOGoNS3PqphW1i9/paaii4lx+x6/nFBKZ0yPn45B437l+Fqjjd0rcwAYPSex3UkPT9Ww/2dCr5DWj9EfXwUuOwT1guBe53xP4eI3OkFZJk0vPG0fUMRINJKGXFMuueGpAPjGKlmhTcuWo9Fq6TN2ovLa9avxD/PEL8QDl1Mm71AlgnA2RAAkCG5oyP6cOKz15Iem5cryl8/l7p/+sjgsrMxWjpHPSmzfLMqOxZk4rE5C43xIHhrarmuc7qz2/xxeonzu637OI6Fnam0fkJfWi0GhgwDYrFEyRfsGZAHNl8GObt+M3WppXAYTx+GFsyUCIEE4S5WF+VTk56JSq0kYPKzFNo7KSszblCCpPft/1uatpcZeQ4RXBMPCWr5HW6qKa0nfUADAmGuSkVTtT3rYoNRk5VChEYDxya0EQE77yfw/fcTyl3D2zrQMtql8H4SkKMtgvWIal8EievUhICISh9XKsW2biauvDp99oBzZJSMIZyICIEE4Sw2zP9GpAzB4tbwPpmb1anA60aekoItz//TWLxnK8tcViVegktz/9dz64wlcLpnYfkFE9+mYIqSbjivvqPtH+RLk3cpR+uxNYK0GrxCIHt4h9xV6hlH1y2CnB0Bjo5Tq79uLtmOPV4Ih3xTl9860bDmSJJE6oSEn0Goie/mj1aupNdoozTV1VfeFC5gIgAThLDWc/moz+3Pj6S/3Nz9XWCoac/9ckeh+7bDiTCMZu0pAgjFXn1vSw1OtPVICwPjktk5/1S9/9b4MVK3kCBKEFjTMAJ2+DyglMIUAfQBmu5m9wcpeOt+APODkMlhDbbCcg/sxV5URk6oEU5n7xDKYcGYiABKEs1BnMpJ/OB2A5FayPzurqjBvVUpk+LRj+WtZ5jIcsoPUoFQS/RPdeq0sy2z+vr7kxahwgqNbOanlJqdLZu1RZQP0JX1b2U8ky3CkPgASy1+Cm8J8DSQGN98HpJJUjI5UUk1sRtkArbUewdAvpXEZzC80jOjU/iDLHNqwtrHQb+ZeEQAJZyYCIEE4Cyd27UCWXYTEJeAb0nIgYFr9Kzgc6Pv0QZ+Q4PY9Fp9QcuhcmXil26/NPlBOwbEq1BoVI2e7Fzy1ZXdOJVW1dvw8tAyN9W+5UdF+qM4FjQckTu6wews9x6gzHYcv3Q2hStoJ3+HK71ZDqZl+E6cCSmmMuH5BSCqJ8rwajGV1XdJ34cIlAiBBOAsnsz+3sfy1XMmC3J7Zn6zqLPaV7UMtqbk8wb3TYy6XzJZFSsmLgVOi8Ql0/+h8a349rCx/Tewdgkbdyp+LI0uVz0mXgM6zw+4t9ByjE1vZBxSp7ANKL0+nMlY5eekbrQQ2DctgvUaNQ6PTU1GQR3VxFpHJSlZoMQsknIkIgAThDBw2G1l7dgGt7/9xVldj3qIsf/m24/h7Q+6fMZFjCPZo46h5C45sLaSiwIzeU8PQy9tXNqM1DQHQJX3b2P+T/qPyWRx/F9rp1H1AVbW2xsdDPEPoHdAbGZkt/srvhdaYhmHgwMZlML2nJ8kjlKWyg+t/JWGQ8rN6Yk9pF49CuNCIAEgQziD34D7sVgveAYGEJSa32Mb06xqw29H3Skaf6P7+nYYAyN3Nzw6bk+0/ZwIw7PJ4DF7tqxrfkoKqOg4XmZAkmNS7lf0/pUeg5CCotCL7s9BuYb4GkkO9kWXYeLzpzE3DLNAmRzUgQelhfC8ZD4BxibL3rF99TqDDm9cT20+ZASo8XoWlxo4gtEYEQIJwBqcuf7Va/LQh+eFl7s/+7C3dS35NPp4aTy6JvcSt1+5bk0dNpRXvQD0DprhfdLUta+pPfw2J8SfQS9dyowPfK5+Tp4JHxxy7F3qmSfU15tYdaTpz0xAAbSlJQw6r3weU4g2SRN3ONOz5+cQOGIxXQCAWk5Gy3IMExyjBlEiKKLRFBECC0AbZ5WrM/9Pq8pfJhHmTcnzd93L3j78vyVTexU6NnYqHxuOsX2epsZO2LBuAUbMT0Wg79vj5mvrlr6kpYS03kGU48J3ydb/2FW0VhAaT+9QHQEdLkeWTiQyHhg3FoDZQWlfKseghAGir0vAcqewJql68BJVaTcr4yQCkr/uVhIHKcplYBhPaIgIgQWhDcWYGNZUVaA0exPQf1GKbml9/Rbbb0SUloU9ueYmsNU6XkxVZKwDc3vy8c1kWtjoHQVHe9B4Z7tZrz8RidzYuRUzp08ryV/EBKD8Gar2o/i6cs5EJgXho1ZSYrKTXZx4H0Kv1DA9Xkmtu8lGWtzi+Ct9ZypKr8Rdl+bihNMaJXTuI6KUcBMhNr8Buc3bVEIQLjAiABKENDctf8YOGoNG2vL/GuFwJYNqT/HBn8U7KLeX46f0YEzHmrF9nLKtj/1olKdyYa5JQdUDJi1NtOVGOxe4iws9ASoRPy40alr96XQoG3w69v9Dz6DVqxiYpm6HXnrYMNj5K2fOzri4XtJ5gKsR3SDSSVov16FEsR44SEhtPaHwSLqeDsuxd+AQacNhd5KZXNLuXIIAIgAShTRk7lJNdycNHt/i8s6YG88aNAPi0IwBalqUcnZ8WOw2t+uw3MG/7+QQuh0xUnwBi67PfdqSG5a/JfUJb3vcky3CwPgDqL5a/hI7RuAx2WgA0OWYyAHtK91FdXxZDXbQFr0lKRXjjLz8DJ2eBDq1fQ8Lg+qSIIiu00AoRAAlCK6pLiinNyUKSVCQMabm+Vc2atcg2G7qEBPS9e7l1fbvL3lj53Z3lr9JcE0e3FwMw9pqkVjdmt5csy6w+1HD8vZXlr4LdUJmlvBvv7f7Gb0FoScNpw7ScSoyWkye4oryjSPZPxik72Rgarzx4bCV+VyhJQ6t/WYzsctF33EQklYrC40cIinIAkLW3DJfT1aXjEC4MIgAShFY0LH9F9U3Fw6flJZ5Tkx+6G4hsLdhKtbWaQEMgw8POvoDolkUZIEOv4aGExnX80tOBfCP5VXV4aNVM6NVKTqKGzc+9LwedV4f3QeiZYoM8SQz2wumS2XSs6czNpOhJAKyT6jM8527De9QgVN7eOAoLqdu1Cy//ABIGDwOgPHsHek8NFrOdwozqLh2HcGEQAZAgtKLx9Fdrtb9qzJjXbwDal/ywYflretx0NCrNWb0m91AFuekVqNQSo+Z0XMHTJv06WAjAlL4hGFo6WeZywcEflK/F8pfQwSbVL4Odvg+oYRlsY+lu7MG9QHaiKtiCz6WXAlD9c9PN0Ic2riV+gLI8fGK3OA0mNCcCIEFogcVcQ96hAwAkDRvZYpuadfXLX3Fx6Pv0cev6VqeVX3N+Bc5++Ut2nSx42n9iFH4hZ39k3h3LDhQBcFm/Vk6WZW8EYx7ofSH50k7pg9BzTa4/dXj6cfgBwQMI0AdgspnYE6vM8nBsFX5XKslDTcuWIdtsJA0bhd7TC1N5Kb5BygbojN2lyC4ZQTiVCIAEoQWZe9JwOZ0ERsUQENFygsGGYow+l13m9vLXpvxN1NhrCPUMZUjokLN6zbGdxZTl1qA1qBk+M96t+52t4yUmMkrN6NSq1vf/7Pqf8rn/taDtuLpjggAwKiEQg1ZFkdHC4SJT4+NqlZoJ0RMAWOuhVx48vhLPESNQhwTjrK6mZuMmNDodfcYo7UqztqM1qDFXWSnOMja7l9CziQBIEFrQsPyV3Mryl8tspmZDw/JXO05/ZSrLX5fFX4ZKOvOvodPuYuuPJwAYOj0OD59WMjOfo4bZn3HJQfgYWjiVVlcFh35Svh56a6f0QejZDFo1Y+prgzXUomvQsAy22ngUWesFNcVIZen4zVTq0DWcBut/iTIzeXz7ZmJSlAK9x3c1vZYgiABIEE7jdNjJ2pMGtL7/p2bdOmSLBW1sLPqUFLeuX2uvZW3eWgBmxJ9dAsED6/MxlVvw9NMxaGqMW/dzx7KDSgB0ef9Wlr/2fwMOC4T2g8ihndYPoWe7NFX5+Vte//PYYFzkOPRqPfk1+RyNr1+aPrYS3/rTYKbVv+KsqSE8qTfBsfE47Da0mgwAMnaVNFlSEwQRAAnCaXIP7MNaa8bTz5+I5Jb39hiXLgWUzc/uLn+tz19PnaOOKO8o+gf3P2N7a52DnUuyABh5RQJafceWvGiQW1HLgXwjKgmmtVb+Ynf98teQW6CDj98LQoNLU8OQJNiXV01BVV3j455az8baYKvrq8NzbAWG/v3QJSYiW62Yli1DkiQGXDIdgPzDm1DrVNRUWCnJNjW7l9BziQBIEE5zdJtS16vXyDFIqua/Is4aMzXr1gPgO9P9EhDLM5W9Q5fFn93eoV3Ls7GY7QSEe5IyNsLt+511v+rfbY9MCCTIW9+8QeE+KNyrVH4feEOn9UMQQnz0DItViuuuOG0WaFrcNABW2+qXtHK3I9WU4Hf1VQBUfack6EyZMAW1VktZTibh8RZAmQUShAYiABKEU7icTo7XZ3/uNWpci21q1qxRTn/Fx7t9+qvGVsP6PCV4mpFw5uCpptLKvtW5AIy+KgmVuvN+ZRsCoBn9Wwmydn+ufO47C7yCOq0fggAnl2GXHyxu8vik6EmoJTVHjZnkRg0GZDj8M35z5oBaTd3u3VhPZOLh7UOvkcpskdOinOgUy2DCqUQAJAinyDt0gDqTEYOPLzGpA1ps07j8NXOG28tfa3LXYHPZiPeNp0/AmYOn7b+cwGF3EZHkR8KgVpISdoBio4Wd2ZUATO/XwvKX3QL7Fipfi83PQhdoSMOwLbOcshpr4+N+er/G4qirI+qLD6f/iDY0FO8Jyumv6kXKLNCAS5QDCvlHtqHWODCWWSjLremqIQjnOREACcIpjm7bDCinv1Tq5nttnCYT5vrTXz7tSH64NFMJnmYknDl4qigwc3izkpRwzDXJHV7y4lQ/7SlAlmF4XAARfi3kFzr8C1iqwDcaEqd0Wj8EoUFMoCcDovxwybD0wGnLYLHKMthKV32G56yNYC7D71olMWfVDz8gOxzEpPbHPywCu6UOv+ACQCyDCSeJAEgQ6skuF8e3KwFQ79aWv379FdluR5eUhL6Xe7W/qq3VbCnYAsDl8WcOnrYsOo4sQ+LgECKS/Ny6l7t+2JMPwFVDWs55xPb3lc9DbgFV52zCFoTTXTlIWY79ZW9Bk8enxU1DJanYV3mEgogBILvg8C/4TJqEOiAAZ2kZNRs3IqlU9J+iHIm3GPcAynF4sQwmwHkQAM2fP5+EhAQMBgPDhg1jQ/2769asW7eOYcOGYTAYSExM5N13323y/Pvvv8+ECRMICAggICCAadOmsX379s4cgnCRyD96CHNVJXpPL2IHDGqxjXFJ/fLXDPeXv1Zlr8IhO+gT0IdE/8S2+3Kkkqz95UgqidFXtd32XB0vMXGwwIhGJTFrQAv7fwp2Q+5WUGlg+J2d2hdBONWsgZEAbM+qoNhoaXw82CO4sX7e8sj6NyLpPyLpdPjNng1Adf1m6H6TpiKpVFTkH0eikuqSOsrzxTKY0M0B0MKFC/nTn/7EE088we7du5kwYQIzZswgJyenxfaZmZnMnDmTCRMmsHv3bh5//HEeeOABvvvuu8Y2a9eu5Te/+Q1r1qxhy5YtxMbGMn36dPLz87tqWMIF6lj98lfSsJGoNc2TADqrq6nZrLRpT/LDpVlK8HSm0henlrzoNyGSgPDOLTb6w27l3fXkPiEEeLWQYHHbf5XP/a4Gn1byAwlCJ4jy92BYXACyDL/sK2zy3GXxyu/gMqeyd40T66C2onEZzLRmDY6KCrwDg0gcOgIAD+9jABzb0XRjtdAzdWsA9K9//Yu7776be+65h5SUFN544w1iYmJ45513Wmz/7rvvEhsbyxtvvEFKSgr33HMPd911F6+++mpjmy+++IL777+fwYMH07dvX95//31cLherV6/uqmEJFyBZlhsDoNZOf5lW/wp2O/pevdAnJ7t1/bK6MnYU7QBO/uFuzfG0EkqyTWj1akbMSnDrPu6SZblx+WvO4BaWv2pK4MC3ytej7uvUvghCS64cqMxK/rin6ZvYaXHTUEtq0qszyAlPBdkJR5Zg6N0bw4AB4HBg/FnJDN2wGdpUuhtZtnN0R7GoDSZ0XwBks9lIS0tj+vTpTR6fPn06m+vfZZ9uy5Ytzdpfdtll7Ny5E7vd3uJramtrsdvtBAYGttoXq9WK0Whs8iH0LEUZRzGVl6LVG4gb1HJtrlNPf7lrRdYKXLKLAcEDiPFpPZOzUvJCyVw79LJYPH07p+RFg7TsSvIq6/DSqVtOfrjtPXDaIGo4RA/v1L4IQkuuHBSJRiWxL6+aY8UnExkGGgIZGa5kg14ecXIZDMD/mqsBJSeQLMskDBmGT3AIdosZST5OTYWVohPVXTsQ4bzTbQFQWVkZTqeTsLCmf3TDwsIoKipq8TVFRUUttnc4HJSVlbX4mscee4yoqCimTZvWal9efPFF/Pz8Gj9iYjqv1IBwfmqY/UkcOgKtrnkSQEdlJeYtygbm9pz+Wp6lJD880+bnA+vzMZY1lLyIdfs+7mqY/bm8fwQeutM2N1trYEf95udxD3Z6XwShJUHe+sYK8d/tajoL1LCcvNRZrjyQsQbqqvCdNQtJr8d69CiWAwdRqdQMmqa8cVGh5AQ6KpbBerxu3wR9+kZSWZbb3FzaUvuWHgf45z//yYIFC/j+++8xGFqvWj1v3jyqq6sbP3Jzc90ZgnCBk2X5ZPbn1k5/rV4NDgf6lBT0Ce4tSxWZi9hVsgsJqc3lL2utnR1LMgEYdWVip5W8aGB3ulhcv6/iqiGRzRvs+hQs1RCYpCQ/FIRucu1QZXl20e48nKcsXU2NnYpWpeWYKYcjYb3BZYdDP6H29cXnUuX0V9V3yhLugEumo1JrqK3OxeUo4nhaCU6nq+sHI5w3ui0ACg4ORq1WN5vtKSkpaTbL0yA8PLzF9hqNhqCgpplpX331Vf7xj3+wYsUKBg4c2GZf9Ho9vr6+TT6EnqM0O5Pq4iI0Wh0JQ4a12Kb6518ApfaXuxpmf4aGDSXMq5UaW0DasmysZgeBkV70HdP5m43XHimlstZOiI+esUmnJVl0WGHzW8rX4x4QR9+FbnVJSih+HlqKjVY2HCttfNxP78ek6EkA/ByRpDy4ZwEA/nPnAmD86WecNTV4+vnTe3T9GxzXfiw1dvIOVXbdIITzTrcFQDqdjmHDhrFy5comj69cuZKxY8e2+JoxY8Y0a79ixQqGDx+OVnvy1M4rr7zCc889x7Jlyxg+XOxbENp2eNM6AOIHD0NnaJ4E0F5QQO22bQD4XXmF29dvSH7Y1vKXqcLCvl/zABhzdeeWvGiwYLty2vLqIVGoVafNoO76DEwF4BMJA2/s9L4IQlv0GjVX1+eo+mp70xn6K5OUSvCLLYU4JBXkbIbyDDxHjUSXlISrtpbqH5W9QYOnKzOZ9rpDyC4LR3e0vN1C6Bm6dQns4Ycf5oMPPuCjjz7i0KFDPPTQQ+Tk5HDffcppk3nz5nHbbbc1tr/vvvvIzs7m4Ycf5tChQ3z00Ud8+OGHPPLII41t/vnPf/K3v/2Njz76iPj4eIqKiigqKqKmRuR9EJqTXS4Ob1Jqc6VMmNxim4bZH8+RI9FGtrBU1IZcYy4Hyw+iklRcGndpq+22/XgCp8NFVG9/4vp3fp2t/Ko61h5RMuL+ZuRpe40cVtjwL+XrCQ+DtvXlY0HoKg0/pysPFVNySk6gCVET8Nf7U2atYFuCsimavV8hSRIBv/kNAJVfLkCWZSL7pBASG4/scuC0HeTEnjLsNmeXj0U4P3RrAHTDDTfwxhtv8OyzzzJ48GDWr1/PkiVLiIuLA6CwsLBJTqCEhASWLFnC2rVrGTx4MM899xxvvvkm1157bWOb+fPnY7PZmDt3LhEREY0fpx6VF4QGeYcPYiovRe/pReKQEc2el2WZ6p9+AsBvzmy3r78saxkAo8JHEeTRcmBTmmPiyHblnejYazu35EWDhdtzcMkwNimIhODT8gydOvsz9LaWLyAIXaxPuA/D4gJwumS+3nlyFkir1jbOrv7sX/87tvcrcLnwu2oOKk9PbBkZ1G7bjiRJDKqfBZId+7FbHGTta/kAjXDx6/ZN0Pfffz9ZWVlYrVbS0tKYOHFi43OffPIJa9eubdJ+0qRJ7Nq1C6vVSmZmZuNsUYOsrCxkWW728fTTT3fBaIQLzaGNawHoNWosGl3zI+eWg+nYMjKQ9Hp8Lmt/8sO2Kr9vWXQcZOg1IozQuM7ff+ZwulhY/w/ITaNOm/2x1sC6l5WvJzwMmuYn4gShu9xUPwu0YHtuk83QDctgq41HMRn8oDoHsjei9vbGt/6NS+WXXwLKTK/OwwOnvQKXI4ej28VpsJ6q2wMgQeguDrudo1s3ApAyfnKLbRr2DvhMvQS1t7db1z9eeZxjlcfQqDRcEntJi21yDpaTe6gSlUZi9JzOLXnRYPXhEoqNVoK9dUxPPW2z9Za3wFwKgYkw7I4u6Y8gnK1ZAyMI8NSSX1XHyvSTgcuA4AEk+SVhcVpZklS/DLZHCXgalsFMq1djLypCZ/AgdeJUAJzWPeQcLKeuxta1AxHOCyIAEnqszD07sZrNeAcEEp3av9nzst2OcfFiAPzmzHH7+g3LX+Mix+Gnb17M1OWS2fy9kvRwwORofINbqMLeCb7cpiwrzx0Wg05zyp+AmhLY9Kby9dSnQN28HIggdCeDVt24F+ijTZmNj0uSxDW9lBIY36nqlAfTfwSrCUPv3ngOHw5OJ1Vffw3A4OkzAXDZT+CwVYlZoB5KBEBCj3V4w1oA+oybhKqFY941mzbhrKhAHRSE17iW8wO1Rpblk8kPW6n9dXhLIeX5Neg9NQyfEe/W9dsrt6KW9fXHiH8z8rSEn6ueAbsZooZB6lVd0h9BcNetY+JQqyS2Z1ZwsOBkNucrk65Eq9JyqCaH9JBEsNc2ZoYOuPkmACq//gbZZiMoOpa4gUMAGad1N4c2FYoK8T2QCICEHslaayZj13ag9eUvY/3mZ99ZM5E0Greuf6DsAFnGLAxqA1NipjR73lbnYOsPyuzP8JnxGLy6Zrbly+05yDJM6BVMXNApm5/zdsKez5WvL38ZumAjtiC0R4SfBzP6K0u3768/0fh4gCGAqbHK0tZ3DTmB0j4FwGfaNDQhITjLyjDWp1IZPusqAJy2/ZTllVOac7LMhtAziABI6JGObduM024nMCqG0Pjme2+cJpNS/BTwm+3+8tdPGUrwdEnsJXhpm1dzT1uWRZ3Jjn+YJwMmR7t9/faosTr4Yms2ALeMjjv5hMsJS/6ifD3oJohpfhpOEM4nv5uoBDg/7S0gs8zc+Pi1vZUTwYvrcqlV6yBvOxTsRtJq8b/+ekA5Eg8QN2goQdGxINtxWvdzaHMhQs8iAiChR2o4/ZUyfnKLx85Ny5cjW63okpIw9Et169o2p63x9NecpObBU3VpLXtWK6ewxl2bjFrTNb+GX23PwWhxkBjixaWnFj7d/j4U7AK9L0x7ukv6IgjnYkC0H1P6hOCSYf6a442PjwwfSaxPLGZHLb/0Gq08uF2pZ+d//XWgVlOXloYlPR1JkhhWPwvksOzm6PZCHCInUI8iAiChx6mpKCfn4D4AUsZParFN9Y/1uX9mz3Y7L8+GvA1UW6sJ9QhlVMSoZs9v/i4Dl0MmJjWQuAGdn/QQwOZw8cEGZdPo7yYmomrI/FyVC6ufVb6e9nfwab1UhyCcT/5vqlIBftHufHIragFQSSpu7KtkLv9SY0MG2P8tmMvQhoXhW5/KovyTTwDlDZCnrz/IJuqq0zmxtxSh5xABkNDjHN68HmSZyD6p+IU2r7lly8ujdscOoH2lL37MUDZezkqahfq0zdV5Ryo5sacUSSUxbm7XJD0EZamgyGghxEfPVfUlBXC54OcHlY3PMaNh2F1d0hdB6AhDYwMYnxyMwyXz7rqMxsevSr4KD40HGeYCtkX1A6dVSe4JBN55JwDGJUuxFxWh0ekYfJmSGNFhSSN9Y0HXD0ToNiIAEnoUWZY5uHYV0Prm56pvlOrRXmPHul36otJSyYa8DQDMTmyaOdrlktn4zTEA+k+IJCjSvbxC7eVyyfx3vfIPxF3jEtBr6oOy7f+FjNWgMcDsN0El/hwIF5b/uyQZgG925lFUrZTH8NH5MDtJ+d37Mrj+Dc6OD8HpwGNAfzxHjACHg4r//Q+AQdNnotZokZ3F5B48iLGsrusHInQL8RdP6FGKMo5SlpuNRquj77iJzZ6XHQ6qv/8eoHHTpDuWZC7BITtIDUolOSC5yXOHNhVQnqccex9xZUL7BtAOa46UcLS4Bm+95mTm5+KDsPIp5evpz0NIny7rjyB0lFGJQYxMCMTmdDWZBbqpr3LsfZ0pgzzvYDDmwVFlX17DLFDVwq+VKvG+fqROUhKVOixpHN4iNkP3FCIAEnqUA2uUI7C9Ro3F4NV8BqZm3TocpaWoAwPxuaT58fUz+TnjZ4DGd6ANrHUOtv2kHNkdMSsBD+/mZTc6gyzLvLNW+Yfh5lGx+HlowW6B7+5RlgZ6TYcR93RJXwShMzxYvxfoi23ZZNWfCEv0T2Rs5FhcsovP4gcoDbe9B4D35EnoEhJw1dRQ9fU3AAybeRUALvtxDqw7iMslcgL1BCIAEnoMu9XSWPm9/5TpLbaprM8U63/N1Ugt1AZrS0ZVBgfLD6KRNM1qf6UtOXnsvf/kqHb0vn02HCtjZ3YlOo2KO8fVzzqtehpK0sErBOa8LXL+CBe0ccnBTOwdgt0p8+LSQ42P39lfmelZZMmjQq2BrA1QuBdJpSLobmW/W8XHH+Oy2QiKjiF+8HAAjMUbRYHUHkIEQEKPcXTrJmx1tfiFhRPTQukLe2Eh5g1KbTD/uXPdvn5D7p/x0eMJNAQ2Pl5ZZGbvr/XH3ucmo1Z3za+dLMu8tuIIALeMiiPczwAHf4Bt7ygN5swH79Au6YsgdKa/zUpBrZJYfrCYLRnlAIwKH0VqUCoWp5UFSUpww8bXAeV0pyY8HEdpKdXfLwJgzLXK6TGn7RBpS/d3/SCELicCIKHH2P+rUpqi/+RLkVrY8Fu5cCG4XHiOHIkuPt6taztdTn7J+AVouvwlyzLrvzqKyykT1z+IuP5dc+wdYGV6MXvzqvHQqvn95CQoOgA//F55cswfoXfLs2CCcKHpHebTWCn+uV/ScbpkJEnirv7KTM+XcjW1kqS8ASg7jqTTEXSX8lz5Bx8gOxxE9u5LVN+BgIv89JWUF9R002iEriICIKFHKM3JIv9wOpJKRf/J05o977JaG/cDBNx8s9vX31q4lZK6Enx1vkyKPplbKGNXKXmHK1FrVEy4oVeXHXu3OVz8Y4myHHDnuHhC1Gb46ialPlLiZJj2TJf0QxC6ykOX9sbHoCG90Mh3aXkATIudRqxPLEaHma8ThwEybHoDAP/r5qIOCMCel4dxyRIAxv9G+d132g6StvRAdwxD6EIiABJ6hL0rlRMgySNG4x3YfBbGuHQpzooKNOHh+Ey9xO3rf3fsOwBmJc5Cp1b2DtksjsZj70Mvi8UvxLO93XfbJ5szySqvJcRHz/2T4uHbO6EqG/zjYO7HoHavtpkgnO8CvXQ8cImyIfqVFUcwWuyoVWruGaBs8v9YXafMAu39CqrzUXl4EHj77QCUvfMustNJdN9+hManAC4Ob/gFa629u4YjdAERAAkXPVtdLenrlbpeg6fPava8LMtUfv4FAAE33uh24dOyujLW5KwBYG7vk3uHdi7JwlxlxTfYwNDL4lp7eYcrNVn5z2qlPMBfpvfGe/XjcGItaD3hxi/BM7DtCwjCBer2sfEkBHtRarLy4pLDAFyRdAUxPjFU2E0siB0ALjtseQuAgFtuRu3nhy0zE+PixQBMvPUWAOx1+9mz8lDLNxIuCiIAEi566RvWYrfUERAZTUy/gc2et+zdi+XAASSdTqkX5KYfjv+AQ3YwKGQQvQN6A1BRaGbvKmXj84Tre6PRqdu6RId6bcURTFYHA6L8mFvzBez8EJDg6nchvPnmb0G4WOg0Kl68Rjn2vmB7DlsyytGqtNw36D4APtbZqJEkSPsEzGWovb0JvPtuAErffhvZ4SC230D8I5IAJ2lLfkAWR+IvWiIAEi5qsiyzd4Xyzm7QtBkt7sGp+EzJCOs7cyaaQPdmR1yyi++OKstfDbM/ysbnI7hcMvEDg4kfGHwuQ3DLgfxqFu5UAq+3eu9Gte4l5YmZr0Cq+1XtBeFCMzoxiJvrE37O+34fdTYnMxNmEu8bT7Wjlv9F9Vb2wtXPAgXefJOyFyg7h+qffkaSJCbdcisAdVW7OLLtRLeNRehcIgASLmo5+/dSlpuNVm+g36SpzZ635eVhXK6cDgu8/Ta3r7+1cCt5NXn4aH24LF4ptHh4SyH5R6rQaFWMv67XuQ3ADU6XzJM/HkCW4anEo8RteVJ5YuKjMPLeLuuHIHS3x2b0JdzXQFZ5LW+sOopGpeEPg/8AwMd6J2VqlZIY0VSEysuLoHuUfUJlb7+Ny2YjadgwvAJjAQebvl7YjSMROpMIgISLWtqSHwDoN3kaBu/mmZ8rPvkUnE68xo3DkJLi9vUXHF4AKPsMPDQe1BptbPpW2X8z4soE/EI82t95N3248QS7c6qYqj/MncX/AGQYdidMebzL+iAI5wMfg5bnr1KWe9/fcILdOZVcFn8ZA4MHUuey8VZ0L2UWaP0rAATc9Bs0ISHY8/Op+uorJEliwm+UWaCqgm1k7svstrEInUcEQMJFqzw/l8zdO0GSGDpzdrPnHZWVVH2nLF8F3XO329fPM+WxLncdADf2VZKobfz6KNZaB8Ex3gyeGnMOvXfP8ZIaXl1xlH5SJu9qX0Ny2iBlNsx6TWR6FnqkaalhzBkciUuGB7/aQ43VwSMjHgFgkcrCMa1W2QtUkYnKw4Pg//sjAGXz38FpMpE6YTReAYmAkzUff9Jt4xA6jwiAhIvW7qVKZuakYaMICG9e1b1ywQLkujr0qSl4jh7t9vUXHlmIjMzYyLEk+iWStb+MYztLkCSYcktfVF2U8dnpknn0271EOAv40uMVtA4zxE+Aa94HVddtvhaE882zc/oT5e9BTkUtT/14kCGhQ7g07lJcyLwcnYjscsCafwDgf8016BITcVZVUf7+B0iSxMRblHIalQW7yNp3tDuHInQCEQAJF6Xa6ioOrlOOvg+b1Xzzr7PGTGX95uegu+52O0Fhrb22MffPTX1vwmZxsG6BUnZi0NQYQuN8z6X7bvloYya5OZl8rnsJP1cVhA9QjrtrDV3WB0E4H/l5aPn3jYNRSbBodz6Ldufx0LCH0Kl0bKOO5V6esP8bKDqApNEQ+ueHAaj49FPshYWkjh+Cd1AqILPqww+6dzBChxMBkHBR2rX0Jxw2K+FJvYhOaX70u/LLL3FWVaGLj8f38svcvv7izMWYbCaivaMZHzWebT+eoKbCik+QgZFXJnbEEM5KeoGRd1bs5lPdP4mRSiAgAW75HgxdF4AJwvlseHwgf5qmpKf426IDuGyB3DNQ2fT8z7AIaiRg9bMAeF9yCR7DhyFbrZS88ioAE2++HVBRXZTO4S3bu2MIQicRAZBw0bGYa9i9TKnLNerqG5rN7jhrzFR8+CEAwff/3u3Ehy7ZxWcHPwPgN31/Q+ExI/vWKKn3J9/cB62+a5adTBY7f/piK/NVr5Cqykb2CoVbF4kCp4Jwmj9MSWZkQiBmm5Pff76L3/S+nVifWEplO28HBsCx5XBsFZIkEf744yBJGJcsoXbnTlLG9cM3bAQAv370Pi6Xs5tHI3QUEQAJF529K5Zgq6slKDqWpGEjmz1f+cUXOKurldmfmTPdvv6anDVkGbPw0flwZcwcfv1UyRbbb0IksaldU+xUlmWe+G4Pfza+zGjVIWSdD9It30FgQpfcXxAuJGqVxL9vHEyQl470QiPP/nSUx0cppyO/8PVmj14Hy/4KDhuG1FT8r78egKLnX0B2OrnkzttB0lNnLCTtlyXdORShA4kASLio2C0W0hb/AMCoq65rVvXdaTJR8dFHAAT/4X63Z39kWeajA8rrb+xzI7t/LMBUYcE32MDYa5PPfQBn6ctt2Yw59AKXqXfiUuuQfrMAIppnuRYEQRHh58FbNw1FrZL4fnc+x7OjmJ00Gxl4MjQUS0UGbJ0PQMifHkTl64v18GEqFy4kcXAMQTFTANj8zefU1Zi6cSRCRxEBkHBR2bXsZ+pMRvzCwukzdmKz58v/+74y+5OY2K7Zn10lu9hXtg+dSsdk1xUc2lQIEky9PQWdoWsKjO7Lq6J6ydP8RrMGFypU134ICRO65N6CcCEbkxTEvBl9AXjul3Smhf6WUI9QsjQq/hPgp+QFMhaiCQgg5MEHACj91+s4SkqZeucNSKogHDYzv378UXcOQ+ggIgASLhoWcw07f1JOZo2dexMqddO9OPaCAio+/RSA0EceQVK7v1fng/3KSZA50deQ9nUBoJz6iuwVcC5dP2slRgvLP36e+1WLAJCueB1Sm+c4EgShZXePT+DKQZE4XDJ/+foo9w/4KwD/8/Vli8oOK58ClMLIhkEDcdXUUPzCC8SkhhCVehUAhzeuoui4OBZ/oRMBkHDRSFv8IxZzDYFRMfQdP6nZ86X//jeyzYbniBF4T5ns9vX3lu5lY/5G1Kjpd3AatUYbAeGejJ7TNae+LHYnH33wJn92KEGYdcJjSMPv6JJ7C8LFQpIkXr52AKkRvpTV2Hh3qZ45SdciS/B4SDDlB7+FzA1IajURzz4LajWmFSswrV7NJbdPQ6VLAWSWzv8PLqfYEH0hEwGQcFGoNVazq77sxbjrb0Z1WgLAugMHqf5RSYwY+uijbuf9AZi/R9kfcIPjPooOmFGpJabdmYpG2/mnvmRZ5v3PP+eh6n+ikmSM/W9Df8ljnX5fQbgYeeo0fHjHcMJ89RwrqSHzyCUk+iVSplHzt5AgXD/9AWxmDH36EHSXkgyx6OlnCPR10nvMtSDpqcjPZOcvi7p5JMK5EAGQcFHY/M2X2OrqCE1IotfIsU2ek51Oip55BgDfK6/EY0DzvEBnsqdkD5sLNhNmjsN/Vx8Axl6T3GUJDxcuWcVtWY+hl+yUx1yK7zVviBIXgnAOIvw8+PD2EXho1Ww6ZiLa/lv0ah0bPT14T66EVcrfjOA//AFdQgKO0lKKnn2OCdcPQuc9GYBNX39BRUF+N45COBciABIueOV5OexbtRSAybfe3ezkV+XChVj270fl7U3Yo39x+/qyLPPW7rfQOvVcmfl7XE6Z+IHBDLwkukP6fybLt+xiwvbf4SfVUuI/mKDb/idKXAhCB+gf5cebvxmCJMHiNBgf8DsA3vH3Y/3+TyFrIyqDgciXXwK1GuOSJbBjLcOvmIFKE4fLYWf5O/8WuYEuUCIAEi54a//3IbLLRfKIMcT0a3oU3FFaSunrbwAQ8tCf0ISEuH39Dfkb2Fa4jUknbkRj8sA7QM/U21PatYzmrm2HMolbdhtRUjllhlhCf7sItF1XYV4QLnaXpobxt1mpAPy4MYrRwVcgSxKPhQRz4mdlKcxj4ECCf/dbAAqffoZBgw34hs0AtBQcTSftlx+6bwBCu4kASLigndi1g6w9aajUmsbChQ1kWabo+RdwmUwY+vcn4MYb3b6+w+XgtZ2vkVIyluSyoUgqien39Mfgpe2oIbTqaH4ZLLyZvlIu1epAAn/7M3gGdvp9BaGnuWtcPLeOjkOWYd2WsSR7p2BSq/iDh5WK5fMACL7vPgz9++OqrqZk3qOMnzsEjedkADZ+9RklWSe6cQRCe4gASLhg2S0WVn/0DgBDZ85uVvHd+MtiTMuXg0ZD+DNPt+vY+/fHvqcuV2JC5lwARs9JJCLJ79w7fwZFVbVkf3g7ozhIreSB4Y5FqALjO/2+gtATSZLE369MZVpKGDaHiuMHridMG0CeVssDBcuw7P8aSacj6vV/ofL2pm73bvw3LyRu4ARU2iRcTidL/vMKdqulu4ciuEEEQMIFa8t3CzCWluATHMLYuTc1ec5eVETRc88BSr0vj3793L5+tbWaj7d8zvQjd6GS1fQaHsqQ6bEd0ve2mCx2Nr7zey51bcSOBtd1/0MfM7jT7ysIPZlGreKtm4YwMiEQU50HFTm/w1vSsteg5y+bnsRechhdTAwRzz8PQMUH7zMioRKD72UgeVGel8vqD9/t5lEI7hABkHBBKs0+eQR16l2/R2swND4nO50UzJuHy2jEMHAgwb/9bbvu8frWfzN633V4OLwJjvFmym2dv+/HYneyaP4TzLX+AIBx+ht4p17aqfcUBEFh0Kr54PbhpET4UlHlj6rsd+hlWOuh4+8/34TLZsb38ssIuOUWAGqefZTBo0PRes0EJA6uW8X+NSu6dxDCWRMBkHDBcTrsLJ3/OrLLRa+RY5sVPC196y1qt2xF8vAg8qUX3a73BZBWmIZxuRfBtVFovSVm/n4gWl3nnrxyOF189v5r3GZ8D4CikY8TNPbWTr2nIAhN+Rq0fHrXCOKCPMkviSbcdDNqWeZnjZ1nvrsKp9NB2F8fxXPUKFy1tQT972+ExfdGY1DSb6z+8F2KTxzv5lEIZ0MEQMIFZ8u3X1GadQKDjy9T7/59k+dMa9ZQ/o4yDR3x7LPoE93P0mx1WPnq0zUkVQxBVrm48vdD8Qk0nPmF58Dlkvngs0+4o/glAAr73kH4jEc79Z6CILQs1MfA/+4aRYiPngP5AxhinoZKlvneVsQTP16HUy0R9cbraKOjceZmk3rkc7Reo1BpE3DabfzwynPUVFZ09zCEMxABkHBBKTh6iO0/fAPApff+AS//kzW4rBkZFPxVyY4ccPPN+F15Rbvu8c4nXxOfNRSAcTd1/qZnWZb58NsfuDnrcXSSk6KYGURc/7pIdCgI3Sg2yJPP7hqJn4eWNbmXMss4CI0ss9h0nEd/vgnZ15uYd+aj8vVFs/NXUtiH1msmkjqQmopyfnzlOew2a3cPQ2iDCICEC0adycgv//4nsuwiZcIUeo8a1/icvbiEnHvvxWU04jFkCGF/bd/syS+LN6LdGQVA2FQYMr7z63z9b+l6rjr4ID5SHSVBIwm//VNQiV9NQehuKRG+fHHPKHwNGr4s+A03VyehlWVWVh3iT0tug8RYYt5+C0mnI+zX/xKur0HrNQdJ7UFRxjGWvPmKqBd2HhN/ZYULguxysfTtf2EqK8U/PIKpd93X+Jyzpobc++7DUVCILi6O6PlvI+l0bt9j3/ZMsn5WjrHW9s9l7nWXdFj/W/PV2l2M3/pbQqRqyr17E3rvt6DRd/p9BUE4O/2j/Pjf3aPw0WuZX3gP91eEoXe5WF9xgN8tvgXbgF5EvvoKkkqi15qX8dR6ovW4Ekml4fiOrax8/21kWe7uYQgtEAGQcEHYtuhrMnfvRKPVceVD89B7egHgNJnIvfserIcOoQ4KIuaD99EEBJzhas1l7C1m3SfHkVCRG7Wf+387t6OH0MyCjen0/fVuElVFVOsjCPrdz2Do/BxDgiC4Z1CMP5/ePRJPvY5XSv7IvHJvvFwu0ioPc9NP11I2KpnIl15E56il387/oNZGovGYAZLEgTUrWPe/D0UQdB4SAZBw3juyZQObvv4cgEvuvo/QeGVZymk0knP3PdTt3YvKz4+Y/76HLibG7esfTyth6bv7UbnUZAXt587fz8BL59WhYzjdws1HSFxxJ4NVJ6jV+OF7z0/gE96p9xQEof2Gxgbw6V0j0Og8eKbszzxf4kWU3UFuXQm3/HwjB4cFE/HiP/AzZdHnyALUul5oPKcBkLb4B9Z9/pEIgs4zIgASzmsFRw+x9O1/ATB05hwGTJkOgL2wkOxbbsWybx9qPz/iPv6oXckOD28pZPkH+5FkFceCdzL9nn70Ce7doWM43TdbjxGx9G5GqQ5jUXvjcdePSCGde09BEM7dsLhAPrlrJC69Hw9U/pUnSoMYYrFgctZx/6rfs7iPmchX/klk2Q6i89ai0Q1A61UfBP2yiDWf/BfZ5ermUQgNRAAknLdKc7JY9PKzOO12koaPYtKtdwFQd/AgWdffgPXoUdQhwcR+8jGG1FS3r79/bR6rPz0EssSh0M30vsaHSxOmdfQwmvh2+wkCFv+Wier92FQe6G//HilySKfeUxCEjjMiPpAv7x2FztOHe6r/zL2lscw21eDExUvbX+JZ7zUE/edVehcsIaR0N2rdQAy+SjLT3ct+5pc3X8Fht3fzKAQQAZBwnqooyOPb5/+GpcZEeHJvZv3fX5AkFZVfLST7NzfhKC1F3yuZhK++wpCS4ta1XU4XGxYeZf1XRwHYF76WiBkSvx9y3xleeW4Wbj5KwM93MU29C7ukQ3vr10ixozr1noIgdLyB0f58/bsx+Pn4cGfNA4ypHMQj5ZVoZJkV2Su4rexV7G/+jYHlS/GrOg7qAXj4zkSlVnN0ywa++8eT1Bqru3sYPZ4IgITzTllOFl8/+zi11VWExCdy7bxnkerqyH/4YYqefhrZZsN70iTivvwSbVSUW9e2mO388tZe9q3JA2B7zGICL7Ezb/S8TitzIcsy763YQ8zS25mq3o1d0qG5aQFSwsROuZ8gCJ2vd5gP3943luggX/5ovodc4/V8UlhGlN1BvrmQ2088zY7nZjNK3oBv9QlkdV8Mnlei0RvISz/AF48/JCrIdzNJFruymjEajfj5+VFdXY2vr293d6dHyT+czqJ/PoPVbCY4Jo7rnnwB+8ZNFL/wD5wVFaDREPrQQwTeeQeSm7lyKovMLJ6/j+qSOuwqK78mf87Q0b14cvSTqFWdU+bC5ZJ59cetTN/9BwarMrCqvdDd+jVS/PhOuZ8gCF2rwmzjt5/tZGd2JSPVR3nb5y1e9pFZ7q0cpBji349HdySwOyMJo18ikr0YSVpKrakCjVbH5NvvYeC0GZ1eZ7CncOffbxEAtUAEQN3j0KZ1rHjn3zjsNiJ7p3D5lXMxvvkWtTt3AqDvlUzECy/gMXCgW9eVZZmD6/PZ+N0xnDYZk76cZX0+4JrRs/i/If/XaX94am0OXvpiCbdkPkZvVT4WrR+GO36AqKGdcj9BELqHxe7kr9/t48c9BYRQxcehX3LEtZtXggIwq1RoJDWP1k5HWp9IuW9vcJoxOH+iylwIQPKIMUy75/4mme2F9hEB0DkSAVDXcjocbPzqM3b+/D0Acb36MrzaSt3KVQBIej1Bv72X4HvvdTvBobnKyq+fHSInXanLk+d3hM19v2He5Ee5POHyjh3IKfIqa3n7ww951PQSAVINdYZQPO78EcLc36wtCML5T5Zl3lx9nDdWH0WWZX4fvJdbXR/ykrfEGi9PAAa4Irl66zWUSL2QZRmv6l+okjJwuVwYvH2Ycvu9pEyYImaDzoEIgM6RCIC6TmVRAUv/8xqFx48A0FfrScLO/UgAkoTf7NmE/OlBtBERbl1Xdskc3V7EmoWHcNaBQ7KzLe5n7KnFvDzpZRL9Oq/ExZbjZaz/4h/82fUxGslFTfAgvG9bCL7ujUEQhAvPuqOl/Omr3VTW2okz1PJZzM8cKV/Ji0EBlGnUIMOdR69BXz4BJBU6Uzou+2pqJOVkWFTffky547eEJSR180guTCIAOkciAOp8LqeTXUt/YtNX/8Nht6FxyQzIKSai2gwqFb6zZhH823vR9+rl9rXzMspZ/sUeLAXKu6hSr1w29vmam8fO5ZbUW9CqtB09HACsDifvLdlGnx1PcplaWbarTZmL5zVvg7Zzq8kLgnD+KKiq4w9f7mJ3ThUAdyWZuF/9GR/WHOQrXx8ckkRseTKzjt2JLHuD04Z3yUIq9aU4JUCS6Dt2ImOvv5mA8MhuHcuFRgRA50gEQJ1Htts5/suPbPjxGyrrzAAE1tQxKKcEby9v/K+bi/8NN6CLjnbrurX2WrYcTmPvknz0maEA2FQW9sasJmlCAPcOuYdwr87LtHy02MTCz+bz+5q3CJaMOCQN8iVPoR3/gKjqLgg9kN3p4t21Gbz56zHsThk/g4Y3hleQXPw+/7FksdTbC4Pdi6lHryfGOBgAvTkTddVPVHgqBVQllYpeI8cyZMaVRPVJFUtjZ0EEQOdIBEAdR3Y6sR4/Tu3WrWSvX8v+4jxKvZRin1qHk77lJlKHjsJv1ky8xo9HpW+7EGiNrYYicxFFtUXkm/I5WnmU3BOl+KTHE18+EBUqZFxkRuwhbqon1w2+mgjvzlt6qq618/mS1STve43LVNsBMPn1xuc3H0H4gE67ryAIF4bDRUYe+WYvB/KNAPQK9eblEWa02fN5s+YwWz08SCgfyIQTc/F0KLUAPap24apdT7XHyazRoXEJDJ11Fb3HjEerEwWTWyMCoHMkAqD2kWUZR1ERlvR06vbuo27vXswH9lOolcgO9qPSywMASZaJNBgImjAY0+hkatVOah211DnqqHPUYbabMdvN1NprqXXUNv53tbWaGnsNABqnjviKAaSUjCHKeHKZzBZVwYAZ4UwaMgqtunOWukA59fHjxj1I617iGnkVGsmFCxV1I/+A1/QnRUV3QRAa2Z0u/rclmzd/PUZVrbLXZ2R8IH8a7MKv+DM+LVjLrzpvBhZMY1DBJWhk5W+X3pSOxriGSkMdrvq0H1qNloTBw+g7eSoJg4ahcfNgyMVOBEDnSARAZ+asMWM9dpSaw+lUHdyD5cgRVCdyUZstOCWo8Pag2NeLQn9v7Bolx46MTGFQDVv6V2Pycrp9T5VLTaSxF6kVo4kt64fGWf+Lr5KJGeLHuJl9CYry7shhNlNqsrJy5RK893/KdNdGDPUbF8sjpxA05x/ilJcgCK2qrrMzf81xPt6Uhc2pzO4kBHtxz8hQRkor+D7ja1ZYXCSUXEJq8Tj0TuVNo8pWjr7qV2qlXKyak9fTqNTEJiYTN2osccNGEhgZ3eOXyS6oAGj+/Pm88sorFBYW0q9fP9544w0mTJjQavt169bx8MMPc/DgQSIjI3n00Ue5776mJQy+++47nnzySTIyMkhKSuKFF17g6quvPus+iQBI4bJYMJ44SsmRvVSfOIw1KxPyijAUVuBVZW1sJwNmvZZKTwMlvp6U+no2vlsBMOsdHI+pITvRid7PBy+tFx4aj8YPT40nHtqT/+2l9VIeU3uiNXrhyNFTmylRneXAZT/54+obbKD3yHBSx0fiE9h5m4xrrA627thG1e6f6FO2ggHSyeytpX4DCJjzIprE1n9mBUEQTlVUbeGTzVl8uS0bo8UBgFolMT45mGt7OZCqFrCqMI2q0pGkFk/E2+YPKLPsKutxtKad1EnF2NVNC6t6qDWEh0cSltyHyCHDCB8wCA9vn64eXre6YAKghQsXcuuttzJ//nzGjRvHe++9xwcffEB6ejqxsbHN2mdmZtK/f3/uvfdefve737Fp0ybuv/9+FixYwLXXXgvAli1bmDBhAs899xxXX301ixYt4qmnnmLjxo2MGnV2dZcu9gBIlmVcJhPmkgJK849TnZeJOT8bW2EBcnEp2tJqPCtq8TY3n6Wxq1WY9VrMei2lPjrKfPTYNAZOr6qi9vEkqF9vYocNo9eQUQR5BqFXt7wsJMsyFrOd6pI6qoprKcuroSTbSGluDQ5r0z54+upIGBxCn1HhhCf6dsq7HbPFxpH0vZQc3oyqII1k0w4SpYKT/w/QUBwzg/Bpf0QTO0pschYEoV3MVgff7Mzl2115jXuEGvQJ82FidCVejsUUlBixlvUjtmIwBqeSYVqWZWRnEZIlHazZ2KRqZKn5P+eekgpfT2/8A4MIiIgiMD4R/+Re+MbF4+Hnf9HNGF0wAdCoUaMYOnQo77zzTuNjKSkpXHXVVbz44ovN2v/1r3/lp59+4tChQ42P3Xfffezdu5ctW7YAcMMNN2A0Glm6dGljm8svv5yAgAAWLFhwVv06XwMg2W7HZbXiqqvDabFgq6vBUluNxWzCUmvEZjZhNVbiMBlxGI24akw4TcoHJjPaajN6kwWDyY7aJeFSSbgkCYdKhV2twlH/YVfVf9aoqNFrMHlosOl1OFRqZFfLvywanZ6wxCSi+/YnfshIAiPjcdhkbBYndosDm9WJrdZBrdFGrdFa/1n5MJVbsNY6Wr6uXk14gi+xqUHE9gskMNKrXb+wssuF1WajrtaMpbaamvJizJXF1FUXYzeVgakQnTEHf0suka5CfKW6Jq+3oyHfbyj6fjOJGHcreAW73QdBEITWZJTW8MveQpYfLCK90NjseR+NhVFBaQQ5ypHMwRhMyQSZY1DVv/mUZQcuRwGyoxCXoxicRbjkmjbvKckyelmFQa3BU6fH08sLg5cXek8vdF7eGLx90Pv6YvDzR+vjg9bHB43BE62nB1pPbzRenmg9PVGpO6eUUHu48++3ps1nO5HNZiMtLY3HHnusyePTp09n8+bNLb5my5YtTJ8+vcljl112GR9++CF2ux2tVsuWLVt46KGHmrV54403Wu2L1WrFaj25pFNdrVTpNRqb/xCeC8vRoxQ+8ww4XchOB7hcyC4XtRYTdVYzkiwjueo/ZFC55PoPULlAfZahqgrQATvjIzDrtbgkkFUaXF5+uLz9IbIdEb9dBpQgRaXxQa32B00AKk0IKk0oTimQ/DyJvFzYuvIYcMztW3j76/EN8SAgzIuQOG+Co33wD/NEpWrorwuTyXTmCy17jOK9K1G57GiwoZdt6LGjlmTUgFf9R1tK0VKgT8YWOhDfXqMJHzSdQINyQsPoBDr4Z0MQhJ4tRA93jgzjzpFhVJhtpGVVsD2rgkOFJo4UG6k25UFmOAAACatJREFUu1hhHtLYXo2NZL+1xKkq8XLpMdgC0VvC8KYvKnV/AGSXFdlZjuyqRnZWIstVytcuM8gWAOoAnDaw1UJNZbv6LskyKllGAiWJLRKSrHwtSShfSyAh1beR0IaFEto7hUvv/UP7/6e1oOHf7bOZ2+m2AKisrAyn00lYWFiTx8PCwigqKmrxNUVFRS22dzgclJWVERER0Wqb1q4J8OKLL/LMM880ezwmJuZsh3N+Ou5+ECKcanv9xwfd3RFBEIRmsrq7Ax3hkcc75bImkwk/P78223RbANTg9OUMWZbbXOJoqf3pj7t7zXnz5vHwww83/rfL5aKiooKgoKAzLrcYjUZiYmLIzc09r5bLOpMYsxjzxUqMWYz5YtVTxizLMiaTicjIM2fQ7rYAKDg4GLVa3WxmpqSkpNkMToPw8PAW22s0GoKCgtps09o1AfR6PfrTEvD5+/uf7VAA8PX1vah/qFoixtwziDH3DGLMPUNPGPOZZn4aqM7cpHPodDqGDRvGypUrmzy+cuVKxo4d2+JrxowZ06z9ihUrGD58OFqtts02rV1TEARBEISep1uXwB5++GFuvfVWhg8fzpgxY/jvf/9LTk5OY16fefPmkZ+fz2effQYoJ77eeustHn74Ye699162bNnChx9+2OR014MPPsjEiRN5+eWXmTNnDj/++COrVq1i48aN3TJGQRAEQRDOP90aAN1www2Ul5fz7LPPUlhYSP/+/VmyZAlxcXEAFBYWkpOT09g+ISGBJUuW8NBDD/H2228TGRnJm2++2ZgDCGDs2LF89dVX/O1vf+PJJ58kKSmJhQsXnnUOIHfp9Xr+/ve/N1tCu5iJMfcMYsw9gxhzz9ATx3wm3Z4JWhAEQRAEoat12x4gQRAEQRCE7iICIEEQBEEQehwRAAmCIAiC0OOIAEgQBEEQhB5HBEDnYPbs2cTGxmIwGIiIiODWW2+loKCgSRtJkpp9vPvuu93U43N3NmPOycnhyiuvxMvLi+DgYB544AFsNls39fjcZGVlcffdd5OQkICHhwdJSUn8/e9/bzaei+n7fLZjvpi+zwAvvPACY8eOxdPTs9VEqBfT9xnObswX2/e5JfHx8c2+r6fXqbzQzZ8/n4SEBAwGA8OGDWPDhg3d3aVu1+2lMC5kU6ZM4fHHHyciIoL8/HweeeQR5s6d26yY68cff8zll1/e+N9nm6XyfHSmMTudTmbNmkVISAgbN26kvLyc22+/HVmW+c9//tPNvXff4cOHcblcvPfeeyQnJ3PgwAHuvfdezGYzr776apO2F8v3+WzGfLF9n0Ep0HzdddcxZswYPvzww1bbXSzfZzjzmC/G73Nrnn32We69997G//b29u7G3nSshQsX8qc//Yn58+czbtw43nvvPWbMmEF6ejqxsbHd3b3uIwsd5scff5QlSZJtNlvjY4C8aNGi7utUJzt9zEuWLJFVKpWcn5/f2GbBggWyXq+Xq6uru6ubHeqf//ynnJCQ0OSxi/37fPqYL+bv88cffyz7+fm1+NzF+n1ubcwX8/f5VHFxcfLrr7/e3d3oNCNHjpTvu+++Jo/17dtXfuyxx7qpR+cHsQTWQSoqKvjiiy8YO3ZsY1mOBn/84x8JDg5mxIgRvPvuu7hcrm7qZcdqacxbtmyhf//+TQrRXXbZZVitVtLS0rqrqx2qurqawMDAZo9frN9naD7mnvB9bs3F/H0+XU/6Pr/88ssEBQUxePBgXnjhhYtmmc9ms5GWlsb06dObPD59+vRmqxU9jVgCO0d//etfeeutt6itrWX06NH88ssvTZ5/7rnnmDp1Kh4eHqxevZo///nPlJWV8be//a2benzu2hpzUVFRs8KzAQEB6HS6ZkVqL0QZGRn85z//4bXXXmvy+MX4fW7Q0pgv9u9zay7m73NLesr3+cEHH2To0KEEBASwfft25s2bR2ZmJh988EF3d+2clZWV4XQ6m30fw8LCLqrvYbt09xTU+ebvf/+7DLT5sWPHjsb2paWl8pEjR+QVK1bI48aNk2fOnCm7XK5Wr//qq6/Kvr6+XTGUs9aRY7733nvl6dOnN7uHVquVFyxY0GVjOhN3xyzLspyfny8nJyfLd9999xmvfzF8n2W59TFfzN/ntpbATnexfJ9bG/OF8n1uSXv+PzT49ttvZUAuKyvr4l53vPz8fBmQN2/e3OTx559/Xu7Tp0839er8IGaATvPHP/6RG2+8sc028fHxjV8HBwcTHBxM7969SUlJISYmhq1btzJmzJgWXzt69GiMRiPFxcXNIvLu0pFjDg8PZ9v/t3f/LqnFYRjAn1vgj5Akc6ilgoJAGqITlNDQWVocpD9AmgyHlqJNqKmphpZm/4iGIAdpqpQwmiNI1KLAJqGQeBtuHbhdzR957+l8v89n9Ae8z3kGX456zvn5H+99enpCrVb7MXmB9jOXy2WYpmndtLcZFXr+KrOqPbdLhZ6/4pSe6/nOcZifnwcAXF9fY3BwsNuj/VfBYBC9vb1/ne15eHj48R3+a1yAPvn4cO+EvN9W7eXlpeFr8vk8PB5Pw7+c2qGbmcPhMHZ2dnB3d4fh4WEAwPHxMdxuNwzD6M7AXdBO5lKpBNM0YRgGUqkUenqa/3TO6T03y6xiz51wes/NOKXner5zHPL5PABYmZ3M5XLBMAyk02ksLy9bj6fTaUSjURsnsx8XoA5ls1lks1ksLCxgYGAANzc32Nrawvj4uHX25/DwEPf39wiHw/B6vchkMkgmk1hdXXXkHXlbyby0tIRQKIRYLIbd3V1UKhVsbm4iHo+jv7/f5gTtK5fLWFxcxMjICPb29vD4+Gg9NzQ0BEC9nlvJrFrPwO/r3VQqFRQKBby+vuLy8hIAMDExAZ/Pp1zPQPPMKvb82enpKc7OzmCaJvx+P3K5HNbX161rnqlgY2MDsVgMs7Oz1hndQqGARCJh92j2svs7OKe6uroS0zQlEAiI2+2WsbExSSQSUiwWrdccHR3J9PS0+Hw+6evrk6mpKdnf35darWbj5J1rJbOIyO3trUQiEfF6vRIIBGRtbU2en59tmvp7UqlUw98PfFCt51Yyi6jVs4jIyspK3cyZTEZE1OtZpHlmEfV6/uzi4kLm5ubE7/eLx+ORyclJ2d7elmq1avdoXXVwcCCjo6PicrlkZmZGTk5O7B7Jdr9E3r/DICIiItIErwNERERE2uECRERERNrhAkRERETa4QJERERE2uECRERERNrhAkRERETa4QJERERE2uECRERERNrhAkRERETa4QJERERE2uECRERERNrhAkRERETaeQPtGQttTzCPaQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "events2 = [e for e in events if e['phase']=='Phase 2']\n",
    "events2 = sorted([(e['cos_sim'],e) for e in events2])\n",
    "_, events2 = zip(*events2)\n",
    "\n",
    "seed = -1 \n",
    "for e in events2:\n",
    "    win = np.array(e['win'])\n",
    "    wout = np.array(e['wout'])\n",
    "    # win = win[win<0]\n",
    "    # wout = wout[wout<0]\n",
    "    \n",
    "    \n",
    "    if seed != e['seed']:\n",
    "        print(f\"win {e['seed']} {e['training_type']}, {e['cos_sim']} min win = {np.quantile(win,.05)}, max win = {np.quantile(win,0.95):}\")\n",
    "        wout = np.log(np.abs(wout))\n",
    "        win =  np.log(np.abs(win))\n",
    "        sns.kdeplot(win, label=f\"win {e['seed']} {e['training_type']}, {e['cos_sim']}\",)\n",
    "        # sns.kdeplot(wout, label=f\"wout {e['seed']} {e['training_type']}, {e['cos_sim']}\",)\n",
    "        # sns.kdeplot(np.log(-win[win<0]), label=f\"win {e['seed']} {e['training_type']}, {e['cos_sim']}\",)\n",
    "        # sns.kdeplot(np.log(-wout[wout<0]), label=f\"wout {e['seed']} {e['training_type']}, {e['cos_sim']}\",)\n",
    "        seed = e['seed']\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e6ede817-25b9-48d8-9587-7345238f1096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.509046275370981"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.log(np.abs(wout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074c361-9fd1-48de-9d66-f1a185ecf661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb6a64-b9b1-4538-9486-3f52fa8c81a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa550-c7e9-454d-9859-583da80780cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c053a9-868b-4426-83d5-2dc57264c40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
