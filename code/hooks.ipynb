{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2d46b-cfd0-41dd-be56-66fbfd942f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577a572c-97c5-4aea-9615-e0aaa7e6e291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLP Model Structure ===\n",
      "layers: ModuleDict\n",
      "layers.linear_0: Linear\n",
      "layers.activation_0: ReLU\n",
      "layers.norm_0: BatchNorm1d\n",
      "layers.dropout_0: Dropout\n",
      "layers.linear_1: Linear\n",
      "layers.activation_1: ReLU\n",
      "layers.norm_1: BatchNorm1d\n",
      "layers.dropout_1: Dropout\n",
      "layers.output: Linear\n",
      "\n",
      "=== MLP Activation Statistics ===\n",
      "\n",
      "ACTIVATIONS DATA STRUCTURE:\n",
      "layers.linear_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0056\n",
      "layers.norm_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.0000\n",
      "layers.activation_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.4078\n",
      "layers.dropout_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.4028\n",
      "layers.linear_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=0.0289\n",
      "layers.norm_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=-0.0000\n",
      "layers.activation_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=0.4041\n",
      "layers.dropout_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=0.3967\n",
      "layers.output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=-0.0627\n",
      "\n",
      "GRADIENTS DATA STRUCTURE:\n",
      "layers.output_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=0.0000\n",
      "layers.dropout_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=-0.0000\n",
      "layers.activation_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=-0.0000\n",
      "layers.norm_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=0.0000\n",
      "layers.linear_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256]), mean=-0.0000\n",
      "layers.dropout_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.0000\n",
      "layers.activation_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0000\n",
      "layers.norm_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0000\n",
      "layers.linear_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0000\n",
      "\n",
      "Activation Statistics:\n",
      "\n",
      "Gradient Statistics (output):\n",
      "layers.linear_1_grad_output[0]: shape=torch.Size([16, 256]), mean=-0.000000, std=0.004405, max=0.023299\n",
      "layers.linear_0_grad_output[0]: shape=torch.Size([16, 512]), mean=-0.000000, std=0.002619, max=0.018382\n",
      "\n",
      "Activation Magnitude Flow:\n",
      "layers.activation_0 | ████████████████████ 0.407780\n",
      "layers.activation_1 | ███████████████████ 0.404096\n",
      "layers.dropout_0    | ███████████████████ 0.402835\n",
      "layers.dropout_1    | ███████████████████ 0.396746\n",
      "layers.linear_0     | ██████████████████████ 0.466735\n",
      "layers.linear_1     | █████████████████ 0.359758\n",
      "layers.norm_0       | ████████████████████████████████████████ 0.815560\n",
      "layers.norm_1       | ███████████████████████████████████████ 0.808191\n",
      "layers.output       | ██████████████████ 0.379134\n",
      "\n",
      "Gradient Magnitude Flow (output):\n",
      "layers.activation_0 | █████ 0.001422\n",
      "layers.activation_1 | ██████ 0.001839\n",
      "layers.dropout_0    | █████ 0.001426\n",
      "layers.dropout_1    | ██████ 0.001857\n",
      "layers.linear_0     | █████ 0.001523\n",
      "layers.linear_1     | █████████ 0.002704\n",
      "layers.norm_0       | ██ 0.000721\n",
      "layers.norm_1       | ███ 0.000904\n",
      "layers.output       | ████████████████████████████████████████ 0.011363\n",
      "\n",
      "=== CNN Model Structure ===\n",
      "layers: ModuleDict\n",
      "layers.conv_0: Conv2d\n",
      "layers.norm_0: BatchNorm2d\n",
      "layers.act_0: ReLU\n",
      "layers.pool_0: MaxPool2d\n",
      "layers.conv_1: Conv2d\n",
      "layers.norm_1: BatchNorm2d\n",
      "layers.act_1: ReLU\n",
      "layers.pool_1: MaxPool2d\n",
      "layers.conv_2: Conv2d\n",
      "layers.norm_2: BatchNorm2d\n",
      "layers.act_2: ReLU\n",
      "layers.pool_2: MaxPool2d\n",
      "layers.flatten: Flatten\n",
      "layers.fc_0: Linear\n",
      "layers.fc_act_0: ReLU\n",
      "layers.fc_dropout_0: Dropout\n",
      "layers.output: Linear\n",
      "\n",
      "=== CNN Activation Statistics ===\n",
      "\n",
      "ACTIVATIONS DATA STRUCTURE:\n",
      "layers.conv_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0042\n",
      "layers.norm_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.act_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.3979\n",
      "layers.pool_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 16, 16]), mean=1.0404\n",
      "layers.conv_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0697\n",
      "layers.norm_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.act_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.3988\n",
      "layers.pool_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 8, 8]), mean=1.0117\n",
      "layers.conv_2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0467\n",
      "layers.norm_2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.act_2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.4006\n",
      "layers.pool_2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 4, 4]), mean=1.0067\n",
      "layers.flatten: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 4096]), mean=1.0067\n",
      "layers.fc_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0322\n",
      "layers.fc_act_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.2662\n",
      "layers.fc_dropout_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.2641\n",
      "layers.output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=0.1298\n",
      "\n",
      "GRADIENTS DATA STRUCTURE:\n",
      "layers.output_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=0.0000\n",
      "layers.fc_dropout_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0000\n",
      "layers.fc_act_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=-0.0000\n",
      "layers.fc_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.0000\n",
      "layers.flatten_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 4096]), mean=0.0000\n",
      "layers.pool_2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 4, 4]), mean=0.0000\n",
      "layers.act_2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.norm_2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.conv_2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.pool_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 8, 8]), mean=0.0000\n",
      "layers.act_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.norm_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.conv_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.pool_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 16, 16]), mean=0.0000\n",
      "layers.act_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.norm_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.conv_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "\n",
      "Activation Statistics:\n",
      "\n",
      "Gradient Statistics (output):\n",
      "layers.conv_2_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=0.000000, std=0.000282, max=0.003042\n",
      "layers.conv_1_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000251, max=0.002512\n",
      "layers.conv_0_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=-0.000000, std=0.000173, max=0.002109\n",
      "\n",
      "Activation Magnitude Flow:\n",
      "layers.act_0        | ███████████████ 0.397911\n",
      "layers.act_1        | ███████████████ 0.398828\n",
      "layers.act_2        | ███████████████ 0.400581\n",
      "layers.conv_0       | █████████████████ 0.462035\n",
      "layers.conv_1       | █████████████████████ 0.554901\n",
      "layers.conv_2       | ████████████████████ 0.522149\n",
      "layers.fc_0         | █████████████████████ 0.564657\n",
      "layers.fc_act_0     | ██████████ 0.266223\n",
      "layers.fc_dropout_0 | ██████████ 0.264089\n",
      "layers.flatten      | ██████████████████████████████████████ 1.006727\n",
      "layers.norm_0       | ██████████████████████████████ 0.795822\n",
      "layers.norm_1       | ██████████████████████████████ 0.797656\n",
      "layers.norm_2       | ██████████████████████████████ 0.801163\n",
      "layers.output       | ██████████ 0.277948\n",
      "layers.pool_0       | ████████████████████████████████████████ 1.040440\n",
      "layers.pool_1       | ██████████████████████████████████████ 1.011698\n",
      "layers.pool_2       | ██████████████████████████████████████ 1.006727\n",
      "\n",
      "Gradient Magnitude Flow (output):\n",
      "layers.act_0        |  0.000039\n",
      "layers.act_1        |  0.000042\n",
      "layers.act_2        |  0.000047\n",
      "layers.conv_0       |  0.000066\n",
      "layers.conv_1       |  0.000097\n",
      "layers.conv_2       |  0.000112\n",
      "layers.fc_0         | ██ 0.000620\n",
      "layers.fc_act_0     | ████ 0.001282\n",
      "layers.fc_dropout_0 | ████ 0.001287\n",
      "layers.flatten      |  0.000190\n",
      "layers.norm_0       |  0.000037\n",
      "layers.norm_1       |  0.000039\n",
      "layers.norm_2       |  0.000044\n",
      "layers.output       | ████████████████████████████████████████ 0.011064\n",
      "layers.pool_0       |  0.000156\n",
      "layers.pool_1       |  0.000168\n",
      "layers.pool_2       |  0.000190\n",
      "\n",
      "=== RESNET Model Structure ===\n",
      "layers: ModuleDict\n",
      "layers.conv1: Conv2d\n",
      "layers.bn1: BatchNorm2d\n",
      "layers.activation: ReLU\n",
      "layers.layer1_block0: BasicBlock\n",
      "layers.layer1_block0.layers: ModuleDict\n",
      "layers.layer1_block0.layers.conv1: Conv2d\n",
      "layers.layer1_block0.layers.bn1: BatchNorm2d\n",
      "layers.layer1_block0.layers.activation: ReLU\n",
      "layers.layer1_block0.layers.conv2: Conv2d\n",
      "layers.layer1_block0.layers.bn2: BatchNorm2d\n",
      "layers.layer1_block1: BasicBlock\n",
      "layers.layer1_block1.layers: ModuleDict\n",
      "layers.layer1_block1.layers.conv1: Conv2d\n",
      "layers.layer1_block1.layers.bn1: BatchNorm2d\n",
      "layers.layer1_block1.layers.activation: ReLU\n",
      "layers.layer1_block1.layers.conv2: Conv2d\n",
      "layers.layer1_block1.layers.bn2: BatchNorm2d\n",
      "layers.layer2_block0: BasicBlock\n",
      "layers.layer2_block0.layers: ModuleDict\n",
      "layers.layer2_block0.layers.conv1: Conv2d\n",
      "layers.layer2_block0.layers.bn1: BatchNorm2d\n",
      "layers.layer2_block0.layers.activation: ReLU\n",
      "layers.layer2_block0.layers.conv2: Conv2d\n",
      "layers.layer2_block0.layers.bn2: BatchNorm2d\n",
      "layers.layer2_block0.layers.downsample: Sequential\n",
      "layers.layer2_block0.layers.downsample.0: Conv2d\n",
      "layers.layer2_block0.layers.downsample.1: BatchNorm2d\n",
      "layers.layer2_block1: BasicBlock\n",
      "layers.layer2_block1.layers: ModuleDict\n",
      "layers.layer2_block1.layers.conv1: Conv2d\n",
      "layers.layer2_block1.layers.bn1: BatchNorm2d\n",
      "layers.layer2_block1.layers.activation: ReLU\n",
      "layers.layer2_block1.layers.conv2: Conv2d\n",
      "layers.layer2_block1.layers.bn2: BatchNorm2d\n",
      "layers.layer3_block0: BasicBlock\n",
      "layers.layer3_block0.layers: ModuleDict\n",
      "layers.layer3_block0.layers.conv1: Conv2d\n",
      "layers.layer3_block0.layers.bn1: BatchNorm2d\n",
      "layers.layer3_block0.layers.activation: ReLU\n",
      "layers.layer3_block0.layers.conv2: Conv2d\n",
      "layers.layer3_block0.layers.bn2: BatchNorm2d\n",
      "layers.layer3_block0.layers.downsample: Sequential\n",
      "layers.layer3_block0.layers.downsample.0: Conv2d\n",
      "layers.layer3_block0.layers.downsample.1: BatchNorm2d\n",
      "layers.layer3_block1: BasicBlock\n",
      "layers.layer3_block1.layers: ModuleDict\n",
      "layers.layer3_block1.layers.conv1: Conv2d\n",
      "layers.layer3_block1.layers.bn1: BatchNorm2d\n",
      "layers.layer3_block1.layers.activation: ReLU\n",
      "layers.layer3_block1.layers.conv2: Conv2d\n",
      "layers.layer3_block1.layers.bn2: BatchNorm2d\n",
      "layers.layer4_block0: BasicBlock\n",
      "layers.layer4_block0.layers: ModuleDict\n",
      "layers.layer4_block0.layers.conv1: Conv2d\n",
      "layers.layer4_block0.layers.bn1: BatchNorm2d\n",
      "layers.layer4_block0.layers.activation: ReLU\n",
      "layers.layer4_block0.layers.conv2: Conv2d\n",
      "layers.layer4_block0.layers.bn2: BatchNorm2d\n",
      "layers.layer4_block0.layers.downsample: Sequential\n",
      "layers.layer4_block0.layers.downsample.0: Conv2d\n",
      "layers.layer4_block0.layers.downsample.1: BatchNorm2d\n",
      "layers.layer4_block1: BasicBlock\n",
      "layers.layer4_block1.layers: ModuleDict\n",
      "layers.layer4_block1.layers.conv1: Conv2d\n",
      "layers.layer4_block1.layers.bn1: BatchNorm2d\n",
      "layers.layer4_block1.layers.activation: ReLU\n",
      "layers.layer4_block1.layers.conv2: Conv2d\n",
      "layers.layer4_block1.layers.bn2: BatchNorm2d\n",
      "layers.avgpool: AdaptiveAvgPool2d\n",
      "layers.flatten: Flatten\n",
      "layers.dropout: Dropout\n",
      "layers.fc: Linear\n",
      "\n",
      "=== RESNET Activation Statistics ===\n",
      "\n",
      "ACTIVATIONS DATA STRUCTURE:\n",
      "layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0002\n",
      "layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.activation: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.3977\n",
      "layers.layer1_block0.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0361\n",
      "layers.layer1_block0.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block0.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.3972\n",
      "  - Item 1: shape=torch.Size([16, 64, 32, 32]), mean=0.6788\n",
      "layers.layer1_block0.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.1317\n",
      "layers.layer1_block0.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.6788\n",
      "layers.layer1_block1.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.1705\n",
      "layers.layer1_block1.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block1.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.3973\n",
      "  - Item 1: shape=torch.Size([16, 64, 32, 32]), mean=0.9092\n",
      "layers.layer1_block1.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0275\n",
      "layers.layer1_block1.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.9092\n",
      "layers.layer2_block0.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.1462\n",
      "layers.layer2_block0.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer2_block0.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.3971\n",
      "  - Item 1: shape=torch.Size([16, 128, 16, 16]), mean=0.5612\n",
      "layers.layer2_block0.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0111\n",
      "layers.layer2_block0.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer2_block0.layers.downsample.0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.1735\n",
      "layers.layer2_block0.layers.downsample.1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.downsample: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.5612\n",
      "layers.layer2_block1.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0747\n",
      "layers.layer2_block1.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block1.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.3969\n",
      "  - Item 1: shape=torch.Size([16, 128, 16, 16]), mean=0.8229\n",
      "layers.layer2_block1.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0254\n",
      "layers.layer2_block1.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer2_block1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.8229\n",
      "layers.layer3_block0.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0589\n",
      "layers.layer3_block0.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block0.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.3970\n",
      "  - Item 1: shape=torch.Size([16, 256, 8, 8]), mean=0.5593\n",
      "layers.layer3_block0.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0351\n",
      "layers.layer3_block0.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.downsample.0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0541\n",
      "layers.layer3_block0.layers.downsample.1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.downsample: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.5593\n",
      "layers.layer3_block1.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0719\n",
      "layers.layer3_block1.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.3964\n",
      "  - Item 1: shape=torch.Size([16, 256, 8, 8]), mean=0.8215\n",
      "layers.layer3_block1.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0208\n",
      "layers.layer3_block1.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.8215\n",
      "layers.layer4_block0.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0534\n",
      "layers.layer4_block0.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.3965\n",
      "  - Item 1: shape=torch.Size([16, 512, 4, 4]), mean=0.5578\n",
      "layers.layer4_block0.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0007\n",
      "layers.layer4_block0.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.downsample.0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0394\n",
      "layers.layer4_block0.layers.downsample.1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.downsample: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.5578\n",
      "layers.layer4_block1.layers.conv1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0134\n",
      "layers.layer4_block1.layers.bn1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1.layers.activation: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.3971\n",
      "  - Item 1: shape=torch.Size([16, 512, 4, 4]), mean=0.8189\n",
      "layers.layer4_block1.layers.conv2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0203\n",
      "layers.layer4_block1.layers.bn2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.8189\n",
      "layers.avgpool: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 1, 1]), mean=0.8189\n",
      "layers.flatten: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.8189\n",
      "layers.dropout: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.8150\n",
      "layers.fc: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=-0.0478\n",
      "\n",
      "GRADIENTS DATA STRUCTURE:\n",
      "layers.fc_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=-0.0000\n",
      "layers.dropout_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.0000\n",
      "layers.flatten_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512]), mean=0.0000\n",
      "layers.avgpool_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 1, 1]), mean=0.0000\n",
      "layers.layer4_block1.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "  - Item 1: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0000\n",
      "layers.layer4_block1.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "  - Item 1: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.downsample.1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.downsample.0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0000\n",
      "layers.layer4_block0.layers.downsample_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer4_block0.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0000\n",
      "layers.layer4_block0.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0000\n",
      "layers.layer4_block0.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=-0.0000\n",
      "layers.layer4_block0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 512, 4, 4]), mean=0.0000\n",
      "layers.layer3_block1.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "  - Item 1: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "  - Item 1: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block0.layers.downsample.1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.downsample.0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.downsample_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block0.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer3_block0.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=0.0000\n",
      "layers.layer3_block0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 256, 8, 8]), mean=-0.0000\n",
      "layers.layer2_block1.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "  - Item 1: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block1.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer2_block1.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block1.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block1.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer2_block0.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "  - Item 1: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.downsample.1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.downsample.0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.downsample_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=0.0000\n",
      "layers.layer2_block0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 128, 16, 16]), mean=-0.0000\n",
      "layers.layer1_block1.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "  - Item 1: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block0.layers.activation_grad_output: List with 2 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "  - Item 1: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block0.layers.bn2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block0.layers.conv2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.layer1_block0.layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block0.layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.layer1_block0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.activation_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=0.0000\n",
      "layers.bn1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "layers.conv1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 32, 32]), mean=-0.0000\n",
      "\n",
      "Activation Statistics:\n",
      "\n",
      "Gradient Statistics (output):\n",
      "layers.layer4_block0.layers.activation_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=0.000002, std=0.000143, max=0.000700\n",
      "layers.layer4_block0.layers.activation_grad_output[1]: shape=torch.Size([16, 512, 4, 4]), mean=0.000000, std=0.000172, max=0.000888\n",
      "layers.layer4_block0.layers.downsample.1_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=0.000001, std=0.000106, max=0.000684\n",
      "layers.layer4_block0.layers.downsample.0_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=-0.000000, std=0.000107, max=0.000694\n",
      "layers.layer4_block0.layers.downsample_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=0.000001, std=0.000106, max=0.000684\n",
      "layers.layer4_block0.layers.bn2_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=0.000001, std=0.000106, max=0.000684\n",
      "layers.layer4_block0.layers.conv2_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=-0.000000, std=0.000142, max=0.000971\n",
      "layers.layer4_block0.layers.bn1_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=-0.000000, std=0.000121, max=0.000888\n",
      "layers.layer4_block0.layers.conv1_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=-0.000000, std=0.000125, max=0.000898\n",
      "layers.layer4_block0_grad_output[0]: shape=torch.Size([16, 512, 4, 4]), mean=0.000002, std=0.000143, max=0.000700\n",
      "layers.layer3_block0.layers.activation_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000157, max=0.001060\n",
      "layers.layer3_block0.layers.activation_grad_output[1]: shape=torch.Size([16, 256, 8, 8]), mean=0.000000, std=0.000195, max=0.001096\n",
      "layers.layer3_block0.layers.downsample.1_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000115, max=0.001060\n",
      "layers.layer3_block0.layers.downsample.0_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000116, max=0.001103\n",
      "layers.layer3_block0.layers.downsample_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000115, max=0.001060\n",
      "layers.layer3_block0.layers.bn2_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000115, max=0.001060\n",
      "layers.layer3_block0.layers.conv2_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=0.000000, std=0.000146, max=0.001336\n",
      "layers.layer3_block0.layers.bn1_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000138, max=0.001096\n",
      "layers.layer3_block0.layers.conv1_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=0.000000, std=0.000139, max=0.001162\n",
      "layers.layer3_block0_grad_output[0]: shape=torch.Size([16, 256, 8, 8]), mean=-0.000000, std=0.000157, max=0.001060\n",
      "layers.layer2_block0.layers.activation_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=-0.000000, std=0.000178, max=0.001300\n",
      "layers.layer2_block0.layers.activation_grad_output[1]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000221, max=0.001443\n",
      "layers.layer2_block0.layers.downsample.1_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000130, max=0.001300\n",
      "layers.layer2_block0.layers.downsample.0_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000127, max=0.001435\n",
      "layers.layer2_block0.layers.downsample_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000130, max=0.001300\n",
      "layers.layer2_block0.layers.bn2_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000130, max=0.001300\n",
      "layers.layer2_block0.layers.conv2_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000159, max=0.001524\n",
      "layers.layer2_block0.layers.bn1_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000156, max=0.001443\n",
      "layers.layer2_block0.layers.conv1_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=0.000000, std=0.000152, max=0.001413\n",
      "layers.layer2_block0_grad_output[0]: shape=torch.Size([16, 128, 16, 16]), mean=-0.000000, std=0.000178, max=0.001300\n",
      "layers.layer1_block0.layers.activation_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=0.000000, std=0.000203, max=0.001626\n",
      "layers.layer1_block0.layers.activation_grad_output[1]: shape=torch.Size([16, 64, 32, 32]), mean=0.000000, std=0.000283, max=0.001832\n",
      "layers.layer1_block0.layers.bn2_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=-0.000000, std=0.000165, max=0.001626\n",
      "layers.layer1_block0.layers.conv2_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=0.000000, std=0.000200, max=0.001967\n",
      "layers.layer1_block0.layers.bn1_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=-0.000000, std=0.000199, max=0.001832\n",
      "layers.layer1_block0.layers.conv1_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=-0.000000, std=0.000246, max=0.002279\n",
      "layers.layer1_block0_grad_output[0]: shape=torch.Size([16, 64, 32, 32]), mean=0.000000, std=0.000203, max=0.001626\n",
      "\n",
      "Activation Magnitude Flow:\n",
      "layers.activation                        | █████████████ 0.397690\n",
      "layers.avgpool                           | ███████████████████████████ 0.818935\n",
      "layers.bn1                               | ██████████████████████████ 0.795380\n",
      "layers.conv1                             | ███████ 0.232582\n",
      "layers.dropout                           | ███████████████████████████ 0.815046\n",
      "layers.fc                                | █████████████ 0.402533\n",
      "layers.flatten                           | ███████████████████████████ 0.818935\n",
      "layers.layer1_block0                     | ██████████████████████ 0.678786\n",
      "layers.layer1_block0.layers.bn1          | ██████████████████████████ 0.794414\n",
      "layers.layer1_block0.layers.bn2          | ██████████████████████████ 0.793719\n",
      "layers.layer1_block0.layers.conv1        | ████████████████████████ 0.734339\n",
      "layers.layer1_block0.layers.conv2        | ██████████████████████████ 0.783880\n",
      "layers.layer1_block1                     | ██████████████████████████████ 0.909207\n",
      "layers.layer1_block1.layers.bn1          | ██████████████████████████ 0.794655\n",
      "layers.layer1_block1.layers.bn2          | ██████████████████████████ 0.792907\n",
      "layers.layer1_block1.layers.conv1        | ████████████████████████████████████████ 1.181947\n",
      "layers.layer1_block1.layers.conv2        | █████████████████████████ 0.746195\n",
      "layers.layer2_block0                     | ██████████████████ 0.561235\n",
      "layers.layer2_block0.layers.bn1          | ██████████████████████████ 0.794108\n",
      "layers.layer2_block0.layers.bn2          | ██████████████████████████ 0.793443\n",
      "layers.layer2_block0.layers.conv1        | █████████████████████████████████████ 1.110533\n",
      "layers.layer2_block0.layers.conv2        | ██████████████████████████ 0.785285\n",
      "layers.layer2_block0.layers.downsample   | ██████████████████████████ 0.790885\n",
      "layers.layer2_block0.layers.downsample.0 | █████████████████████████████████████ 1.118321\n",
      "layers.layer2_block0.layers.downsample.1 | ██████████████████████████ 0.790885\n",
      "layers.layer2_block1                     | ███████████████████████████ 0.822851\n",
      "layers.layer2_block1.layers.bn1          | ██████████████████████████ 0.793829\n",
      "layers.layer2_block1.layers.bn2          | ██████████████████████████ 0.792122\n",
      "layers.layer2_block1.layers.conv1        | ████████████████████████████████████ 1.067541\n",
      "layers.layer2_block1.layers.conv2        | ██████████████████████████ 0.774093\n",
      "layers.layer3_block0                     | ██████████████████ 0.559264\n",
      "layers.layer3_block0.layers.bn1          | ██████████████████████████ 0.793961\n",
      "layers.layer3_block0.layers.bn2          | ██████████████████████████ 0.792825\n",
      "layers.layer3_block0.layers.conv1        | ██████████████████████████████████ 1.019569\n",
      "layers.layer3_block0.layers.conv2        | ████████████████████████ 0.735968\n",
      "layers.layer3_block0.layers.downsample   | ██████████████████████████ 0.790980\n",
      "layers.layer3_block0.layers.downsample.0 | ██████████████████████████████████ 1.010845\n",
      "layers.layer3_block0.layers.downsample.1 | ██████████████████████████ 0.790980\n",
      "layers.layer3_block1                     | ███████████████████████████ 0.821506\n",
      "layers.layer3_block1.layers.bn1          | ██████████████████████████ 0.792894\n",
      "layers.layer3_block1.layers.bn2          | ██████████████████████████ 0.791284\n",
      "layers.layer3_block1.layers.conv1        | ███████████████████████████████████ 1.035873\n",
      "layers.layer3_block1.layers.conv2        | ████████████████████████ 0.714018\n",
      "layers.layer4_block0                     | ██████████████████ 0.557801\n",
      "layers.layer4_block0.layers.bn1          | ██████████████████████████ 0.792908\n",
      "layers.layer4_block0.layers.bn2          | ██████████████████████████ 0.792607\n",
      "layers.layer4_block0.layers.conv1        | ████████████████████████████████ 0.951315\n",
      "layers.layer4_block0.layers.conv2        | ██████████████████████ 0.658469\n",
      "layers.layer4_block0.layers.downsample   | ██████████████████████████ 0.790570\n",
      "layers.layer4_block0.layers.downsample.0 | █████████████████████████████████ 0.998017\n",
      "layers.layer4_block0.layers.downsample.1 | ██████████████████████████ 0.790570\n",
      "layers.layer4_block1                     | ███████████████████████████ 0.818935\n",
      "layers.layer4_block1.layers.bn1          | ██████████████████████████ 0.794256\n",
      "layers.layer4_block1.layers.bn2          | ██████████████████████████ 0.793755\n",
      "layers.layer4_block1.layers.conv1        | ████████████████████████████████ 0.954170\n",
      "layers.layer4_block1.layers.conv2        | ███████████████████████ 0.683712\n",
      "\n",
      "Gradient Magnitude Flow (output):\n",
      "layers.activation                        | █ 0.000287\n",
      "layers.avgpool                           | ████ 0.001335\n",
      "layers.bn1                               |  0.000146\n",
      "layers.conv1                             | █ 0.000514\n",
      "layers.dropout                           | ████ 0.001338\n",
      "layers.fc                                | ████████████████████████████████████████ 0.011439\n",
      "layers.flatten                           | ████ 0.001335\n",
      "layers.layer1_block0                     |  0.000152\n",
      "layers.layer1_block0.layers.activation   |  0.000152\n",
      "layers.layer1_block0.layers.bn1          |  0.000107\n",
      "layers.layer1_block0.layers.bn2          |  0.000097\n",
      "layers.layer1_block0.layers.conv1        |  0.000132\n",
      "layers.layer1_block0.layers.conv2        |  0.000119\n",
      "layers.layer1_block1                     |  0.000101\n",
      "layers.layer1_block1.layers.activation   |  0.000101\n",
      "layers.layer1_block1.layers.bn1          |  0.000074\n",
      "layers.layer1_block1.layers.bn2          |  0.000069\n",
      "layers.layer1_block1.layers.conv1        |  0.000063\n",
      "layers.layer1_block1.layers.conv2        |  0.000085\n",
      "layers.layer2_block0                     |  0.000134\n",
      "layers.layer2_block0.layers.activation   |  0.000134\n",
      "layers.layer2_block0.layers.bn1          |  0.000084\n",
      "layers.layer2_block0.layers.bn2          |  0.000070\n",
      "layers.layer2_block0.layers.conv1        |  0.000083\n",
      "layers.layer2_block0.layers.conv2        |  0.000086\n",
      "layers.layer2_block0.layers.downsample   |  0.000070\n",
      "layers.layer2_block0.layers.downsample.0 |  0.000068\n",
      "layers.layer2_block0.layers.downsample.1 |  0.000070\n",
      "layers.layer2_block1                     |  0.000093\n",
      "layers.layer2_block1.layers.activation   |  0.000093\n",
      "layers.layer2_block1.layers.bn1          |  0.000065\n",
      "layers.layer2_block1.layers.bn2          |  0.000060\n",
      "layers.layer2_block1.layers.conv1        |  0.000057\n",
      "layers.layer2_block1.layers.conv2        |  0.000074\n",
      "layers.layer3_block0                     |  0.000120\n",
      "layers.layer3_block0.layers.activation   |  0.000120\n",
      "layers.layer3_block0.layers.bn1          |  0.000076\n",
      "layers.layer3_block0.layers.bn2          |  0.000062\n",
      "layers.layer3_block0.layers.conv1        |  0.000078\n",
      "layers.layer3_block0.layers.conv2        |  0.000081\n",
      "layers.layer3_block0.layers.downsample   |  0.000062\n",
      "layers.layer3_block0.layers.downsample.0 |  0.000064\n",
      "layers.layer3_block0.layers.downsample.1 |  0.000062\n",
      "layers.layer3_block1                     |  0.000083\n",
      "layers.layer3_block1.layers.activation   |  0.000083\n",
      "layers.layer3_block1.layers.bn1          |  0.000059\n",
      "layers.layer3_block1.layers.bn2          |  0.000054\n",
      "layers.layer3_block1.layers.conv1        |  0.000053\n",
      "layers.layer3_block1.layers.conv2        |  0.000069\n",
      "layers.layer4_block0                     |  0.000112\n",
      "layers.layer4_block0.layers.activation   |  0.000112\n",
      "layers.layer4_block0.layers.bn1          |  0.000068\n",
      "layers.layer4_block0.layers.bn2          |  0.000059\n",
      "layers.layer4_block0.layers.conv1        |  0.000072\n",
      "layers.layer4_block0.layers.conv2        |  0.000085\n",
      "layers.layer4_block0.layers.downsample   |  0.000059\n",
      "layers.layer4_block0.layers.downsample.0 |  0.000064\n",
      "layers.layer4_block0.layers.downsample.1 |  0.000059\n",
      "layers.layer4_block1                     |  0.000083\n",
      "layers.layer4_block1.layers.activation   |  0.000083\n",
      "layers.layer4_block1.layers.bn1          |  0.000053\n",
      "layers.layer4_block1.layers.bn2          |  0.000054\n",
      "layers.layer4_block1.layers.conv1        |  0.000052\n",
      "layers.layer4_block1.layers.conv2        |  0.000078\n",
      "\n",
      "=== VIT Model Structure ===\n",
      "layers: ModuleDict\n",
      "layers.patch_embed: PatchEmbedding\n",
      "layers.patch_embed.layers: ModuleDict\n",
      "layers.patch_embed.layers.proj: Conv2d\n",
      "layers.pos_drop: Dropout\n",
      "layers.block_0: TransformerBlock\n",
      "layers.block_0.layers: ModuleDict\n",
      "layers.block_0.layers.norm1: LayerNorm\n",
      "layers.block_0.layers.attn: Attention\n",
      "layers.block_0.layers.attn.layers: ModuleDict\n",
      "layers.block_0.layers.attn.layers.qkv: Linear\n",
      "layers.block_0.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_0.layers.attn.layers.proj: Linear\n",
      "layers.block_0.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_0.layers.norm2: LayerNorm\n",
      "layers.block_0.layers.mlp: TransformerMLP\n",
      "layers.block_0.layers.mlp.layers: ModuleDict\n",
      "layers.block_0.layers.mlp.layers.fc1: Linear\n",
      "layers.block_0.layers.mlp.layers.act: GELU\n",
      "layers.block_0.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_0.layers.mlp.layers.fc2: Linear\n",
      "layers.block_0.layers.mlp.layers.drop2: Dropout\n",
      "layers.block_1: TransformerBlock\n",
      "layers.block_1.layers: ModuleDict\n",
      "layers.block_1.layers.norm1: LayerNorm\n",
      "layers.block_1.layers.attn: Attention\n",
      "layers.block_1.layers.attn.layers: ModuleDict\n",
      "layers.block_1.layers.attn.layers.qkv: Linear\n",
      "layers.block_1.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_1.layers.attn.layers.proj: Linear\n",
      "layers.block_1.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_1.layers.norm2: LayerNorm\n",
      "layers.block_1.layers.mlp: TransformerMLP\n",
      "layers.block_1.layers.mlp.layers: ModuleDict\n",
      "layers.block_1.layers.mlp.layers.fc1: Linear\n",
      "layers.block_1.layers.mlp.layers.act: GELU\n",
      "layers.block_1.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_1.layers.mlp.layers.fc2: Linear\n",
      "layers.block_1.layers.mlp.layers.drop2: Dropout\n",
      "layers.block_2: TransformerBlock\n",
      "layers.block_2.layers: ModuleDict\n",
      "layers.block_2.layers.norm1: LayerNorm\n",
      "layers.block_2.layers.attn: Attention\n",
      "layers.block_2.layers.attn.layers: ModuleDict\n",
      "layers.block_2.layers.attn.layers.qkv: Linear\n",
      "layers.block_2.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_2.layers.attn.layers.proj: Linear\n",
      "layers.block_2.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_2.layers.norm2: LayerNorm\n",
      "layers.block_2.layers.mlp: TransformerMLP\n",
      "layers.block_2.layers.mlp.layers: ModuleDict\n",
      "layers.block_2.layers.mlp.layers.fc1: Linear\n",
      "layers.block_2.layers.mlp.layers.act: GELU\n",
      "layers.block_2.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_2.layers.mlp.layers.fc2: Linear\n",
      "layers.block_2.layers.mlp.layers.drop2: Dropout\n",
      "layers.block_3: TransformerBlock\n",
      "layers.block_3.layers: ModuleDict\n",
      "layers.block_3.layers.norm1: LayerNorm\n",
      "layers.block_3.layers.attn: Attention\n",
      "layers.block_3.layers.attn.layers: ModuleDict\n",
      "layers.block_3.layers.attn.layers.qkv: Linear\n",
      "layers.block_3.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_3.layers.attn.layers.proj: Linear\n",
      "layers.block_3.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_3.layers.norm2: LayerNorm\n",
      "layers.block_3.layers.mlp: TransformerMLP\n",
      "layers.block_3.layers.mlp.layers: ModuleDict\n",
      "layers.block_3.layers.mlp.layers.fc1: Linear\n",
      "layers.block_3.layers.mlp.layers.act: GELU\n",
      "layers.block_3.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_3.layers.mlp.layers.fc2: Linear\n",
      "layers.block_3.layers.mlp.layers.drop2: Dropout\n",
      "layers.block_4: TransformerBlock\n",
      "layers.block_4.layers: ModuleDict\n",
      "layers.block_4.layers.norm1: LayerNorm\n",
      "layers.block_4.layers.attn: Attention\n",
      "layers.block_4.layers.attn.layers: ModuleDict\n",
      "layers.block_4.layers.attn.layers.qkv: Linear\n",
      "layers.block_4.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_4.layers.attn.layers.proj: Linear\n",
      "layers.block_4.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_4.layers.norm2: LayerNorm\n",
      "layers.block_4.layers.mlp: TransformerMLP\n",
      "layers.block_4.layers.mlp.layers: ModuleDict\n",
      "layers.block_4.layers.mlp.layers.fc1: Linear\n",
      "layers.block_4.layers.mlp.layers.act: GELU\n",
      "layers.block_4.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_4.layers.mlp.layers.fc2: Linear\n",
      "layers.block_4.layers.mlp.layers.drop2: Dropout\n",
      "layers.block_5: TransformerBlock\n",
      "layers.block_5.layers: ModuleDict\n",
      "layers.block_5.layers.norm1: LayerNorm\n",
      "layers.block_5.layers.attn: Attention\n",
      "layers.block_5.layers.attn.layers: ModuleDict\n",
      "layers.block_5.layers.attn.layers.qkv: Linear\n",
      "layers.block_5.layers.attn.layers.attn_drop: Dropout\n",
      "layers.block_5.layers.attn.layers.proj: Linear\n",
      "layers.block_5.layers.attn.layers.proj_drop: Dropout\n",
      "layers.block_5.layers.norm2: LayerNorm\n",
      "layers.block_5.layers.mlp: TransformerMLP\n",
      "layers.block_5.layers.mlp.layers: ModuleDict\n",
      "layers.block_5.layers.mlp.layers.fc1: Linear\n",
      "layers.block_5.layers.mlp.layers.act: GELU\n",
      "layers.block_5.layers.mlp.layers.drop1: Dropout\n",
      "layers.block_5.layers.mlp.layers.fc2: Linear\n",
      "layers.block_5.layers.mlp.layers.drop2: Dropout\n",
      "layers.norm: LayerNorm\n",
      "layers.head: Linear\n",
      "\n",
      "=== VIT Activation Statistics ===\n",
      "\n",
      "ACTIVATIONS DATA STRUCTURE:\n",
      "layers.patch_embed.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 192, 8, 8]), mean=-0.0007\n",
      "layers.patch_embed: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 192]), mean=-0.0007\n",
      "layers.pos_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0009\n",
      "layers.block_0.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=0.0017\n",
      "layers.block_0.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_0.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_0.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_0.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_0.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_0.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0012\n",
      "layers.block_0.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0291\n",
      "layers.block_0.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0290\n",
      "layers.block_0.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0021\n",
      "layers.block_0.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0021\n",
      "layers.block_0.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0021\n",
      "layers.block_0: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0013\n",
      "layers.block_1.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0012\n",
      "layers.block_1.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_1.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0008\n",
      "layers.block_1.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0007\n",
      "layers.block_1.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0007\n",
      "layers.block_1.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0009\n",
      "layers.block_1.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0298\n",
      "layers.block_1.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0298\n",
      "layers.block_1.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_1.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0003\n",
      "layers.block_1.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0003\n",
      "layers.block_1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0009\n",
      "layers.block_2.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0020\n",
      "layers.block_2.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_2.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0004\n",
      "layers.block_2.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0295\n",
      "layers.block_2.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0295\n",
      "layers.block_2.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0008\n",
      "layers.block_3.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=0.0044\n",
      "layers.block_3.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_3.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0001\n",
      "layers.block_3.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0001\n",
      "layers.block_3.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0001\n",
      "layers.block_3.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0014\n",
      "layers.block_3.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0287\n",
      "layers.block_3.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0287\n",
      "layers.block_3.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0006\n",
      "layers.block_3.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0006\n",
      "layers.block_3.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0006\n",
      "layers.block_3: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0015\n",
      "layers.block_4.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0025\n",
      "layers.block_4.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_4.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0008\n",
      "layers.block_4.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0008\n",
      "layers.block_4.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0008\n",
      "layers.block_4.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0001\n",
      "layers.block_4.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0297\n",
      "layers.block_4.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0297\n",
      "layers.block_4.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0005\n",
      "layers.block_4.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0006\n",
      "layers.block_4.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0006\n",
      "layers.block_4: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0001\n",
      "layers.block_5.layers.norm1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5.layers.attn.layers.qkv: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=0.0015\n",
      "layers.block_5.layers.attn.layers.attn_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0154\n",
      "layers.block_5.layers.attn.layers.proj: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_5.layers.attn.layers.proj_drop: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_5.layers.attn: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0002\n",
      "layers.block_5.layers.norm2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5.layers.mlp.layers.fc1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0006\n",
      "layers.block_5.layers.mlp.layers.act: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0294\n",
      "layers.block_5.layers.mlp.layers.drop1: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0294\n",
      "layers.block_5.layers.mlp.layers.fc2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0023\n",
      "layers.block_5.layers.mlp.layers.drop2: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0023\n",
      "layers.block_5.layers.mlp: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0023\n",
      "layers.block_5: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0021\n",
      "layers.norm: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.head: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=0.0272\n",
      "\n",
      "GRADIENTS DATA STRUCTURE:\n",
      "layers.head_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 10]), mean=0.0000\n",
      "layers.norm_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_5.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_5.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_5.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_5.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_5.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_5.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_5.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=-0.0000\n",
      "layers.block_5.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_5.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_5.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_5_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_4.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_4.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_4.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_4.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_4.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_4.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0000\n",
      "layers.block_4.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_4.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_4.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_4_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_3.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_3.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_3.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0000\n",
      "layers.block_3.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_3.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0000\n",
      "layers.block_3.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_3.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_3.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_3_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_2.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0000\n",
      "layers.block_2.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0000\n",
      "layers.block_2.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=-0.0000\n",
      "layers.block_2.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_2.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_2.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_1.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=-0.0000\n",
      "layers.block_1.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_1.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_1.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_1.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_1.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_1.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=-0.0000\n",
      "layers.block_1.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_1.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_1.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.mlp.layers.drop2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.mlp.layers.fc2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=-0.0000\n",
      "layers.block_0.layers.mlp.layers.drop1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_0.layers.mlp.layers.act_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_0.layers.mlp.layers.fc1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 768]), mean=0.0000\n",
      "layers.block_0.layers.mlp_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.norm2_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.attn.layers.proj_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.attn.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.attn.layers.attn_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 8, 65, 65]), mean=0.0000\n",
      "layers.block_0.layers.attn.layers.qkv_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 576]), mean=-0.0000\n",
      "layers.block_0.layers.attn_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0.layers.norm1_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.block_0_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.pos_drop_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 65, 192]), mean=0.0000\n",
      "layers.patch_embed_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 64, 192]), mean=0.0000\n",
      "layers.patch_embed.layers.proj_grad_output: List with 1 tensors\n",
      "  - Item 0: shape=torch.Size([16, 192, 8, 8]), mean=0.0000\n",
      "\n",
      "Activation Statistics:\n",
      "\n",
      "Gradient Statistics (output):\n",
      "layers.block_5.layers.mlp.layers.drop2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000610, max=0.017819\n",
      "layers.block_5.layers.mlp.layers.fc2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000646, max=0.019799\n",
      "layers.block_5.layers.mlp.layers.drop1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000000, std=0.000179, max=0.006339\n",
      "layers.block_5.layers.mlp.layers.act_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000000, std=0.000190, max=0.007044\n",
      "layers.block_5.layers.mlp.layers.fc1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000000, std=0.000101, max=0.004644\n",
      "layers.block_5.layers.mlp_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000610, max=0.017819\n",
      "layers.block_5.layers.norm2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000056, max=0.001657\n",
      "layers.block_5.layers.attn.layers.proj_drop_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000667, max=0.019253\n",
      "layers.block_5.layers.attn.layers.proj_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000707, max=0.020740\n",
      "layers.block_5.layers.attn.layers.attn_drop_grad_output[0]: shape=torch.Size([16, 8, 65, 65]), mean=-0.000000, std=0.000257, max=0.008025\n",
      "layers.block_5.layers.attn.layers.qkv_grad_output[0]: shape=torch.Size([16, 65, 576]), mean=-0.000000, std=0.000014, max=0.000105\n",
      "layers.block_5.layers.attn_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000667, max=0.019253\n",
      "layers.block_5.layers.norm1_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000007, max=0.000037\n",
      "layers.block_5_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000610, max=0.017819\n",
      "layers.block_2.layers.mlp.layers.drop2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000852, max=0.026210\n",
      "layers.block_2.layers.mlp.layers.fc2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000896, max=0.029122\n",
      "layers.block_2.layers.mlp.layers.drop1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000000, std=0.000249, max=0.007920\n",
      "layers.block_2.layers.mlp.layers.act_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=-0.000000, std=0.000262, max=0.008800\n",
      "layers.block_2.layers.mlp.layers.fc1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=-0.000000, std=0.000140, max=0.006834\n",
      "layers.block_2.layers.mlp_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000852, max=0.026210\n",
      "layers.block_2.layers.norm2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000078, max=0.002575\n",
      "layers.block_2.layers.attn.layers.proj_drop_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.001032, max=0.031372\n",
      "layers.block_2.layers.attn.layers.proj_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.001084, max=0.034857\n",
      "layers.block_2.layers.attn.layers.attn_drop_grad_output[0]: shape=torch.Size([16, 8, 65, 65]), mean=-0.000001, std=0.000406, max=0.013793\n",
      "layers.block_2.layers.attn.layers.qkv_grad_output[0]: shape=torch.Size([16, 65, 576]), mean=-0.000000, std=0.000022, max=0.000171\n",
      "layers.block_2.layers.attn_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.001032, max=0.031372\n",
      "layers.block_2.layers.norm1_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000011, max=0.000059\n",
      "layers.block_2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.000852, max=0.026210\n",
      "layers.block_0.layers.mlp.layers.drop2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.001425, max=0.046355\n",
      "layers.block_0.layers.mlp.layers.fc2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=-0.000000, std=0.001503, max=0.051505\n",
      "layers.block_0.layers.mlp.layers.drop1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000001, std=0.000414, max=0.016044\n",
      "layers.block_0.layers.mlp.layers.act_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000001, std=0.000434, max=0.017827\n",
      "layers.block_0.layers.mlp.layers.fc1_grad_output[0]: shape=torch.Size([16, 65, 768]), mean=0.000001, std=0.000238, max=0.011013\n",
      "layers.block_0.layers.mlp_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.001425, max=0.046355\n",
      "layers.block_0.layers.norm2_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.000130, max=0.004253\n",
      "layers.block_0.layers.attn.layers.proj_drop_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.004421, max=0.140456\n",
      "layers.block_0.layers.attn.layers.proj_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000005, std=0.004683, max=0.156062\n",
      "layers.block_0.layers.attn.layers.attn_drop_grad_output[0]: shape=torch.Size([16, 8, 65, 65]), mean=0.000006, std=0.001700, max=0.070495\n",
      "layers.block_0.layers.attn.layers.qkv_grad_output[0]: shape=torch.Size([16, 65, 576]), mean=-0.000001, std=0.000093, max=0.000785\n",
      "layers.block_0.layers.attn_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.004421, max=0.140456\n",
      "layers.block_0.layers.norm1_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000001, std=0.000045, max=0.000262\n",
      "layers.block_0_grad_output[0]: shape=torch.Size([16, 65, 192]), mean=0.000000, std=0.001425, max=0.046355\n",
      "layers.patch_embed_grad_output[0]: shape=torch.Size([16, 64, 192]), mean=0.000000, std=0.000088, max=0.000576\n",
      "layers.patch_embed.layers.proj_grad_output[0]: shape=torch.Size([16, 192, 8, 8]), mean=0.000000, std=0.000088, max=0.000576\n",
      "\n",
      "Activation Magnitude Flow:\n",
      "layers.block_0                              | ███████████████████████ 0.467299\n",
      "layers.block_0.layers.attn                  |  0.010911\n",
      "layers.block_0.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_0.layers.attn.layers.proj      |  0.010904\n",
      "layers.block_0.layers.attn.layers.proj_drop |  0.010911\n",
      "layers.block_0.layers.attn.layers.qkv       | ███████████ 0.220458\n",
      "layers.block_0.layers.mlp                   | ███ 0.068867\n",
      "layers.block_0.layers.mlp.layers.act        | █████ 0.110555\n",
      "layers.block_0.layers.mlp.layers.drop1      | █████ 0.110510\n",
      "layers.block_0.layers.mlp.layers.drop2      | ███ 0.068867\n",
      "layers.block_0.layers.mlp.layers.fc1        | ███████████ 0.221627\n",
      "layers.block_0.layers.mlp.layers.fc2        | ███ 0.068958\n",
      "layers.block_0.layers.norm1                 | ███████████████████████████████████████ 0.764583\n",
      "layers.block_0.layers.norm2                 | ███████████████████████████████████████ 0.765443\n",
      "layers.block_1                              | ████████████████████████ 0.476240\n",
      "layers.block_1.layers.attn                  |  0.011115\n",
      "layers.block_1.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_1.layers.attn.layers.proj      |  0.011122\n",
      "layers.block_1.layers.attn.layers.proj_drop |  0.011115\n",
      "layers.block_1.layers.attn.layers.qkv       | ███████████ 0.221352\n",
      "layers.block_1.layers.mlp                   | ███ 0.068704\n",
      "layers.block_1.layers.mlp.layers.act        | █████ 0.110319\n",
      "layers.block_1.layers.mlp.layers.drop1      | █████ 0.110359\n",
      "layers.block_1.layers.mlp.layers.drop2      | ███ 0.068704\n",
      "layers.block_1.layers.mlp.layers.fc1        | ███████████ 0.220341\n",
      "layers.block_1.layers.mlp.layers.fc2        | ███ 0.068725\n",
      "layers.block_1.layers.norm1                 | ███████████████████████████████████████ 0.771595\n",
      "layers.block_1.layers.norm2                 | ███████████████████████████████████████ 0.771727\n",
      "layers.block_2                              | ████████████████████████ 0.482391\n",
      "layers.block_2.layers.attn                  |  0.012628\n",
      "layers.block_2.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_2.layers.attn.layers.proj      |  0.012654\n",
      "layers.block_2.layers.attn.layers.proj_drop |  0.012628\n",
      "layers.block_2.layers.attn.layers.qkv       | ███████████ 0.221303\n",
      "layers.block_2.layers.mlp                   | ███ 0.068307\n",
      "layers.block_2.layers.mlp.layers.act        | █████ 0.110121\n",
      "layers.block_2.layers.mlp.layers.drop1      | █████ 0.110149\n",
      "layers.block_2.layers.mlp.layers.drop2      | ███ 0.068307\n",
      "layers.block_2.layers.mlp.layers.fc1        | ███████████ 0.220208\n",
      "layers.block_2.layers.mlp.layers.fc2        | ███ 0.068179\n",
      "layers.block_2.layers.norm1                 | ███████████████████████████████████████ 0.775069\n",
      "layers.block_2.layers.norm2                 | ███████████████████████████████████████ 0.775115\n",
      "layers.block_3                              | ████████████████████████ 0.488534\n",
      "layers.block_3.layers.attn                  |  0.011301\n",
      "layers.block_3.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_3.layers.attn.layers.proj      |  0.011302\n",
      "layers.block_3.layers.attn.layers.proj_drop |  0.011301\n",
      "layers.block_3.layers.attn.layers.qkv       | ███████████ 0.223052\n",
      "layers.block_3.layers.mlp                   | ███ 0.068270\n",
      "layers.block_3.layers.mlp.layers.act        | █████ 0.110146\n",
      "layers.block_3.layers.mlp.layers.drop1      | █████ 0.110112\n",
      "layers.block_3.layers.mlp.layers.drop2      | ███ 0.068270\n",
      "layers.block_3.layers.mlp.layers.fc1        | ███████████ 0.220589\n",
      "layers.block_3.layers.mlp.layers.fc2        | ███ 0.068287\n",
      "layers.block_3.layers.norm1                 | ███████████████████████████████████████ 0.777418\n",
      "layers.block_3.layers.norm2                 | ███████████████████████████████████████ 0.777468\n",
      "layers.block_4                              | █████████████████████████ 0.495155\n",
      "layers.block_4.layers.attn                  |  0.013133\n",
      "layers.block_4.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_4.layers.attn.layers.proj      |  0.013134\n",
      "layers.block_4.layers.attn.layers.proj_drop |  0.013133\n",
      "layers.block_4.layers.attn.layers.qkv       | ███████████ 0.222334\n",
      "layers.block_4.layers.mlp                   | ███ 0.068349\n",
      "layers.block_4.layers.mlp.layers.act        | █████ 0.111041\n",
      "layers.block_4.layers.mlp.layers.drop1      | █████ 0.110925\n",
      "layers.block_4.layers.mlp.layers.drop2      | ███ 0.068349\n",
      "layers.block_4.layers.mlp.layers.fc1        | ███████████ 0.222145\n",
      "layers.block_4.layers.mlp.layers.fc2        | ███ 0.068396\n",
      "layers.block_4.layers.norm1                 | ███████████████████████████████████████ 0.779383\n",
      "layers.block_4.layers.norm2                 | ███████████████████████████████████████ 0.779504\n",
      "layers.block_5                              | █████████████████████████ 0.500758\n",
      "layers.block_5.layers.attn                  |  0.012295\n",
      "layers.block_5.layers.attn.layers.attn_drop |  0.015385\n",
      "layers.block_5.layers.attn.layers.proj      |  0.012298\n",
      "layers.block_5.layers.attn.layers.proj_drop |  0.012295\n",
      "layers.block_5.layers.attn.layers.qkv       | ███████████ 0.221397\n",
      "layers.block_5.layers.mlp                   | ███ 0.069325\n",
      "layers.block_5.layers.mlp.layers.act        | █████ 0.110746\n",
      "layers.block_5.layers.mlp.layers.drop1      | █████ 0.110826\n",
      "layers.block_5.layers.mlp.layers.drop2      | ███ 0.069325\n",
      "layers.block_5.layers.mlp.layers.fc1        | ███████████ 0.221752\n",
      "layers.block_5.layers.mlp.layers.fc2        | ███ 0.069267\n",
      "layers.block_5.layers.norm1                 | ███████████████████████████████████████ 0.780919\n",
      "layers.block_5.layers.norm2                 | ███████████████████████████████████████ 0.780965\n",
      "layers.head                                 | ██████████ 0.203565\n",
      "layers.norm                                 | ████████████████████████████████████████ 0.782239\n",
      "layers.patch_embed                          | ███████████████████████ 0.462797\n",
      "layers.patch_embed.layers.proj              | ███████████████████████ 0.462797\n",
      "layers.pos_drop                             | ███████████████████████ 0.456102\n",
      "\n",
      "Gradient Magnitude Flow (output):\n",
      "layers.block_0                              |  0.000167\n",
      "layers.block_0.layers.attn                  | █ 0.000460\n",
      "layers.block_0.layers.attn.layers.attn_drop |  0.000177\n",
      "layers.block_0.layers.attn.layers.proj      | █ 0.000465\n",
      "layers.block_0.layers.attn.layers.proj_drop | █ 0.000460\n",
      "layers.block_0.layers.attn.layers.qkv       |  0.000045\n",
      "layers.block_0.layers.mlp                   |  0.000167\n",
      "layers.block_0.layers.mlp.layers.act        |  0.000049\n",
      "layers.block_0.layers.mlp.layers.drop1      |  0.000049\n",
      "layers.block_0.layers.mlp.layers.drop2      |  0.000167\n",
      "layers.block_0.layers.mlp.layers.fc1        |  0.000025\n",
      "layers.block_0.layers.mlp.layers.fc2        |  0.000167\n",
      "layers.block_0.layers.norm1                 |  0.000035\n",
      "layers.block_0.layers.norm2                 |  0.000015\n",
      "layers.block_1                              |  0.000123\n",
      "layers.block_1.layers.attn                  |  0.000161\n",
      "layers.block_1.layers.attn.layers.attn_drop |  0.000063\n",
      "layers.block_1.layers.attn.layers.proj      |  0.000161\n",
      "layers.block_1.layers.attn.layers.proj_drop |  0.000161\n",
      "layers.block_1.layers.attn.layers.qkv       |  0.000014\n",
      "layers.block_1.layers.mlp                   |  0.000123\n",
      "layers.block_1.layers.mlp.layers.act        |  0.000036\n",
      "layers.block_1.layers.mlp.layers.drop1      |  0.000036\n",
      "layers.block_1.layers.mlp.layers.drop2      |  0.000123\n",
      "layers.block_1.layers.mlp.layers.fc1        |  0.000018\n",
      "layers.block_1.layers.mlp.layers.fc2        |  0.000122\n",
      "layers.block_1.layers.norm1                 |  0.000011\n",
      "layers.block_1.layers.norm2                 |  0.000011\n",
      "layers.block_2                              |  0.000101\n",
      "layers.block_2.layers.attn                  |  0.000119\n",
      "layers.block_2.layers.attn.layers.attn_drop |  0.000046\n",
      "layers.block_2.layers.attn.layers.proj      |  0.000118\n",
      "layers.block_2.layers.attn.layers.proj_drop |  0.000119\n",
      "layers.block_2.layers.attn.layers.qkv       |  0.000011\n",
      "layers.block_2.layers.mlp                   |  0.000101\n",
      "layers.block_2.layers.mlp.layers.act        |  0.000029\n",
      "layers.block_2.layers.mlp.layers.drop1      |  0.000029\n",
      "layers.block_2.layers.mlp.layers.drop2      |  0.000101\n",
      "layers.block_2.layers.mlp.layers.fc1        |  0.000015\n",
      "layers.block_2.layers.mlp.layers.fc2        |  0.000101\n",
      "layers.block_2.layers.norm1                 |  0.000008\n",
      "layers.block_2.layers.norm2                 |  0.000009\n",
      "layers.block_3                              |  0.000087\n",
      "layers.block_3.layers.attn                  |  0.000097\n",
      "layers.block_3.layers.attn.layers.attn_drop |  0.000038\n",
      "layers.block_3.layers.attn.layers.proj      |  0.000097\n",
      "layers.block_3.layers.attn.layers.proj_drop |  0.000097\n",
      "layers.block_3.layers.attn.layers.qkv       |  0.000009\n",
      "layers.block_3.layers.mlp                   |  0.000087\n",
      "layers.block_3.layers.mlp.layers.act        |  0.000025\n",
      "layers.block_3.layers.mlp.layers.drop1      |  0.000025\n",
      "layers.block_3.layers.mlp.layers.drop2      |  0.000087\n",
      "layers.block_3.layers.mlp.layers.fc1        |  0.000013\n",
      "layers.block_3.layers.mlp.layers.fc2        |  0.000087\n",
      "layers.block_3.layers.norm1                 |  0.000007\n",
      "layers.block_3.layers.norm2                 |  0.000008\n",
      "layers.block_4                              |  0.000075\n",
      "layers.block_4.layers.attn                  |  0.000083\n",
      "layers.block_4.layers.attn.layers.attn_drop |  0.000033\n",
      "layers.block_4.layers.attn.layers.proj      |  0.000082\n",
      "layers.block_4.layers.attn.layers.proj_drop |  0.000083\n",
      "layers.block_4.layers.attn.layers.qkv       |  0.000008\n",
      "layers.block_4.layers.mlp                   |  0.000075\n",
      "layers.block_4.layers.mlp.layers.act        |  0.000022\n",
      "layers.block_4.layers.mlp.layers.drop1      |  0.000022\n",
      "layers.block_4.layers.mlp.layers.drop2      |  0.000075\n",
      "layers.block_4.layers.mlp.layers.fc1        |  0.000011\n",
      "layers.block_4.layers.mlp.layers.fc2        |  0.000075\n",
      "layers.block_4.layers.norm1                 |  0.000006\n",
      "layers.block_4.layers.norm2                 |  0.000007\n",
      "layers.block_5                              |  0.000060\n",
      "layers.block_5.layers.attn                  |  0.000066\n",
      "layers.block_5.layers.attn.layers.attn_drop |  0.000025\n",
      "layers.block_5.layers.attn.layers.proj      |  0.000066\n",
      "layers.block_5.layers.attn.layers.proj_drop |  0.000066\n",
      "layers.block_5.layers.attn.layers.qkv       |  0.000007\n",
      "layers.block_5.layers.mlp                   |  0.000060\n",
      "layers.block_5.layers.mlp.layers.act        |  0.000018\n",
      "layers.block_5.layers.mlp.layers.drop1      |  0.000018\n",
      "layers.block_5.layers.mlp.layers.drop2      |  0.000060\n",
      "layers.block_5.layers.mlp.layers.fc1        |  0.000009\n",
      "layers.block_5.layers.mlp.layers.fc2        |  0.000061\n",
      "layers.block_5.layers.norm1                 |  0.000006\n",
      "layers.block_5.layers.norm2                 |  0.000006\n",
      "layers.head                                 | ████████████████████████████████████████ 0.011080\n",
      "layers.norm                                 |  0.000014\n",
      "layers.patch_embed                          |  0.000066\n",
      "layers.patch_embed.layers.proj              |  0.000066\n",
      "layers.pos_drop                             | █ 0.000498\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combined file with all neural network models adapted to use ModuleDict\n",
    "and work with NetworkMonitor for tracking activations and gradients.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Utility Functions and Custom Layers\n",
    "###########################################\n",
    "\n",
    "def get_activation(activation_name):\n",
    "    \"\"\"\n",
    "    Returns the activation function based on name\n",
    "    \n",
    "    Parameters:\n",
    "        activation_name (str): Name of the activation function\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: PyTorch activation module\n",
    "    \"\"\"\n",
    "    activations = {\n",
    "        'relu': nn.ReLU(inplace=False),  # Use inplace=False for compatibility with hooks\n",
    "        'leaky_relu': nn.LeakyReLU(0.1, inplace=False),\n",
    "        'tanh': nn.Tanh(),\n",
    "        'sigmoid': nn.Sigmoid(),\n",
    "        'gelu': nn.GELU(),\n",
    "        'elu': nn.ELU(inplace=False),\n",
    "        'selu': nn.SELU(inplace=False),\n",
    "        'none': nn.Identity()\n",
    "    }\n",
    "    \n",
    "    if activation_name.lower() not in activations:\n",
    "        raise ValueError(f\"Activation {activation_name} not supported. \"\n",
    "                         f\"Choose from: {list(activations.keys())}\")\n",
    "    \n",
    "    return activations[activation_name.lower()]\n",
    "\n",
    "def get_normalization(norm_name, num_features):\n",
    "    \"\"\"\n",
    "    Returns the normalization layer based on name\n",
    "    \n",
    "    Parameters:\n",
    "        norm_name (str): Name of the normalization ('batch', 'layer', etc.)\n",
    "        num_features (int): Number of features for the normalization layer\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: PyTorch normalization module or None\n",
    "    \"\"\"\n",
    "    if norm_name is None:\n",
    "        return None\n",
    "        \n",
    "    normalizations = {\n",
    "        'batch': nn.BatchNorm1d(num_features),\n",
    "        'batch2d': nn.BatchNorm2d(num_features),\n",
    "        'layer': nn.LayerNorm(num_features),\n",
    "        'instance': nn.InstanceNorm1d(num_features),\n",
    "        'instance2d': nn.InstanceNorm2d(num_features, affine=True),\n",
    "        'group': nn.GroupNorm(min(32, num_features), num_features),\n",
    "        'none': nn.Identity()\n",
    "    }\n",
    "    \n",
    "    norm_key = str(norm_name).lower()\n",
    "    if norm_key not in normalizations:\n",
    "        raise ValueError(f\"Normalization {norm_name} not supported. \"\n",
    "                         f\"Choose from: {list(normalizations.keys())}\")\n",
    "    \n",
    "    return normalizations[norm_key]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization from the paper:\n",
    "    \"Root Mean Square Layer Normalization\"\n",
    "    https://arxiv.org/abs/1910.07467\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate RMS\n",
    "        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)\n",
    "        x_normalized = x / rms\n",
    "        # Scale\n",
    "        return self.scale * x_normalized\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding for Vision Transformer.\n",
    "    Adapted to work with ModuleDict and NetworkMonitor.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Use ModuleDict for components\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'proj': nn.Conv2d(\n",
    "                in_channels,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['proj'](x)  # (B, E, H', W')\n",
    "        # Rearrange to sequence of patches: [B, C, H, W] -> [B, N, C]\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, N, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Network Monitor\n",
    "###########################################\n",
    "\n",
    "class NetworkMonitor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activations = defaultdict(list)\n",
    "        self.gradients = defaultdict(list)\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name != '':  # Skip the root module\n",
    "                # Forward hook\n",
    "                def make_fwd_hook(name=name):\n",
    "                    def hook(module, input, output):\n",
    "                        # print(f\"Forward hook called for {name}\")\n",
    "                        self.activations[name].append(output.clone().detach().cpu())\n",
    "                    return hook\n",
    "                \n",
    "                # Backward hook - FIXED to avoid nested lists\n",
    "                def make_bwd_hook(name=name):\n",
    "                    def hook(module, grad_input, grad_output):\n",
    "                        # print(f\"Backward hook called for {name}\")\n",
    "                        # Just take the first gradient tensor directly\n",
    "                        if len(grad_output) > 0 and grad_output[0] is not None:\n",
    "                            self.gradients[f\"{name}_grad_output\"].append(grad_output[0].clone().detach().cpu())\n",
    "                        return grad_input\n",
    "                    return hook\n",
    "                \n",
    "                # Register both hooks\n",
    "                h1 = module.register_forward_hook(make_fwd_hook())\n",
    "                h2 = module.register_full_backward_hook(make_bwd_hook())\n",
    "                self.fwd_hooks.append(h1)\n",
    "                self.bwd_hooks.append(h2)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.fwd_hooks + self.bwd_hooks:\n",
    "            h.remove()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "        \n",
    "    def clear_data(self):\n",
    "        self.activations = defaultdict(list)\n",
    "        self.gradients = defaultdict(list)\n",
    "\n",
    "    def get_activations(self):\n",
    "        \"\"\"Get the recorded activations\"\"\"\n",
    "        return {k: v[0] if len(v)==1 else v for k, v in self.activations.items()}\n",
    "    \n",
    "    def get_gradients(self):\n",
    "        \"\"\"Get the recorded gradients\"\"\"\n",
    "        return {k: v[0] if len(v)==1 else v for k, v in self.gradients.items()}\n",
    "    \n",
    "    def show_data_structure(self):\n",
    "        print(\"\\nACTIVATIONS DATA STRUCTURE:\")\n",
    "        for name, acts in self.activations.items():\n",
    "            print(f\"{name}: List with {len(acts)} tensors\")\n",
    "            for i, act in enumerate(acts):\n",
    "                print(f\"  - Item {i}: shape={act.shape}, mean={act.mean().item():.4f}\")\n",
    "        \n",
    "        print(\"\\nGRADIENTS DATA STRUCTURE:\")\n",
    "        for name, grads in self.gradients.items():\n",
    "            print(f\"{name}: List with {len(grads)} tensors\")\n",
    "            for i, g in enumerate(grads):\n",
    "                print(f\"  - Item {i}: shape={g.shape}, mean={g.mean().item():.4f}\")\n",
    "\n",
    "    def print_activation_stats(self, layers=None):\n",
    "        \"\"\"Print statistics about activations\"\"\"\n",
    "        activations = self.get_activations()\n",
    "        print(\"\\nActivation Statistics:\")\n",
    "        \n",
    "        # Filter by layers if specified\n",
    "        if layers:\n",
    "            filtered_activations = {k: v for k, v in activations.items() if k in layers}\n",
    "        else:\n",
    "            filtered_activations = activations\n",
    "            \n",
    "        for name, act in filtered_activations.items():\n",
    "            if isinstance(act, torch.Tensor):\n",
    "                print(f\"{name}: shape={act.shape}, mean={act.mean().item():.6f}, \"\n",
    "                      f\"std={act.std().item():.6f}, min={act.min().item():.6f}, \"\n",
    "                      f\"max={act.max().item():.6f}\")\n",
    "            else:\n",
    "                print(f\"{name}: {type(act)}\")\n",
    "    \n",
    "    def print_gradient_stats(self, layers=None, grad_type=\"output\"):\n",
    "        \"\"\"Print statistics about gradients\"\"\"\n",
    "        gradients = self.get_gradients()\n",
    "        print(f\"\\nGradient Statistics ({grad_type}):\")\n",
    "        \n",
    "        # Filter by layers and gradient type\n",
    "        keys = [k for k in gradients.keys() if f\"_grad_{grad_type}\" in k]\n",
    "        if layers:\n",
    "            keys = [k for k in keys if any(layer in k for layer in layers)]\n",
    "            \n",
    "        for key in keys:\n",
    "            grads = gradients[key]\n",
    "            if not isinstance(grads, list):\n",
    "                grads = [grads]\n",
    "                \n",
    "            for i, g in enumerate(grads):\n",
    "                if g is not None:\n",
    "                    print(f\"{key}[{i}]: shape={g.shape}, mean={g.mean().item():.6f}, \"\n",
    "                          f\"std={g.std().item():.6f}, max={g.abs().max().item():.6f}\")\n",
    "                else:\n",
    "                    print(f\"{key}[{i}]: None\")\n",
    "\n",
    "    def visualize_activation_flow(self, layer_names=None):\n",
    "        \"\"\"Visualize activation magnitudes across layers\"\"\"\n",
    "        activations = self.get_activations()\n",
    "        \n",
    "        # Filter by layer names if specified\n",
    "        if layer_names:\n",
    "            filtered_acts = {k: v for k, v in activations.items() if k in layer_names}\n",
    "        else:\n",
    "            # Filter out non-tensor activations and exclude input\n",
    "            filtered_acts = {k: v for k, v in activations.items() \n",
    "                            if isinstance(v, torch.Tensor) and k != 'input'}\n",
    "        \n",
    "        # Calculate mean activation magnitudes\n",
    "        magnitudes = {name: float(act.abs().mean().item()) \n",
    "                     for name, act in filtered_acts.items()}\n",
    "        \n",
    "        # Sort by layer name (assuming sequential naming like conv1, conv2, etc.)\n",
    "        sorted_items = sorted(magnitudes.items())\n",
    "        \n",
    "        # Print a simple text-based visualization\n",
    "        print(\"\\nActivation Magnitude Flow:\")\n",
    "        max_name_len = max(len(name) for name in magnitudes.keys())\n",
    "        max_mag = max(magnitudes.values())\n",
    "        \n",
    "        for name, mag in sorted_items:\n",
    "            bar_len = int((mag / max_mag) * 40)\n",
    "            print(f\"{name.ljust(max_name_len)} | {'█' * bar_len} {mag:.6f}\")\n",
    "    \n",
    "    def visualize_gradient_flow(self, filter_type=\"output\"):\n",
    "        \"\"\"Visualize gradient magnitudes across layers\"\"\"\n",
    "        gradients = self.get_gradients()\n",
    "        \n",
    "        # Filter gradient keys by type (input or output)\n",
    "        grad_keys = [k for k in gradients.keys() if f\"_grad_{filter_type}\" in k]\n",
    "        \n",
    "        # Extract mean gradient magnitudes\n",
    "        magnitudes = {}\n",
    "        for key in grad_keys:\n",
    "            grads = gradients[key]\n",
    "            if not isinstance(grads, list):\n",
    "                grads = [grads]\n",
    "            \n",
    "            # Get the first non-None gradient\n",
    "            for g in grads:\n",
    "                if g is not None:\n",
    "                    # Extract the module name from the gradient key\n",
    "                    name = key.split('_grad_')[0]\n",
    "                    magnitudes[name] = float(g.abs().mean().item())\n",
    "                    break\n",
    "        \n",
    "        # Sort and visualize\n",
    "        sorted_items = sorted(magnitudes.items())\n",
    "        \n",
    "        print(f\"\\nGradient Magnitude Flow ({filter_type}):\")\n",
    "        max_name_len = max(len(name) for name in magnitudes.keys())\n",
    "        max_mag = max(magnitudes.values()) if magnitudes else 1.0\n",
    "        \n",
    "        for name, mag in sorted_items:\n",
    "            bar_len = int((mag / max_mag) * 40)\n",
    "            print(f\"{name.ljust(max_name_len)} | {'█' * bar_len} {mag:.6f}\")\n",
    "\n",
    "###########################################\n",
    "# Usage Example\n",
    "###########################################\n",
    "\n",
    "def test_model_with_monitor(model_name='mlp'):\n",
    "    \"\"\"\n",
    "    Create and test a model with NetworkMonitor.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): One of 'mlp', 'cnn', 'resnet', 'vit'\n",
    "    \"\"\"\n",
    "    # Create model based on model_name\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP(\n",
    "            input_size=784,  # MNIST flattened size\n",
    "            hidden_sizes=[512, 256],\n",
    "            output_size=10,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2,\n",
    "            normalization='batch'\n",
    "        )\n",
    "        input_shape = (16, 1, 28, 28)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'cnn':\n",
    "        model = CNN(\n",
    "            in_channels=3,\n",
    "            conv_channels=[64, 128, 256],\n",
    "            kernel_sizes=[3, 3, 3],\n",
    "            strides=[1, 1, 1],\n",
    "            paddings=[1, 1, 1],\n",
    "            fc_hidden_units=[512],\n",
    "            num_classes=10,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'resnet':\n",
    "        model = ResNet(\n",
    "            layers=[2, 2, 2, 2],  # ResNet18\n",
    "            num_classes=10,\n",
    "            in_channels=3,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'vit':\n",
    "        model = VisionTransformer(\n",
    "            img_size=32,\n",
    "            patch_size=4,\n",
    "            in_channels=3,\n",
    "            num_classes=10,\n",
    "            embed_dim=192,\n",
    "            depth=6,\n",
    "            n_heads=8\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    # Print model structure\n",
    "    print(f\"\\n=== {model_name.upper()} Model Structure ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(name) > 0:\n",
    "            print(f\"{name}: {module.__class__.__name__}\")\n",
    "    \n",
    "    # Create NetworkMonitor and register hooks\n",
    "    monitor = NetworkMonitor(model)\n",
    "    monitor.register_hooks()\n",
    "    \n",
    "    # Generate dummy data\n",
    "    x = torch.randn(*input_shape)\n",
    "    target = torch.randint(0, 10, (input_shape[0],))\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n=== {model_name.upper()} Activation Statistics ===\")\n",
    "    important_layers = []\n",
    "    # Get a few important layers based on the model type\n",
    "    if model_name == 'mlp':\n",
    "        for i in range(len(model.hidden_sizes)):\n",
    "            important_layers.append(f'linear_{i}')\n",
    "    elif model_name == 'cnn':\n",
    "        for i in range(3):\n",
    "            important_layers.append(f'conv_{i}')\n",
    "    elif model_name == 'resnet':\n",
    "        important_layers = ['layer1_block0', 'layer2_block0', 'layer3_block0', 'layer4_block0']\n",
    "    elif model_name == 'vit':\n",
    "        important_layers = ['patch_embed', 'block_0', 'block_2', 'block_5']\n",
    "    \n",
    "    # Print statistics for selected layers\n",
    "    monitor.print_activation_stats(layers=important_layers)\n",
    "    monitor.print_gradient_stats(layers=important_layers)\n",
    "    \n",
    "    # Visualize activation and gradient flow\n",
    "    monitor.visualize_activation_flow()\n",
    "    monitor.visualize_gradient_flow()\n",
    "    \n",
    "    # Clean up\n",
    "    monitor.remove_hooks()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "#  MLP\n",
    "###########################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size=784, \n",
    "                 hidden_sizes=[512, 256, 128], \n",
    "                 output_size=10, \n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 normalization=None,\n",
    "                 norm_after_activation=False,\n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "        Fully  MLP that supports various activations and normalizations.\n",
    "        \n",
    "        Parameters:\n",
    "            input_size (int): Dimensionality of input features\n",
    "            hidden_sizes (list): List of hidden layer dimensions\n",
    "            output_size (int): Number of output classes\n",
    "            activation (str): Activation function to use ('relu', 'tanh', 'sigmoid', etc.)\n",
    "            dropout_p (float): Dropout probability (0 to disable)\n",
    "            normalization (str): Normalization to use ('batch', 'layer', None)\n",
    "            norm_after_activation (bool): If True, apply normalization after activation\n",
    "            bias (bool): Whether to include bias terms in linear layers\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        \n",
    "        # Build network using ModuleDict\n",
    "        self.layers = nn.ModuleDict()\n",
    "        in_features = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # Linear layer\n",
    "            self.layers[f'linear_{i}'] = nn.Linear(in_features, hidden_size, bias=bias)\n",
    "            \n",
    "            # Activation\n",
    "            self.layers[f'activation_{i}'] = get_activation(activation)\n",
    "            \n",
    "            # Normalization\n",
    "            if normalization:\n",
    "                self.layers[f'norm_{i}'] = get_normalization(normalization, hidden_size)\n",
    "            \n",
    "            # Dropout\n",
    "            if dropout_p > 0:\n",
    "                self.layers[f'dropout_{i}'] = nn.Dropout(dropout_p)\n",
    "            \n",
    "            in_features = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers['output'] = nn.Linear(in_features, output_size, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass without activation storage.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input data with shape [batch_size, input_size]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits\n",
    "        \"\"\"\n",
    "        # Flatten input if needed\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply hidden layers\n",
    "        for i in range(len(self.hidden_sizes)):\n",
    "            # Linear\n",
    "            x = self.layers[f'linear_{i}'](x)\n",
    "            \n",
    "            # Apply norm before activation if configured that way\n",
    "            if not self.norm_after_activation and f'norm_{i}' in self.layers:\n",
    "                x = self.layers[f'norm_{i}'](x)\n",
    "                \n",
    "            # Activation\n",
    "            x = self.layers[f'activation_{i}'](x)\n",
    "            \n",
    "            # Apply norm after activation if configured that way\n",
    "            if self.norm_after_activation and f'norm_{i}' in self.layers:\n",
    "                x = self.layers[f'norm_{i}'](x)\n",
    "                \n",
    "            # Dropout (if present)\n",
    "            if f'dropout_{i}' in self.layers:\n",
    "                x = self.layers[f'dropout_{i}'](x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.layers['output'](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "#  CNN\n",
    "###########################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 conv_channels=[64, 128, 256], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[1, 1, 1],\n",
    "                 paddings=[1, 1, 1],\n",
    "                 fc_hidden_units=[512],\n",
    "                 num_classes=10, \n",
    "                 input_size=32,\n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 pool_type='max',\n",
    "                 pool_size=2,\n",
    "                 use_batchnorm=True,\n",
    "                 norm_after_activation=False):\n",
    "        \"\"\"\n",
    "         CNN with  layers, activations, and normalizations.\n",
    "        \n",
    "        Parameters:\n",
    "            in_channels (int): Number of input channels (3 for RGB images)\n",
    "            conv_channels (list): List of convolutional layer output channels\n",
    "            kernel_sizes (list): List of kernel sizes for each conv layer\n",
    "            strides (list): List of stride values for each conv layer\n",
    "            paddings (list): List of padding values for each conv layer\n",
    "            fc_hidden_units (list): List of hidden units for fully connected layers\n",
    "            num_classes (int): Number of output classes\n",
    "            input_size (int): Height/width of the input images (assumed square)\n",
    "            activation (str): Activation function to use ('relu', 'tanh', etc.)\n",
    "            dropout_p (float): Dropout probability (0 to disable)\n",
    "            pool_type (str): Type of pooling ('max', 'avg', or None)\n",
    "            pool_size (int): Size of the pooling window\n",
    "            use_batchnorm (bool): Whether to use batch normalization\n",
    "            norm_after_activation (bool): Apply normalization after activation\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Check if input lists are of the same length\n",
    "        assert len(conv_channels) == len(kernel_sizes) == len(strides) == len(paddings), \\\n",
    "            \"Convolutional parameters (channels, kernels, strides, paddings) must have the same length\"\n",
    "        \n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        \n",
    "        # Create a ModuleDict to store all layers\n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        channels = in_channels\n",
    "        for i, (out_channels, kernel_size, stride, padding) in enumerate(\n",
    "                zip(conv_channels, kernel_sizes, strides, paddings)):\n",
    "            # Conv layer\n",
    "            self.layers[f'conv_{i}'] = nn.Conv2d(channels, out_channels, kernel_size, stride, padding)\n",
    "            \n",
    "            # Normalization\n",
    "            if use_batchnorm:\n",
    "                self.layers[f'norm_{i}'] = nn.BatchNorm2d(out_channels)\n",
    "            \n",
    "            # Activation\n",
    "            self.layers[f'act_{i}'] = get_activation(activation)\n",
    "            \n",
    "            # Pooling\n",
    "            if pool_type == 'max':\n",
    "                self.layers[f'pool_{i}'] = nn.MaxPool2d(pool_size, pool_size)\n",
    "            elif pool_type == 'avg':\n",
    "                self.layers[f'pool_{i}'] = nn.AvgPool2d(pool_size, pool_size)\n",
    "            \n",
    "            channels = out_channels\n",
    "        \n",
    "        # Calculate the size after all pooling operations\n",
    "        num_pools = len(conv_channels) if pool_type in ['max', 'avg'] else 0\n",
    "        final_size = input_size // (pool_size ** num_pools)\n",
    "        self.flattened_size = conv_channels[-1] * final_size * final_size\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.layers['flatten'] = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_input_size = self.flattened_size\n",
    "        for i, hidden_units in enumerate(fc_hidden_units):\n",
    "            self.layers[f'fc_{i}'] = nn.Linear(fc_input_size, hidden_units)\n",
    "            self.layers[f'fc_act_{i}'] = get_activation(activation)\n",
    "            \n",
    "            if dropout_p > 0:\n",
    "                self.layers[f'fc_dropout_{i}'] = nn.Dropout(dropout_p)\n",
    "                \n",
    "            fc_input_size = hidden_units\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers['output'] = nn.Linear(fc_input_size, num_classes)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_conv_layers = len(conv_channels)\n",
    "        self.num_fc_layers = len(fc_hidden_units)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.has_pool = pool_type in ['max', 'avg']\n",
    "        self.dropout_p = dropout_p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass without activation storage.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input data [batch_size, in_channels, height, width]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits\n",
    "        \"\"\"\n",
    "        # Process conv layers\n",
    "        for i in range(self.num_conv_layers):\n",
    "            # Conv\n",
    "            x = self.layers[f'conv_{i}'](x)\n",
    "            \n",
    "            # Normalization (before activation)\n",
    "            if self.use_batchnorm and not self.norm_after_activation:\n",
    "                if f'norm_{i}' in self.layers:\n",
    "                    x = self.layers[f'norm_{i}'](x)\n",
    "            \n",
    "            # Activation\n",
    "            x = self.layers[f'act_{i}'](x)\n",
    "            \n",
    "            # Normalization (after activation)\n",
    "            if self.use_batchnorm and self.norm_after_activation:\n",
    "                if f'norm_{i}' in self.layers:\n",
    "                    x = self.layers[f'norm_{i}'](x)\n",
    "            \n",
    "            # Pooling\n",
    "            if self.has_pool and f'pool_{i}' in self.layers:\n",
    "                x = self.layers[f'pool_{i}'](x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.layers['flatten'](x)\n",
    "        \n",
    "        # FC layers\n",
    "        for i in range(self.num_fc_layers):\n",
    "            x = self.layers[f'fc_{i}'](x)\n",
    "            x = self.layers[f'fc_act_{i}'](x)\n",
    "            \n",
    "            if self.dropout_p > 0 and f'fc_dropout_{i}' in self.layers:\n",
    "                x = self.layers[f'fc_dropout_{i}'](x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.layers['output'](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# ResNet\n",
    "###########################################\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic ResNet block with  activation and normalization.\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1, activation='relu', \n",
    "                 use_batchnorm=True, norm_after_activation=False, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        \n",
    "        # Use ModuleDict for all components\n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        # First convolution\n",
    "        self.layers['conv1'] = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, \n",
    "                                        padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        # Normalization for first conv\n",
    "        if use_batchnorm:\n",
    "            self.layers['bn1'] = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Activation\n",
    "        self.layers['activation'] = get_activation(activation)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.layers['conv2'] = nn.Conv2d(planes, planes, kernel_size=3, stride=1, \n",
    "                                        padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        # Normalization for second conv\n",
    "        if use_batchnorm:\n",
    "            self.layers['bn2'] = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Downsample if needed (for shortcut connection)\n",
    "        if downsample is not None:\n",
    "            self.layers['downsample'] = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # Apply conv1\n",
    "        out = self.layers['conv1'](x)\n",
    "        \n",
    "        # Apply norm1 if exists (before activation)\n",
    "        if 'bn1' in self.layers and not self.norm_after_activation:\n",
    "            out = self.layers['bn1'](out)\n",
    "        \n",
    "        # Apply activation\n",
    "        out = self.layers['activation'](out)\n",
    "        \n",
    "        # Apply norm1 if exists (after activation)\n",
    "        if 'bn1' in self.layers and self.norm_after_activation:\n",
    "            out = self.layers['bn1'](out)\n",
    "            \n",
    "        # Apply conv2\n",
    "        out = self.layers['conv2'](out)\n",
    "        \n",
    "        # Apply norm2 if exists (before final addition)\n",
    "        if 'bn2' in self.layers and not self.norm_after_activation:\n",
    "            out = self.layers['bn2'](out)\n",
    "            \n",
    "        # Handle shortcut connection\n",
    "        if 'downsample' in self.layers:\n",
    "            identity = self.layers['downsample'](x)\n",
    "            \n",
    "        # Add identity - FIXED: don't use in-place operation (+=)\n",
    "        # Instead use out = out + identity\n",
    "        out = out + identity  # This line was changed from out += identity\n",
    "        \n",
    "        # Final activation\n",
    "        out = self.layers['activation'](out)\n",
    "        \n",
    "        # Apply norm2 if exists (after final activation)\n",
    "        if 'bn2' in self.layers and self.norm_after_activation:\n",
    "            out = self.layers['bn2'](out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "     ResNet architecture for continual learning experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 block=BasicBlock,\n",
    "                 layers=[2, 2, 2, 2],  # ResNet18 by default\n",
    "                 num_classes=10,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 use_batchnorm=True,\n",
    "                 norm_after_activation=False):\n",
    "        \"\"\"\n",
    "        Initialize the ResNet.\n",
    "        \n",
    "        Parameters:\n",
    "            block (nn.Module): The block type to use (BasicBlock)\n",
    "            layers (list): Number of blocks in each layer\n",
    "            num_classes (int): Number of output classes\n",
    "            in_channels (int): Number of input channels (3 for RGB images)\n",
    "            base_channels (int): Base number of channels (first layer)\n",
    "            activation (str): Activation function to use\n",
    "            dropout_p (float): Dropout probability before final layer\n",
    "            use_batchnorm (bool): Whether to use batch normalization\n",
    "            norm_after_activation (bool): Apply normalization after activation\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        self.in_planes = base_channels\n",
    "        \n",
    "        # Use ModuleDict for all layers\n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.layers['conv1'] = nn.Conv2d(in_channels, base_channels, kernel_size=3, \n",
    "                                        stride=1, padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        # Batch norm after first conv\n",
    "        if use_batchnorm:\n",
    "            self.layers['bn1'] = nn.BatchNorm2d(base_channels)\n",
    "        \n",
    "        # Activation\n",
    "        self.layers['activation'] = get_activation(activation)\n",
    "        \n",
    "        # ResNet layers\n",
    "        self._make_layer(block, base_channels, layers[0], stride=1, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm, \n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer1')\n",
    "        self._make_layer(block, base_channels*2, layers[1], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm, \n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer2')\n",
    "        self._make_layer(block, base_channels*4, layers[2], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm,\n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer3')\n",
    "        self._make_layer(block, base_channels*8, layers[3], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm,\n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer4')\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.layers['avgpool'] = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Flatten operation\n",
    "        self.layers['flatten'] = nn.Flatten()\n",
    "        \n",
    "        # Dropout if needed\n",
    "        if dropout_p > 0:\n",
    "            self.layers['dropout'] = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.layers['fc'] = nn.Linear(base_channels*8*block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        # Store layer information\n",
    "        self.num_layers = len(layers)\n",
    "        self.blocks_per_layer = layers\n",
    "                \n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1, activation='relu', \n",
    "                    use_batchnorm=True, norm_after_activation=False, layer_name='layer'):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample_layers = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes * block.expansion, \n",
    "                         kernel_size=1, stride=stride, bias=not use_batchnorm)\n",
    "            )\n",
    "            \n",
    "            if use_batchnorm:\n",
    "                downsample_layers.add_module('1', nn.BatchNorm2d(planes * block.expansion))\n",
    "                \n",
    "            downsample = downsample_layers\n",
    "        \n",
    "        # Add first block with stride and downsample\n",
    "        self.layers[f'{layer_name}_block0'] = block(\n",
    "            self.in_planes, planes, stride, activation, \n",
    "            use_batchnorm, norm_after_activation, downsample\n",
    "        )\n",
    "        \n",
    "        self.in_planes = planes * block.expansion\n",
    "        \n",
    "        # Add remaining blocks\n",
    "        for i in range(1, num_blocks):\n",
    "            self.layers[f'{layer_name}_block{i}'] = block(\n",
    "                self.in_planes, planes, 1, activation, \n",
    "                use_batchnorm, norm_after_activation\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass without activation storage.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input data\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits\n",
    "        \"\"\"\n",
    "        # Initial conv\n",
    "        x = self.layers['conv1'](x)\n",
    "        \n",
    "        # Apply normalization (before activation)\n",
    "        if self.use_batchnorm and not self.norm_after_activation:\n",
    "            if 'bn1' in self.layers:\n",
    "                x = self.layers['bn1'](x)\n",
    "                \n",
    "        # Activation\n",
    "        x = self.layers['activation'](x)\n",
    "        \n",
    "        # Apply normalization (after activation)\n",
    "        if self.use_batchnorm and self.norm_after_activation:\n",
    "            if 'bn1' in self.layers:\n",
    "                x = self.layers['bn1'](x)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        for layer_idx in range(1, self.num_layers + 1):\n",
    "            for block_idx in range(self.blocks_per_layer[layer_idx - 1]):\n",
    "                block_name = f'layer{layer_idx}_block{block_idx}'\n",
    "                x = self.layers[block_name](x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.layers['avgpool'](x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.layers['flatten'](x)\n",
    "        \n",
    "        # Dropout if specified\n",
    "        if 'dropout' in self.layers:\n",
    "            x = self.layers['dropout'](x)\n",
    "            \n",
    "        # Final classifier\n",
    "        x = self.layers['fc'](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# ViT Components\n",
    "###########################################\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        head_dim = dim // n_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # Use ModuleDict\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'qkv': nn.Linear(dim, dim * 3, bias=qkv_bias),\n",
    "            'attn_drop': nn.Dropout(attn_drop),\n",
    "            'proj': nn.Linear(dim, dim),\n",
    "            'proj_drop': nn.Dropout(proj_drop)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.layers['qkv'](x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.layers['attn_drop'](attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.layers['proj'](x)\n",
    "        x = self.layers['proj_drop'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerMLP(nn.Module):\n",
    "    \"\"\"MLP module with  activation.\"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, \n",
    "                 activation='gelu', drop=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use ModuleDict\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(in_features, hidden_features),\n",
    "            'act': get_activation(activation),\n",
    "            'drop1': nn.Dropout(drop) if drop > 0 else nn.Identity(),\n",
    "            'fc2': nn.Linear(hidden_features, out_features),\n",
    "            'drop2': nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['act'](x)\n",
    "        x = self.layers['drop1'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        x = self.layers['drop2'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with  components.\"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., \n",
    "                 attn_drop=0., activation='gelu', normalization='layer'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use ModuleDict\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'norm1': get_normalization(normalization, dim),\n",
    "            'attn': Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, \n",
    "                             attn_drop=attn_drop, proj_drop=drop),\n",
    "            'norm2': get_normalization(normalization, dim),\n",
    "            'mlp': TransformerMLP(dim, int(dim * mlp_ratio), dim, \n",
    "                       activation=activation, drop=drop)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention branch\n",
    "        norm_x = self.layers['norm1'](x)\n",
    "        attn_out = self.layers['attn'](norm_x)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # MLP branch\n",
    "        norm_x = self.layers['norm2'](x)\n",
    "        mlp_out = self.layers['mlp'](norm_x)\n",
    "        x = x + mlp_out\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) model with  architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size=32, \n",
    "                 patch_size=4, \n",
    "                 in_channels=3, \n",
    "                 num_classes=10, \n",
    "                 embed_dim=192,\n",
    "                 depth=12, \n",
    "                 n_heads=8, \n",
    "                 mlp_ratio=4., \n",
    "                 qkv_bias=True, \n",
    "                 drop_rate=0.1,\n",
    "                 attn_drop_rate=0.0,\n",
    "                 activation='gelu',\n",
    "                 normalization='layer'):\n",
    "        \"\"\"\n",
    "        Initialize Vision Transformer.\n",
    "        \n",
    "        Parameters:\n",
    "            img_size (int): Input image size\n",
    "            patch_size (int): Patch size for splitting image\n",
    "            in_channels (int): Number of image channels\n",
    "            num_classes (int): Number of output classes\n",
    "            embed_dim (int): Embedding dimension\n",
    "            depth (int): Number of transformer blocks\n",
    "            n_heads (int): Number of attention heads\n",
    "            mlp_ratio (float): Ratio for MLP hidden dimension\n",
    "            qkv_bias (bool): Whether to use bias in QKV projection\n",
    "            drop_rate (float): Dropout rate\n",
    "            attn_drop_rate (float): Attention dropout rate\n",
    "            activation (str): Activation function to use\n",
    "            normalization (str): Normalization method to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use ModuleDict for all components\n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.layers['patch_embed'] = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.layers['patch_embed'].n_patches\n",
    "\n",
    "        # Class token and position embeddings (these aren't in ModuleDict as they're parameters)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        \n",
    "        # Position dropout\n",
    "        self.layers['pos_drop'] = nn.Dropout(drop_rate)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for i in range(depth):\n",
    "            self.layers[f'block_{i}'] = TransformerBlock(\n",
    "                embed_dim, n_heads, mlp_ratio, qkv_bias, \n",
    "                drop_rate, attn_drop_rate, activation, normalization\n",
    "            )\n",
    "\n",
    "        # Final normalization\n",
    "        self.layers['norm'] = get_normalization(normalization, embed_dim)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.layers['head'] = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "        # Save configuration for forward pass\n",
    "        self.depth = depth\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize position embedding and class token\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        # Initialize other weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass without activation storage.\n",
    "        \n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input images [batch_size, channels, height, width]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits\n",
    "        \"\"\"\n",
    "        # Patch embedding\n",
    "        x = self.layers['patch_embed'](x)\n",
    "        \n",
    "        # Add class token\n",
    "        B = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.layers['pos_drop'](x)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for i in range(self.depth):\n",
    "            x = self.layers[f'block_{i}'](x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.layers['norm'](x)\n",
    "        \n",
    "        # Extract class token and classify\n",
    "        x = x[:, 0]  # Use only the cls token for classification\n",
    "        x = self.layers['head'](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def example_test():\n",
    "    # Setup model and monitor\n",
    "    model = SimpleModel()\n",
    "    monitor = NetworkMonitor(model)\n",
    "    monitor.register_hooks()\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"=== BATCH 1 ===\")\n",
    "    # Create batch 1\n",
    "    x1 = torch.randn(2, 3, 8, 8)\n",
    "    y1 = torch.tensor([0, 3])\n",
    "    \n",
    "    # Forward and backward pass for batch 1\n",
    "    output1 = model(x1)\n",
    "    loss1 = criterion(output1, y1)\n",
    "    loss1.backward()\n",
    "    \n",
    "    print(\"\\nAfter batch 1, let's examine the data structure:\")\n",
    "    monitor.show_data_structure()\n",
    "    \n",
    "    print(\"\\n=== BATCH 2 ===\")\n",
    "    # Create batch 2 (with different data)\n",
    "    x2 = torch.randn(3, 3, 8, 8)\n",
    "    y2 = torch.tensor([2, 4, 5])\n",
    "    \n",
    "    # Forward and backward pass for batch 2\n",
    "    output2 = model(x2)\n",
    "    loss2 = criterion(output2, y2)\n",
    "    loss2.backward()\n",
    "    \n",
    "    print(\"\\nAfter batch 2, let's examine the data structure:\")\n",
    "    monitor.show_data_structure()\n",
    "    \n",
    "    # Clean up\n",
    "    monitor.remove_hooks()\n",
    "\n",
    "\"\"\"\n",
    "Example of how to use the adapted models with NetworkMonitor.\n",
    "\"\"\"\n",
    "\n",
    "def test_model_with_monitor(model_name='mlp'):\n",
    "    \"\"\"\n",
    "    Create and test a model with NetworkMonitor.\n",
    "    \n",
    "    Parameters:\n",
    "        model_name (str): One of 'mlp', 'cnn', 'resnet', 'vit'\n",
    "    \"\"\"\n",
    "    # Create model based on model_name\n",
    "    if model_name == 'mlp':\n",
    "        model = MLP(\n",
    "            input_size=784,  # MNIST flattened size\n",
    "            hidden_sizes=[512, 256],\n",
    "            output_size=10,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2,\n",
    "            normalization='batch'\n",
    "        )\n",
    "        input_shape = (16, 1, 28, 28)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'cnn':\n",
    "        model = CNN(\n",
    "            in_channels=3,\n",
    "            conv_channels=[64, 128, 256],\n",
    "            kernel_sizes=[3, 3, 3],\n",
    "            strides=[1, 1, 1],\n",
    "            paddings=[1, 1, 1],\n",
    "            fc_hidden_units=[512],\n",
    "            num_classes=10,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'resnet':\n",
    "        model = ResNet(\n",
    "            layers=[2, 2, 2, 2],  # ResNet18\n",
    "            num_classes=10,\n",
    "            in_channels=3,\n",
    "            activation='relu',\n",
    "            dropout_p=0.2\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    elif model_name == 'vit':\n",
    "        model = VisionTransformer(\n",
    "            img_size=32,\n",
    "            patch_size=4,\n",
    "            in_channels=3,\n",
    "            num_classes=10,\n",
    "            embed_dim=192,\n",
    "            depth=6,\n",
    "            n_heads=8\n",
    "        )\n",
    "        input_shape = (16, 3, 32, 32)  # batch_size, channels, height, width\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    # Print model structure\n",
    "    print(f\"\\n=== {model_name.upper()} Model Structure ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(name) > 0:\n",
    "            print(f\"{name}: {module.__class__.__name__}\")\n",
    "    \n",
    "    # Create NetworkMonitor and register hooks\n",
    "    monitor = NetworkMonitor(model)\n",
    "    monitor.register_hooks()\n",
    "    \n",
    "    # Generate dummy data\n",
    "    x = torch.randn(*input_shape)\n",
    "    target = torch.randint(0, 10, (input_shape[0],))\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n=== {model_name.upper()} Activation Statistics ===\")\n",
    "    important_layers = []\n",
    "    # Get a few important layers based on the model type\n",
    "    if model_name == 'mlp':\n",
    "        for i in range(len(model.layers) // 3):\n",
    "            important_layers.append(f'linear_{i}')\n",
    "    elif model_name == 'cnn':\n",
    "        for i in range(3):\n",
    "            important_layers.append(f'conv_{i}')\n",
    "    elif model_name == 'resnet':\n",
    "        important_layers = ['layer1_block0', 'layer2_block0', 'layer3_block0', 'layer4_block0']\n",
    "    elif model_name == 'vit':\n",
    "        important_layers = ['patch_embed', 'block_0', 'block_2', 'block_5']\n",
    "    \n",
    "    # Print statistics for selected layers\n",
    "    monitor.show_data_structure()\n",
    "    monitor.print_activation_stats(layers=important_layers)\n",
    "    monitor.print_gradient_stats(layers=important_layers)\n",
    "    \n",
    "    # Visualize activation and gradient flow\n",
    "    monitor.visualize_activation_flow()\n",
    "    monitor.visualize_gradient_flow()\n",
    "    \n",
    "    # Clean up\n",
    "    monitor.remove_hooks()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test each model\n",
    "    for model_name in ['mlp', 'cnn', 'resnet', 'vit']:\n",
    "        test_model_with_monitor(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612bc0c-f57f-426d-92a8-1df60113e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8572d-2fce-46d0-88de-d84e58fbcd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CIFAR10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating model...\n",
      "\n",
      "Model Architecture:\n",
      "layers: ModuleDict\n",
      "layers.conv_0: Conv2d\n",
      "layers.norm_0: BatchNorm2d\n",
      "layers.act_0: ReLU\n",
      "layers.pool_0: MaxPool2d\n",
      "layers.conv_1: Conv2d\n",
      "layers.norm_1: BatchNorm2d\n",
      "layers.act_1: ReLU\n",
      "layers.pool_1: MaxPool2d\n",
      "layers.conv_2: Conv2d\n",
      "layers.norm_2: BatchNorm2d\n",
      "layers.act_2: ReLU\n",
      "layers.pool_2: MaxPool2d\n",
      "layers.flatten: Flatten\n",
      "layers.fc_0: Linear\n",
      "layers.fc_act_0: ReLU\n",
      "layers.output: Linear\n",
      "\n",
      "Starting training...\n",
      "Measuring baseline metrics before training...\n",
      "\n",
      "=== Training Batch Metrics (before training) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   11.315\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   11.312\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   27.140\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   25.894\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   61.011\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   60.738\n",
      "layers.act_1   : Dead:    0.023, Dup:    0.000, EffRank:   64.058\n",
      "layers.pool_1  : Dead:    0.008, Dup:    0.000, EffRank:   57.884\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:  121.874\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:  121.732\n",
      "layers.act_2   : Dead:    0.125, Dup:    0.000, EffRank:  116.576\n",
      "layers.pool_2  : Dead:    0.070, Dup:    0.000, EffRank:  107.150\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.000, EffRank:   50.441\n",
      "layers.fc_act_0: Dead:    0.248, Dup:    0.014, EffRank:   49.505\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    5.719\n",
      "\n",
      "=== Validation Batch Metrics (before training) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   11.256\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   11.276\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   27.030\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   26.375\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   59.979\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   59.275\n",
      "layers.act_1   : Dead:    0.023, Dup:    0.000, EffRank:   62.053\n",
      "layers.pool_1  : Dead:    0.016, Dup:    0.000, EffRank:   56.659\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:  120.006\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:  120.155\n",
      "layers.act_2   : Dead:    0.113, Dup:    0.000, EffRank:  115.729\n",
      "layers.pool_2  : Dead:    0.066, Dup:    0.000, EffRank:  106.245\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.000, EffRank:   50.944\n",
      "layers.fc_act_0: Dead:    0.248, Dup:    0.020, EffRank:   49.506\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    5.471\n",
      "\n",
      "Step 100: Measuring metrics...\n",
      "\n",
      "=== Training Batch Metrics (step 100) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   11.636\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.165\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   33.839\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   31.426\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   41.142\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   53.682\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   70.473\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   62.367\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   54.748\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   66.412\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  104.548\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:   91.437\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.783, EffRank:    6.711\n",
      "layers.fc_act_0: Dead:    0.811, Dup:    0.004, EffRank:   21.536\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.651\n",
      "\n",
      "=== Validation Batch Metrics (step 100) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   12.134\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   13.728\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   33.165\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   31.140\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   40.021\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   51.316\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   68.120\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   59.857\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   48.674\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   60.629\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:   94.169\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:   83.655\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.789, EffRank:    6.455\n",
      "layers.fc_act_0: Dead:    0.811, Dup:    0.008, EffRank:   20.064\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.133\n",
      "\n",
      "Step 200: Measuring metrics...\n",
      "\n",
      "=== Training Batch Metrics (step 200) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   12.219\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.399\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   35.019\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   31.880\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   37.850\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   45.522\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   77.778\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   68.679\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   53.156\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   64.340\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  126.610\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  112.868\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.738, EffRank:   11.325\n",
      "layers.fc_act_0: Dead:    0.801, Dup:    0.000, EffRank:   28.178\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.752\n",
      "\n",
      "=== Validation Batch Metrics (step 200) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   12.512\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.461\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   34.212\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   31.529\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   37.131\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   43.966\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   77.150\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   69.564\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   51.831\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   63.228\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  119.506\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  106.795\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.762, EffRank:   10.047\n",
      "layers.fc_act_0: Dead:    0.801, Dup:    0.008, EffRank:   25.192\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.349\n",
      "\n",
      "Step 300: Measuring metrics...\n",
      "\n",
      "=== Training Batch Metrics (step 300) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   13.327\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.584\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   35.019\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   32.191\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   38.038\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   44.215\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   77.506\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   71.265\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   53.324\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   62.931\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  136.608\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  122.536\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.729, EffRank:   14.050\n",
      "layers.fc_act_0: Dead:    0.799, Dup:    0.000, EffRank:   31.235\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    7.168\n",
      "\n",
      "=== Validation Batch Metrics (step 300) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   13.079\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.871\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   34.742\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   31.901\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   37.133\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   42.974\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   78.899\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   70.997\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.000, EffRank:   51.458\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   61.513\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  129.125\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  115.322\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.752, EffRank:   12.464\n",
      "layers.fc_act_0: Dead:    0.793, Dup:    0.000, EffRank:   27.546\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.735\n",
      "\n",
      "Epoch 1/5:\n",
      "Train Loss: 1.5085 | Test Acc: 55.67%\n",
      "Time: 74.86s\n",
      "\n",
      "Step 400: Measuring metrics...\n",
      "\n",
      "=== Training Batch Metrics (step 400) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   13.215\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   14.736\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   34.948\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   32.036\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   37.615\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   48.973\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   77.597\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   68.286\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.008, EffRank:   45.668\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   68.226\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  136.967\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  122.055\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.779, EffRank:   11.338\n",
      "layers.fc_act_0: Dead:    0.807, Dup:    0.000, EffRank:   33.088\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.921\n",
      "\n",
      "=== Validation Batch Metrics (step 400) ===\n",
      "layers.conv_0  : Dead:    0.000, Dup:    0.000, EffRank:   13.112\n",
      "layers.norm_0  : Dead:    0.000, Dup:    0.000, EffRank:   15.011\n",
      "layers.act_0   : Dead:    0.000, Dup:    0.000, EffRank:   35.617\n",
      "layers.pool_0  : Dead:    0.000, Dup:    0.000, EffRank:   32.743\n",
      "layers.conv_1  : Dead:    0.000, Dup:    0.000, EffRank:   37.219\n",
      "layers.norm_1  : Dead:    0.000, Dup:    0.000, EffRank:   47.915\n",
      "layers.act_1   : Dead:    0.000, Dup:    0.000, EffRank:   75.349\n",
      "layers.pool_1  : Dead:    0.000, Dup:    0.000, EffRank:   66.536\n",
      "layers.conv_2  : Dead:    0.000, Dup:    0.008, EffRank:   42.851\n",
      "layers.norm_2  : Dead:    0.000, Dup:    0.000, EffRank:   65.465\n",
      "layers.act_2   : Dead:    0.000, Dup:    0.000, EffRank:  126.392\n",
      "layers.pool_2  : Dead:    0.000, Dup:    0.000, EffRank:  112.619\n",
      "layers.fc_0    : Dead:    0.000, Dup:    0.787, EffRank:    9.662\n",
      "layers.fc_act_0: Dead:    0.809, Dup:    0.000, EffRank:   28.768\n",
      "layers.output  : Dead:    0.000, Dup:    0.000, EffRank:    6.584\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Import NetworkMonitor and metric functions from your code\n",
    "from collections import defaultdict\n",
    "\n",
    "###########################################\n",
    "# Metric Functions \n",
    "###########################################\n",
    "\n",
    "def measure_dead_neurons(layer_act, dead_threshold=0.95):\n",
    "    \"\"\"\n",
    "    For a given activation tensor (B, C, ...) flatten spatial dims and compute\n",
    "    the fraction of neurons that output zero (or nearly zero) for most inputs.\n",
    "    \n",
    "    Uses consistent flattening approach across all metrics:\n",
    "    (B, C, H, W) -> (B*H*W, C) where each row is one spatial location across all batches,\n",
    "    and each column represents one channel/neuron.\n",
    "    \"\"\"\n",
    "    shape = layer_act.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C) format, similar to measure_effective_rank\n",
    "    if len(shape) > 2:\n",
    "        # e.g., for (B, C, H, W): rearrange to (B*H*W, C)\n",
    "        layer_act = layer_act.permute(0, 2, 3, 1).contiguous().view(-1, shape[1])\n",
    "    else:\n",
    "        layer_act = layer_act.view(-1, shape[1])\n",
    "    \n",
    "    # Check which values are nearly zero\n",
    "    is_zero = (layer_act.abs() < 1e-7)\n",
    "    \n",
    "    # Average over all spatial locations for each channel\n",
    "    frac_zero_per_neuron = is_zero.float().mean(dim=0)\n",
    "    \n",
    "    # A neuron is considered \"dead\" if it's zero for more than threshold% of inputs\n",
    "    dead_mask = (frac_zero_per_neuron > dead_threshold)\n",
    "    dead_fraction = dead_mask.float().mean().item()\n",
    "    \n",
    "    return dead_fraction\n",
    "\n",
    "def measure_duplicate_neurons(layer_act, corr_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Measure the fraction of neurons that are nearly duplicates (i.e. their outputs\n",
    "    are highly correlated) across the batch.\n",
    "    \n",
    "    Uses consistent flattening approach across all metrics:\n",
    "    (B, C, H, W) -> (B*H*W, C) where each row is one spatial location across all batches,\n",
    "    and each column represents one channel/neuron.\n",
    "    \"\"\"\n",
    "    shape = layer_act.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C) format, similar to measure_effective_rank\n",
    "    if len(shape) > 2:\n",
    "        # e.g., for (B, C, H, W): rearrange to (B*H*W, C)\n",
    "        layer_act = layer_act.permute(0, 2, 3, 1).contiguous().view(-1, shape[1])\n",
    "    else:\n",
    "        layer_act = layer_act.view(-1, shape[1])\n",
    "    \n",
    "    # Now we have data in (Locations, Channels) format\n",
    "    # For duplicate analysis, we want to compare channels, so we transpose to (C, Locations)\n",
    "    layer_act = layer_act.t()  # Shape becomes (C, B*H*W)\n",
    "    C = layer_act.shape[0]  # Number of channels/features\n",
    "    \n",
    "    # Normalize each neuron's outputs for cosine similarity calculation\n",
    "    layer_act = torch.nn.functional.normalize(layer_act, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix - entry (i,j) is the cosine similarity between neurons i and j\n",
    "    similarity_matrix = torch.matmul(layer_act, layer_act.t())\n",
    "    \n",
    "    # Mark duplicates: high similarity but not self (diagonal)\n",
    "    dup_mask = (similarity_matrix > corr_threshold) & (~torch.eye(C, dtype=torch.bool, device=similarity_matrix.device))\n",
    "    \n",
    "    # A neuron is a duplicate if it's highly similar to any other neuron\n",
    "    neuron_is_dup = dup_mask.any(dim=1)\n",
    "    \n",
    "    # Calculate the fraction of neurons that are duplicates\n",
    "    fraction_dup = neuron_is_dup.float().mean().item()\n",
    "    \n",
    "    return fraction_dup\n",
    "\n",
    "def measure_effective_rank(layer_act, svd_sample_size=1024):\n",
    "    \"\"\"\n",
    "    Compute the effective rank of the activation matrix.\n",
    "    Effective rank = exp(-sum_i p_i * log(p_i)) where p_i = sigma_i/sum_j sigma_j.\n",
    "    \"\"\"\n",
    "    shape = layer_act.shape\n",
    "    if len(shape) > 2:\n",
    "        # e.g., for (B, C, H, W): rearrange to (B*H*W, C)\n",
    "        layer_act = layer_act.permute(0, 2, 3, 1).contiguous().view(-1, shape[1])\n",
    "    else:\n",
    "        layer_act = layer_act.view(-1, shape[1])\n",
    "    N = layer_act.shape[0]\n",
    "    if N > svd_sample_size:\n",
    "        idx = torch.randperm(N)[:svd_sample_size]\n",
    "        layer_act = layer_act[idx]\n",
    "    U, S, Vt = torch.linalg.svd(layer_act, full_matrices=False)\n",
    "    S_sum = S.sum()\n",
    "    if S_sum < 1e-9:\n",
    "        return 0.0\n",
    "    p = S / S_sum\n",
    "    p_log_p = p * torch.log(p + 1e-12)\n",
    "    eff_rank = torch.exp(-p_log_p.sum()).item()\n",
    "    return eff_rank\n",
    "\n",
    "def get_activations_for_batch(model, monitor, x_batch, y_batch=None, criterion=None):\n",
    "    \"\"\"\n",
    "    Clears monitor data, runs a forward pass (and backward pass if criterion provided),\n",
    "    then returns a dict of final activations from each hooked layer.\n",
    "    \"\"\"\n",
    "    monitor.clear_data()\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(criterion is not None):\n",
    "        output = model(x_batch)\n",
    "        if criterion is not None and y_batch is not None:\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "    final_acts = {name: acts_list[-1] for name, acts_list in monitor.activations.items()}\n",
    "    return final_acts\n",
    "\n",
    "def evaluate_layer_metrics(model, monitor, x_batch, y_batch=None, criterion=None,\n",
    "                           dead_threshold=0.95, corr_threshold=0.99):\n",
    "    \"\"\"\n",
    "    Run a forward (and optionally backward) pass on x_batch (and y_batch),\n",
    "    then compute for each layer:\n",
    "      - Fraction of dead neurons\n",
    "      - Fraction of duplicate neurons\n",
    "      - Effective rank of the activations\n",
    "    Returns a dictionary of metrics per layer.\n",
    "    \"\"\"\n",
    "    final_acts = get_activations_for_batch(model, monitor, x_batch, y_batch, criterion)\n",
    "    results = {}\n",
    "    for layer_name, act in final_acts.items():\n",
    "        if not isinstance(act, torch.Tensor):\n",
    "            continue\n",
    "        # Skip certain non-meaningful layers\n",
    "        if layer_name.startswith('dropout') or 'flatten' in layer_name or 'shortcut' in layer_name:\n",
    "            continue\n",
    "            \n",
    "        dead_frac = measure_dead_neurons(act, dead_threshold)\n",
    "        dup_frac = measure_duplicate_neurons(act, corr_threshold)\n",
    "        eff_rank = measure_effective_rank(act)\n",
    "        results[layer_name] = {\n",
    "            'dead_fraction': dead_frac,\n",
    "            'dup_fraction': dup_frac,\n",
    "            'eff_rank': eff_rank\n",
    "        }\n",
    "    return results\n",
    "\n",
    "###########################################\n",
    "# Training and Evaluation Functions\n",
    "###########################################\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_cifar10_data(batch_size=128):\n",
    "    \"\"\"Load CIFAR10 dataset with standard transformations\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Create fixed batches for consistent metric measurement\n",
    "    # 1. Training fixed batch\n",
    "    train_indices = list(range(500))  # Use first 500 samples for fixed training batch\n",
    "    fixed_train_set = Subset(trainset, train_indices)\n",
    "    fixed_trainloader = DataLoader(fixed_train_set, batch_size=100, shuffle=False)\n",
    "    \n",
    "    # 2. Validation fixed batch\n",
    "    val_indices = list(range(500))  # Use first 500 samples for fixed validation batch\n",
    "    fixed_val_set = Subset(testset, val_indices)\n",
    "    fixed_valloader = DataLoader(fixed_val_set, batch_size=100, shuffle=False)\n",
    "    \n",
    "    return trainloader, testloader, fixed_trainloader, fixed_valloader\n",
    "\n",
    "def train_and_evaluate(model, trainloader, testloader, fixed_trainloader, fixed_valloader, \n",
    "                       monitor, num_epochs=20, metrics_frequency=100, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model and periodically measure metrics on fixed batches\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Initialize metric tracking\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    # Metrics storage\n",
    "    training_metrics_history = defaultdict(lambda: defaultdict(list))\n",
    "    validation_metrics_history = defaultdict(lambda: defaultdict(list))\n",
    "    step_history = []\n",
    "    \n",
    "    # Fixed batches for consistent metric evaluation\n",
    "    fixed_train_batch, fixed_train_targets = next(iter(fixed_trainloader))\n",
    "    fixed_val_batch, fixed_val_targets = next(iter(fixed_valloader))\n",
    "    \n",
    "    # Move fixed batches to device\n",
    "    fixed_train_batch, fixed_train_targets = fixed_train_batch.to(device), fixed_train_targets.to(device)\n",
    "    fixed_val_batch, fixed_val_targets = fixed_val_batch.to(device), fixed_val_targets.to(device)\n",
    "    \n",
    "    # Baseline metrics (before training)\n",
    "    print(\"Measuring baseline metrics before training...\")\n",
    "    train_metrics = evaluate_layer_metrics(model, monitor, fixed_train_batch)\n",
    "    val_metrics = evaluate_layer_metrics(model, monitor, fixed_val_batch)\n",
    "    \n",
    "    # Print baseline metrics\n",
    "    print(\"\\n=== Training Batch Metrics (before training) ===\")\n",
    "    for layer_name in (train_metrics.keys()):\n",
    "        metrics = train_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Validation Batch Metrics (before training) ===\")\n",
    "    for layer_name in (val_metrics.keys()):\n",
    "        metrics = val_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "    \n",
    "    # Add baseline metrics to history\n",
    "    step_history.append(0)\n",
    "    for layer_name, metrics in train_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            training_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    for layer_name, metrics in val_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            validation_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    total_steps = 0\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Increment step counter\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Measure metrics periodically\n",
    "            if total_steps % metrics_frequency == 0:\n",
    "                # Measure metrics on fixed batches\n",
    "                print(f\"\\nStep {total_steps}: Measuring metrics...\")\n",
    "                \n",
    "                # Training metrics\n",
    "                train_metrics = evaluate_layer_metrics(model, monitor, fixed_train_batch)\n",
    "                \n",
    "                # Validation metrics\n",
    "                val_metrics = evaluate_layer_metrics(model, monitor, fixed_val_batch)\n",
    "                \n",
    "                # Store metrics\n",
    "                step_history.append(total_steps)\n",
    "                for layer_name, metrics in train_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        training_metrics_history[layer_name][metric_name].append(value)\n",
    "                \n",
    "                for layer_name, metrics in val_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        validation_metrics_history[layer_name][metric_name].append(value)\n",
    "                \n",
    "                # Print metrics for all layers\n",
    "                print(f\"\\n=== Training Batch Metrics (step {total_steps}) ===\")\n",
    "                for layer_name in (train_metrics.keys()):\n",
    "                    metrics = train_metrics[layer_name]\n",
    "                    print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "                          f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "                          f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "                \n",
    "                print(f\"\\n=== Validation Batch Metrics (step {total_steps}) ===\")\n",
    "                for layer_name in (val_metrics.keys()):\n",
    "                    metrics = val_metrics[layer_name]\n",
    "                    print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "                          f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "                          f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "        \n",
    "        # Evaluate on test set at the end of each epoch\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        test_acc = 100. * correct / total\n",
    "        \n",
    "        # Store epoch statistics\n",
    "        train_losses.append(train_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "        print(f'Time: {time.time() - start_time:.2f}s')\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal metrics:\")\n",
    "    \n",
    "    # Training metrics\n",
    "    train_metrics = evaluate_layer_metrics(model, monitor, fixed_train_batch)\n",
    "    \n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_layer_metrics(model, monitor, fixed_val_batch)\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\n=== Final Training Batch Metrics ===\")\n",
    "    for layer_name in (train_metrics.keys()):\n",
    "        metrics = train_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Final Validation Batch Metrics ===\")\n",
    "    for layer_name in (val_metrics.keys()):\n",
    "        metrics = val_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}\")\n",
    "    \n",
    "    # Store final metrics\n",
    "    step_history.append(total_steps)\n",
    "    for layer_name, metrics in train_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            training_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    for layer_name, metrics in val_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            validation_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'test_accs': test_accs,\n",
    "        'training_metrics_history': dict(training_metrics_history),\n",
    "        'validation_metrics_history': dict(validation_metrics_history),\n",
    "        'step_history': step_history\n",
    "    }\n",
    "\n",
    "###########################################\n",
    "# Visualization Functions\n",
    "###########################################\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training loss and test accuracy curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(history['train_losses'])\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    ax2.plot(history['test_accs'])\n",
    "    ax2.set_title('Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_evolution(history, metric_name, layer_names=None, is_train=True):\n",
    "    \"\"\"Plot the evolution of a specific metric for selected layers\"\"\"\n",
    "    prefix = \"Training\" if is_train else \"Validation\"\n",
    "    metrics_history = history['training_metrics_history'] if is_train else history['validation_metrics_history']\n",
    "    steps = history['step_history']\n",
    "    \n",
    "    # If no layers specified, use all available\n",
    "    if layer_names is None:\n",
    "        layer_names = list(metrics_history.keys())\n",
    "    \n",
    "    # Filter to layers that exist\n",
    "    layer_names = [layer for layer in layer_names if layer in metrics_history]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for layer in layer_names:\n",
    "        if layer in metrics_history and metric_name in metrics_history[layer]:\n",
    "            plt.plot(steps, metrics_history[layer][metric_name], label=layer)\n",
    "    \n",
    "    plt.title(f'{prefix} {metric_name.replace(\"_\", \" \").title()} Evolution')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel(metric_name.replace(\"_\", \" \").title())\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_metrics(history, layer_names=None):\n",
    "    \"\"\"Plot the evolution of all metrics for selected layers\"\"\"\n",
    "    # Get all metrics\n",
    "    metrics = ['dead_fraction', 'dup_fraction', 'eff_rank']\n",
    "    \n",
    "    # Plot training metrics\n",
    "    for metric in metrics:\n",
    "        plot_metric_evolution(history, metric, layer_names, is_train=True)\n",
    "    \n",
    "    # Plot validation metrics\n",
    "    for metric in metrics:\n",
    "        plot_metric_evolution(history, metric, layer_names, is_train=False)\n",
    "\n",
    "###########################################\n",
    "# Main Function\n",
    "###########################################\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data loaders\n",
    "    print(\"Loading CIFAR10 dataset...\")\n",
    "    trainloader, testloader, fixed_trainloader, fixed_valloader = get_cifar10_data(batch_size=128)\n",
    "    \n",
    "    # Create CNN model\n",
    "    print(\"Creating model...\")\n",
    "    model = CNN(\n",
    "        in_channels=3,\n",
    "        conv_channels=[64, 128, 256],\n",
    "        kernel_sizes=[3, 3, 3],\n",
    "        strides=[1, 1, 1],\n",
    "        paddings=[1, 1, 1],\n",
    "        fc_hidden_units=[512],\n",
    "        num_classes=10,\n",
    "        activation='relu',\n",
    "        dropout_p=0.0,\n",
    "        use_batchnorm=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create monitor\n",
    "    monitor = NetworkMonitor(model)\n",
    "    monitor.register_hooks()\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(name) > 0:  # Skip the root module\n",
    "            print(f\"{name}: {module.__class__.__name__}\")\n",
    "    \n",
    "    # Training settings\n",
    "    num_epochs = 5\n",
    "    metrics_frequency = 100  # Measure metrics every 100 steps\n",
    "    \n",
    "    # Train and evaluate\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train_and_evaluate(\n",
    "        model, trainloader, testloader, fixed_trainloader, fixed_valloader,\n",
    "        monitor, num_epochs, metrics_frequency, device\n",
    "    )\n",
    "    \n",
    "    # Clean up monitor\n",
    "    monitor.remove_hooks()\n",
    "    \n",
    "    # Plot results\n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_training_curves(history)\n",
    "    \n",
    "    # Select important layers to visualize\n",
    "    important_layers = ['conv_0', 'conv_1', 'conv_2', 'fc_0', 'output']\n",
    "    \n",
    "    # Plot metrics evolution for important layers\n",
    "    plot_all_metrics(history, important_layers)\n",
    "    \n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba8992-4e74-4a31-987e-c9858ef2d497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
