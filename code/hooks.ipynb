{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2d46b-cfd0-41dd-be56-66fbfd942f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a572c-97c5-4aea-9615-e0aaa7e6e291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/NN-dynamic-scaling/code/wandb/run-20250305_125437-nsel27px</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amirjoudaki/CL-plasticity/runs/nsel27px' target=\"_blank\">driven-resonance-4</a></strong> to <a href='https://wandb.ai/amirjoudaki/CL-plasticity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amirjoudaki/CL-plasticity' target=\"_blank\">https://wandb.ai/amirjoudaki/CL-plasticity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amirjoudaki/CL-plasticity/runs/nsel27px' target=\"_blank\">https://wandb.ai/amirjoudaki/CL-plasticity/runs/nsel27px</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading CIFAR10 dataset...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Combined file with all neural network models adapted to use ModuleDict,\n",
    "NetworkMonitor for tracking activations and gradients, and wandb for logging experiments.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import wandb  # Import wandb for logging experiments\n",
    "\n",
    "###########################################\n",
    "# Utility Functions and Custom Layers\n",
    "###########################################\n",
    "\n",
    "def get_activation(activation_name):\n",
    "    \"\"\"\n",
    "    Returns the activation function based on name.\n",
    "    \n",
    "    Parameters:\n",
    "        activation_name (str): Name of the activation function.\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: PyTorch activation module.\n",
    "    \"\"\"\n",
    "    activations = {\n",
    "        'relu': nn.ReLU(inplace=False),  # Use inplace=False for compatibility with hooks\n",
    "        'leaky_relu': nn.LeakyReLU(0.1, inplace=False),\n",
    "        'tanh': nn.Tanh(),\n",
    "        'sigmoid': nn.Sigmoid(),\n",
    "        'gelu': nn.GELU(),\n",
    "        'elu': nn.ELU(inplace=False),\n",
    "        'selu': nn.SELU(inplace=False),\n",
    "        'none': nn.Identity()\n",
    "    }\n",
    "    \n",
    "    if activation_name.lower() not in activations:\n",
    "        raise ValueError(f\"Activation {activation_name} not supported. \"\n",
    "                         f\"Choose from: {list(activations.keys())}\")\n",
    "    \n",
    "    return activations[activation_name.lower()]\n",
    "\n",
    "def get_normalization(norm_name, num_features, affine=True):\n",
    "    \"\"\"\n",
    "    Returns the normalization layer based on name.\n",
    "    \n",
    "    Parameters:\n",
    "        norm_name (str): Name of the normalization ('batch', 'layer', etc.)\n",
    "        num_features (int): Number of features for the normalization layer.\n",
    "        affine (bool): Whether the normalization layer should have learnable parameters.\n",
    "        \n",
    "    Returns:\n",
    "        nn.Module: PyTorch normalization module or None.\n",
    "    \"\"\"\n",
    "    if norm_name is None:\n",
    "        return None\n",
    "        \n",
    "    normalizations = {\n",
    "        'batch': nn.BatchNorm1d(num_features, affine=affine),\n",
    "        'batch2d': nn.BatchNorm2d(num_features, affine=affine),\n",
    "        'layer': nn.LayerNorm(num_features, elementwise_affine=affine),\n",
    "        'instance': nn.InstanceNorm1d(num_features, affine=affine),\n",
    "        'instance2d': nn.InstanceNorm2d(num_features, affine=affine),\n",
    "        'group': nn.GroupNorm(min(32, num_features), num_features, affine=affine),\n",
    "        'none': nn.Identity()\n",
    "    }\n",
    "    \n",
    "    norm_key = str(norm_name).lower()\n",
    "    if norm_key not in normalizations:\n",
    "        raise ValueError(f\"Normalization {norm_name} not supported. \"\n",
    "                         f\"Choose from: {list(normalizations.keys())}\")\n",
    "    \n",
    "    return normalizations[norm_key]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size=784, \n",
    "                 hidden_sizes=[512, 256, 128], \n",
    "                 output_size=10, \n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 normalization=None,\n",
    "                 norm_after_activation=False,\n",
    "                 bias=True,\n",
    "                 normalization_affine=True):\n",
    "        \"\"\"\n",
    "        Fully connected MLP that supports various activations and normalizations.\n",
    "        \n",
    "        Parameters:\n",
    "            input_size (int): Dimensionality of input features.\n",
    "            hidden_sizes (list): List of hidden layer dimensions.\n",
    "            output_size (int): Number of output classes.\n",
    "            activation (str): Activation function to use.\n",
    "            dropout_p (float): Dropout probability (0 to disable).\n",
    "            normalization (str): Normalization to use ('batch', 'layer', or None).\n",
    "            norm_after_activation (bool): If True, apply normalization after activation.\n",
    "            bias (bool): Whether to include bias terms in linear layers.\n",
    "            normalization_affine (bool): Whether normalization layers have learnable parameters.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "        in_features = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            self.layers[f'linear_{i}'] = nn.Linear(in_features, hidden_size, bias=bias)\n",
    "            \n",
    "            if norm_after_activation:\n",
    "                self.layers[f'act_{i}'] = get_activation(activation)\n",
    "                if normalization:\n",
    "                    self.layers[f'norm_{i}'] = get_normalization(normalization, hidden_size, affine=normalization_affine)\n",
    "            else:\n",
    "                if normalization:\n",
    "                    self.layers[f'norm_{i}'] = get_normalization(normalization, hidden_size, affine=normalization_affine)\n",
    "                self.layers[f'act_{i}'] = get_activation(activation)\n",
    "            \n",
    "            if dropout_p > 0:\n",
    "                self.layers[f'drop_{i}'] = nn.Dropout(dropout_p)\n",
    "            \n",
    "            in_features = hidden_size\n",
    "        \n",
    "        self.layers['out'] = nn.Linear(in_features, output_size, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        \n",
    "        for k, l in self.layers.items():\n",
    "            x = l(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "#  CNN\n",
    "###########################################\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 conv_channels=[64, 128, 256], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[1, 1, 1],\n",
    "                 paddings=[1, 1, 1],\n",
    "                 fc_hidden_units=[512],\n",
    "                 num_classes=10, \n",
    "                 input_size=32,\n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 pool_type='max',\n",
    "                 pool_size=2,\n",
    "                 use_batchnorm=True,\n",
    "                 norm_after_activation=False,\n",
    "                 normalization_affine=True):\n",
    "        \"\"\"\n",
    "         CNN with layers, activations, and normalizations.\n",
    "        \n",
    "        Parameters:\n",
    "            in_channels (int): Number of input channels.\n",
    "            conv_channels (list): List of convolutional layer output channels.\n",
    "            kernel_sizes (list): List of kernel sizes for each conv layer.\n",
    "            strides (list): List of stride values for each conv layer.\n",
    "            paddings (list): List of padding values for each conv layer.\n",
    "            fc_hidden_units (list): List of hidden units for fully connected layers.\n",
    "            num_classes (int): Number of output classes.\n",
    "            input_size (int): Height/width of the input images.\n",
    "            activation (str): Activation function to use.\n",
    "            dropout_p (float): Dropout probability.\n",
    "            pool_type (str): Type of pooling ('max', 'avg', or None).\n",
    "            pool_size (int): Size of the pooling window.\n",
    "            use_batchnorm (bool): Whether to use batch normalization.\n",
    "            norm_after_activation (bool): Whether to apply normalization after activation.\n",
    "            normalization_affine (bool): Whether normalization layers have learnable parameters.\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        assert len(conv_channels) == len(kernel_sizes) == len(strides) == len(paddings), \\\n",
    "            \"Convolutional parameters must have the same length\"\n",
    "        \n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        channels = in_channels\n",
    "        for i, (out_channels, kernel_size, stride, padding) in enumerate(\n",
    "                zip(conv_channels, kernel_sizes, strides, paddings)):\n",
    "            self.layers[f'conv_{i}'] = nn.Conv2d(channels, out_channels, kernel_size, stride, padding)\n",
    "            \n",
    "            if use_batchnorm:\n",
    "                self.layers[f'norm_{i}'] = get_normalization('batch2d', out_channels, affine=normalization_affine)\n",
    "            \n",
    "            self.layers[f'act_{i}'] = get_activation(activation)\n",
    "            \n",
    "            if pool_type == 'max':\n",
    "                self.layers[f'pool_{i}'] = nn.MaxPool2d(pool_size, pool_size)\n",
    "            elif pool_type == 'avg':\n",
    "                self.layers[f'pool_{i}'] = nn.AvgPool2d(pool_size, pool_size)\n",
    "            \n",
    "            channels = out_channels\n",
    "        \n",
    "        num_pools = len(conv_channels) if pool_type in ['max', 'avg'] else 0\n",
    "        final_size = input_size // (pool_size ** num_pools)\n",
    "        self.flattened_size = conv_channels[-1] * final_size * final_size\n",
    "        \n",
    "        self.layers['flatten'] = nn.Flatten()\n",
    "        \n",
    "        fc_input_size = self.flattened_size\n",
    "        for i, hidden_units in enumerate(fc_hidden_units):\n",
    "            self.layers[f'fc_{i}'] = nn.Linear(fc_input_size, hidden_units)\n",
    "            self.layers[f'fc_act_{i}'] = get_activation(activation)\n",
    "            \n",
    "            if dropout_p > 0:\n",
    "                self.layers[f'fc_dropout_{i}'] = nn.Dropout(dropout_p)\n",
    "                \n",
    "            fc_input_size = hidden_units\n",
    "        \n",
    "        self.layers['output'] = nn.Linear(fc_input_size, num_classes)\n",
    "        \n",
    "        self.num_conv_layers = len(conv_channels)\n",
    "        self.num_fc_layers = len(fc_hidden_units)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.has_pool = pool_type in ['max', 'avg']\n",
    "        self.dropout_p = dropout_p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_conv_layers):\n",
    "            x = self.layers[f'conv_{i}'](x)\n",
    "            \n",
    "            if self.use_batchnorm and not self.norm_after_activation:\n",
    "                if f'norm_{i}' in self.layers:\n",
    "                    x = self.layers[f'norm_{i}'](x)\n",
    "            \n",
    "            x = self.layers[f'act_{i}'](x)\n",
    "            \n",
    "            if self.use_batchnorm and self.norm_after_activation:\n",
    "                if f'norm_{i}' in self.layers:\n",
    "                    x = self.layers[f'norm_{i}'](x)\n",
    "            \n",
    "            if self.has_pool and f'pool_{i}' in self.layers:\n",
    "                x = self.layers[f'pool_{i}'](x)\n",
    "        \n",
    "        x = self.layers['flatten'](x)\n",
    "        \n",
    "        for i in range(self.num_fc_layers):\n",
    "            x = self.layers[f'fc_{i}'](x)\n",
    "            x = self.layers[f'fc_act_{i}'](x)\n",
    "            \n",
    "            if self.dropout_p > 0 and f'fc_dropout_{i}' in self.layers:\n",
    "                x = self.layers[f'fc_dropout_{i}'](x)\n",
    "        \n",
    "        x = self.layers['output'](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# ResNet\n",
    "###########################################\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic ResNet block with activation and normalization.\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1, activation='relu', \n",
    "                 use_batchnorm=True, norm_after_activation=False, downsample=None,\n",
    "                 normalization_affine=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        self.layers['conv1'] = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, \n",
    "                                        padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        if use_batchnorm:\n",
    "            self.layers['bn1'] = get_normalization('batch2d', planes, affine=normalization_affine)\n",
    "        \n",
    "        self.layers['activation'] = get_activation(activation)\n",
    "        \n",
    "        self.layers['conv2'] = nn.Conv2d(planes, planes, kernel_size=3, stride=1, \n",
    "                                        padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        if use_batchnorm:\n",
    "            self.layers['bn2'] = get_normalization('batch2d', planes, affine=normalization_affine)\n",
    "        \n",
    "        if downsample is not None:\n",
    "            self.layers['downsample'] = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.layers['conv1'](x)\n",
    "        \n",
    "        if 'bn1' in self.layers and not self.norm_after_activation:\n",
    "            out = self.layers['bn1'](out)\n",
    "        \n",
    "        out = self.layers['activation'](out)\n",
    "        \n",
    "        if 'bn1' in self.layers and self.norm_after_activation:\n",
    "            out = self.layers['bn1'](out)\n",
    "            \n",
    "        out = self.layers['conv2'](out)\n",
    "        \n",
    "        if 'bn2' in self.layers and not self.norm_after_activation:\n",
    "            out = self.layers['bn2'](out)\n",
    "            \n",
    "        if 'downsample' in self.layers:\n",
    "            identity = self.layers['downsample'](x)\n",
    "            \n",
    "        out = out + identity\n",
    "        out = self.layers['activation'](out)\n",
    "        \n",
    "        if 'bn2' in self.layers and self.norm_after_activation:\n",
    "            out = self.layers['bn2'](out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "     ResNet architecture for continual learning experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 block=BasicBlock,\n",
    "                 layers=[2, 2, 2, 2],\n",
    "                 num_classes=10,\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 activation='relu',\n",
    "                 dropout_p=0.0,\n",
    "                 use_batchnorm=True,\n",
    "                 norm_after_activation=False,\n",
    "                 normalization_affine=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.norm_after_activation = norm_after_activation\n",
    "        self.in_planes = base_channels\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        self.layers['conv1'] = nn.Conv2d(in_channels, base_channels, kernel_size=3, \n",
    "                                        stride=1, padding=1, bias=not use_batchnorm)\n",
    "        \n",
    "        if use_batchnorm:\n",
    "            self.layers['bn1'] = get_normalization('batch2d', base_channels, affine=normalization_affine)\n",
    "        \n",
    "        self.layers['activation'] = get_activation(activation)\n",
    "        \n",
    "        self._make_layer(block, base_channels, layers[0], stride=1, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm, \n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer1',\n",
    "                        normalization_affine=normalization_affine)\n",
    "        self._make_layer(block, base_channels*2, layers[1], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm, \n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer2',\n",
    "                        normalization_affine=normalization_affine)\n",
    "        self._make_layer(block, base_channels*4, layers[2], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm,\n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer3',\n",
    "                        normalization_affine=normalization_affine)\n",
    "        self._make_layer(block, base_channels*8, layers[3], stride=2, \n",
    "                        activation=activation, use_batchnorm=use_batchnorm,\n",
    "                        norm_after_activation=norm_after_activation, \n",
    "                        layer_name='layer4',\n",
    "                        normalization_affine=normalization_affine)\n",
    "        \n",
    "        self.layers['avgpool'] = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.layers['flatten'] = nn.Flatten()\n",
    "        \n",
    "        if dropout_p > 0:\n",
    "            self.layers['dropout'] = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.layers['fc'] = nn.Linear(base_channels*8*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        self.num_layers = len(layers)\n",
    "        self.blocks_per_layer = layers\n",
    "                \n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1, activation='relu', \n",
    "                    use_batchnorm=True, norm_after_activation=False, layer_name='layer',\n",
    "                    normalization_affine=True):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample_layers = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes * block.expansion, \n",
    "                         kernel_size=1, stride=stride, bias=not use_batchnorm)\n",
    "            )\n",
    "            \n",
    "            if use_batchnorm:\n",
    "                downsample_layers.add_module('1', get_normalization('batch2d', planes * block.expansion, affine=normalization_affine))\n",
    "                \n",
    "            downsample = downsample_layers\n",
    "        \n",
    "        self.layers[f'{layer_name}_block0'] = block(\n",
    "            self.in_planes, planes, stride, activation, \n",
    "            use_batchnorm, norm_after_activation, downsample,\n",
    "            normalization_affine=normalization_affine\n",
    "        )\n",
    "        \n",
    "        self.in_planes = planes * block.expansion\n",
    "        \n",
    "        for i in range(1, num_blocks):\n",
    "            self.layers[f'{layer_name}_block{i}'] = block(\n",
    "                self.in_planes, planes, 1, activation, \n",
    "                use_batchnorm, norm_after_activation,\n",
    "                normalization_affine=normalization_affine\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers['conv1'](x)\n",
    "        \n",
    "        if self.use_batchnorm and not self.norm_after_activation:\n",
    "            if 'bn1' in self.layers:\n",
    "                x = self.layers['bn1'](x)\n",
    "                \n",
    "        x = self.layers['activation'](x)\n",
    "        \n",
    "        if self.use_batchnorm and self.norm_after_activation:\n",
    "            if 'bn1' in self.layers:\n",
    "                x = self.layers['bn1'](x)\n",
    "        \n",
    "        for layer_idx in range(1, self.num_layers + 1):\n",
    "            for block_idx in range(self.blocks_per_layer[layer_idx - 1]):\n",
    "                block_name = f'layer{layer_idx}_block{block_idx}'\n",
    "                x = self.layers[block_name](x)\n",
    "        \n",
    "        x = self.layers['avgpool'](x)\n",
    "        x = self.layers['flatten'](x)\n",
    "        \n",
    "        if 'dropout' in self.layers:\n",
    "            x = self.layers['dropout'](x)\n",
    "            \n",
    "        x = self.layers['fc'](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# ViT Components\n",
    "###########################################\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "    def __init__(self, dim, n_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        head_dim = dim // n_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'qkv': nn.Linear(dim, dim * 3, bias=qkv_bias),\n",
    "            'attn_drop': nn.Dropout(attn_drop),\n",
    "            'proj': nn.Linear(dim, dim),\n",
    "            'proj_drop': nn.Dropout(proj_drop)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.layers['qkv'](x).reshape(B, N, 3, self.n_heads, C // self.n_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.layers['attn_drop'](attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.layers['proj'](x)\n",
    "        x = self.layers['proj_drop'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerMLP(nn.Module):\n",
    "    \"\"\"MLP module with activation.\"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, \n",
    "                 activation='gelu', drop=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(in_features, hidden_features),\n",
    "            'act': get_activation(activation),\n",
    "            'drop1': nn.Dropout(drop) if drop > 0 else nn.Identity(),\n",
    "            'fc2': nn.Linear(hidden_features, out_features),\n",
    "            'drop2': nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['fc1'](x)\n",
    "        x = self.layers['act'](x)\n",
    "        x = self.layers['drop1'](x)\n",
    "        x = self.layers['fc2'](x)\n",
    "        x = self.layers['drop2'](x)\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding for Vision Transformer.\n",
    "    Adapted to work with ModuleDict and NetworkMonitor.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.layers = nn.ModuleDict({\n",
    "            'proj': nn.Conv2d(\n",
    "                in_channels,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['proj'](x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with components.\"\"\"\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4., qkv_bias=True, drop=0., \n",
    "                 attn_drop=0., activation='gelu', normalization='layer',\n",
    "                 normalization_affine=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleDict({\n",
    "            'norm1': get_normalization(normalization, dim, affine=normalization_affine),\n",
    "            'attn': Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, \n",
    "                             attn_drop=attn_drop, proj_drop=drop),\n",
    "            'norm2': get_normalization(normalization, dim, affine=normalization_affine),\n",
    "            'mlp': TransformerMLP(dim, int(dim * mlp_ratio), dim, \n",
    "                       activation=activation, drop=drop)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.layers['norm1'](x)\n",
    "        attn_out = self.layers['attn'](norm_x)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        norm_x = self.layers['norm2'](x)\n",
    "        mlp_out = self.layers['mlp'](norm_x)\n",
    "        x = x + mlp_out\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) model with architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_size=32, \n",
    "                 patch_size=4, \n",
    "                 in_channels=3, \n",
    "                 num_classes=10, \n",
    "                 embed_dim=192,\n",
    "                 depth=12, \n",
    "                 n_heads=8, \n",
    "                 mlp_ratio=4., \n",
    "                 qkv_bias=True, \n",
    "                 drop_rate=0.1,\n",
    "                 attn_drop_rate=0.0,\n",
    "                 activation='gelu',\n",
    "                 normalization='layer',\n",
    "                 normalization_affine=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleDict()\n",
    "        \n",
    "        self.layers['patch_embed'] = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        n_patches = self.layers['patch_embed'].n_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        \n",
    "        self.layers['pos_drop'] = nn.Dropout(drop_rate)\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers[f'block_{i}'] = TransformerBlock(\n",
    "                embed_dim, n_heads, mlp_ratio, qkv_bias, \n",
    "                drop_rate, attn_drop_rate, activation, normalization,\n",
    "                normalization_affine=normalization_affine\n",
    "            )\n",
    "\n",
    "        self.layers['norm'] = get_normalization(normalization, embed_dim, affine=normalization_affine)\n",
    "        self.layers['head'] = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "        self.depth = depth\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers['patch_embed'](x)\n",
    "        \n",
    "        B = x.shape[0]\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        x = self.layers['pos_drop'](x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.layers[f'block_{i}'](x)\n",
    "        \n",
    "        x = self.layers['norm'](x)\n",
    "        x = x[:, 0]\n",
    "        x = self.layers['head'](x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "###########################################\n",
    "# NetworkMonitor Class with Enhanced Control\n",
    "###########################################\n",
    "\n",
    "class NetworkMonitor:\n",
    "    def __init__(self, model, filter_func=None):\n",
    "        \"\"\"\n",
    "        Initialize the network monitor.\n",
    "        \n",
    "        Parameters:\n",
    "            model: The neural network model to monitor.\n",
    "            filter_func: Function that takes a layer name and returns \n",
    "                         True if the layer should be monitored.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.filter_func = filter_func if filter_func is not None else lambda name: True\n",
    "        self.activations = defaultdict(list)\n",
    "        self.gradients = defaultdict(list)\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "        self.hooks_active = False\n",
    "        \n",
    "    def set_filter(self, filter_func):\n",
    "        was_active = self.hooks_active\n",
    "        if was_active:\n",
    "            self.remove_hooks()\n",
    "        self.filter_func = filter_func if filter_func is not None else lambda name: True\n",
    "        if was_active:\n",
    "            self.register_hooks()\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        if not self.hooks_active:\n",
    "            for name, module in self.model.named_modules():\n",
    "                if name != '' and self.filter_func(name):\n",
    "                    def make_fwd_hook(name=name):\n",
    "                        def hook(module, input, output):\n",
    "                            self.activations[f\"{name}\"].append(output.clone().detach().cpu())\n",
    "                        return hook\n",
    "                    \n",
    "                    def make_bwd_hook(name=name):\n",
    "                        def hook(module, grad_input, grad_output):\n",
    "                            if len(grad_output) > 0 and grad_output[0] is not None:\n",
    "                                self.gradients[f\"{name}\"].append(grad_output[0].clone().detach().cpu())\n",
    "                            return grad_input\n",
    "                        return hook\n",
    "                    \n",
    "                    h1 = module.register_forward_hook(make_fwd_hook())\n",
    "                    h2 = module.register_full_backward_hook(make_bwd_hook())\n",
    "                    self.fwd_hooks.append(h1)\n",
    "                    self.bwd_hooks.append(h2)\n",
    "            \n",
    "            self.hooks_active = True\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        if self.hooks_active:\n",
    "            for h in self.fwd_hooks + self.bwd_hooks:\n",
    "                h.remove()\n",
    "            self.fwd_hooks = []\n",
    "            self.bwd_hooks = []\n",
    "            self.hooks_active = False\n",
    "        \n",
    "    def clear_data(self):\n",
    "        self.activations = defaultdict(list)\n",
    "        self.gradients = defaultdict(list)\n",
    "        \n",
    "    def get_latest_activations(self):\n",
    "        latest_acts = {}\n",
    "        for name, acts_list in self.activations.items():\n",
    "            if acts_list:\n",
    "                latest_acts[name] = acts_list[-1]\n",
    "        return latest_acts\n",
    "    \n",
    "    def get_latest_gradients(self):\n",
    "        latest_grads = {}\n",
    "        for name, grads_list in self.gradients.items():\n",
    "            if grads_list:\n",
    "                latest_grads[name] = grads_list[-1]\n",
    "        return latest_grads\n",
    "\n",
    "###########################################\n",
    "# Utility Functions\n",
    "###########################################\n",
    "\n",
    "def flatten_activations(layer_act):\n",
    "    shape = layer_act.shape\n",
    "    if len(shape) == 4:\n",
    "        return layer_act.permute(0, 2, 3, 1).contiguous().view(-1, shape[1])\n",
    "    elif len(shape) == 3:\n",
    "        return layer_act.contiguous().view(-1, shape[2])\n",
    "    else:\n",
    "        return layer_act.view(-1, shape[1])\n",
    "\n",
    "###########################################\n",
    "# Metric Functions \n",
    "###########################################\n",
    "\n",
    "def measure_dead_neurons(layer_act, dead_threshold=0.95):\n",
    "    flattened_act = flatten_activations(layer_act)\n",
    "    is_zero = (flattened_act.abs() < 1e-7)\n",
    "    frac_zero_per_neuron = is_zero.float().mean(dim=0)\n",
    "    dead_mask = (frac_zero_per_neuron > dead_threshold)\n",
    "    dead_fraction = dead_mask.float().mean().item()\n",
    "    return dead_fraction\n",
    "\n",
    "def measure_duplicate_neurons(layer_act, corr_threshold=0.90):\n",
    "    flattened_act = flatten_activations(layer_act)\n",
    "    flattened_act = flattened_act.t()  \n",
    "    flattened_act = torch.nn.functional.normalize(flattened_act, p=2, dim=1)\n",
    "    similarity_matrix = torch.matmul(flattened_act, flattened_act.t())\n",
    "    upper_tri_mask = torch.triu(torch.ones_like(similarity_matrix), diagonal=1).bool()\n",
    "    dup_pairs = (similarity_matrix > corr_threshold) & upper_tri_mask\n",
    "    neuron_is_dup = dup_pairs.any(dim=1)\n",
    "    fraction_dup = neuron_is_dup.float().mean().item()\n",
    "    return fraction_dup\n",
    "\n",
    "def measure_effective_rank(layer_act, svd_sample_size=1024):\n",
    "    flattened_act = flatten_activations(layer_act)\n",
    "    N = flattened_act.shape[0]\n",
    "    if N > svd_sample_size:\n",
    "        idx = torch.randperm(N)[:svd_sample_size]\n",
    "        flattened_act = flattened_act[idx]\n",
    "    U, S, Vt = torch.linalg.svd(flattened_act, full_matrices=False)\n",
    "    S_sum = S.sum()\n",
    "    if S_sum < 1e-9:\n",
    "        return 0.0\n",
    "    p = S / S_sum\n",
    "    p_log_p = p * torch.log(p + 1e-12)\n",
    "    eff_rank = torch.exp(-p_log_p.sum()).item()\n",
    "    return eff_rank\n",
    "\n",
    "def measure_stable_rank(layer_act, sample_size=1024, use_gram=True):\n",
    "    flattened_act = flatten_activations(layer_act)\n",
    "    N, D = flattened_act.shape\n",
    "    if N > sample_size:\n",
    "        idx = torch.randperm(N)[:sample_size]\n",
    "        flattened_act = flattened_act[idx]\n",
    "        N = sample_size\n",
    "    flattened_act = flattened_act - flattened_act.mean(dim=0, keepdim=True)\n",
    "    if use_gram or D < N:\n",
    "        frob_norm_sq = torch.sum(flattened_act**2).item()\n",
    "        gram = torch.matmul(flattened_act.t(), flattened_act)\n",
    "        trace_gram_squared = torch.sum(gram**2).item()\n",
    "        if trace_gram_squared < 1e-9:\n",
    "            return 0.0\n",
    "        stable_rank = (frob_norm_sq**2) / trace_gram_squared\n",
    "    else:\n",
    "        cov = torch.matmul(flattened_act, flattened_act.t())\n",
    "        trace_cov = torch.trace(cov).item()\n",
    "        trace_cov_squared = torch.sum(cov**2).item()\n",
    "        if trace_cov_squared < 1e-9:\n",
    "            return 0.0\n",
    "        stable_rank = (trace_cov**2) / trace_cov_squared\n",
    "    return stable_rank\n",
    "\n",
    "###########################################\n",
    "# Analysis with Single Monitor\n",
    "###########################################\n",
    "\n",
    "def analyze_fixed_batch(model, monitor, fixed_batch, fixed_targets=None, \n",
    "                        criterion=None, dead_threshold=0.95, \n",
    "                        corr_threshold=0.99, device='cpu'):\n",
    "    if fixed_batch.device != device:\n",
    "        fixed_batch = fixed_batch.to(device)\n",
    "    if fixed_targets is not None and fixed_targets.device != device:\n",
    "        fixed_targets = fixed_targets.to(device)\n",
    "    \n",
    "    hooks_were_active = monitor.hooks_active\n",
    "    monitor.register_hooks()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.set_grad_enabled(criterion is not None):\n",
    "        outputs = model(fixed_batch)\n",
    "        if criterion is not None and fixed_targets is not None:\n",
    "            loss = criterion(outputs, fixed_targets)\n",
    "            loss.backward()\n",
    "    \n",
    "    metrics = {}\n",
    "    latest_acts = monitor.get_latest_activations()\n",
    "    \n",
    "    for layer_name, act in latest_acts.items():\n",
    "        if not isinstance(act, torch.Tensor) or \\\n",
    "           layer_name.startswith('dropout') or 'flatten' in layer_name or 'shortcut' in layer_name:\n",
    "            continue\n",
    "        \n",
    "        dead_frac = measure_dead_neurons(act, dead_threshold)\n",
    "        dup_frac = measure_duplicate_neurons(act, corr_threshold)\n",
    "        eff_rank = measure_effective_rank(act)\n",
    "        stable_rank = measure_stable_rank(act)\n",
    "        \n",
    "        metrics[layer_name] = {\n",
    "            'dead_fraction': dead_frac,\n",
    "            'dup_fraction': dup_frac,\n",
    "            'eff_rank': eff_rank,\n",
    "            'stable_rank': stable_rank\n",
    "        }\n",
    "    \n",
    "    if not hooks_were_active:\n",
    "        monitor.remove_hooks()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "###########################################\n",
    "# Training and Evaluation Functions\n",
    "###########################################\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_cifar10_data(batch_size=128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    train_indices = list(range(500))\n",
    "    fixed_train_set = Subset(trainset, train_indices)\n",
    "    fixed_trainloader = DataLoader(fixed_train_set, batch_size=100, shuffle=False)\n",
    "    \n",
    "    val_indices = list(range(500))\n",
    "    fixed_val_set = Subset(testset, val_indices)\n",
    "    fixed_valloader = DataLoader(fixed_val_set, batch_size=100, shuffle=False)\n",
    "    \n",
    "    return trainloader, testloader, fixed_trainloader, fixed_valloader\n",
    "\n",
    "\n",
    "def get_cifar10_data_with_class_selection(batch_size=128, sample_classes=None):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    if sample_classes:\n",
    "        class_mapping = {original_class: i for i, original_class in enumerate(sample_classes)}\n",
    "        \n",
    "        train_data = []\n",
    "        train_targets = []\n",
    "        for image, label in trainset:\n",
    "            if label in sample_classes:\n",
    "                new_label = class_mapping[label]\n",
    "                train_data.append(image)\n",
    "                train_targets.append(new_label)\n",
    "        \n",
    "        test_data = []\n",
    "        test_targets = []\n",
    "        for image, label in testset:\n",
    "            if label in sample_classes:\n",
    "                new_label = class_mapping[label]\n",
    "                test_data.append(image)\n",
    "                test_targets.append(new_label)\n",
    "        \n",
    "        from torch.utils.data import TensorDataset\n",
    "        trainset = TensorDataset(torch.stack(train_data), torch.tensor(train_targets))\n",
    "        testset = TensorDataset(torch.stack(test_data), torch.tensor(test_targets))\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "    fixed_train_set = Subset(trainset, range(min(500, len(trainset))))\n",
    "    fixed_trainloader = DataLoader(fixed_train_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    fixed_val_set = Subset(testset, range(min(500, len(testset))))\n",
    "    fixed_valloader = DataLoader(fixed_val_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return trainloader, testloader, fixed_trainloader, fixed_valloader\n",
    "\n",
    "def train_with_separate_monitors(model, trainloader, testloader, fixed_trainloader, fixed_valloader,\n",
    "                                 train_monitor, val_monitor, learning_rate=0.001,\n",
    "                                 num_epochs=20, metrics_frequency=100, device='cpu'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    training_metrics_history = defaultdict(lambda: defaultdict(list))\n",
    "    validation_metrics_history = defaultdict(lambda: defaultdict(list))\n",
    "    step_history = []\n",
    "    \n",
    "    fixed_train_batch, fixed_train_targets = next(iter(fixed_trainloader))\n",
    "    fixed_val_batch, fixed_val_targets = next(iter(fixed_valloader))\n",
    "    \n",
    "    fixed_train_batch, fixed_train_targets = fixed_train_batch.to(device), fixed_train_targets.to(device)\n",
    "    fixed_val_batch, fixed_val_targets = fixed_val_batch.to(device), fixed_val_targets.to(device)\n",
    "    \n",
    "    print(\"Measuring baseline metrics before training...\")\n",
    "    \n",
    "    train_metrics = analyze_fixed_batch(model, train_monitor, fixed_train_batch, fixed_train_targets, criterion, device=device)\n",
    "    val_metrics = analyze_fixed_batch(model, val_monitor, fixed_val_batch, device=device)\n",
    "    \n",
    "    print(\"\\n=== Training Batch Metrics (before training) ===\")\n",
    "    for layer_name in train_metrics.keys():\n",
    "        metrics = train_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "              f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Validation Batch Metrics (before training) ===\")\n",
    "    for layer_name in val_metrics.keys():\n",
    "        metrics = val_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "              f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "    \n",
    "    step_history.append(0)\n",
    "    for layer_name, metrics in train_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            training_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    for layer_name, metrics in val_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            validation_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    total_steps = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        train_monitor.remove_hooks()\n",
    "        val_monitor.remove_hooks()\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            total_steps += 1\n",
    "            \n",
    "            if total_steps % metrics_frequency == 0:\n",
    "                print(f\"\\nStep {total_steps}: Measuring metrics...\")\n",
    "                \n",
    "                train_monitor.clear_data()\n",
    "                val_monitor.clear_data()\n",
    "                \n",
    "                train_metrics = analyze_fixed_batch(model, train_monitor, fixed_train_batch, fixed_train_targets, criterion, device=device)\n",
    "                val_metrics = analyze_fixed_batch(model, val_monitor, fixed_val_batch, device=device)\n",
    "                \n",
    "                step_history.append(total_steps)\n",
    "                for layer_name, metrics in train_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        training_metrics_history[layer_name][metric_name].append(value)\n",
    "                for layer_name, metrics in val_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        validation_metrics_history[layer_name][metric_name].append(value)\n",
    "                \n",
    "                # Log fixed batch metrics to wandb\n",
    "                fixed_metrics_log = {\"step\": total_steps}\n",
    "                for layer_name, metrics in train_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        fixed_metrics_log[f\"fixed_train/{layer_name}/{metric_name}\"] = value\n",
    "                for layer_name, metrics in val_metrics.items():\n",
    "                    for metric_name, value in metrics.items():\n",
    "                        fixed_metrics_log[f\"fixed_val/{layer_name}/{metric_name}\"] = value\n",
    "                wandb.log(fixed_metrics_log)\n",
    "                \n",
    "                print(f\"\\n=== Training Batch Metrics (step {total_steps}) ===\")\n",
    "                for layer_name in train_metrics.keys():\n",
    "                    metrics = train_metrics[layer_name]\n",
    "                    print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "                          f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "                          f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "                          f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "                \n",
    "                print(f\"\\n=== Validation Batch Metrics (step {total_steps}) ===\")\n",
    "                for layer_name in val_metrics.keys():\n",
    "                    metrics = val_metrics[layer_name]\n",
    "                    print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "                          f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "                          f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "                          f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        test_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "        print(f'Time: {elapsed:.2f}s')\n",
    "        \n",
    "        # Log epoch-level metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"elapsed_time\": elapsed\n",
    "        })\n",
    "    \n",
    "    print(\"\\nFinal metrics:\")\n",
    "    \n",
    "    train_monitor.clear_data()\n",
    "    val_monitor.clear_data()\n",
    "    \n",
    "    train_metrics = analyze_fixed_batch(model, train_monitor, fixed_train_batch, fixed_train_targets, criterion, device=device)\n",
    "    val_metrics = analyze_fixed_batch(model, val_monitor, fixed_val_batch, device=device)\n",
    "    \n",
    "    print(\"\\n=== Final Training Batch Metrics ===\")\n",
    "    for layer_name in train_metrics.keys():\n",
    "        metrics = train_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "              f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "    \n",
    "    print(\"\\n=== Final Validation Batch Metrics ===\")\n",
    "    for layer_name in val_metrics.keys():\n",
    "        metrics = val_metrics[layer_name]\n",
    "        print(f\"{layer_name:15}: Dead: {metrics['dead_fraction']:8.3f}, \" +\n",
    "              f\"Dup: {metrics['dup_fraction']:8.3f}, \" +\n",
    "              f\"EffRank: {metrics['eff_rank']:8.3f}, \" +\n",
    "              f\"StableRank: {metrics['stable_rank']:8.3f}\")\n",
    "    \n",
    "    step_history.append(total_steps)\n",
    "    for layer_name, metrics in train_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            training_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    for layer_name, metrics in val_metrics.items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            validation_metrics_history[layer_name][metric_name].append(value)\n",
    "    \n",
    "    train_monitor.remove_hooks()\n",
    "    val_monitor.remove_hooks()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'test_accs': test_accs,\n",
    "        'training_metrics_history': dict(training_metrics_history),\n",
    "        'validation_metrics_history': dict(validation_metrics_history),\n",
    "        'step_history': step_history,\n",
    "        'train_monitor': train_monitor,\n",
    "        'val_monitor': val_monitor\n",
    "    }\n",
    "\n",
    "###########################################\n",
    "# Visualization Functions\n",
    "###########################################\n",
    "\n",
    "def plot_training_curves(history, save_path=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.plot(history['train_losses'])\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(history['test_accs'])\n",
    "    ax2.set_title('Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_evolution(history, metric_name, layer_names=None, is_train=True, save_path=None):\n",
    "    prefix = \"Training\" if is_train else \"Validation\"\n",
    "    metrics_history = history['training_metrics_history'] if is_train else history['validation_metrics_history']\n",
    "    steps = history['step_history']\n",
    "    \n",
    "    if layer_names is None:\n",
    "        layer_names = list(metrics_history.keys())\n",
    "    \n",
    "    layer_names = [layer for layer in layer_names if layer in metrics_history]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for layer in layer_names:\n",
    "        if layer in metrics_history and metric_name in metrics_history[layer]:\n",
    "            plt.plot(steps, metrics_history[layer][metric_name], label=layer)\n",
    "    \n",
    "    plt.title(f'{prefix} {metric_name.replace(\"_\", \" \").title()} Evolution')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel(metric_name.replace(\"_\", \" \").title())\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/{prefix.lower()}_{metric_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_metrics(history, layer_names=None, save_path=None):\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    metrics = ['dead_fraction', 'dup_fraction', 'eff_rank', 'stable_rank']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plot_metric_evolution(history, metric, layer_names, is_train=True, save_path=save_path)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plot_metric_evolution(history, metric, layer_names, is_train=False, save_path=save_path)\n",
    "        \n",
    "def plot_comparison_metrics(history, metric_names=None, layer_names=None, save_path=None):\n",
    "    if metric_names is None:\n",
    "        metric_names = ['dead_fraction', 'dup_fraction', 'eff_rank', 'stable_rank']\n",
    "    \n",
    "    if layer_names is None:\n",
    "        layer_names = [layer_name for layer_name in history['training_metrics_history'].keys()\n",
    "                      if not (layer_name.startswith('dropout') or 'flatten' in layer_name)]\n",
    "                      \n",
    "    final_index = -1\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        valid_layers = [layer for layer in layer_names \n",
    "                        if layer in history['training_metrics_history'] \n",
    "                        and layer in history['validation_metrics_history']\n",
    "                        and metric in history['training_metrics_history'][layer]\n",
    "                        and metric in history['validation_metrics_history'][layer]]\n",
    "        \n",
    "        train_values = [history['training_metrics_history'][layer][metric][final_index] for layer in valid_layers]\n",
    "        val_values = [history['validation_metrics_history'][layer][metric][final_index] for layer in valid_layers]\n",
    "        \n",
    "        x = np.arange(len(valid_layers))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, train_values, width, label='Training')\n",
    "        plt.bar(x + width/2, val_values, width, label='Validation')\n",
    "        \n",
    "        plt.xlabel('Layer')\n",
    "        plt.ylabel(metric.replace('_', ' ').title())\n",
    "        plt.title(f'Comparison of {metric.replace(\"_\", \" \").title()} Between Training and Validation')\n",
    "        plt.xticks(x, [layer.split('.')[-1] for layer in valid_layers], rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}/comparison_{metric}.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "###########################################\n",
    "# Main Function with Centralized Configuration and wandb Logging\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Centralized configuration dictionary\n",
    "    config = {\n",
    "        \"seed\": 41,\n",
    "        \"sample_classes\": [0, 1, 2, 3, 4],\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_epochs\": 50,\n",
    "        \"metrics_frequency\": 100,\n",
    "        \"model_type\": \"MLP\",  # Options: \"MLP\", \"CNN\", \"VisionTransformer\",\n",
    "        \"model\": {\n",
    "            \"input_size\": 3 * 32 * 32,\n",
    "            \"hidden_sizes\": [1024] * 10,\n",
    "            \"activation\": \"tanh\",\n",
    "            \"normalization\": \"batch\",\n",
    "            \"norm_after_activation\": False,\n",
    "            \"normalization_affine\": False,\n",
    "            \"dropout_p\": 0,\n",
    "            # output_size is set dynamically based on number of sample classes\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Initialize wandb run (specify project and optionally entity)\n",
    "    wandb.init(project=\"CL-plasticity\", config=config)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    def set_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    set_seed(config[\"seed\"])\n",
    "    \n",
    "    num_classes = len(config[\"sample_classes\"])\n",
    "    \n",
    "    print(\"Loading CIFAR10 dataset...\")\n",
    "    trainloader, testloader, fixed_trainloader, fixed_valloader = get_cifar10_data_with_class_selection(\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        sample_classes=config[\"sample_classes\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Creating model...\")\n",
    "    if config[\"model_type\"] == \"MLP\":\n",
    "        config[\"model\"][\"output_size\"] = num_classes\n",
    "        model = MLP(**config[\"model\"])\n",
    "    # Options for CNN or VisionTransformer can be added here similarly.\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    def module_filter(name):\n",
    "        return name[-4:] == '.mlp' or 'linear' in name\n",
    "    train_monitor = NetworkMonitor(model, module_filter)\n",
    "    val_monitor = NetworkMonitor(model, module_filter)\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(name) > 0:\n",
    "            print(f\"{name}: {module.__class__.__name__}\")\n",
    "    \n",
    "    print(\"\\nStarting training with separate monitors...\")\n",
    "    history = train_with_separate_monitors(\n",
    "        model, trainloader, testloader, fixed_trainloader, fixed_valloader,\n",
    "        train_monitor, val_monitor,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        metrics_frequency=config[\"metrics_frequency\"],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Log final metrics to wandb\n",
    "    wandb.log({\n",
    "        \"final_train_loss\": history[\"train_losses\"][-1],\n",
    "        \"final_test_acc\": history[\"test_accs\"][-1]\n",
    "    })\n",
    "    \n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_training_curves(history, save_path=results_dir)\n",
    "    plot_all_metrics(history, save_path=results_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48a5c8-7f73-4fa0-86ba-1c91dc688849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd18e20-f284-47dd-9651-0312d9d73361",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in fixed_trainloader:\n",
    "    break\n",
    "inputs.shape, targets.shape\n",
    "k = 8\n",
    "X = train_monitor.activations['layers.block_11.layers.mlp'][0][:k]\n",
    "X = X.flatten(0,1).t()\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.imshow(X[:,:]); \n",
    "Y = (X/X.norm(dim=1,keepdim=True))\n",
    "C = Y @ Y.t()\n",
    "C[C<0.8] = 0\n",
    "plt.figure()\n",
    "plt.imshow(C)\n",
    "plt.colorbar()\n",
    "X.shape, targets[:k], C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b534a62-8a06-403a-8f49-bd0611b881f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layers.linear_0', 'layers.norm_0', 'layers.act_0', 'layers.linear_1', 'layers.norm_1', 'layers.act_1', 'layers.linear_2', 'layers.norm_2', 'layers.act_2', 'layers.linear_3', 'layers.norm_3', 'layers.act_3', 'layers.linear_4', 'layers.norm_4', 'layers.act_4', 'layers.linear_5', 'layers.norm_5', 'layers.act_5', 'layers.linear_6', 'layers.norm_6', 'layers.act_6', 'layers.linear_7', 'layers.norm_7', 'layers.act_7', 'layers.linear_8', 'layers.norm_8', 'layers.act_8', 'layers.linear_9', 'layers.norm_9', 'layers.act_9', 'layers.out'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs, targets in fixed_trainloader:\n",
    "    break\n",
    "\n",
    "def module_filter(name):\n",
    "    return True\n",
    "monitor = NetworkMonitor(model, module_filter)\n",
    "inputs.shape, targets.shape\n",
    "monitor.register_hooks()\n",
    "output = model(inputs.to('cuda'));\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "loss = criterion(output, targets.to('cuda'))\n",
    "loss.backward()\n",
    "monitor.activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "675fcc4a-53c4-4e48-b893-a6d707ac8d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layers.out', 'layers.act_9', 'layers.norm_9', 'layers.linear_9', 'layers.act_8', 'layers.norm_8', 'layers.linear_8', 'layers.act_7', 'layers.norm_7', 'layers.linear_7', 'layers.act_6', 'layers.norm_6', 'layers.linear_6', 'layers.act_5', 'layers.norm_5', 'layers.linear_5', 'layers.act_4', 'layers.norm_4', 'layers.linear_4', 'layers.act_3', 'layers.norm_3', 'layers.linear_3', 'layers.act_2', 'layers.norm_2', 'layers.linear_2', 'layers.act_1', 'layers.norm_1', 'layers.linear_1', 'layers.act_0', 'layers.norm_0', 'layers.linear_0'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.gradients.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea002307-4c09-4155-87bc-901c8c8e2d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1024])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = train_monitor.gradients['layers.norm_9'][0]\n",
    "# plt.hist(X.flatten())\n",
    "for f in range(10):\n",
    "    plt.hist(X[:,f],label=f'feature {f}',alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "for b in [0,1]:\n",
    "    plt.hist(X[b,:],label=f'batch {b}',alpha=0.5)\n",
    "plt.legend()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ff3abd-723c-4421-980f-ec306a4e0402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70533c-1e1c-4e2f-ac55-1ecae3b2bdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9630e-ebfd-4d25-9e3c-d9de73be8947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63906e-dfda-4bd6-8ad1-c2f79b326f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9d92d-4d04-4408-ba65-1af1da4640bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def plot_layer_metrics_lines(history, metric_name, is_train=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a line plot visualization with:\n",
    "    - x-axis: Layer index (simple order of layers in the model)\n",
    "    - multiple lines: one for each training step\n",
    "    - line color: corresponds to the step (from early to late training)\n",
    "    \"\"\"\n",
    "    prefix = \"Training\" if is_train else \"Validation\"\n",
    "    metrics_history = history['training_metrics_history'] if is_train else history['validation_metrics_history']\n",
    "    steps = history['step_history']\n",
    "    \n",
    "    # Get layer names in the order they appear in the dictionary\n",
    "    layer_names = list(metrics_history.keys())\n",
    "    \n",
    "    # Filter out layers without the requested metric\n",
    "    valid_layers = [layer for layer in layer_names if metric_name in metrics_history[layer]]\n",
    "    \n",
    "    # Check if we have layers to plot\n",
    "    if not valid_layers:\n",
    "        print(f\"No valid layers found for {metric_name}!\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with GridSpec to leave room for colorbar\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    gs = GridSpec(1, 2, width_ratios=[20, 1])  # Main plot and colorbar\n",
    "    ax = fig.add_subplot(gs[0])  # Main plot\n",
    "    cax = fig.add_subplot(gs[1])  # Colorbar axes\n",
    "    \n",
    "    # Get colormap for steps\n",
    "    cmap = matplotlib.colormaps['viridis']\n",
    "    \n",
    "    # Determine which steps to highlight\n",
    "    if len(steps) > 10:\n",
    "        # If we have many steps, only label a few of them for clarity\n",
    "        highlight_indices = [0, len(steps)//4, len(steps)//2, 3*len(steps)//4, len(steps)-1]\n",
    "    else:\n",
    "        highlight_indices = range(len(steps))\n",
    "    \n",
    "    # Plot each step as a separate line\n",
    "    for i, step in enumerate(steps):\n",
    "        # Get metric values for this step across all layers\n",
    "        values = [metrics_history[layer][metric_name][i] for layer in valid_layers]\n",
    "        \n",
    "        # Normalize step index for color mapping\n",
    "        color = cmap(i / max(1, len(steps)-1))\n",
    "        \n",
    "        # Plot with varying linewidth and marker for highlighted steps\n",
    "        if i in highlight_indices:\n",
    "            ax.plot(range(len(valid_layers)), values, '-o', \n",
    "                   color=color, linewidth=2, markersize=6,\n",
    "                   label=f'Step {step}')\n",
    "        else:\n",
    "            ax.plot(range(len(valid_layers)), values, '-', \n",
    "                   color=color, linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Layer Index (Forward Pass Order)')\n",
    "    ax.set_ylabel(metric_name.replace('_', ' ').title())\n",
    "    ax.set_title(f'{prefix} {metric_name.replace(\"_\", \" \").title()} Evolution Across Layers')\n",
    "    \n",
    "    # Add layer names as x tick labels\n",
    "    short_names = [layer.split('.')[-1] if '.' in layer else layer for layer in valid_layers]\n",
    "    ax.set_xticks(range(len(valid_layers)))\n",
    "    ax.set_xticklabels(short_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    # Add colorbar to show step progression\n",
    "    norm = plt.Normalize(min(steps), max(steps))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, cax=cax, label='Training Steps')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/lines_{prefix.lower()}_{metric_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_all_metrics_lines(history, save_path=None):\n",
    "    \"\"\"Plot line visualizations of all metrics\"\"\"\n",
    "    # Create save directory if specified\n",
    "    if save_path:\n",
    "        import os\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    # Get all metrics\n",
    "    metrics = ['dead_fraction', 'dup_fraction', 'eff_rank', 'stable_rank']\n",
    "    \n",
    "    # Plot training metrics\n",
    "    for metric in metrics:\n",
    "        plot_layer_metrics_lines(history, metric, is_train=True, save_path=save_path)\n",
    "    \n",
    "    # Plot validation metrics\n",
    "    for metric in metrics:\n",
    "        plot_layer_metrics_lines(history, metric, is_train=False, save_path=save_path)\n",
    "\n",
    "# Alternative simpler version without a colorbar\n",
    "def plot_layer_metrics_simple(history, metric_name, is_train=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Simpler version without colorbar - uses line styles to differentiate steps\n",
    "    \"\"\"\n",
    "    prefix = \"Training\" if is_train else \"Validation\"\n",
    "    metrics_history = history['training_metrics_history'] if is_train else history['validation_metrics_history']\n",
    "    steps = history['step_history']\n",
    "    \n",
    "    # Get layer names in the order they appear in the dictionary\n",
    "    layer_names = list(metrics_history.keys())\n",
    "    \n",
    "    # Filter out layers without the requested metric\n",
    "    valid_layers = [layer for layer in layer_names if metric_name in metrics_history[layer]]\n",
    "    \n",
    "    if not valid_layers:\n",
    "        print(f\"No valid layers found for {metric_name}!\")\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Choose steps to plot (if too many)\n",
    "    if len(steps) > 6:\n",
    "        # Just select a few representative steps\n",
    "        step_indices = [0, len(steps)//5, 2*len(steps)//5, 3*len(steps)//5, 4*len(steps)//5, len(steps)-1]\n",
    "        selected_steps = [steps[i] for i in step_indices]\n",
    "    else:\n",
    "        step_indices = range(len(steps))\n",
    "        selected_steps = steps\n",
    "    \n",
    "    # Different line styles and colors\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    markers = ['o', 's', '^', 'D', 'x', '*', '+']\n",
    "    \n",
    "    # Plot each selected step\n",
    "    for i, step_idx in enumerate(step_indices):\n",
    "        step = steps[step_idx]\n",
    "        values = [metrics_history[layer][metric_name][step_idx] for layer in valid_layers]\n",
    "        \n",
    "        plt.plot(range(len(valid_layers)), values, \n",
    "                 marker=markers[i % len(markers)],\n",
    "                 color=colors[i % len(colors)], \n",
    "                 label=f'Step {step}')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Layer Index (Forward Pass Order)')\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title())\n",
    "    plt.title(f'{prefix} {metric_name.replace(\"_\", \" \").title()} Evolution Across Layers')\n",
    "    \n",
    "    # Add layer names as x tick labels\n",
    "    short_names = [layer.split('.')[-1] if '.' in layer else layer for layer in valid_layers]\n",
    "    plt.xticks(range(len(valid_layers)), short_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add grid and legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/simple_{prefix.lower()}_{metric_name}.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_all_metrics_simple(history, save_path=None):\n",
    "    \"\"\"Plot simplified line visualizations of all metrics\"\"\"\n",
    "    # Create save directory if specified\n",
    "    if save_path:\n",
    "        import os\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    metrics = ['dead_fraction', 'dup_fraction', 'eff_rank', 'stable_rank']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        plot_layer_metrics_simple(history, metric, is_train=True, save_path=save_path)\n",
    "        plot_layer_metrics_simple(history, metric, is_train=False, save_path=save_path)\n",
    "\n",
    "# Example usage:\n",
    "plot_all_metrics_lines(history, save_path='./results')  # With colorbar\n",
    "# plot_all_metrics_simple(history, save_path='./results')  # Simpler version\n",
    "\n",
    "\n",
    "\n",
    "# Create a directory for saving plots\n",
    "results_dir = './results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Plot results\n",
    "print(\"\\nPlotting results...\")\n",
    "plot_training_curves(history, save_path=results_dir)\n",
    "\n",
    "# Plot metrics evolution for all layers\n",
    "plot_all_metrics(history, save_path=results_dir)\n",
    "\n",
    "# Plot comparison of metrics between training and validation\n",
    "plot_comparison_metrics(history, save_path=results_dir)\n",
    "\n",
    "# Additional analysis comparing training vs validation metrics\n",
    "print(\"\\nComparing final training vs validation metrics:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
