{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d31e34-e123-4f62-9139-b550e92c0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def num_connected_components(A, tol=1e-8, thresh = 0.98):\n",
    "    # eps = 1e-8\n",
    "    A = A - A.mean(dim=1,keepdim=True)\n",
    "    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "    Corr = A_norm @ A_norm.T\n",
    "    # print(Corr[:2,:2], Corr.shape)\n",
    "    Corr.fill_diagonal_(0)\n",
    "    Corr = Corr.abs()\n",
    "    Adj = (Corr>0.98).float()\n",
    "    # Compute the degree vector and degree matrix D\n",
    "    degrees = torch.sum(Adj, dim=1)\n",
    "    D = torch.diag(degrees)\n",
    "    # Compute the Laplacian L = D - Adj\n",
    "    L = D - Adj\n",
    "    # Compute eigenvalues of L. Since L is symmetric, use eigvalsh.\n",
    "    eigenvalues = torch.linalg.eigvalsh(L)\n",
    "    # Count the number of eigenvalues that are close to zero.\n",
    "    num_components = torch.sum(eigenvalues < tol).item()\n",
    "    return num_components\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_effective_rank(activation_matrix, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute the effective rank of an activation matrix in a numerically stable manner.\n",
    "    \n",
    "    activation_matrix: Tensor of shape (batch_size, feature_dim)\n",
    "    eps: Small constant to prevent division by zero or log(0)\n",
    "    \n",
    "    Returns: effective rank (float)\n",
    "    \"\"\"\n",
    "    # Use double precision for stability\n",
    "    act = activation_matrix.double()\n",
    "    # Compute SVD\n",
    "    U, S, V = torch.linalg.svd(act, full_matrices=False)\n",
    "    S_sum = S.sum() + eps  # Avoid division by zero\n",
    "    p = S / S_sum         # Normalized singular values\n",
    "    # Clamp probabilities to avoid log(0)\n",
    "    p_clamped = p.clamp(min=eps)\n",
    "    # Compute entropy and effective rank\n",
    "    entropy = -(p * torch.log(p_clamped)).sum()\n",
    "    eff_rank = torch.exp(entropy)\n",
    "    return eff_rank.item()\n",
    "\n",
    "# ---------------------\n",
    "# Define a Wide, Deep MLP with Hooks to Record Activations\n",
    "# ---------------------\n",
    "class WideDeepMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3*32*32, hidden_dim=1000, num_layers=10, num_classes=10):\n",
    "        super(WideDeepMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # First Linear Layer + ReLU\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        # Additional hidden layers: each has a Linear layer followed by ReLU\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim, bias=False))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        # Final classifier layer (not included in activation collection)\n",
    "        self.layers.append ( nn.Linear(hidden_dim, num_classes, bias=False) ) \n",
    "        # Dictionary to store activations (from each hidden Linear layer output)\n",
    "        self.activations = {}\n",
    "        # Register hooks on each Linear layer in self.layers (skip ReLU modules)\n",
    "        layer_idx = 0\n",
    "        for module in self.layers:\n",
    "            # store pre-activations (after applying linear )\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                module.register_forward_hook(self._get_activation_hook(layer_idx))\n",
    "                layer_idx += 1\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=(1.0/0.39)**0.5/layer.weight.shape[0]**0.5)\n",
    "\n",
    "    def _get_activation_hook(self, idx):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[f\"layer_{idx}\"] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------\n",
    "# Data Preparation\n",
    "# ---------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=6000, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# ---------------------\n",
    "# Initialize Model, Loss, Optimizer\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WideDeepMLP(num_layers=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "th = 0.999 # threshold for counting the connected components \n",
    "\n",
    "# ---------------------\n",
    "# Training Loop\n",
    "# ---------------------\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase, skip epoch 0, only report validation metrics \n",
    "    if epoch==0:\n",
    "        avg_train_loss = 0\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        num_train_batches = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            num_train_batches += 1\n",
    "        avg_train_loss = train_loss_total / num_train_batches\n",
    "\n",
    "    # Validation phase: compute average loss on validation set\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    num_val_batches = 0\n",
    "    # Also, capture activations from one batch for effective rank\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "            num_val_batches += 1\n",
    "            # For effective rank metrics, only use the first batch\n",
    "            if num_val_batches == 1:\n",
    "                val_batch_activations = {k: v for k, v in model.activations.items()}\n",
    "    avg_val_loss = val_loss_total / num_val_batches\n",
    "    print(f'epoch {epoch} CC stats using threshold = {th:.3f}')\n",
    "    for k,A in model.activations.items():\n",
    "        print(f\"layer {k} feature dim = {A.shape[1]} # of connected components: {num_connected_components(A.T,thresh=th)}\")\n",
    "\n",
    "    # Compute effective rank for each recorded hidden layer on the first validation batch\n",
    "    erank_dict = {}\n",
    "    for layer_name, act in val_batch_activations.items():\n",
    "        act_matrix = act.view(act.size(0), -1)\n",
    "        erank_dict[layer_name] = compute_effective_rank(act_matrix)\n",
    "\n",
    "    # Report training loss, validation loss, and effective rank per layer for this epoch\n",
    "    erank_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in erank_dict.items()])\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | Effective Rank per layer: {erank_str}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
