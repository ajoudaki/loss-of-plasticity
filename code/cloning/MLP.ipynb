{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb42d0d-ddb0-4de7-a43d-e79ca8d5261c",
   "metadata": {},
   "source": [
    "# Summary of the MLP Setup, Forward and Backward Passes, and Kernels\n",
    "\n",
    "## Multilayer Perceptron (MLP) Setup\n",
    "An MLP is a feedforward neural network composed of multiple layers of linear transformations followed by non-linear activation functions. In this setup:\n",
    "\n",
    "- **Layers**: The MLP consists of $L$ layers.\n",
    "- **Weights**: Each layer $l$ has a weight matrix $W^{(l)}$.\n",
    "- **No Biases**: For simplicity, we assume there are no bias terms.\n",
    "- **Activation Function**: A non-linear activation function $f$ is applied element-wise.\n",
    "\n",
    "## Architecture\n",
    "- **Input**: $x \\in \\mathbb{R}^n$\n",
    "- **Layer 1**:\n",
    "    - Pre-activation: $z^{(1)} = W^{(1)} x$\n",
    "    - Activation: $a^{(1)} = f(z^{(1)})$\n",
    "- **Layer $l$** (for $l = 2, \\dots, L$):\n",
    "    - Pre-activation: $z^{(l)} = W^{(l)} a^{(l-1)}$\n",
    "    - Activation: $a^{(l)} = f(z^{(l)})$\n",
    "- **Output**: The activations of the last layer $a^{(L)}$ represent the output of the MLP.\n",
    "\n",
    "## Forward Pass\n",
    "The forward pass computes the activations of each layer given an input $x$.\n",
    "- **Layer 1**:\n",
    "  - $z^{(1)} = W^{(1)} x$\n",
    "  - $a^{(1)} = f(z^{(1)})$\n",
    "- **Layer $l$** (for $l = 2, \\dots, L$):\n",
    "  - $z^{(l)} = W^{(l)} a^{(l-1)}$\n",
    "  - $a^{(l)} = f(z^{(l)})$\n",
    "\n",
    "## Backward Pass\n",
    "The backward pass computes the gradients of a loss function $L$ with respect to the weights using backpropagation.\n",
    "- **Initialize Gradient at Output Layer**:\n",
    "  - $\\delta^{(L)} = \\nabla_{a^{(L)}} L \\circ f'(z^{(L)})$\n",
    "  - where $\\circ$ denotes element-wise multiplication and $f'$ is the derivative of the activation function.\n",
    "\n",
    "- **Backpropagate Through Layers $l = L, L-1, \\dots, 1$**:\n",
    "- $\\delta^{(l)} = (W^{(l+1)\\top} \\delta^{(l+1)}) \\circ f'(z^{(l)})$\n",
    "\n",
    "- **Gradient with Respect to Weights**:\n",
    "  - $\\nabla_{W^{(l)}} L = \\delta^{(l)} (a^{(l-1)})^\\top$\n",
    "  - For $l = 1$, $a^{(0)} = x$.\n",
    "\n",
    "## Forward Kernel\n",
    "The forward kernel measures the similarity between the activations of two inputs $x$ and $Y$ at each layer.\n",
    "\n",
    "### Definition\n",
    "At layer $l$, the forward kernel $K^{(l)}(x,Y)$ is defined as:\n",
    "$$\n",
    "K^{(l)}(x,Y) = \\mathbb{E}[a^{(l)}(x)^\\top a^{(l)}(Y)]\n",
    "$$\n",
    "The expectation $\\mathbb{E}$ can be over the randomness in the weights or inputs if they are stochastic.\n",
    "\n",
    "### Properties\n",
    "- **Recursive Computation**: The forward kernel can be computed recursively using the kernels from the previous layer and the properties of the activation function.\n",
    "- **Influence on Learning**: The forward kernel captures how similar the representations of different inputs are at each layer, influencing the network's ability to generalize.\n",
    "\n",
    "## Backward Kernel\n",
    "The backward kernel measures the similarity between the gradients of the loss with respect to the inputs or weights for two different inputs.\n",
    "\n",
    "### Definition\n",
    "At layer $l$, the backward kernel $B^{(l)}(x,Y)$ is defined as:\n",
    "$$\n",
    "B^{(l)}(x,Y) = \\mathbb{E}[\\delta^{(l)}(x)^\\top \\delta^{(l)}(Y)]\n",
    "$$\n",
    "where $\\delta^{(l)}(x)$ is the backpropagated error at layer $l$ for input $x$.\n",
    "\n",
    "### Properties\n",
    "- **Captures Gradient Alignment**: The backward kernel indicates how aligned the gradients are for different inputs, affecting convergence during training.\n",
    "- **Dependency on Activation Function**: The computation of the backward kernel depends on the activation function and its derivative.\n",
    "\n",
    "## Kernel Equivalence Between Models\n",
    "To ensure that two models $M_1$ and $M_2$ have equal forward and backward kernels:\n",
    "\n",
    "- **Activation Functions**: Both models must use the same activation function $f$.\n",
    "- **Weight Initialization**: Weights must be initialized such that the statistical properties (variances and covariances) of the pre-activations and activations match between the models.\n",
    "- **Input Mapping**: If the input dimensions differ, inputs must be mapped appropriately (e.g., duplication and scaling) to preserve input variances.\n",
    "\n",
    "## Constructing $M_2$ from $M_1$\n",
    "Given $M_1$ with weight matrix $W_1 \\in \\mathbb{R}^{d \\times d}$, construct $M_2$ with $W_2 \\in \\mathbb{R}^{2d \\times 2d}$ as:\n",
    "$$\n",
    "W_2 = \\frac{1}{2} \\begin{bmatrix} W_1 & W_1 \\\\ W_1 & W_1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Input Mapping:\n",
    "$$\n",
    "X_2 = \\frac{1}{2} \\begin{bmatrix} x \\\\ x \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Activation Function Requirements:\n",
    "- $f(0) = 0$\n",
    "- $f$ should be homogeneous or satisfy specific symmetry properties (e.g., ReLU).\n",
    "\n",
    "### Result\n",
    "With the above construction:\n",
    "\n",
    "- **Forward Pass**: The activations in $M_2$ corresponding to the first $d$ neurons replicate those in $M_1$.\n",
    "- **Backward Pass**: The gradients with respect to the weights in $M_2$ align with those in $M_1$ for the corresponding weights.\n",
    "- **Kernel Equivalence**: Both the forward and backward kernels of $M_1$ and $M_2$ are equal, ensuring similar training dynamics.\n",
    "\n",
    "## Practical Implementation\n",
    "- **PyTorch Models**: Implement $M_1$ and $M_2$ as PyTorch models without bias terms.\n",
    "- **Initialization**: Initialize $M_2$ weights based on $M_1$ to satisfy the kernel equivalence conditions.\n",
    "- **Verification**: Compute the forward and backward passes for a given input and verify that the activations and gradients match as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02397e-68b6-4ac5-a471-8cca401cbd8f",
   "metadata": {},
   "source": [
    "# Empirical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db87fbbf-0092-46c8-8da8-5f42c65ca2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1, Test loss: 0.0075, Accuracy: 13.19%\n",
      "Epoch: 2, Test loss: 0.0072, Accuracy: 16.94%\n",
      "Epoch: 3, Test loss: 0.0070, Accuracy: 18.62%\n",
      "Epoch: 4, Test loss: 0.0068, Accuracy: 19.43%\n",
      "Epoch: 5, Test loss: 0.0067, Accuracy: 21.16%\n",
      "Epoch: 6, Test loss: 0.0067, Accuracy: 20.46%\n",
      "Epoch: 7, Test loss: 0.0067, Accuracy: 22.00%\n",
      "Epoch: 8, Test loss: 0.0066, Accuracy: 22.16%\n",
      "Epoch: 9, Test loss: 0.0066, Accuracy: 22.27%\n",
      "Epoch: 10, Test loss: 0.0066, Accuracy: 23.13%\n",
      "Files already downloaded and verified\n",
      "Hyper cloned model Test loss: 0.0066, Accuracy: 23.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "activation_functions = {\n",
    "    'relu': nn.ReLU,\n",
    "    'leaky_relu': nn.LeakyReLU,\n",
    "    'prelu': nn.PReLU,\n",
    "    'rrelu': nn.RReLU,\n",
    "    'relu6': nn.ReLU6,\n",
    "    'elu': nn.ELU,\n",
    "    'selu': nn.SELU,\n",
    "    'celu': nn.CELU,\n",
    "    'gelu': nn.GELU,\n",
    "    'sigmoid': nn.Sigmoid,\n",
    "    'tanh': nn.Tanh,\n",
    "    'softmax': nn.Softmax,\n",
    "    'log_softmax': nn.LogSoftmax,\n",
    "}\n",
    "\n",
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation='relu', norm_layer='none'):\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        input_linear = nn.Linear(input_dim, hidden_dims[0])\n",
    "        input_linear.name = \"linear_in\"  # Optional naming\n",
    "        self.layers.append(input_linear)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            linear = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            linear.name = f\"linear_{i}\"  # Optional naming\n",
    "            self.layers.append(linear)\n",
    "            \n",
    "            # Add normalization layer\n",
    "            if norm_layer == 'layernorm':\n",
    "                norm = nn.LayerNorm(hidden_dims[i+1])\n",
    "                norm.name = f\"norm_{i}\"  # Optional naming\n",
    "                self.layers.append(norm)\n",
    "            elif norm_layer == 'batchnorm':\n",
    "                norm = nn.BatchNorm1d(hidden_dims[i+1])\n",
    "                norm.name = f\"norm_{i}\"  # Optional naming\n",
    "                self.layers.append(norm)\n",
    "            elif norm_layer == 'rmsnorm':\n",
    "                norm = nn.GroupNorm(1, hidden_dims[i+1])  # RMSNorm is GroupNorm with num_groups=1\n",
    "                norm.name = f\"norm_{i}\"  # Optional naming\n",
    "                self.layers.append(norm)\n",
    "            \n",
    "            # Add activation layer\n",
    "            act = activation_functions[activation]()\n",
    "            act.name = f\"act_{i}\"  # Optional naming\n",
    "            self.layers.append(act)\n",
    "        \n",
    "        # Output layer\n",
    "        output_linear = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        output_linear.name = \"linear_out\"  # Optional naming\n",
    "        self.layers.append(output_linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def double_mlp_size(original_mlp, config, noise=0.01):\n",
    "\n",
    "    _, input_dim, output_dim = get_dataset(config['dataset'],)\n",
    "    # Create new MLP with doubled hidden layer sizes\n",
    "    hidden_dims = [dim * 2 for dim in config['hidden_dims']]\n",
    "    new_mlp = ConfigurableMLP(input_dim, hidden_dims, output_dim, config['activation'], config['norm_layer'])\n",
    "\n",
    "    # Initialize weights by cloning\n",
    "    with torch.no_grad():\n",
    "        # Input layer (mx2n)\n",
    "        new_mlp.layers[0].weight.data = torch.cat([\n",
    "            original_mlp.layers[0].weight.data,\n",
    "            original_mlp.layers[0].weight.data\n",
    "        ], dim=0) \n",
    "        new_mlp.layers[0].bias.data = torch.cat([\n",
    "            original_mlp.layers[0].bias.data,\n",
    "            original_mlp.layers[0].bias.data,\n",
    "        ],)\n",
    "\n",
    "        # Hidden layers (2mx2n)\n",
    "        for i in range(1, len(hidden_dims) + 1):\n",
    "            orig_layer = [l for l in original_mlp.layers if isinstance(l, nn.Linear)][i]\n",
    "            new_layer = [l for l in new_mlp.layers if isinstance(l, nn.Linear)][i]\n",
    "\n",
    "            new_layer.weight.data = torch.cat([\n",
    "                orig_layer.weight.data,\n",
    "                orig_layer.weight.data\n",
    "            ],dim=0) \n",
    "            new_layer.weight.data = torch.cat([\n",
    "                new_layer.weight.data,\n",
    "                new_layer.weight.data\n",
    "            ],dim=1) * 0.5\n",
    "            new_layer.bias.data = torch.cat([\n",
    "                orig_layer.bias.data,\n",
    "                orig_layer.bias.data\n",
    "            ])\n",
    "\n",
    "        # Output layer (2mxn)\n",
    "        new_mlp.layers[-1].weight.data = torch.cat([\n",
    "            original_mlp.layers[-1].weight.data,\n",
    "            original_mlp.layers[-1].weight.data\n",
    "        ], dim=1) * 0.5\n",
    "        new_mlp.layers[-1].bias.data = original_mlp.layers[-1].bias.data.clone()\n",
    "    for l in new_mlp.layers:\n",
    "        if isinstance(l,nn.Linear):\n",
    "            sd = torch.std(l.weight.data)\n",
    "            l.weight.data += torch.randn_like(l.weight.data) * sd * noise\n",
    "            sd = torch.std(l.bias.data)\n",
    "            l.bias.data += torch.randn_like(l.bias.data) * sd * noise\n",
    "\n",
    "    return new_mlp\n",
    "\n",
    "def get_dataset(name, train=True):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    if name == 'CIFAR10':\n",
    "        dataset = datasets.CIFAR10(root='./data', train=train, download=True, transform=transform)\n",
    "        input_dim = 3 * 32 * 32\n",
    "        output_dim = 10\n",
    "    elif name == 'CIFAR100':\n",
    "        dataset = datasets.CIFAR100(root='./data', train=train, download=True, transform=transform)\n",
    "        input_dim = 3 * 32 * 32\n",
    "        output_dim = 100\n",
    "    elif name == 'MNIST':\n",
    "        dataset = datasets.MNIST(root='./data', train=train, download=True, transform=transform)\n",
    "        input_dim = 28 * 28\n",
    "        output_dim = 10\n",
    "    elif name == 'FashionMNIST':\n",
    "        dataset = datasets.FashionMNIST(root='./data', train=train, download=True, transform=transform)\n",
    "        input_dim = 28 * 28\n",
    "        output_dim = 10\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "    \n",
    "    return dataset, input_dim, output_dim\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "    \n",
    "def main(config):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    train_dataset, input_dim, output_dim = get_dataset(config['dataset'], train=True)\n",
    "    test_dataset, _, _ = get_dataset(config['dataset'], train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = ConfigurableMLP(input_dim, config['hidden_dims'], output_dim, config['activation'], config['norm_layer']).to(device)\n",
    "    \n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {config['optimizer']}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, accuracy = test(model, test_loader, criterion, device)\n",
    "        print(f'Epoch: {epoch+1}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "    return model, train_loader, test_loader, criterion, device\n",
    "\n",
    "\n",
    "def get_default_config():\n",
    "    return {\n",
    "        'dataset': 'CIFAR100',\n",
    "        'hidden_dims': [512,]*5,\n",
    "        'activation': 'tanh',\n",
    "        'norm_layer': 'none',\n",
    "        'lr': 0.001,\n",
    "        'epochs': 1,\n",
    "        'optimizer': 'adam',\n",
    "        'batch_size': 512\n",
    "    }\n",
    "\n",
    "\n",
    "# config = get_default_config()\n",
    "# config['epochs'] = 10\n",
    "model, train_loader, test_loader, criterion, device = main(config)\n",
    "model2 = double_mlp_size(model, config, noise=0.1)\n",
    "test_loss, accuracy = test(model2.to(device), test_loader, criterion, device)\n",
    "print(f'Hyper cloned model Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "    # Use default configuration\n",
    "    # config = get_default_config()\n",
    "\n",
    "    # You can modify the configuration here\n",
    "    # config['dataset'] = 'CIFAR10'\n",
    "    # config['hidden_dims'] = [256, 128, 64]\n",
    "    # config['activation'] = 'tanh'\n",
    "    # config['norm_layer'] = 'batchnorm'\n",
    "    # config['lr'] = 0.0005\n",
    "    # config['epochs'] = 20\n",
    "    # config['optimizer'] = 'sgd'\n",
    "    # config['batch_size'] = 32\n",
    "\n",
    "    # main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31e0e5-e40a-4917-a2bf-9dda6761b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "def test_models(model1, model2, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    error = 0\n",
    "    correct1, correct2 = 0, 0\n",
    "    loss1, loss2 = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            o1 = model(data)\n",
    "            o2 = model2(data)\n",
    "            error += torch.mean((o1-o2)**2)\n",
    "            loss1 += criterion(o1, target).item()\n",
    "            loss2 += criterion(o2, target).item()\n",
    "            pred = o1.argmax(dim=1, keepdim=True)\n",
    "            correct1 += pred.eq(target.view_as(pred)).sum().item()            \n",
    "            pred = o2.argmax(dim=1, keepdim=True)\n",
    "            correct2 += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss1 /= len(test_loader.dataset)\n",
    "    loss2 /= len(test_loader.dataset)\n",
    "    acc1 = 100. * correct1 / len(test_loader.dataset)\n",
    "    acc2 = 100. * correct2 / len(test_loader.dataset)\n",
    "    error /= len(test_loader)\n",
    "    return loss1,acc1, loss2,acc2,error \n",
    "\n",
    "\n",
    "config = {\n",
    "    'dataset': 'CIFAR100',\n",
    "    'hidden_dims': [256,]*5,\n",
    "    'activation': 'tanh',\n",
    "    'norm_layer': 'rmsnorm',\n",
    "    'lr': 0.001,\n",
    "    'epochs': 3,\n",
    "    'gram_epochs': 3,\n",
    "    'freeze_others': True,\n",
    "    'doubling_noise': 0.0,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 512\n",
    "}\n",
    "model, train_loader, test_loader = main(config)\n",
    "model2 = double_mlp_size(model, config, noise=config['doubling_noise']).to(device)\n",
    "opt1 = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "opt2 = optim.Adam(model2.parameters(), lr=config['lr'])\n",
    "\n",
    "epoch = 0\n",
    "loss1,acc1, loss2,acc2,err  = test_models(model, model2, criterion, test_loader, device)\n",
    "print(f\"Epoch {epoch}, model discrepancy={err:.8f}, loss1={loss1:.2f},acc1={acc1:.1f}, loss2={loss2:.2f},acc2={acc2:.1f}\")\n",
    "while epoch < 3:\n",
    "    epoch += 1\n",
    "    train(model, train_loader, opt1, criterion, device)\n",
    "    train(model2, train_loader, opt2, criterion, device)\n",
    "    \n",
    "    loss1,acc1, loss2,acc2,err  = test_models(model, model2, criterion, test_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch}, model discrepancy={err:.8f}, loss1={loss1:.2f},acc1={acc1:.1f}, loss2={loss2:.2f},acc2={acc2:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "52ad1177-d93d-4471-a9b7-a34cdd7fcb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000) tensor(1.0000)\n",
      "tensor(1.0000) tensor(1.0000)\n",
      "tensor(1.0000) tensor(1.0000)\n",
      "tensor(1.0000) tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "for l in model2.layers:\n",
    "    if isinstance(l,nn.Linear):\n",
    "        if l.weight.shape[0]!=l.weight.shape[1]:\n",
    "            continue\n",
    "        w = l.weight.detach().cpu()\n",
    "        d = w.shape[0]//2\n",
    "        print(cos(w[:d,:d].flatten(),w[d:,d:].flatten()),cos(w[:d,:d].flatten(),w[:d,d:].flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "16b2900e-bc00-4b2c-81dc-fa0d1129fe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "Epoch: 1, Test loss: 0.0033, Accuracy: 42.13%\n",
      "training model\n",
      "Epoch: 2, Test loss: 0.0033, Accuracy: 42.13%\n",
      "training model\n",
      "Epoch: 3, Test loss: 0.0033, Accuracy: 42.13%\n",
      "training model\n",
      "Epoch: 4, Test loss: 0.0033, Accuracy: 42.13%\n",
      "training model\n",
      "Epoch: 5, Test loss: 0.0033, Accuracy: 42.13%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, accuracy = test(model, test_loader, criterion, device)\n",
    "    print(f'Epoch: {epoch+1}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5097c16e-b087-48a3-9976-1bc7855c5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_in\n",
      "linear_0\n",
      "act_0\n",
      "linear_1\n",
      "act_1\n",
      "linear_2\n",
      "act_2\n",
      "linear_out\n",
      "linear_in\n",
      "linear_0\n",
      "act_0\n",
      "linear_1\n",
      "act_1\n",
      "linear_2\n",
      "act_2\n",
      "linear_out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hidden_layers(model, x, layer_name):\n",
    "    hidden_layers = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        hidden_layers.append((module.name, output.detach().cpu()))\n",
    "    \n",
    "    hooks = []\n",
    "    for layer in model.layers:\n",
    "        print(layer.name)\n",
    "        hooks.append(layer.register_forward_hook(hook))\n",
    "    \n",
    "    # Forward pass\n",
    "    model(x)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return hidden_layers\n",
    "\n",
    "for batch in train_loader:\n",
    "    data, _ = batch\n",
    "    data = data.view(data.size(0), -1).to('cuda')\n",
    "    break\n",
    "\n",
    "hidden = get_hidden_layers(model, data, layer_name='linear')\n",
    "first_hidden = model.layers[0](data).cpu()\n",
    "\n",
    "hidden2 = get_hidden_layers(model2, data, layer_name='linear')\n",
    "first_hidden2 = model2.layers[0](data).cpu()\n",
    "hidden[0][1]-first_hidden, hidden2[0][1]-first_hidden2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "86a7b430-e0b2-4e12-8f97-fec878d8f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(model, batch, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    model.zero_grad()  # Zero out any existing gradients\n",
    "\n",
    "    # Unpack the batch\n",
    "    inputs, targets = batch\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    # Reshape the input if necessary (e.g., for MNIST or CIFAR)\n",
    "    inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Now the gradients are stored in the .grad attribute of each parameter\n",
    "    gradients = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append((name, param.grad.clone().cpu()))\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "gradients = compute_gradients(model, batch, criterion, device)\n",
    "gradients2 = compute_gradients(model2, batch, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7439696a-4cc2-4280-b476-d0e3eac72592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('layers.1.bias', torch.Size([500]), torch.Size([1000]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 3\n",
    "name, gradient = gradients[l]\n",
    "name, gradient2 = gradients2[l]\n",
    "name, gradient.shape, gradient2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "c3c8340e-9a06-49d6-9f18-b40e5c7569c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "M1 - Epoch: 1, Test loss: 0.0074, Accuracy: 14.82%\n",
      "M1 - Epoch: 2, Test loss: 0.0071, Accuracy: 17.26%\n",
      "M1 - Epoch: 3, Test loss: 0.0068, Accuracy: 19.41%\n",
      "M1 - Epoch: 4, Test loss: 0.0067, Accuracy: 19.70%\n",
      "M1 - Epoch: 5, Test loss: 0.0066, Accuracy: 21.55%\n",
      "M1 - Epoch: 6, Test loss: 0.0065, Accuracy: 22.43%\n",
      "M1 - Epoch: 7, Test loss: 0.0065, Accuracy: 22.38%\n",
      "Epoch 1, Train Loss: 4.018192, Test Gram Loss: 1.355158\n",
      "M2 - Final Test loss: 0.0125, Accuracy: 0.45%\n",
      "Training after kernel transfer (M3 = M2 from scratch)\n",
      "M2 - Epoch: 1, Test loss: 0.0074, Accuracy: 14.22%\n",
      "M2 - Epoch: 2, Test loss: 0.0072, Accuracy: 15.91%\n",
      "M2 - Epoch: 3, Test loss: 0.0070, Accuracy: 17.86%\n",
      "M2 - Epoch: 4, Test loss: 0.0068, Accuracy: 19.75%\n",
      "M2 - Epoch: 5, Test loss: 0.0067, Accuracy: 20.42%\n",
      "M2 - Epoch: 6, Test loss: 0.0066, Accuracy: 21.30%\n",
      "M2 - Epoch: 7, Test loss: 0.0065, Accuracy: 21.99%\n",
      "M3 - Epoch: 1, Test loss: 0.0075, Accuracy: 13.98%\n",
      "M3 - Epoch: 2, Test loss: 0.0072, Accuracy: 16.56%\n",
      "M3 - Epoch: 3, Test loss: 0.0069, Accuracy: 19.23%\n",
      "M3 - Epoch: 4, Test loss: 0.0068, Accuracy: 19.83%\n",
      "M3 - Epoch: 5, Test loss: 0.0067, Accuracy: 20.93%\n",
      "M3 - Epoch: 6, Test loss: 0.0066, Accuracy: 22.34%\n",
      "M3 - Epoch: 7, Test loss: 0.0065, Accuracy: 22.11%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "078ae4c4-0c13-45fb-9f71-0c88f5cb4562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21680.8848, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>),\n",
       " tensor(60140.7812, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for data, label in test_loader:\n",
    "    i += 1\n",
    "    data = data.view(data.size(0), -1)\n",
    "    data = data.to(device)\n",
    "    if i==11:\n",
    "        break\n",
    "out = M1(data)\n",
    "out2 = M2(data)\n",
    "G = out @ out.t()\n",
    "G2 = out2 @ out2.t()\n",
    "(G - G2).norm(), G.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "84e6f8a6-7d89-417b-afe3-1d91da2f4668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 - Epoch: 6, Test loss: 0.0032, Accuracy: 43.91%\n",
      "M1 - Epoch: 7, Test loss: 0.0030, Accuracy: 46.60%\n",
      "M1 - Epoch: 8, Test loss: 0.0029, Accuracy: 48.73%\n",
      "M1 - Epoch: 9, Test loss: 0.0028, Accuracy: 50.22%\n",
      "M1 - Epoch: 10, Test loss: 0.0028, Accuracy: 50.80%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5c026199-9172-4050-a3df-c73417f517f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2 - Epoch: 6, Test loss: 0.0132, Accuracy: 22.18%\n",
      "M2 - Epoch: 7, Test loss: 0.0132, Accuracy: 22.32%\n",
      "M2 - Epoch: 8, Test loss: 0.0132, Accuracy: 22.44%\n",
      "M2 - Epoch: 9, Test loss: 0.0132, Accuracy: 22.07%\n",
      "M2 - Epoch: 10, Test loss: 0.0132, Accuracy: 22.47%\n"
     ]
    }
   ],
   "source": [
    "# Train M1 using the original training method\n",
    "optimizer_M2 = optim.Adam(M2.parameters(), lr=config['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train(M2, train_loader, optimizer_M2, criterion, device)\n",
    "    test_loss, accuracy = test(M2, test_loader, criterion, device)\n",
    "    print(f'M2 - Epoch: {config[\"epochs\"]+epoch+1}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bbd58-7fc2-4709-9c83-352d5bf74080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
