{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c83211-6057-4202-9c9f-71f65f49ff5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56071c8c-add6-449d-9ad2-8554ef524ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "\n",
    "# ... (keep the existing imports and ConfigurableMLP class)\n",
    "\n",
    "def compute_gram_matrix(activations):\n",
    "    G = activations @ activations.t()\n",
    "    return G / activations.size(1)\n",
    "    # b, n = activations.size()\n",
    "    # return torch.bmm(activations.view(b, n, 1), activations.view(b, 1, n))\n",
    "\n",
    "def get_layer_activations(model, x):\n",
    "    activations = []\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            activations.append(x)\n",
    "    return activations\n",
    "\n",
    "def compute_gram_loss(M1, M2, data_loader, device, layer_idx):\n",
    "    M1.eval()\n",
    "    M2.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            \n",
    "            M1_activations = get_layer_activations(M1, data)[:layer_idx+1]\n",
    "            M2_activations = get_layer_activations(M2, data)[:layer_idx+1]\n",
    "            \n",
    "            G1 = compute_gram_matrix(M1_activations[-1])\n",
    "            G2 = compute_gram_matrix(M2_activations[-1])\n",
    "            \n",
    "            loss = torch.mean((G1 - G2) ** 2)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def freeze_all_layers(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_layer(model, layer_idx):\n",
    "    for param in model.layers[layer_idx].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def train_gram_matrix(M1, M2, train_loader, test_loader, optimizer, device, num_epochs=1, tolerance=1e-4, freeze_others=False):\n",
    "    M1.eval()  # Freeze M1\n",
    "    M2.train()\n",
    "    linear_layers = [li for li,l in enumerate(M1.layers) if isinstance(l, nn.Linear)]\n",
    "    for layer_idx in linear_layers:\n",
    "        print(f\"Training layer {layer_idx + 1}\")\n",
    "\n",
    "        if freeze_others:\n",
    "            # Freeze all layers\n",
    "            freeze_all_layers(M2)\n",
    "            # Unfreeze the current layer\n",
    "            unfreeze_layer(M2, layer_idx)\n",
    "            # Create a new optimizer for the unfrozen layer\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, M2.parameters()), lr=optimizer.param_groups[0]['lr'])\n",
    " \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (data, _) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                data = data.view(data.size(0), -1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get activations up to the current layer\n",
    "                with torch.no_grad():\n",
    "                    M1_activations = get_layer_activations(M1, data)[:layer_idx+1]\n",
    "                M2_activations = get_layer_activations(M2, data)[:layer_idx+1]\n",
    "                \n",
    "                # Compute Gram matrices\n",
    "                if layer_idx==linear_layers[-1]:\n",
    "                    G1 = M1_activations[-1]\n",
    "                    G2 = M2_activations[-1]\n",
    "                else:\n",
    "                    G1 = compute_gram_matrix(M1_activations[-1])\n",
    "                    G2 = compute_gram_matrix(M2_activations[-1])\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = torch.mean((G1 - G2) ** 2)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Backpropagate and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Compute test Gram loss\n",
    "            test_gram_loss = compute_gram_loss(M1, M2, test_loader, device, layer_idx)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}, Layer {layer_idx+1}, Train Loss: {avg_train_loss:.6f}, Test Gram Loss: {test_gram_loss:.6f}\")\n",
    "            \n",
    "            if avg_train_loss < tolerance:\n",
    "                print(f\"Converged at epoch {epoch+1} for layer {layer_idx+1}\")\n",
    "                break\n",
    "    \n",
    "    return M2\n",
    "\n",
    "def compute_total_gram_loss(M1, M2, data_loader, device):\n",
    "    M1.eval()\n",
    "    M2.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, _) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            \n",
    "            M1_activations = get_layer_activations(M1, data)\n",
    "            M2_activations = get_layer_activations(M2, data)\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for i in range(len(M1_activations)):\n",
    "                G1 = compute_gram_matrix(M1_activations[i])\n",
    "                G2 = compute_gram_matrix(M2_activations[i])\n",
    "                batch_loss += torch.mean((G1 - G2) ** 2)\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def train_gram_matrix_holistic(M1, M2, train_loader, test_loader, optimizer, device, num_epochs=1, tolerance=1e-4):\n",
    "    M1.eval()  # Freeze M1\n",
    "    M2.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get activations for all layers\n",
    "            with torch.no_grad():\n",
    "                M1_activations = get_layer_activations(M1, data)\n",
    "            M2_activations = get_layer_activations(M2, data)\n",
    "            \n",
    "            # Compute Gram matrices and loss for all layers\n",
    "            batch_loss = 0\n",
    "            for i in range(len(M1_activations)):\n",
    "                # for output, consider the logits themselves rather than Gram \n",
    "                if False and i==len(M1_activations)-1:\n",
    "                    G1 = M1_activations[i]\n",
    "                    G2 = M2_activations[i]\n",
    "                else:\n",
    "                    G1 = compute_gram_matrix(M1_activations[i])\n",
    "                    G2 = compute_gram_matrix(M2_activations[i])\n",
    "                batch_loss += torch.mean((G1 - G2) ** 2)\n",
    "            \n",
    "            # Backpropagate and optimize\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += batch_loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        \n",
    "        # Compute test Gram loss\n",
    "        test_gram_loss = compute_total_gram_loss(M1, M2, test_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.6f}, Test Gram Loss: {test_gram_loss:.6f}\")\n",
    "        \n",
    "        if avg_train_loss < tolerance:\n",
    "            print(f\"Converged at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return M2\n",
    "\n",
    "def main(config):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    train_dataset, input_dim, output_dim = get_dataset(config['dataset'], train=True)\n",
    "    test_dataset, _, _ = get_dataset(config['dataset'], train=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Create M1 (original model)\n",
    "    M1 = ConfigurableMLP(input_dim, config['hidden_dims'], output_dim, config['activation'], config['norm_layer']).to(device)\n",
    "    \n",
    "    # Train M1 using the original training method\n",
    "    optimizer_M1 = optim.Adam(M1.parameters(), lr=config['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        train(M1, train_loader, optimizer_M1, criterion, device)\n",
    "        test_loss, accuracy = test(M1, test_loader, criterion, device)\n",
    "        print(f'M1 - Epoch: {epoch+1}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Create M2 (model to be trained based on M1)\n",
    "    config['hidden_dims'] = [2*h for h in config['hidden_dims']]\n",
    "    M2 = ConfigurableMLP(input_dim, config['hidden_dims'], output_dim, config['activation'], config['norm_layer']).to(device)\n",
    "    \n",
    "    # Train M2 using the new Gram matrix method\n",
    "    optimizer_M2 = optim.Adam(M2.parameters(), lr=config['gram_lr'])\n",
    "    # M2 = train_gram_matrix(M1, M2, train_loader, test_loader, optimizer_M2, device, \n",
    "    #                        num_epochs=config['gram_epochs'], freeze_others=config['freeze_others'], tolerance=config['tolerance'])\n",
    "    M2 = train_gram_matrix_holistic(M1, M2, train_loader, test_loader, optimizer_M2, device, \n",
    "                                    num_epochs=config['gram_epochs'], tolerance=config['tolerance'])\n",
    "    \n",
    "    # Test M2\n",
    "    test_loss, accuracy = test(M2, test_loader, criterion, device)\n",
    "    print(f'M2 - Final Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    M3 = ConfigurableMLP(input_dim, config['hidden_dims'], output_dim, config['activation'], config['norm_layer']).to(device)\n",
    "\n",
    "    print('Training after kernel transfer (M3 = M2 from scratch)')\n",
    "    for model,name in [(M2, 'M2'), (M3,'M3'), ]:\n",
    "        # Train M1 using the original training method\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(config['epochs']):\n",
    "            train(model, train_loader, optimizer, criterion, device)\n",
    "            test_loss, accuracy = test(model, test_loader, criterion, device)\n",
    "            print(f'{name} - Epoch: {epoch+1}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return M1, M2, train_loader, test_loader\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "config = {\n",
    "    'dataset': 'CIFAR100',\n",
    "    'hidden_dims': [256]*5,\n",
    "    'activation': 'selu',\n",
    "    'norm_layer': 'rmsnorm',\n",
    "    'lr': 0.001,\n",
    "    'gram_lr': 0.001,\n",
    "    'epochs': 7,\n",
    "    'gram_epochs': 1,\n",
    "    'tolerance': 0.1,\n",
    "    'freeze_others': False,\n",
    "    'optimizer': 'adam',\n",
    "    'batch_size': 512\n",
    "}\n",
    "\n",
    "M1, M2, train_loader, test_loader = main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
