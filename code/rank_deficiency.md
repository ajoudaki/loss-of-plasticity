Thanks for the clarification! I will look into theoretical and semi-theoretical results on conditions that lead to stable rank deficiencies in neural networks, particularly in the middle or penultimate layers. The focus will be on:

- Rank collapse or stable low-rank representations under training.
- Loss functions like cross-entropy and MSE.
- The role of nonlinear activations (especially ReLU, sigmoid).
- The effect of standard stochastic gradient descent (SGD).
- Scenarios relevant to overparameterized MLPs.

I will get back to you with findings on theoretical guarantees or conditions that enforce such stable rank deficiencies.

# Stable Rank Deficiencies in Hidden Layers of Neural Networks

**Overview:** Deep neural networks often learn **low-rank representations** in their hidden layers, especially in later layers. Theory and empirical evidence suggest that as training progresses (and with sufficient overparameterization), the **effective rank** of hidden-layer activations or weight matrices can remain low or even *collapse*, rather than utilizing the full potential rank. Below, we outline key factors and findings that explain **when and why hidden layers maintain a stable, low rank** during training, focusing on theoretical conditions, loss functions, activation types, SGD dynamics, and overparameterization. 

## Monotonic Rank Reduction in Deep Networks (Theoretical Conditions)

**Composition Limits Rank:** A fundamental theoretical insight is that the **rank of a composed function (multiple layers)** cannot increase with depth – it tends to *monotonically decrease* or stay the same ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)). In other words, as inputs propagate through layers, the dimension of the learned feature manifold is non-increasing. Any layer’s output is constrained by the information in previous layers, so once a representation loses rank (becomes low-dimensional), subsequent layers cannot recover the lost dimensions. Feng *et al.* (2022) formally prove a **“universal monotonic decreasing property”** of network rank based on differential and algebraic composition rules ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)). This means **if a hidden layer becomes rank-deficient, that deficiency tends to persist or worsen in deeper layers**. For example, common operations like pooling, downsampling, or even standard fully-connected layers can significantly **drop the rank** of their outputs ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)). This structural constraint helps explain why certain middle or penultimate layers might **lock in a low rank** — once features collapse onto a low-dimensional subspace, later layers can only use those dimensions (barring injection of new information).

**Intrinsic Data Constraints:** Theoretical conditions for low rank also relate to the data and function being learned. If the target function or data manifold is inherently low-dimensional, a network might **find a low-rank representation that suffices**. In fact, the rank of a layer’s output (viewed as a function) measures the “volume of independent information” it carries ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=In%20mathematics%2C%20the%20rank%20of,learning%20that%20underlies%20many%20tasks)). Under mild assumptions, an optimal network will not inflate rank beyond what’s needed to represent the underlying data structure. This connects to ideas like the **Information Bottleneck**, where networks compress intermediate representations. Patel & Shwartz-Ziv (2024) define a “local rank” measure of feature-manifold dimensionality and show that hidden-layer rank tends to **decrease in later training, forming an emergent bottleneck** ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)) ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=particularly%20multilayer%20perceptrons%20,2023a)). In summary, **deep networks naturally compress information**, and theory indicates that **once a layer’s rank becomes low (e.g. due to a bottleneck or saturating behavior), it remains stably low or further collapses as training continues** ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)).

## Implicit Low-Rank Bias in Overparameterized Models

**Gradient Descent Favors Low Rank:** When neural networks are *overparameterized* (more parameters than data or than strictly needed), there are infinitely many solutions that fit the training data. Theory shows that standard training algorithms have an **implicit bias toward “simpler” (lower-complexity) solutions**, often reflected in low-rank structure. In particular, for deep *linear* networks (a simplified case with no nonlinear activations), it’s proven that gradient descent (or flow) on the squared loss converges to solutions with minimal **effective rank**. Gunasekar *et al.* (2017) demonstrated that a depth-2 linear network trained on a matrix factorization task converges to the minimum nuclear-norm solution ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)) – essentially the **lowest-rank weight matrix** that fits the data. More generally, later works showed that gradient descent tends to act as a *greedy rank minimizer* in linear matrix factorization problems ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)). In deep linear networks, this means if the training data can be fit by a low-rank mapping, gradient-based optimization will converge to that low-rank solution, leaving the weight matrices **rank-deficient** (many singular values driven to zero). This implicit bias arises *without* any explicit rank regularization – it’s a property of the dynamics of overparameterized models.

**Depth Amplifies Rank Bias:** Depth itself contributes to the bias. Adding more hidden layers (even linear ones) can strengthen the tendency toward low-rank solutions. Arora *et al.* (2019) found that in deep linear networks, **singular values of the effective mapping decay faster with increased depth**, indicating that deeper architectures induce a stronger preference for concentrating information in a few directions ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=solutions,rank%20solutions.%20Pennington)). In practice, **deeper networks often end up using only a subset of their neurons or directions effectively**, yielding low-rank feature matrices in intermediate and penultimate layers. Empirically, researchers observed that simply increasing depth (without changing the training objective) biases the network toward learning embeddings with lower rank ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=work%20on%20over%02parameterization%20and%20highlights,matrix%20has%20a%20low%201)). In fact, random initialized deep models already tend to map data into a low-rank feature space (as measured by the Gram matrix of features) and this bias remains after training ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=work%20on%20over%02parameterization%20and%20highlights,matrix%20has%20a%20low%201)). This “**low-rank simplicity bias**” means that among the many possible solutions in an overparameterized setting, deep networks prefer ones where features live in a smaller subspace ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=hypothesis%20that%20deeper%20networks%20are,wide%20variety%20of%20commonly%20used)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). Notably, this bias appears *robust*: it occurs across different initializations, hyperparameters, and even different optimizers ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)). In short, **overparameterization (especially increased depth) combined with gradient-based training often leads to persistent rank deficiencies** in hidden layers – the network finds a solution that uses far fewer independent dimensions than the layer width, and stays there.

**Permanent Rank Collapse:** Once a network converges to such a low-rank solution, those rank deficiencies tend to be “permanent” in the sense that continued training doesn’t reintroduce dropped dimensions. Instead, extended training can further reinforce the collapsed structure. For example, in overparameterized classifiers, one often observes **Neural Collapse** at the penultimate layer: features for each class collapse to their class mean, and those means become maximally spaced in a $C$-dimensional simplex (for $C$ classes). This implies the penultimate layer’s output has rank at most $C$ (much lower than its potential dimension). Importantly, this collapsed configuration emerges *in the late stage of training* and then remains stable ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Any extra degrees of freedom (e.g. additional neurons) simply align redundantly with this low-dimensional structure rather than expanding it. Thus, in highly overparameterized Multi-Layer Perceptrons (MLPs), it’s common to see entire directions in weight space or neuron activations effectively unused – **the network has more capacity than it needs, and gradient descent naturally finds a solution that leaves a “rank gap”**. The theoretical and empirical works above provide conditions for this: **if a low-rank solution exists (e.g. data lies on a low-dimensional manifold, or fewer than $N$ independent features are needed to classify $N$ classes), an overparameterized network will often converge to that solution, making the hidden-layer rank deficit permanent** ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). In summary, **overparameterization doesn’t increase the utilized rank – paradoxically, it often ensures some neurons or directions are redundant, locking in a low-rank representation** for middle and penultimate layers.

## Loss Functions (Cross-Entropy vs. MSE) and Rank Stability

**Cross-Entropy and Neural Collapse:** The choice of loss function can influence training dynamics and feature geometry, but common losses in classification (cross-entropy) and regression (mean squared error) ultimately can lead to similar low-rank feature outcomes. Cross-entropy (with softmax output) is known to drive networks into the **“neural collapse”** regime during the terminal phase of training, especially in classification tasks. Under cross-entropy loss, as training error approaches zero, the network continues to sharpen the separations between classes: penultimate layer features for each class become nearly identical (collapsed to their mean), and different class means maximize their mutual distances in feature space. This is a highly symmetric, low-rank configuration (features span roughly a $C-1$ dimensional subspace for $C$ classes). Papyan *et al.* (2020) observed this phenomenon empirically, and it has since been analyzed theoretically. In particular, for sufficiently large networks trained to convergence on cross-entropy, **the only global minimizers are those exhibiting neural collapse** ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). That means the **optimal solution inherently has low-rank feature structure** (the rank of the class-feature matrix equals the number of classes, not the number of features). Any features beyond that subspace are essentially unused.

**MSE Loss and Other Losses:** One might suspect that mean squared error (MSE) loss, which doesn’t push outputs to extremes the way cross-entropy does, might behave differently. However, recent theoretical work shows that **MSE loss can also lead to neural collapse at optimality** for overparameterized models. Zhou *et al.* (2022) compare cross-entropy vs. MSE and find that **both losses (and even variants like label smoothing or focal loss) yield the same neural collapse structure in the learned features**, given a large enough network trained to minimal loss ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=global%20solution%20and%20landscape%20analyses,FL%20loss%20near%20the%20optimal)). In other words, the global optimum features under MSE classification loss still have all samples of a class coincident at the class mean, and class means maximally separated (simplex configuration). This result implies that **the low-rank collapse of penultimate-layer features is not specific to cross-entropy** – it is a property of the *classification problem* itself at the optimum, rather than the particular loss formula ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Empirically, networks trained with either cross-entropy or MSE (on the same classification task) tend to end up with very similar penultimate-layer geometry and test performance, as long as they are sufficiently overparameterized and trained long enough ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=model%20assumption%2C%20we%20provide%20either,large%20and%20trained%20until%20convergence)).

**Dynamics Differences:** While the *final outcomes* under cross-entropy and MSE can be similar (both can collapse features to a low-rank configuration), the **training dynamics** may differ. Cross-entropy is an **exponential-type loss** that continues to penalize even tiny classification errors, which often drives weights to grow in norm and features to become extremely pure (one-hot like probabilities). This can encourage faster or more pronounced collapse of features during the later stages of training. In contrast, MSE (for classification) treats the problem more like regression to one-hot targets; once the network fits the training points, there’s no additional push to exaggerate the features. As a result, some studies noted that **neural collapse emerges more clearly or earlier with cross-entropy** (which implicitly maximizes class separation margin), whereas with MSE, collapse may still occur but perhaps requires more epochs or stronger overparameterization to mirror the same effect ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)). Nonetheless, in either case the penultimate layer ends up **low-rank (approximately rank = number of classes)** at convergence. For regression tasks, MSE can also lead to low-rank internal representations if the target function is low-dimensional. Overall, common loss functions like cross-entropy and MSE *do not prevent* rank deficiency in hidden layers; at optimum they often **demand it**, by driving the network toward a simplified, structured solution (neural collapse being a prime example) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).

## Activation Functions (ReLU vs. Sigmoid) and Rank Collapse

**ReLU Networks:** The choice of activation affects how information flows and whether it’s preserved or squashed, which in turn influences rank. **ReLU (Rectified Linear Unit)** activations are piecewise-linear and can introduce **dead neurons or linear dependencies** that reduce rank. For example, if a ReLU neuron’s input is negative for all training samples, that neuron outputs all zeros – effectively removing one dimension from that layer’s output (a rank drop). Even when active, ReLUs output either a linear scaling of the input or zero, so large groups of neurons can end up encoding redundant directions (especially if their weight vectors are correlated). From a theoretical perspective, understanding rank in nonlinear networks is harder than in linear ones. Recent work by Timor *et al.* (2023) shows that in contrast to linear networks, **gradient flow on ReLU networks doesn’t always minimize rank** – in fact, they construct scenarios where a shallow ReLU network does *not* find the lowest-rank solution ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=what%20extent%20this%20gen%02eralizes%20to,solutions%20in%20several%20reasonable%20settings)). This is a “negative result” indicating that ReLU’s piecewise linearity can sometimes preserve or create just enough variation to avoid trivial rank collapse in small cases. **However, on the positive side, deeper ReLU networks **are** biased toward low-rank solutions in many settings ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=datasets%20of%20size%202%29,solutions%20in%20several%20reasonable%20settings)).** In other words, depth appears to restore the implicit rank minimization tendency even with ReLUs. Sufficiently deep ReLU MLPs have been proven to favor low-rank function mappings under certain assumptions ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=datasets%20of%20size%202%29,solutions%20in%20several%20reasonable%20settings)), aligning with the empirical findings discussed earlier (depth-driven low-rank bias). Moreover, experiments show that the phenomenon of rank reduction with depth holds **across a variety of activation functions** – ReLU included. Rosenfeld *et al.* (2021) observed that increasing the number of layers **decreases the effective rank of the feature matrix for ReLU networks**, just as it does for smooth activations ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). In summary, ReLU’s on-off behavior can cause **rank collapse by deactivating neurons or duplicating features**, and while a shallow ReLU net might not automatically minimize rank, a deep ReLU net trained with SGD still tends to learn a compressed (low-rank) representation in later layers.

**Sigmoid and Saturating Activations:** **Sigmoid or tanh activations** (smooth, squashing non-linearities) have their own influence on rank. These functions saturate at extreme values (output approaching 0 or 1 for sigmoid, -1 or 1 for tanh), which can effectively **flatten variations** in input. During training, it is often observed that later in training (or in early layers of very deep networks), many sigmoid/tanh neurons enter saturation for a wide range of inputs. A neuron stuck near 0 or 1 for all inputs contributes almost no meaningful variability – it’s nearly a constant output, reducing the rank of that layer’s activation matrix (similar to a dead or saturated unit). This behavior ties in with the **Information Bottleneck (IB) theory**: Tishby and colleagues (2017) reported that networks with saturating activations show phases of training where **mutual information between the layer and the input drops**, implying the layer is discarding information and compressing its representation. This compression often corresponds to many neurons saturating, hence fewer effective degrees of freedom (a lower-dimensional manifold of activations). In practice, sigmoid networks were found to undergo an initial fitting phase followed by a **compression phase**, where the hidden-layer information (and empirically, the variance or entropy of activations) collapses significantly. This suggests that **sigmoid/tanh networks may exhibit an even stronger rank-collapse tendency in later layers** compared to ReLU, since saturation can make large portions of the layer’s output almost constant. Indeed, Patel & Shwartz-Ziv (2024) note that networks compress their feature manifolds in later training, and this was originally observed in saturating networks consistent with IB predictions ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)).

**Activation-Type Comparisons:** Empirical studies directly comparing activation functions find that **the trend of low-rank representations is quite general**. For instance, one study computed the “effective rank” of the feature Gram matrix for networks with ReLU, leaky ReLU, tanh, GELU, and even sinusoidal activations, across various depths. The result was universal: **adding more layers consistently lowered the effective rank of the penultimate-layer features for all activation types tested** ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)). While the absolute level of rank may differ (e.g. some activations might retain slightly more information in shallow layers), the *qualitative behavior* – a bias toward low-rank, structured representations – appears across the board ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=depth%20of%20the%20model,By%20hierarchically)). Therefore, common nonlinearities like ReLU and sigmoid both permit rank-deficient solutions. ReLU may allow networks to retain more piecewise-linear information in some cases (not *always* minimizing rank), but in deep and overparameterized regimes it still converges to low-rank feature mappings. Sigmoidal activations naturally encourage compression via saturation, often leading to **“rank collapse” as training goes on** (neurons saturate and outputs cluster). In both cases, once neurons either saturate or become redundant, those dimensions effectively drop out of the model’s representation. No matter the activation, a deep network that has more capacity than needed will tend to **only use a few dominant directions** in each layer’s output – yielding a stable, low-rank representation by the penultimate layer.

## Role of SGD and Training Dynamics in Rank Deficiency

**SGD as an Implicit Regularizer:** Interestingly, the stochastic nature of training (stochastic gradient descent, SGD) itself plays a role in enforcing low-rank structures. Recent theoretical analyses point out that **mini-batch SGD injects noise that biases the solution toward low-complexity (low-rank) weights**. Galanti & Poggio (2022) proved that when training deep networks with small-batch SGD (especially with common tweaks like weight decay), the noise in the gradients creates an *implicit constraint* that favors low-rank solutions ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). In fact, they show the very source of SGD noise can be viewed as a form of *rank regularization*: all else equal, SGD tends to drive the weight matrices to smaller effective rank over training ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). Their theory connects batch size, learning rate, and weight decay to the rank of the learned weight matrices ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Specifically, **smaller batch sizes and the use of weight decay act as strong low-rank regularizers** ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Intuitively, gradient noise continually perturbs the solution within the space of zero training error solutions, and it preferentially guides the weights toward configurations that are “simpler” (analogous to how noise in linear regression can favor solutions with smaller norm). In deep networks, that simplicity manifests as weight matrices with many singular values effectively zero – i.e. **SGD implicitly pushes toward rank deficiency** in each layer’s weights ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). This complements the implicit bias of gradient descent itself; even without noise, gradient descent finds a low-rank solution in many cases, and **with noise (SGD) this bias is even more pronounced**.

**Empirical Evidence in SGD Dynamics:** Empirically, one finds that SGD-trained networks often learn features in a stage-wise fashion, capturing the most significant structure first. Early in training, networks latch onto the largest variance or easiest features in the data (sometimes called *spectral bias* or *dominant feature first* learning). As Pezeshki *et al.* (2020) observed, **SGD tends to learn statistically dominant features first, which leads to learning low-rank solutions** for the data ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)). In practical terms, this means the network might initially increase the rank of representations to fit the data variation, but once it has fit the major patterns, additional training **compresses the representation**, aligning with those dominant patterns and ignoring minor ones. This is consistent with a reduction in rank as training continues. Some works also note that different optimization algorithms (SGD vs. adaptive methods like Adam) yield similar low-rank phenomena in deep models ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)), suggesting the effect is not solely due to SGD’s noise but also due to the parameterization. Nevertheless, **SGD’s stochastic noise reinforces symmetry-breaking and flat minima selection** that often coincides with low-rank weight configurations. The *theorem* by Galanti & Poggio even implies that as long as there is some SGD noise (e.g. mini-batches) and weight decay, the training will *never exactly converge* but instead keep hovering around a solution, effectively preventing the network from utilizing extra degrees of freedom that aren’t needed ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=only%20assumed%20to%20be%20differentiable,rank)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Sec,4)). In other words, the noise keeps the model near a minimal-rank solution. 

**Breaking vs. Enforcing Rank Deficiency:** One might wonder if SGD noise could ever *break* a rank deficiency (for example, jostle the network out of a bad symmetric solution where two neurons are identical, thereby increasing rank). In practice, SGD **does break exact symmetric degeneracies** (it’s rare for two neurons to remain perfectly identical during SGD training because tiny gradient differences will separate them). However, the net effect of SGD’s randomness is not to maximize rank, but rather to explore solutions of similar performance and favor the ones with smoother or simpler structure. If a certain low-rank configuration suffices to fit the data, SGD is unlikely to kick the network into a higher-rank regime without a clear benefit. In fact, small-batch SGD will add noise that, on average, drives the weights toward the flat region of the loss landscape that often corresponds to compressive solutions ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)). Thus, **SGD more often enforces rank deficiency than alleviates it**. The overall training dynamic typically sees the rank of hidden-layer activations **initially high (when learning diverse features), then stabilizing or decreasing as training converges**, especially under SGD. This aligns with the “compression phase” idea and has been measured directly: for example, Patel (2024) found that the *local rank* of features in each layer dropped during the final phase of training, indicating SGD was fine-tuning the model by compressing representations further ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=Deep%20neural%20networks%20tend%20to,This%20work%20bridges%20the%20gap)) ([Learning to Compress: Local Rank and Information Compression in Deep Neural Networks](https://arxiv.org/html/2410.07687v1#:~:text=dimensionality%20and%20demonstrate%2C%20both%20theoretically,information%20bottlenecks%20and%20representation%20learning)). In summary, **SGD (with typical settings) implicitly regularizes the network toward low-rank solutions**, making rank-deficient hidden layers a persistent outcome of the training process ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)).

## Overparameterized MLPs and Conditions for Rank Collapse

**Excess Capacity Leads to Redundancy:** In multi-layer perceptrons with more neurons or layers than necessary, certain conditions practically guarantee rank deficiencies. If an MLP has layers much wider than the intrinsic dimension of the problem, it has the freedom to realize a solution where many neurons are simply not needed. **Gradient descent will often utilize only a subset of neurons (or a subset of independent directions in those neurons)** to solve the task, leaving the rest effectively redundant. For instance, if an MLP could solve a task with an internal representation of dimension $d$, giving it $d \times 2$ neurons in that layer doesn’t force it to use all $2d$ directions – it may well use only $d$ of them (making the layer’s output rank $d$). The other neurons might end up as linear combinations of the first $d$ or stuck at zero weights. **This is a common scenario for “permanent” rank deficiency**: the network finds a low-rank configuration early (or by midpoint of training) and never needs to activate the extra capacity. Once those extra neurons settle into redundancy (e.g., duplicating another neuron’s behavior or outputting near-constant), the rank of that layer stays low. Any slight perturbation (like SGD noise) doesn’t overcome the bias to keep them redundant, because deviating would not improve the loss.

**Bottleneck Layers and Architecture:** Some networks explicitly include bottleneck layers (fewer neurons) in the middle by design; those obviously enforce low rank at that point. But even in **uniformly wide networks, an “effective bottleneck” can emerge**. Rangamani *et al.* (2023) empirically found that as one goes deeper into a trained classifier, the **within-class variability of features shrinks and class means become the dominant components** ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)). In effect, by the penultimate layer, most of the variation in features is between classes rather than within – which means the representation has roughly one degree of freedom per class (a very low rank structure) ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)). This happens even if every hidden layer had the same width; the *network learned* to create an information bottleneck near the end. Such **self-induced bottlenecks** are a hallmark of overparameterized models: they have enough layers/neurons to first separate the classes and then compress each class cluster tightly. The penultimate layer in these cases is severely rank-deficient (often close to rank $C$ for $C$ classes). Notably, this low-rank state is maintained – it doesn’t revert or expand – even if training continues longer (the clusters just tighten further). Conditions that encourage this include having **much more model capacity than the minimum required**, and optimizing to near-zero training loss (so the network can afford to project data onto a structured low-dimensional subspace that cleanly separates classes).

**Explicit Regularization and Rank:** It’s worth mentioning that some explicit regularization techniques can also encourage low-rank solutions (e.g. weight decay, which is commonly used, biases toward smaller weights that often imply fewer independent components). However, what we’ve discussed are *implicit* phenomena – even in the absence of explicit rank penalties, overparameterized MLPs tend to end up with stable rank deficiencies. In fact, adding too strong an explicit rank penalty is often unnecessary or even harmful; simply relying on the network’s inductive biases and SGD tends to find a good low-rank solution on its own ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=maps%20to%20low,parameters%2C%20and)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=shows%20that%20linear%20models%20with,demonstrate%20the%20practical%20applications%20of)). Researchers have demonstrated that **deliberately overparameterizing a model (especially in depth) can improve generalization by leveraging this implicit low-rank bias**, rather than explicitly constraining rank ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=optimizers,parameterizing%20standard%20models%20at%20training)). For example, inserting extra linear layers (which add depth but no new nonlinearity) during training was shown to yield lower-rank features and better generalization, even though the model’s theoretical capacity didn’t change ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=optimizers,parameterizing%20standard%20models%20at%20training)). This underscores that **overparameterization itself (when coupled with SGD) is a form of regularization** that often realizes as low-rank internal representations.

**Summary of Conditions:** In an overparameterized MLP, you are likely to see **permanent rank collapse** in hidden layers when: (1) the model has significantly more parameters than needed to fit the data, (2) the training is run to near convergence (zero or negligible training error), and (3) standard losses (like cross-entropy or MSE) and optimizers (SGD or similar) are used. Under these conditions, theoretical and empirical studies indicate the network will converge to a solution where hidden layers (especially the penultimate layer) have **stable, low rank** – often determined by the problem’s inherent dimensionality (such as number of classes or principal components of the data). The **rank remains low throughout the end of training** because neither the architecture nor the training dynamics provide an incentive to reinflate it. On the contrary, deep architectures and SGD training both *favor* collapsing dimensions and finding efficient, low-dimensional encodings ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)). This is why we observe phenomena like neural collapse and rank-deficient features in practice. In conclusion, hidden-layer rank deficiency in neural networks is backed by several semi-theoretical and theoretical insights: **deep compositions naturally restrict rank, gradient descent implicitly seeks low-rank solutions (especially in overparameterized setups), common loss functions do not oppose (and in fact often drive) rank collapse, nonlinear activations (ReLU or sigmoid) still end up compressing information, and SGD’s stochastic nature further encourages simplicity**. All these factors together explain **when/why a network’s middle or penultimate layers might maintain a low, stable rank despite ongoing training** ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)).

**References:**

- R. Feng *et al.*, *“Rank Diminishing in Deep Neural Networks,”* NeurIPS 2022 – establishes that network rank **monotonically decreases** with depth and analyzes rank deficiencies per layer ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=rank%2C%20focusing%20particularly%20on%20the,MLPs%2C%20and%20Transformers%20on%20ImageNet)) ([](https://proceedings.neurips.cc/paper_files/paper/2022/file/d5cd70b708f726737e2ebace18c3f71b-Paper-Conference.pdf#:~:text=decreasing%20comes%20from%20the%20structural,layer%20can%20loose%20ranks%20considerably)).  
- S. Gunasekar *et al.*, *“Implicit Regularization in Matrix Factorization,”* NeurIPS 2017 – shows gradient descent on overparametrized linear models converges to **minimum-nuclear-norm (low-rank)** solutions ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=Notable%20work%20from%20Arora%20et,%282020%3B%202021%29%20argues)).  
- J. Zhou *et al.*, *“Are All Losses Created Equal? A Neural Collapse Perspective,”* NeurIPS 2022 – proves that both cross-entropy and MSE losses (and others) yield **Neural Collapse** at global optima, implying low-rank penultimate features for sufficiently large networks ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).  
- N. Timor *et al.*, *“Implicit Regularization Towards Rank Minimization in ReLU Networks,”* ALT 2023 – finds that while shallow ReLU nets may not always minimize rank, **deeper ReLU nets are biased towards low-rank solutions** under gradient flow ([](https://proceedings.mlr.press/v201/timor23a/timor23a.pdf#:~:text=what%20extent%20this%20gen%02eralizes%20to,solutions%20in%20several%20reasonable%20settings)).  
- T. Galanti & T. Poggio, *“SGD Noise and Implicit Low-Rank Bias in Deep Neural Networks,”* CBMM Memo 2022 – theoretically shows mini-batch **SGD + weight decay imposes a low-rank constraint** on weight matrices; smaller batches and higher weight decay strengthen this effect ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=We%20analyze%20deep%20ReLU%20neural,Our%20analysis)) ([](https://cbmm.mit.edu/sites/default/files/publications/Implicit%20Rank%20Minimization.pdf#:~:text=%E2%80%A2%20In%20Thm,weight%20decay%2C%20optimization%20and%20rank)).  
- A. Rangamani *et al.*, *“Feature Learning in Deep Classifiers through Intermediate Neural Collapse,”* ICML 2023 – empirically demonstrates that **intermediate layers progressively collapse** class-wise: deeper layers have much lower within-class variance (effectively lower rank) relative to between-class variance ([Feature Learning in Deep Classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a/rangamani23a.pdf#:~:text=these%20properties%20extend%20to%20intermediate,class%20means%20aligns%20with%20the)).  
- M. Rosenfeld *et al.* (OpenReview preprint 2021), *“The Low-Rank Simplicity Bias in Deep Networks,”* – provides empirical evidence that **increasing depth consistently reduces the effective rank** of learned features across various architectures and activations, and that this bias is robust to different optimizers ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=gelu%20relu%20leaky%20relu%20tanh,matrix%20on%20a%20variety%20of)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=simplicity%20bias%20exists%20even%20after,Practically%2C%20we)).  
- Additional references: S. Arora *et al.* 2019; B. Pezeshki *et al.* 2020; H. Papyan *et al.* 2020; and others as cited above, which further support these points on rank collapse and implicit biases in deep learning ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=solutions,rank%20solutions.%20Pennington)) ([](https://minyoungg.github.io/overparam/resources/overparam-v2.pdf#:~:text=compression%2C%20Guo%20et%20al,an%20implicit%20regularizer%20during%20training)) ([Are All Losses Created Equal: A Neural Collapse Perspective | OpenReview](https://openreview.net/forum?id=8rfYWE3nyXl#:~:text=a%20recent%20line%20work%20showing,produce%20equivalent)).