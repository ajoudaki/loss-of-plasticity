{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3419b6-664f-404a-b1a0-10a14db8463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:16<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CC  =   82, e-rank =   19, soft rank = 3.902, dead features =    0 / 128\n",
      "# CC  =   55, e-rank =   31, soft rank = 2.777, dead features =    0 / 128\n",
      "# CC  =   59, e-rank =   28, soft rank = 3.066, dead features =    0 / 128\n",
      "# CC  =   78, e-rank =   30, soft rank = 3.530, dead features =    0 / 128\n",
      "# CC  =  122, e-rank =   42, soft rank = 5.018, dead features =    0 / 128\n",
      "# CC  =  128, e-rank =   63, soft rank = 7.722, dead features =    0 / 128\n",
      "# CC  =  128, e-rank =   73, soft rank = 8.130, dead features =    0 / 128\n",
      "Accuracy on test set after epoch 1: 2.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████████████████████████████████▍                                                                | 111/196 [00:09<00:07, 11.84it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 262, in pil_loader\n    with open(path, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: './tiny-imagenet-200/train/n02165456/images/n02165456_118.JPEG'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 184\u001b[0m\n\u001b[1;32m    182\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    183\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(trainloader, \u001b[38;5;241m0\u001b[39m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainloader)):\n\u001b[1;32m    185\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    187\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n    sample = self.loader(path)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 284, in default_loader\n    return pil_loader(path)\n  File \"/home/amir/miniconda3/lib/python3.10/site-packages/torchvision/datasets/folder.py\", line 262, in pil_loader\n    with open(path, \"rb\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: './tiny-imagenet-200/train/n02165456/images/n02165456_118.JPEG'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# -------------------------\n",
    "# Configurable CNN with Batch Normalization and Hidden Activations Collection\n",
    "# -------------------------\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, conv_channels, fc_hidden_units=512, dropout_p=0.25,\n",
    "                 num_classes=10, input_size=32, input_channels=3, use_batchnorm=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conv_channels (list of int): List of output channels for each convolutional layer.\n",
    "            fc_hidden_units (int): Number of neurons in the hidden fully connected layer.\n",
    "            dropout_p (float): Dropout probability.\n",
    "            num_classes (int): Number of output classes.\n",
    "            input_size (int): Height/width of the input images (assumed square).\n",
    "            input_channels (int): Number of channels in the input images.\n",
    "            use_batchnorm (bool): Whether to use batch normalization after each convolution.\n",
    "        \"\"\"\n",
    "        super(ConfigurableCNN, self).__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if self.use_batchnorm:\n",
    "            self.bn_layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = input_channels  # For colored images, this is 3.\n",
    "        self.num_pool = len(conv_channels)  # One pooling per conv layer\n",
    "        \n",
    "        # Create convolutional layers along with optional batch normalization.\n",
    "        for out_channels in conv_channels:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            if self.use_batchnorm:\n",
    "                self.bn_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Define a max pooling layer (2x2) applied after each conv block.\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Compute the spatial size after all pooling operations.\n",
    "        final_size = input_size // (2 ** self.num_pool)\n",
    "        self.flattened_size = conv_channels[-1] * final_size * final_size\n",
    "        \n",
    "        # Fully connected layers.\n",
    "        self.fc1 = nn.Linear(self.flattened_size, fc_hidden_units)\n",
    "        self.fc2 = nn.Linear(fc_hidden_units, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.act = F.tanh  # You can change this activation if desired\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        hidden_activations = []  # List to collect hidden activations\n",
    "\n",
    "        # Pass through each convolutional layer\n",
    "        for idx, conv in enumerate(self.conv_layers):\n",
    "            x = conv(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bn_layers[idx](x)\n",
    "            if return_hidden:\n",
    "                hidden_activations.append(x.detach().cpu())\n",
    "            x = self.act(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        # First fully connected layer with activation\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final fully connected layer (logits)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_hidden:\n",
    "            return x, hidden_activations\n",
    "        return x\n",
    "\n",
    "\n",
    "def eval_features(model, testloader, thresh=0.9, tol=1e-10, rank_atol=1e-2, dead_tol=0.1):\n",
    "    model.eval()\n",
    "    sample_inputs, _ = next(iter(testloader))\n",
    "    sample_inputs = sample_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, hidden_activations = model(sample_inputs, return_hidden=True)\n",
    "        \n",
    "    for act in hidden_activations:\n",
    "        # Reshape: (batch, channels, H, W) --> (channels, batch * H * W)\n",
    "        A = act.transpose(0, 1).flatten(1)\n",
    "        # Normalize each row (avoid division by zero with a small epsilon)\n",
    "        # A = A - A.mean(dim=1,keepdim=True)\n",
    "        A = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "        stds = A.std(dim=1) / A.abs().mean(dim=1)\n",
    "        # print(stds.shape, stds)\n",
    "        dead_features = (stds<dead_tol).sum()\n",
    "        # Compute cosine similarity matrix\n",
    "        C = A @ A.t()\n",
    "        rank = torch.linalg.matrix_rank(C, atol=rank_atol)\n",
    "        soft_rank = torch.trace(C)**2 / torch.trace(C @ C)\n",
    "        # Remove self-similarity by zeroing the diagonal and take absolute value.\n",
    "        C.fill_diagonal_(0)\n",
    "        C = C.abs()\n",
    "        # Create an adjacency matrix by thresholding.\n",
    "        Adj = (C > thresh).float()\n",
    "        \n",
    "        # Convert to numpy array (scipy works with numpy arrays)\n",
    "        Adj_np = Adj.numpy()\n",
    "        # Compute the number of connected components using SciPy's stable routine.\n",
    "        n_components, labels = connected_components(csgraph=Adj_np, directed=False)\n",
    "        R = Adj_np.shape[0]\n",
    "        print(f'# CC  = {n_components:4}, e-rank = {rank:4}, soft rank = {soft_rank:4.3f}, dead features = {dead_features:4} / {R}')\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Data Preparation (Tiny ImageNet with Selected Classes)\n",
    "# -------------------------\n",
    "# Specify which classes to use.\n",
    "# For Tiny ImageNet, the classes are the subfolder names in the training folder.\n",
    "# Here we select classes by their numeric index (after ImageFolder sorts the folders).\n",
    "# For example, to use the first 5 classes:\n",
    "selected_classes = range(50)  # Set to None to use all available classes\n",
    "\n",
    "# Define the image size for resizing\n",
    "input_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize images to input_size x input_size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the Tiny ImageNet datasets using ImageFolder.\n",
    "# Adjust the root paths to where you have Tiny ImageNet stored.\n",
    "trainset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/train', transform=transform)\n",
    "testset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/val', transform=transform)\n",
    "\n",
    "# If selected_classes is specified, filter the dataset to include only those classes.\n",
    "if selected_classes is not None:\n",
    "    train_indices = [i for i, (_, label) in enumerate(trainset.samples) if label in selected_classes]\n",
    "    trainset = torch.utils.data.Subset(trainset, train_indices)\n",
    "    test_indices = [i for i, (_, label) in enumerate(testset.samples) if label in selected_classes]\n",
    "    testset = torch.utils.data.Subset(testset, test_indices)\n",
    "    num_used_classes = len(selected_classes)\n",
    "else:\n",
    "    num_used_classes = len(trainset.classes)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# -------------------------\n",
    "# Model Configuration and Instantiation\n",
    "# -------------------------\n",
    "conv_channels = [128] * 7  # Example configuration\n",
    "fc_hidden_units = 128\n",
    "dropout_p = 0.25\n",
    "use_batchnorm = True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ConfigurableCNN(conv_channels, fc_hidden_units, dropout_p,\n",
    "                      num_classes=num_used_classes, input_size=input_size, input_channels=3,\n",
    "                      use_batchnorm=use_batchnorm).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "num_epochs = 100  # Adjust the number of epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm.tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = net(inputs)   # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {running_loss / len(trainloader):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Optionally, evaluate feature connectivity\n",
    "    eval_features(net, testloader, thresh=0.95)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Validation after each epoch\n",
    "    # -------------------------\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set after epoch {epoch + 1}: {accuracy:.2f}%')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# -------------------------\n",
    "# Example: Obtaining Hidden Activations\n",
    "# -------------------------\n",
    "net.eval()\n",
    "sample_inputs, _ = next(iter(testloader))\n",
    "sample_inputs = sample_inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    output, hidden_activations = net(sample_inputs, return_hidden=True)\n",
    "print(\"Collected {} hidden activations.\".format(len(hidden_activations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722580a-72c7-465b-9090-8727c6a71f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b8a5bea-cf76-473b-92dc-6247337d3dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf3d26-0202-4c11-9aa7-7b0aeeabb577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8e32d-a47c-4f59-b60b-a35a0ef2dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec7dc8-5b84-4ccf-a739-509042a84e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
