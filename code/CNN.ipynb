{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3419b6-664f-404a-b1a0-10a14db8463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:32<00:00, 11.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CC  = 11 / 128, soft rank = 1.558 / 128\n",
      "# CC  = 58 / 128, soft rank = 3.639 / 128\n",
      "# CC  = 99 / 128, soft rank = 6.255 / 128\n",
      "# CC  = 121 / 128, soft rank = 9.526 / 128\n",
      "# CC  = 119 / 128, soft rank = 9.317 / 128\n",
      "# CC  = 118 / 128, soft rank = 6.739 / 128\n",
      "# CC  = 95 / 128, soft rank = 5.652 / 128\n",
      "Accuracy on test set after epoch 1: 48.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 391/391 [00:32<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# CC  = 14 / 128, soft rank = 2.290 / 128\n",
      "# CC  = 71 / 128, soft rank = 5.417 / 128\n",
      "# CC  = 110 / 128, soft rank = 8.334 / 128\n",
      "# CC  = 126 / 128, soft rank = 11.632 / 128\n",
      "# CC  = 128 / 128, soft rank = 11.152 / 128\n",
      "# CC  = 128 / 128, soft rank = 5.240 / 128\n",
      "# CC  = 113 / 128, soft rank = 5.048 / 128\n",
      "Accuracy on test set after epoch 2: 65.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 355/391 [00:29<00:03, 11.98it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm \n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Configurable CNN with Batch Normalization and Hidden Activations Collection\n",
    "# -------------------------\n",
    "class ConfigurableCNN(nn.Module):\n",
    "    def __init__(self, conv_channels, fc_hidden_units=512, dropout_p=0.25,\n",
    "                 num_classes=10, input_size=32, input_channels=3, use_batchnorm=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conv_channels (list of int): List of output channels for each convolutional layer.\n",
    "            fc_hidden_units (int): Number of neurons in the hidden fully connected layer.\n",
    "            dropout_p (float): Dropout probability.\n",
    "            num_classes (int): Number of output classes.\n",
    "            input_size (int): Height/width of the input images (assumed square).\n",
    "            input_channels (int): Number of channels in the input images (3 for colored images).\n",
    "            use_batchnorm (bool): Whether to use batch normalization after each convolution.\n",
    "        \"\"\"\n",
    "        super(ConfigurableCNN, self).__init__()\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if self.use_batchnorm:\n",
    "            self.bn_layers = nn.ModuleList()\n",
    "        \n",
    "        in_channels = input_channels  # For colored images, this is 3.\n",
    "        self.num_pool = len(conv_channels)  # One pooling per conv layer\n",
    "        \n",
    "        # Create convolutional layers along with optional batch normalization.\n",
    "        for out_channels in conv_channels:\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            if self.use_batchnorm:\n",
    "                self.bn_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Define a max pooling layer (2x2) applied after each conv block.\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Compute the spatial size after all pooling operations.\n",
    "        final_size = input_size // (2 ** self.num_pool)\n",
    "        self.flattened_size = conv_channels[-1] * final_size * final_size\n",
    "        \n",
    "        # Fully connected layers.\n",
    "        self.fc1 = nn.Linear(self.flattened_size, fc_hidden_units)\n",
    "        self.fc2 = nn.Linear(fc_hidden_units, num_classes)\n",
    "        \n",
    "        # Dropout layer for regularization.\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.act = F.relu\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        hidden_activations = []  # List to collect hidden activations\n",
    "\n",
    "        # Pass through each convolutional layer\n",
    "        for idx, conv in enumerate(self.conv_layers):\n",
    "            x = conv(x)\n",
    "            if self.use_batchnorm:\n",
    "                x = self.bn_layers[idx](x)\n",
    "            if return_hidden:\n",
    "                hidden_activations.append(x)\n",
    "            x = self.act(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        # First fully connected layer with ReLU\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # if return_hidden:\n",
    "        #     hidden_activations.append(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final fully connected layer (logits)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_hidden:\n",
    "            return x, hidden_activations\n",
    "        return x\n",
    "\n",
    "\n",
    "def eval_features(model, testloader, thresh=0.95, tol=1e-10):\n",
    "    model.eval()\n",
    "    sample_inputs, _ = next(iter(testloader))\n",
    "    sample_inputs = sample_inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, hidden_activations = model(sample_inputs, return_hidden=True)\n",
    "        \n",
    "    for act in hidden_activations:\n",
    "        # Reshape: (batch, channels, H, W) --> (channels, batch * H * W)\n",
    "        A = act.detach().cpu().transpose(0, 1).flatten(1)\n",
    "        # Normalize each row (avoid division by zero with a small epsilon)\n",
    "        A = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "        # Compute cosine similarity matrix\n",
    "        C = A @ A.t()\n",
    "        soft_rank = torch.trace(C)**2 / torch.trace(C @ C)\n",
    "        # Remove self-similarity by zeroing the diagonal and take absolute value.\n",
    "        C.fill_diagonal_(0)\n",
    "        C = C.abs()\n",
    "        # Create an adjacency matrix by thresholding.\n",
    "        Adj = (C > thresh).float()\n",
    "        \n",
    "        # Convert to numpy array (scipy works with numpy arrays)\n",
    "        Adj_np = Adj.numpy()\n",
    "        # Compute the number of connected components using SciPy's stable routine.\n",
    "        n_components, labels = connected_components(csgraph=Adj_np, directed=False)\n",
    "        print(f'# CC  = {n_components} / {Adj_np.shape[0]}, soft rank = {soft_rank:.3f} / {Adj_np.shape[0]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Model Configuration and Instantiation\n",
    "# -------------------------\n",
    "conv_channels = [128]*7  # Example configuration\n",
    "fc_hidden_units = 128\n",
    "dropout_p = 0.25\n",
    "use_batchnorm = True\n",
    "input_size = 128\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ConfigurableCNN(conv_channels, fc_hidden_units, dropout_p,\n",
    "                      num_classes=10, input_size=input_size, input_channels=3,  # Use 3 for colored images\n",
    "                      use_batchnorm=use_batchnorm).to(device)\n",
    "\n",
    "# -------------------------\n",
    "# Data Preparation (Colored CIFAR-10 resized to 64x64)\n",
    "# -------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize images to 64x64\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for 3 channels\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# -------------------------\n",
    "# Training Loop\n",
    "# -------------------------\n",
    "num_epochs = 100  # Adjust the number of epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm.tqdm(enumerate(trainloader, 0),total=len(trainloader)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = net(inputs)  # Forward pass (default: do not collect hidden activations)\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # if i % 100 == 99:  # Print every 100 mini-batches\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {running_loss / len(trainloader):.3f}')\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Optionally, if you have a feature evaluation function:\n",
    "    eval_features(net, testloader, thresh=0.9)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Validation after each epoch\n",
    "    # -------------------------\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on test set after epoch {epoch + 1}: {accuracy:.2f}%')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# -------------------------\n",
    "# Example: Obtaining Hidden Activations\n",
    "# -------------------------\n",
    "# To obtain the hidden activations for a batch of inputs:\n",
    "net.eval()\n",
    "sample_inputs, _ = next(iter(testloader))\n",
    "sample_inputs = sample_inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    output, hidden_activations = net(sample_inputs, return_hidden=True)\n",
    "print(\"Collected {} hidden activations.\".format(len(hidden_activations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b722580a-72c7-465b-9090-8727c6a71f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:\u001b[38;5;241m10000\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b8a5bea-cf76-473b-92dc-6247337d3dfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 510).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigvalsh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# C = C / torch.diag(C).mean()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m C\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.eigh: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated eigenvalues (error code: 510)."
     ]
    }
   ],
   "source": [
    "# torch.linalg.eigvalsh(C)\n",
    "# C = C / torch.diag(C).mean()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fddf3d26-0202-4c11-9aa7-7b0aeeabb577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy input shape: torch.Size([2, 3, 4, 4])\n",
      "\n",
      "BatchNorm2d epsilon (eps): 1e-05\n",
      "\n",
      "Input statistics per channel:\n",
      " Channel 0: mean = 12.7500, var = 71.1875\n",
      " Channel 1: mean = 12.7500, var = 71.1875\n",
      " Channel 2: mean = 24.0000, var = 276.5000\n",
      "\n",
      "Output statistics per channel after BatchNorm2d:\n",
      " Channel 0: mean = -0.0000, var = 1.0000\n",
      " Channel 1: mean = 0.0000, var = 1.0000\n",
      " Channel 2: mean = 0.0000, var = 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a toy tensor with shape (N, C, H, W)\n",
    "# For example, N=2 (batch size), C=3 (channels), H=W=4 (spatial dimensions)\n",
    "toy_input = torch.tensor([\n",
    "    [\n",
    "        [[1.0,  2.0,  3.0,  4.0],\n",
    "         [5.0,  6.0,  7.0,  8.0],\n",
    "         [9.0, 10.0, 11.0, 12.0],\n",
    "         [13.0, 14.0, 15.0, 16.0]],\n",
    "        \n",
    "        [[16.0, 15.0, 14.0, 13.0],\n",
    "         [12.0, 11.0, 10.0,  9.0],\n",
    "         [8.0,   7.0,  6.0,  5.0],\n",
    "         [4.0,   3.0,  2.0,  1.0]],\n",
    "        \n",
    "        [[1.0,  3.0,  5.0,  7.0],\n",
    "         [9.0, 11.0, 13.0, 15.0],\n",
    "         [17.0, 19.0, 21.0, 23.0],\n",
    "         [25.0, 27.0, 29.0, 31.0]]\n",
    "    ],\n",
    "    [\n",
    "        [[2.0,  4.0,  6.0,  8.0],\n",
    "         [10.0, 12.0, 14.0, 16.0],\n",
    "         [18.0, 20.0, 22.0, 24.0],\n",
    "         [26.0, 28.0, 30.0, 32.0]],\n",
    "        \n",
    "        [[32.0, 30.0, 28.0, 26.0],\n",
    "         [24.0, 22.0, 20.0, 18.0],\n",
    "         [16.0, 14.0, 12.0, 10.0],\n",
    "         [8.0,   6.0,  4.0,  2.0]],\n",
    "        \n",
    "        [[2.0,  6.0, 10.0, 14.0],\n",
    "         [18.0, 22.0, 26.0, 30.0],\n",
    "         [34.0, 38.0, 42.0, 46.0],\n",
    "         [50.0, 54.0, 58.0, 62.0]]\n",
    "    ]\n",
    "])\n",
    "print(\"Toy input shape:\", toy_input.shape)\n",
    "\n",
    "# Define a BatchNorm2d layer for 3 channels.\n",
    "# We set affine=False so that no additional scaling (gamma) or shifting (beta) is applied;\n",
    "# this lets us see the raw normalization: (x - mean) / sqrt(var + eps).\n",
    "bn = nn.BatchNorm2d(num_features=3, affine=False)\n",
    "\n",
    "# Set the BatchNorm layer to training mode so it uses the batch statistics of toy_input.\n",
    "bn.train()\n",
    "\n",
    "# Apply BatchNorm2d on the toy tensor.\n",
    "toy_output = bn(toy_input)\n",
    "\n",
    "# Print the small epsilon value used for numerical stability.\n",
    "print(\"\\nBatchNorm2d epsilon (eps):\", bn.eps)\n",
    "\n",
    "# Helper function to compute per-channel statistics over the batch and spatial dimensions.\n",
    "def compute_channel_stats(x):\n",
    "    # x shape: (N, C, H, W)\n",
    "    N, C, H, W = x.shape\n",
    "    stats = {}\n",
    "    for c in range(C):\n",
    "        # Compute mean and variance over dimensions (N, H, W) for each channel.\n",
    "        channel_data = x[:, c, :, :]\n",
    "        mean = channel_data.mean().item()\n",
    "        var = channel_data.var(unbiased=False).item()  # population variance\n",
    "        stats[c] = (mean, var)\n",
    "    return stats\n",
    "\n",
    "# Compute and display the input statistics.\n",
    "input_stats = compute_channel_stats(toy_input)\n",
    "print(\"\\nInput statistics per channel:\")\n",
    "for c in range(3):\n",
    "    mean, var = input_stats[c]\n",
    "    print(f\" Channel {c}: mean = {mean:.4f}, var = {var:.4f}\")\n",
    "\n",
    "# Compute and display the output statistics.\n",
    "output_stats = compute_channel_stats(toy_output)\n",
    "print(\"\\nOutput statistics per channel after BatchNorm2d:\")\n",
    "for c in range(3):\n",
    "    mean, var = output_stats[c]\n",
    "    print(f\" Channel {c}: mean = {mean:.4f}, var = {var:.4f}\")\n",
    "\n",
    "# For clarity, here's what BatchNorm2d is doing:\n",
    "# For each channel, it subtracts the mean (computed over N, H, W) and divides by the standard deviation.\n",
    "# That is:\n",
    "#    normalized = (x - mean) / sqrt(variance + eps)\n",
    "# As you can see from the output statistics, the normalized tensor has (approximately) zero mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17c8e32d-a47c-4f59-b60b-a35a0ef2dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input statistics per channel:\n",
      " Channel 0: mean = -0.1977, var = 1.1304\n",
      " Channel 1: mean = -0.2741, var = 0.9774\n",
      " Channel 2: mean = -0.2215, var = 0.8813\n"
     ]
    }
   ],
   "source": [
    "# Compute and display the input statistics.\n",
    "input_stats = compute_channel_stats(A)\n",
    "print(\"\\nInput statistics per channel:\")\n",
    "for c in range(3):\n",
    "    mean, var = input_stats[c]\n",
    "    print(f\" Channel {c}: mean = {mean:.4f}, var = {var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec7dc8-5b84-4ccf-a739-509042a84e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
