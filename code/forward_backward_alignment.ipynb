{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf0453b-3a4a-423c-b126-4acfa4f86a57",
   "metadata": {},
   "source": [
    "# Recap and background\n",
    "Here we recap the findings we have and review some background on  continual learning from the point of view of Ricahrd Sutton. \n",
    "\n",
    "*Outline* We proved in the draft that if we duplicate a hyperclone a model in a super-symmetric way (more accurately, forward and backward symmetry hold), then the forward and backward vectors of the network are cloned. More concretely, the forward and backward of the model are essentially the cloned (duplicated) versions of a smaller model from which they are cloned. This situation has a very dramatic consequence that, we can perfectly predict the training dynamics of the larger model with a smaller model, with the only caveat that the learning rate for different layers are set in a layer and module-dependent manner. This suggests that this particular way of cloning may catasrophically limit the model's ability to learn, because it is at best as good as the smaller model. This brings this result closer to the notion of *loss of plasticity* in continual learning literature, which we will very briefly review here. \n",
    "\n",
    "*Richard Sutton's view on Continual Learning (CL)*: Through a series of works on Richard Sutton, he proposes that one of the most fundumental p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8440f324-d8cb-4a48-9464-5afe04d2445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian for single input shape: torch.Size([10, 256])\n",
      "Jacobian for batch input shape: torch.Size([5, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.func as functorch\n",
    "\n",
    "# Define a customizable MLP with a Sequential container\n",
    "class CustomizableMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_sizes (list of int): Sizes of the hidden layers.\n",
    "            output_size (int): Number of output features.\n",
    "            activation (nn.Module): Activation function (e.g., nn.ReLU, nn.Tanh).\n",
    "        \"\"\"\n",
    "        super(CustomizableMLP, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(activation())\n",
    "            in_features = hidden_size\n",
    "        # Output layer (no activation here, but you can add one if desired)\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 784\n",
    "hidden_sizes = [256, 128, 64]\n",
    "output_size = 10\n",
    "model = CustomizableMLP(input_size, hidden_sizes, output_size, activation=nn.ReLU)\n",
    "\n",
    "# --- Slicing the model ---\n",
    "# Let's say we want the Jacobian from the input up to the output of the first hidden block.\n",
    "# Since our Sequential container is a list of modules, you can slice it.\n",
    "# For example, if we want to use the first two modules (a Linear layer and its activation):\n",
    "# layer_slice = model.layers[0:2]  # This represents: Linear(input_size, 256) followed by ReLU\n",
    "slice1 = model.layers[0:1]\n",
    "slice2 = model.layers[1:]\n",
    "\n",
    "# For a single input vector (shape: [input_size]), the Jacobian will have shape [output_dim, input_size]\n",
    "x_single = torch.randn(input_size)\n",
    "h_single = slice1(x_single) \n",
    "jacobian_single = functorch.jacrev(slice2)(h_single)\n",
    "print(\"Jacobian for single input shape:\", jacobian_single.shape)\n",
    "# For example, if the Linear layer outputs 256 features, then jacobian_single.shape will be [256, 784]\n",
    "\n",
    "# For a batch of inputs, we can use vmap to compute a Jacobian per input.\n",
    "# Assume a batch of 5 inputs (shape: [5, input_size])\n",
    "x_batch = torch.randn(5, input_size)\n",
    "h_batch = slice1(x_batch)\n",
    "jacobian_batch = functorch.vmap(functorch.jacrev(slice2))(h_batch)\n",
    "print(\"Jacobian for batch input shape:\", jacobian_batch.shape)\n",
    "# Expected shape: [batch, output_dim, input_size], e.g., [5, 256, 784]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fdb72-24a7-4d62-b88b-226400dfc6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
