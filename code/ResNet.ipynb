{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ff1f8-8213-4a55-969b-ff92ee7a6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# Utility Functions\n",
    "# -------------------------------\n",
    "def num_connected_components(A, tol=1e-8, thresh=0.98):\n",
    "    A = A - A.mean(dim=1,keepdim=True)\n",
    "    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "    Corr = A_norm @ A_norm.T\n",
    "    Corr.fill_diagonal_(0)\n",
    "    Corr = Corr.abs()\n",
    "    Adj = (Corr > thresh).float()\n",
    "    degrees = torch.sum(Adj, dim=1)\n",
    "    D = torch.diag(degrees)\n",
    "    L = D - Adj\n",
    "    eigenvalues = torch.linalg.eigvalsh(L)\n",
    "    num_components = torch.sum(eigenvalues < tol).item()\n",
    "    return num_components\n",
    "\n",
    "def compute_effective_rank(activation_matrix, eps=1e-12):\n",
    "    act = activation_matrix.double()\n",
    "    U, S, V = torch.linalg.svd(act, full_matrices=False)\n",
    "    S_sum = S.sum() + eps\n",
    "    p = S / S_sum\n",
    "    p_clamped = p.clamp(min=eps)\n",
    "    entropy = -(p * torch.log(p_clamped)).sum()\n",
    "    eff_rank = torch.exp(entropy)\n",
    "    return eff_rank.item()\n",
    "\n",
    "# -------------------------------\n",
    "# Define a ResNet with Hooks to Record Activations\n",
    "# -------------------------------\n",
    "# We use torchvision.models.resnet18 as our candidate ResNet.\n",
    "# We insert forward hooks into chosen layers (e.g., after layer1, layer2, layer3, and layer4).\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNetWithHooks(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNetWithHooks, self).__init__()\n",
    "        # Load a pre-defined resnet18\n",
    "        self.resnet = resnet18(pretrained=False, num_classes=num_classes)\n",
    "        # Dictionary to store activations\n",
    "        self.activations = {}\n",
    "        # Register hooks on chosen layers: here we choose layer1, layer2, layer3, and layer4.\n",
    "        self.resnet.layer1.register_forward_hook(self._get_activation_hook('layer1'))\n",
    "        self.resnet.layer2.register_forward_hook(self._get_activation_hook('layer2'))\n",
    "        self.resnet.layer3.register_forward_hook(self._get_activation_hook('layer3'))\n",
    "        self.resnet.layer4.register_forward_hook(self._get_activation_hook('layer4'))\n",
    "\n",
    "    def _get_activation_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            # Save output activation\n",
    "            self.activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Data Preparation\n",
    "# -------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=1024, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize Model, Loss, Optimizer\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetWithHooks(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                     momentum=0.9, weight_decay=5e-4)\n",
    "th = 0.90  # threshold for counting connected components\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 0:\n",
    "        avg_train_loss = 0.0\n",
    "    # Training phase (skip reporting metrics at epoch 0)\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        num_train_batches = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            num_train_batches += 1\n",
    "        avg_train_loss = train_loss_total / num_train_batches\n",
    "\n",
    "    # Validation phase: compute average loss and record activations from one batch\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    num_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "            num_val_batches += 1\n",
    "            # Capture activations from the first validation batch\n",
    "            if num_val_batches == 1:\n",
    "                val_batch_activations = {k: v for k, v in model.activations.items()}\n",
    "    avg_val_loss = val_loss_total / num_val_batches\n",
    "\n",
    "    print(f'\\nEpoch {epoch} Connected Components stats (threshold = {th:.3f}):')\n",
    "    # For each hooked layer, compute number of connected components.\n",
    "    for name, A in model.activations.items():\n",
    "        # Flatten spatial dimensions if necessary: A shape is [B, C, H, W]\n",
    "        if A.dim() > 2:\n",
    "            A_flat = A.transpose(0,1).flatten(1).transpose(0,1)  # shape [BH*W, C]\n",
    "            # print(A.shape, A_flat.shape)\n",
    "            # assert False\n",
    "            # We average over spatial locations\n",
    "            # A_flat = A_flat.mean(dim=2)  # shape [B, C]\n",
    "        else:\n",
    "            A_flat = A\n",
    "        # Transpose so each row corresponds to a feature.\n",
    "        num_cc = num_connected_components(A_flat.T, thresh=th)\n",
    "        print(f\"Layer {name} feature dim = {A_flat.shape[1]}  # connected components: {num_cc}\")\n",
    "\n",
    "    # Compute effective rank for each recorded layer from the first validation batch\n",
    "    erank_dict = {}\n",
    "    for name, act in val_batch_activations.items():\n",
    "        if act.dim() > 2:\n",
    "            act_flat = act.flatten(2).mean(dim=2)\n",
    "        else:\n",
    "            act_flat = act\n",
    "        erank = compute_effective_rank(act_flat)\n",
    "        erank_dict[name] = erank\n",
    "    erank_str = \", \".join([f\"{name}: {val:.2f}\" for name, val in erank_dict.items()])\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | Effective Rank per layer: {erank_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65078a9d-bde5-434a-a35c-5c952646613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def collect_activations(model, dataloader, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    Collect activations for multiple batches of data\n",
    "    \n",
    "    Args:\n",
    "        model: ResNetActivations model\n",
    "        dataloader: DataLoader instance\n",
    "        device: torch device\n",
    "        num_batches: Number of batches to process (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        list of dictionaries containing activations for each batch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_batch_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            if num_batches is not None and batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Collect activations\n",
    "            batch_activations = model.get_all_activations()\n",
    "            batch_activations['labels'] = labels.cpu()\n",
    "            all_batch_activations.append(batch_activations)\n",
    "    \n",
    "    return all_batch_activations\n",
    "\n",
    "def num_connected_components(A, tol=1e-8, thresh=0.98):\n",
    "    A = A - A.mean(dim=1,keepdim=True)\n",
    "    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "    Corr = A_norm @ A_norm.T\n",
    "    # print(f'Corr dims = {Corr.shape}')\n",
    "    Corr.fill_diagonal_(0)\n",
    "    Corr = Corr.abs()\n",
    "    # print(f'Corr dims = {Corr.shape} Corr max (off-diag) median per unit: {Corr.max(dim=1).values.median():.3f}')\n",
    "    Adj = (Corr > thresh).float()\n",
    "    degrees = torch.sum(Adj, dim=1)\n",
    "    D = torch.diag(degrees)\n",
    "    L = D - Adj\n",
    "    eigenvalues = torch.linalg.eigvalsh(L)\n",
    "    num_components = torch.sum(eigenvalues < tol).item()\n",
    "    return num_components\n",
    "\n",
    "def report_CC_stats(model, loader, thresh, num_batches = 10):\n",
    "    activations = collect_activations(model, loader, device, num_batches=num_batches)\n",
    "    \n",
    "    for k in activations[0]['main'].keys():\n",
    "        \n",
    "        A = torch.concat([act['main'][k].detach().cpu() for act in activations])\n",
    "        A_flat = A.flatten(1)\n",
    "        # A = A.transpose(0,1).flatten(1)#.transpose(0,1)\n",
    "        # print('A shape = ', A.shape, ' A flat shape = ', A_flat.shape)\n",
    "        try:\n",
    "            CC = num_connected_components(A_flat,thresh=thresh)\n",
    "            print('key = ', k, ' CC = ', CC, ' rank(A) = ', min(A_flat.shape))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "valset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                     download=True, transform=transform_val)\n",
    "valloader = DataLoader(valset, batch_size=128,\n",
    "                      shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define ResNet model\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=False)  # Changed to not use inplace\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=False)  # Changed to not use inplace\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Store activations\n",
    "        self.activations = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        self.activations['relu1'] = out.detach()\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu2(out)\n",
    "        self.activations['relu2'] = out.detach()\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1,\n",
    "                              bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=False)  # Changed to not use inplace\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 512, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 512, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Store all activations\n",
    "        self.activations = {}\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                         kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        self.activations['initial_relu'] = x.detach()\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        self.activations['layer1'] = x.detach()\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        self.activations['layer2'] = x.detach()\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        self.activations['layer3'] = x.detach()\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        self.activations['layer4'] = x.detach()\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        self.activations['avgpool'] = x.detach()\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.fc(x)\n",
    "        self.activations['logits'] = logits.detach()\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def get_all_activations(self):\n",
    "        \"\"\"Collect all activations from the model, including those from BasicBlocks\"\"\"\n",
    "        all_activations = defaultdict(dict)\n",
    "        \n",
    "        # Get main activations\n",
    "        for name, activation in self.activations.items():\n",
    "            all_activations['main'][name] = activation\n",
    "        \n",
    "        # Get activations from each BasicBlock\n",
    "        for layer_idx, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):\n",
    "            for block_idx, block in enumerate(layer):\n",
    "                for act_name, activation in block.activations.items():\n",
    "                    all_activations[f'layer{layer_idx+1}_block{block_idx+1}'][act_name] = activation\n",
    "        \n",
    "        return all_activations\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = trainloader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = valloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase=='val':\n",
    "                print(f'report CC stats for phase {phase}')\n",
    "                report_CC_stats(model, dataloader, thresh=0.9,num_batches=10)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2]).to(device)  # ResNet18\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
    "                     momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Train and evaluate\n",
    "model = train_model(model, criterion, optimizer, num_epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
