{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d31e34-e123-4f62-9139-b550e92c0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "epoch 0 CC stats using threshold = 0.999\n",
      "layer layer_0 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_1 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_2 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_3 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_4 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_5 feature dim = 1000 # of connected components: 1000\n",
      "layer layer_6 feature dim = 1000 # of connected components: 1000\n",
      "Epoch 1: Train Loss: 0.0000, Val Loss: 15.2584 | Effective Rank per layer: layer_0: 658.19, layer_1: 671.23, layer_2: 675.84, layer_3: 678.75, layer_4: 681.63, layer_5: 684.59, layer_6: 687.17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def num_connected_components(A, tol=1e-8, thresh = 0.98):\n",
    "    # eps = 1e-8\n",
    "    A = A - A.mean(dim=1,keepdim=True)\n",
    "    A_norm = A / (A.norm(dim=1, keepdim=True) + tol)\n",
    "    Corr = A_norm @ A_norm.T\n",
    "    # print(Corr[:2,:2], Corr.shape)\n",
    "    Corr.fill_diagonal_(0)\n",
    "    Corr = Corr.abs()\n",
    "    Adj = (Corr>0.98\n",
    "          ).float()\n",
    "    # Compute the degree vector and degree matrix D\n",
    "    degrees = torch.sum(Adj, dim=1)\n",
    "    D = torch.diag(degrees)\n",
    "    # Compute the Laplacian L = D - Adj\n",
    "    L = D - Adj\n",
    "    # Compute eigenvalues of L. Since L is symmetric, use eigvalsh.\n",
    "    eigenvalues = torch.linalg.eigvalsh(L)\n",
    "    # Count the number of eigenvalues that are close to zero.\n",
    "    num_components = torch.sum(eigenvalues < tol).item()\n",
    "    return num_components\n",
    "\n",
    "\n",
    "def compute_effective_rank(activation_matrix, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute the effective rank of an activation matrix in a numerically stable manner.\n",
    "    \n",
    "    activation_matrix: Tensor of shape (batch_size, feature_dim)\n",
    "    eps: Small constant to prevent division by zero or log(0)\n",
    "    \n",
    "    Returns: effective rank (float)\n",
    "    \"\"\"\n",
    "    # Use double precision for stability\n",
    "    act = activation_matrix.double()\n",
    "    # Compute SVD\n",
    "    U, S, V = torch.linalg.svd(act, full_matrices=False)\n",
    "    S_sum = S.sum() + eps  # Avoid division by zero\n",
    "    p = S / S_sum         # Normalized singular values\n",
    "    # Clamp probabilities to avoid log(0)\n",
    "    p_clamped = p.clamp(min=eps)\n",
    "    # Compute entropy and effective rank\n",
    "    entropy = -(p * torch.log(p_clamped)).sum()\n",
    "    eff_rank = torch.exp(entropy)\n",
    "    return eff_rank.item()\n",
    "\n",
    "# ---------------------\n",
    "# Define a Wide, Deep MLP with Hooks to Record Activations\n",
    "# ---------------------\n",
    "class WideDeepMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3*32*32, hidden_dim=1000, num_layers=10, num_classes=10):\n",
    "        super(WideDeepMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # First Linear Layer + ReLU\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        # Additional hidden layers: each has a Linear layer followed by ReLU\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim, bias=False))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        # Final classifier layer (not included in activation collection)\n",
    "        self.layers.append ( nn.Linear(hidden_dim, num_classes, bias=False) ) \n",
    "        # Dictionary to store activations (from each hidden Linear layer output)\n",
    "        self.activations = {}\n",
    "        # Register hooks on each Linear layer in self.layers (skip ReLU modules)\n",
    "        layer_idx = 0\n",
    "        for module in self.layers:\n",
    "            # store pre-activations (after applying linear )\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                module.register_forward_hook(self._get_activation_hook(layer_idx))\n",
    "                layer_idx += 1\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=(1.0/0.39)**0.5/layer.weight.shape[0]**0.5)\n",
    "\n",
    "    def _get_activation_hook(self, idx):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[f\"layer_{idx}\"] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------\n",
    "# Data Preparation\n",
    "# ---------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_set   = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=512, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_set, batch_size=6000, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# ---------------------\n",
    "# Initialize Model, Loss, Optimizer\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WideDeepMLP(num_layers=7).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "th = 0.999 # threshold for counting the connected components \n",
    "\n",
    "# ---------------------\n",
    "# Training Loop`\n",
    "# ---------------------\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase, skip epoch 0, only report validation metrics \n",
    "    if epoch==0:\n",
    "        avg_train_loss = 0\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        num_train_batches = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            num_train_batches += 1\n",
    "        avg_train_loss = train_loss_total / num_train_batches\n",
    "\n",
    "    # Validation phase: compute average loss on validation set\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    num_val_batches = 0\n",
    "    # Also, capture activations from one batch for effective rank\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss_total += loss.item()\n",
    "            num_val_batches += 1\n",
    "            # For effective rank metrics, only use the first batch\n",
    "            if num_val_batches == 1:\n",
    "                val_batch_activations = {k: v for k, v in model.activations.items()}\n",
    "    avg_val_loss = val_loss_total / num_val_batches\n",
    "    print(f'epoch {epoch} CC stats using threshold = {th:.3f}')\n",
    "    for k,A in model.activations.items():\n",
    "        print(f\"layer {k} feature dim = {A.shape[1]} # of connected components: {num_connected_components(A.T,thresh=th)}\")\n",
    "\n",
    "    # Compute effective rank for each recorded hidden layer on the first validation batch\n",
    "    erank_dict = {}\n",
    "    for layer_name, act in val_batch_activations.items():\n",
    "        act_matrix = act.view(act.size(0), -1)\n",
    "        erank_dict[layer_name] = compute_effective_rank(act_matrix)\n",
    "\n",
    "    # Report training loss, validation loss, and effective rank per layer for this epoch\n",
    "    erank_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in erank_dict.items()])\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f} | Effective Rank per layer: {erank_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97446086-11df-4316-b981-c6c419f33a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 samples\n",
      "layers.0.weight: torch.Size([1000, 3072])\n",
      "layers.2.weight: torch.Size([1000, 1000])\n",
      "layers.4.weight: torch.Size([1000, 1000])\n",
      "layers.6.weight: torch.Size([1000, 1000])\n",
      "layers.8.weight: torch.Size([1000, 1000])\n",
      "layers.10.weight: torch.Size([1000, 1000])\n",
      "layers.12.weight: torch.Size([1000, 1000])\n",
      "layers.14.weight: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "def compute_per_sample_gradients(model, data_loader, criterion, device, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Compute gradients with memory optimization.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model\n",
    "        data_loader (DataLoader): Data loader\n",
    "        criterion (callable): Loss function\n",
    "        device (torch.device): Device for computation\n",
    "        max_samples (int): Maximum number of samples to process\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    per_sample_grads = []\n",
    "    samples_processed = 0\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Process smaller mini-batches\n",
    "        mini_batch_size = 10  # Adjust this based on your GPU memory\n",
    "        for i in range(0, batch_size, mini_batch_size):\n",
    "            if samples_processed >= max_samples:\n",
    "                break\n",
    "                \n",
    "            end_idx = min(i + mini_batch_size, batch_size)\n",
    "            mini_images = images[i:end_idx].to(device)\n",
    "            mini_labels = labels[i:end_idx].to(device)\n",
    "            \n",
    "            for j in range(mini_images.size(0)):\n",
    "                # Process single sample\n",
    "                sample_image = mini_images[j:j+1]\n",
    "                sample_label = mini_labels[j:j+1]\n",
    "                \n",
    "                outputs = model(sample_image)\n",
    "                loss = criterion(outputs, sample_label)\n",
    "                \n",
    "                # Compute and store gradients\n",
    "                grads = torch.autograd.grad(loss, model.parameters(), create_graph=False)\n",
    "                \n",
    "                sample_grad = {}\n",
    "                for (name, _), grad in zip(model.named_parameters(), grads):\n",
    "                    # Store gradients as CPU tensors to save GPU memory\n",
    "                    sample_grad[name] = grad.detach().cpu()\n",
    "                \n",
    "                per_sample_grads.append(sample_grad)\n",
    "                samples_processed += 1\n",
    "                \n",
    "                # Clear some memory\n",
    "                del grads, outputs, loss\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Clear mini-batch tensors\n",
    "            del mini_images, mini_labels\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if samples_processed >= max_samples:\n",
    "                break\n",
    "                \n",
    "        if samples_processed >= max_samples:\n",
    "            break\n",
    "    \n",
    "    return per_sample_grads\n",
    "\n",
    "# Usage example:\n",
    "max_samples = 100  # Adjust based on your needs\n",
    "sample_gradients = compute_per_sample_gradients(model, train_loader, criterion, device, max_samples=max_samples)\n",
    "\n",
    "# To analyze gradients:\n",
    "print(f\"Processed {len(sample_gradients)} samples\")\n",
    "first_sample_grads = sample_gradients[0]\n",
    "for param_name, grad in first_sample_grads.items():\n",
    "    print(f\"{param_name}: {grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "547b7936-a4f8-4cb0-be9b-76d6d78e3e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = sample_gradients[0].keys()\n",
    "N = len(sample_gradients)\n",
    "ntk = {k:torch.zeros((N,N)) for k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83893b16-ea14-42f6-a7ad-d645c4a5e3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a385e-d3f8-47b2-87ca-261896750458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
