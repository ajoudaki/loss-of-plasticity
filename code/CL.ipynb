{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5c51f9-cc01-4f52-8f3f-404034067949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:l0vgixpe) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CL_MLP_3_tasks</strong> at: <a href='https://wandb.ai/amirjoudaki/continual-learning-experiment/runs/l0vgixpe' target=\"_blank\">https://wandb.ai/amirjoudaki/continual-learning-experiment/runs/l0vgixpe</a><br/> View project at: <a href='https://wandb.ai/amirjoudaki/continual-learning-experiment' target=\"_blank\">https://wandb.ai/amirjoudaki/continual-learning-experiment</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250316_135504-l0vgixpe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:l0vgixpe). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/amir/Codes/NN-dynamic-scaling/code/wandb/run-20250316_140529-mm69bpvt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amirjoudaki/continual-learning-experiment/runs/mm69bpvt' target=\"_blank\">CL_MLP_3_tasks</a></strong> to <a href='https://wandb.ai/amirjoudaki/continual-learning-experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amirjoudaki/continual-learning-experiment' target=\"_blank\">https://wandb.ai/amirjoudaki/continual-learning-experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amirjoudaki/continual-learning-experiment/runs/mm69bpvt' target=\"_blank\">https://wandb.ai/amirjoudaki/continual-learning-experiment/runs/mm69bpvt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing CIFAR10 data for continual learning...\n",
      "Class sequence: [[0, 1], [2, 3], [4, 5, 6]]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Creating MLP model...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "from collections import defaultdict\n",
    "\n",
    "from continual_learning import * \n",
    "\n",
    "# Import the necessary classes from the provided code\n",
    "# MLP, CNN, ResNet, VisionTransformer, NetworkMonitor, etc. would be imported here\n",
    "\n",
    "###########################################\n",
    "# Continual Learning Dataset Functions\n",
    "###########################################\n",
    "\n",
    "class SubsetDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for class subset selection\"\"\"\n",
    "    def __init__(self, dataset, class_indices):\n",
    "        self.dataset = dataset\n",
    "        self.class_indices = class_indices\n",
    "        self.indices = self._get_indices()\n",
    "        \n",
    "    def _get_indices(self):\n",
    "        indices = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            _, label = self.dataset[i]\n",
    "            if label in self.class_indices:\n",
    "                indices.append(i)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[self.indices[idx]]\n",
    "        # Map the original label to a new index if needed\n",
    "        # This part can be modified to remap labels based on the continual learning strategy\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def prepare_continual_learning_data(dataset, class_sequence, batch_size=128, val_split=0.2):\n",
    "    \"\"\"\n",
    "    Prepare dataloaders for continual learning on a sequence of class subsets.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The full dataset (e.g., CIFAR10)\n",
    "        class_sequence: List of lists, where each inner list contains class indices for a task\n",
    "        batch_size: Batch size for dataloaders\n",
    "        val_split: Fraction of data to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping task_id -> (train_loader, val_loader, fixed_train_loader, fixed_val_loader)\n",
    "    \"\"\"\n",
    "    dataloaders = {}\n",
    "    all_seen_classes = set()\n",
    "    \n",
    "    for task_id, classes in enumerate(class_sequence):\n",
    "        current_classes = set(classes)\n",
    "        \n",
    "        # Create current task dataset\n",
    "        current_dataset = SubsetDataset(dataset, classes)\n",
    "        \n",
    "        # Split into training and validation\n",
    "        dataset_size = len(current_dataset)\n",
    "        val_size = int(val_split * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        \n",
    "        indices = list(range(dataset_size))\n",
    "        random.shuffle(indices)\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "        \n",
    "        train_subset = Subset(current_dataset, train_indices)\n",
    "        val_subset = Subset(current_dataset, val_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Fixed batches for metrics\n",
    "        fixed_train = Subset(train_subset, range(min(500, len(train_subset))))\n",
    "        fixed_val = Subset(val_subset, range(min(500, len(val_subset))))\n",
    "        \n",
    "        fixed_train_loader = DataLoader(fixed_train, batch_size=batch_size, shuffle=False)\n",
    "        fixed_val_loader = DataLoader(fixed_val, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # For previous tasks (old classes)\n",
    "        old_loaders = {}\n",
    "        if task_id > 0:\n",
    "            old_classes = all_seen_classes - current_classes\n",
    "            if old_classes:\n",
    "                old_dataset = SubsetDataset(dataset, list(old_classes))\n",
    "                old_size = len(old_dataset)\n",
    "                old_indices = list(range(old_size))\n",
    "                random.shuffle(old_indices)\n",
    "                \n",
    "                old_train_size = int((1 - val_split) * old_size)\n",
    "                old_train_indices = old_indices[:old_train_size]\n",
    "                old_val_indices = old_indices[old_train_size:]\n",
    "                \n",
    "                old_train_subset = Subset(old_dataset, old_train_indices)\n",
    "                old_val_subset = Subset(old_dataset, old_val_indices)\n",
    "                \n",
    "                old_train_loader = DataLoader(old_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "                old_val_loader = DataLoader(old_val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "                \n",
    "                # Fixed old batches for metrics\n",
    "                fixed_old_train = Subset(old_train_subset, range(min(500, len(old_train_subset))))\n",
    "                fixed_old_val = Subset(old_val_subset, range(min(500, len(old_val_subset))))\n",
    "                \n",
    "                fixed_old_train_loader = DataLoader(fixed_old_train, batch_size=batch_size, shuffle=False)\n",
    "                fixed_old_val_loader = DataLoader(fixed_old_val, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                old_loaders = {\n",
    "                    'train': old_train_loader,\n",
    "                    'val': old_val_loader,\n",
    "                    'fixed_train': fixed_old_train_loader,\n",
    "                    'fixed_val': fixed_old_val_loader\n",
    "                }\n",
    "        \n",
    "        # Store the dataloaders for this task\n",
    "        dataloaders[task_id] = {\n",
    "            'current': {\n",
    "                'train': train_loader,\n",
    "                'val': val_loader,\n",
    "                'fixed_train': fixed_train_loader,\n",
    "                'fixed_val': fixed_val_loader,\n",
    "                'classes': classes\n",
    "            },\n",
    "            'old': old_loaders\n",
    "        }\n",
    "        \n",
    "        # Update the set of all seen classes\n",
    "        all_seen_classes.update(current_classes)\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def get_cifar10_continual_data(class_sequence, batch_size=128):\n",
    "    \"\"\"\n",
    "    Get CIFAR10 data prepared for continual learning.\n",
    "    \n",
    "    Args:\n",
    "        class_sequence: List of lists, where each inner list contains class indices for a task\n",
    "        batch_size: Batch size for dataloaders\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping task_id -> task data loaders\n",
    "    \"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    \n",
    "    train_dataloaders = prepare_continual_learning_data(trainset, class_sequence, batch_size)\n",
    "    test_dataloaders = prepare_continual_learning_data(testset, class_sequence, batch_size)\n",
    "    \n",
    "    return train_dataloaders, test_dataloaders\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Continual Learning Training Functions\n",
    "###########################################\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset.\n",
    "    \n",
    "    Returns:\n",
    "        loss, accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "\n",
    "def train_continual_learning(model, task_dataloaders, config, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train a model using continual learning on a sequence of tasks.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        task_dataloaders: Dictionary mapping task_id -> task data loaders\n",
    "        config: Configuration dictionary\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    # For monitoring metrics\n",
    "    train_monitor = NetworkMonitor(model)\n",
    "    val_monitor = NetworkMonitor(model)\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'tasks': {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting continual learning with {len(task_dataloaders)} tasks...\")\n",
    "    \n",
    "    for task_id, task_data in task_dataloaders.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting Task {task_id}: Classes {task_data['current']['classes']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        current_train_loader = task_data['current']['train']\n",
    "        current_val_loader = task_data['current']['val']\n",
    "        current_fixed_train = task_data['current']['fixed_train']\n",
    "        current_fixed_val = task_data['current']['fixed_val']\n",
    "        \n",
    "        task_history = {\n",
    "            'epochs': [],\n",
    "            'current': {\n",
    "                'train_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_loss': [],\n",
    "                'val_acc': []\n",
    "            },\n",
    "            'old': {\n",
    "                'train_loss': [],\n",
    "                'train_acc': [],\n",
    "                'val_loss': [],\n",
    "                'val_acc': []\n",
    "            },\n",
    "            'training_metrics_history': defaultdict(lambda: defaultdict(list)),\n",
    "            'validation_metrics_history': defaultdict(lambda: defaultdict(list))\n",
    "        }\n",
    "        \n",
    "        # Get a fixed batch for metrics\n",
    "        try:\n",
    "            fixed_train_batch, fixed_train_targets = next(iter(current_fixed_train))\n",
    "            fixed_val_batch, fixed_val_targets = next(iter(current_fixed_val))\n",
    "            \n",
    "            fixed_train_batch = fixed_train_batch.to(device)\n",
    "            fixed_train_targets = fixed_train_targets.to(device)\n",
    "            fixed_val_batch = fixed_val_batch.to(device)\n",
    "            fixed_val_targets = fixed_val_targets.to(device)\n",
    "            \n",
    "            # Initial metrics\n",
    "            print(\"Measuring initial metrics...\")\n",
    "            \n",
    "            train_metrics = analyze_fixed_batch(model, train_monitor, fixed_train_batch, \n",
    "                                              fixed_train_targets, criterion, device=device)\n",
    "            val_metrics = analyze_fixed_batch(model, val_monitor, fixed_val_batch, \n",
    "                                            fixed_val_targets, criterion, device=device)\n",
    "            \n",
    "            for layer_name, metrics in train_metrics.items():\n",
    "                for metric_name, value in metrics.items():\n",
    "                    task_history['training_metrics_history'][layer_name][metric_name].append(value)\n",
    "            \n",
    "            for layer_name, metrics in val_metrics.items():\n",
    "                for metric_name, value in metrics.items():\n",
    "                    task_history['validation_metrics_history'][layer_name][metric_name].append(value)\n",
    "        except StopIteration:\n",
    "            print(\"Warning: Not enough samples for fixed batch metrics\")\n",
    "        \n",
    "        # If there are old classes (previous tasks)\n",
    "        has_old_data = 'old' in task_data and task_data['old']\n",
    "        if has_old_data:\n",
    "            old_train_loader = task_data['old']['train']\n",
    "            old_val_loader = task_data['old']['val']\n",
    "            \n",
    "            # Evaluate on old data before training on new data\n",
    "            old_train_loss, old_train_acc = evaluate_model(model, old_train_loader, criterion, device)\n",
    "            old_val_loss, old_val_acc = evaluate_model(model, old_val_loader, criterion, device)\n",
    "            \n",
    "            print(f\"Initial performance on OLD classes:\")\n",
    "            print(f\"  Train Loss: {old_train_loss:.4f}, Train Acc: {old_train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {old_val_loss:.4f}, Val Acc: {old_val_acc:.2f}%\")\n",
    "        \n",
    "        # Evaluate on current task before training\n",
    "        current_train_loss, current_train_acc = evaluate_model(model, current_train_loader, criterion, device)\n",
    "        current_val_loss, current_val_acc = evaluate_model(model, current_val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Initial performance on CURRENT classes:\")\n",
    "        print(f\"  Train Loss: {current_train_loss:.4f}, Train Acc: {current_train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {current_val_loss:.4f}, Val Acc: {current_val_acc:.2f}%\")\n",
    "        \n",
    "        # Record initial metrics\n",
    "        task_history['epochs'].append(0)\n",
    "        task_history['current']['train_loss'].append(current_train_loss)\n",
    "        task_history['current']['train_acc'].append(current_train_acc)\n",
    "        task_history['current']['val_loss'].append(current_val_loss)\n",
    "        task_history['current']['val_acc'].append(current_val_acc)\n",
    "        \n",
    "        if has_old_data:\n",
    "            task_history['old']['train_loss'].append(old_train_loss)\n",
    "            task_history['old']['train_acc'].append(old_train_acc)\n",
    "            task_history['old']['val_loss'].append(old_val_loss)\n",
    "            task_history['old']['val_acc'].append(old_val_acc)\n",
    "        \n",
    "        # Training loop for this task\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, config[\"epochs_per_task\"] + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, targets in current_train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            epoch_train_loss = running_loss / len(current_train_loader)\n",
    "            epoch_train_acc = 100. * correct / total\n",
    "            \n",
    "            # Evaluate on current task\n",
    "            current_val_loss, current_val_acc = evaluate_model(model, current_val_loader, criterion, device)\n",
    "            \n",
    "            # Record current task metrics\n",
    "            task_history['epochs'].append(epoch)\n",
    "            task_history['current']['train_loss'].append(epoch_train_loss)\n",
    "            task_history['current']['train_acc'].append(epoch_train_acc)\n",
    "            task_history['current']['val_loss'].append(current_val_loss)\n",
    "            task_history['current']['val_acc'].append(current_val_acc)\n",
    "            \n",
    "            # If there are old classes, evaluate on them too\n",
    "            if has_old_data:\n",
    "                old_train_loss, old_train_acc = evaluate_model(model, old_train_loader, criterion, device)\n",
    "                old_val_loss, old_val_acc = evaluate_model(model, old_val_loader, criterion, device)\n",
    "                \n",
    "                task_history['old']['train_loss'].append(old_train_loss)\n",
    "                task_history['old']['train_acc'].append(old_train_acc)\n",
    "                task_history['old']['val_loss'].append(old_val_loss)\n",
    "                task_history['old']['val_acc'].append(old_val_acc)\n",
    "            \n",
    "            # Periodically collect network metrics\n",
    "            if epoch % config[\"metrics_frequency\"] == 0 or epoch == config[\"epochs_per_task\"]:\n",
    "                try:\n",
    "                    train_monitor.clear_data()\n",
    "                    val_monitor.clear_data()\n",
    "                    \n",
    "                    train_metrics = analyze_fixed_batch(model, train_monitor, fixed_train_batch, \n",
    "                                                      fixed_train_targets, criterion, device=device)\n",
    "                    val_metrics = analyze_fixed_batch(model, val_monitor, fixed_val_batch, \n",
    "                                                    fixed_val_targets, criterion, device=device)\n",
    "                    \n",
    "                    for layer_name, metrics in train_metrics.items():\n",
    "                        for metric_name, value in metrics.items():\n",
    "                            task_history['training_metrics_history'][layer_name][metric_name].append(value)\n",
    "                    \n",
    "                    for layer_name, metrics in val_metrics.items():\n",
    "                        for metric_name, value in metrics.items():\n",
    "                            task_history['validation_metrics_history'][layer_name][metric_name].append(value)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error collecting metrics: {e}\")\n",
    "            \n",
    "            # Log to wandb\n",
    "            log_data = {\n",
    "                \"task\": task_id,\n",
    "                \"epoch\": epoch,\n",
    "                \"current_train_loss\": epoch_train_loss,\n",
    "                \"current_train_acc\": epoch_train_acc,\n",
    "                \"current_val_loss\": current_val_loss,\n",
    "                \"current_val_acc\": current_val_acc\n",
    "            }\n",
    "            \n",
    "            if has_old_data:\n",
    "                log_data.update({\n",
    "                    \"old_train_loss\": old_train_loss,\n",
    "                    \"old_train_acc\": old_train_acc,\n",
    "                    \"old_val_loss\": old_val_loss,\n",
    "                    \"old_val_acc\": old_val_acc\n",
    "                })\n",
    "            \n",
    "            wandb.log(log_data)\n",
    "            \n",
    "            # Print progress\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Task {task_id}, Epoch {epoch}/{config[\"epochs_per_task\"]}:')\n",
    "            print(f'  CURRENT: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, '\n",
    "                 f'Val Loss: {current_val_loss:.4f}, Val Acc: {current_val_acc:.2f}%')\n",
    "            \n",
    "            if has_old_data:\n",
    "                print(f'  OLD: Train Loss: {old_train_loss:.4f}, Train Acc: {old_train_acc:.2f}%, '\n",
    "                     f'Val Loss: {old_val_loss:.4f}, Val Acc: {old_val_acc:.2f}%')\n",
    "            \n",
    "            print(f'  Time: {elapsed:.2f}s')\n",
    "        \n",
    "        # Store task history\n",
    "        history['tasks'][task_id] = {\n",
    "            'classes': task_data['current']['classes'],\n",
    "            'history': task_history\n",
    "        }\n",
    "        \n",
    "        # Log task summary\n",
    "        wandb.log({\n",
    "            \"task_completed\": task_id,\n",
    "            \"final_current_train_acc\": task_history['current']['train_acc'][-1],\n",
    "            \"final_current_val_acc\": task_history['current']['val_acc'][-1],\n",
    "            \"elapsed_time\": time.time() - start_time\n",
    "        })\n",
    "        \n",
    "        if has_old_data:\n",
    "            wandb.log({\n",
    "                \"final_old_train_acc\": task_history['old']['train_acc'][-1],\n",
    "                \"final_old_val_acc\": task_history['old']['val_acc'][-1],\n",
    "            })\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Visualization Functions\n",
    "###########################################\n",
    "\n",
    "def plot_continual_learning_curves(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot learning curves for continual learning experiment.\n",
    "    \"\"\"\n",
    "    num_tasks = len(history['tasks'])\n",
    "    fig, axs = plt.subplots(num_tasks, 2, figsize=(15, 5 * num_tasks))\n",
    "    \n",
    "    if num_tasks == 1:\n",
    "        axs = np.array([axs])\n",
    "    \n",
    "    for task_id, task_data in history['tasks'].items():\n",
    "        task_history = task_data['history']\n",
    "        classes = task_data['classes']\n",
    "        \n",
    "        # Loss plot\n",
    "        ax = axs[task_id, 0]\n",
    "        ax.plot(task_history['epochs'], task_history['current']['train_loss'], label='Current Train')\n",
    "        ax.plot(task_history['epochs'], task_history['current']['val_loss'], label='Current Val')\n",
    "        \n",
    "        if 'old' in task_history and task_history['old']['train_loss']:\n",
    "            ax.plot(task_history['epochs'], task_history['old']['train_loss'], label='Old Train')\n",
    "            ax.plot(task_history['epochs'], task_history['old']['val_loss'], label='Old Val')\n",
    "        \n",
    "        ax.set_title(f'Task {task_id} (Classes {classes}) Loss')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax = axs[task_id, 1]\n",
    "        ax.plot(task_history['epochs'], task_history['current']['train_acc'], label='Current Train')\n",
    "        ax.plot(task_history['epochs'], task_history['current']['val_acc'], label='Current Val')\n",
    "        \n",
    "        if 'old' in task_history and task_history['old']['train_acc']:\n",
    "            ax.plot(task_history['epochs'], task_history['old']['train_acc'], label='Old Train')\n",
    "            ax.plot(task_history['epochs'], task_history['old']['val_acc'], label='Old Val')\n",
    "        \n",
    "        ax.set_title(f'Task {task_id} (Classes {classes}) Accuracy')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/continual_learning_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_forgetting_curve(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot forgetting curve for old tasks as training progresses.\n",
    "    \"\"\"\n",
    "    num_tasks = len(history['tasks'])\n",
    "    \n",
    "    if num_tasks <= 1:\n",
    "        print(\"Need at least 2 tasks to plot forgetting curve\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # For each task, plot old validation accuracy\n",
    "    for task_id in range(1, num_tasks):\n",
    "        task_data = history['tasks'][task_id]\n",
    "        task_history = task_data['history']\n",
    "        \n",
    "        if 'old' in task_history and task_history['old']['val_acc']:\n",
    "            plt.plot(task_history['epochs'], task_history['old']['val_acc'], \n",
    "                     label=f'Task {task_id} (Old Classes)')\n",
    "    \n",
    "    plt.title('Forgetting Curve (Old Classes Validation Accuracy)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/forgetting_curve.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_task_transition(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot accuracy transitions between tasks.\n",
    "    \"\"\"\n",
    "    num_tasks = len(history['tasks'])\n",
    "    \n",
    "    if num_tasks <= 1:\n",
    "        print(\"Need at least 2 tasks to plot task transition\")\n",
    "        return\n",
    "    \n",
    "    # Extract final accuracies for each task\n",
    "    current_val_acc = []\n",
    "    old_val_acc = []\n",
    "    \n",
    "    for task_id in range(num_tasks):\n",
    "        task_data = history['tasks'][task_id]\n",
    "        task_history = task_data['history']\n",
    "        \n",
    "        current_val_acc.append(task_history['current']['val_acc'][-1])\n",
    "        \n",
    "        if task_id > 0 and 'old' in task_history and task_history['old']['val_acc']:\n",
    "            old_val_acc.append(task_history['old']['val_acc'][-1])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot current validation accuracy\n",
    "    plt.plot(range(num_tasks), current_val_acc, 'o-', label='Current Classes')\n",
    "    \n",
    "    # Plot old validation accuracy\n",
    "    if old_val_acc:\n",
    "        plt.plot(range(1, num_tasks), old_val_acc, 'o-', label='Old Classes')\n",
    "    \n",
    "    plt.title('Task Transition (Final Validation Accuracy)')\n",
    "    plt.xlabel('Task')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(range(num_tasks))\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/task_transition.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Main Function\n",
    "###########################################\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import from our continual learning module\n",
    "from continual_learning import *\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Configuration for the experiment\n",
    "    config = {\n",
    "        \"model_type\": \"MLP\",\n",
    "        \"model_config\": {\n",
    "            \"input_size\": 3 * 32 * 32,\n",
    "            \"hidden_sizes\": [1024, 512],  # Moderately sized network\n",
    "            \"output_size\": 10,            # Total number of classes in CIFAR10\n",
    "            \"activation\": \"relu\",\n",
    "            \"dropout_p\": 0.0,             # No dropout to better observe forgetting\n",
    "            \"normalization\": \"batch\",\n",
    "            \"norm_after_activation\": False\n",
    "        },\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs_per_task\": 10,            # Train for 20 epochs per task\n",
    "        \"metrics_frequency\": 5,           # Measure detailed metrics every 5 epochs\n",
    "        \"class_sequence\": [\n",
    "            [0, 1],                       # Task 0: airplane, automobile\n",
    "            [2, 3],                       # Task 1: bird, cat\n",
    "            [4, 5, 6]                     # Task 2: deer, dog, frog\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Initialize wandb with a descriptive name\n",
    "    run_name = f\"CL_MLP_{len(config['class_sequence'])}_tasks\"\n",
    "    wandb.init(project=\"continual-learning-experiment\", config=config, name=run_name)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data for continual learning\n",
    "    print(\"Preparing CIFAR10 data for continual learning...\")\n",
    "    print(f\"Class sequence: {config['class_sequence']}\")\n",
    "    train_dataloaders, test_dataloaders = get_cifar10_continual_data(\n",
    "        config[\"class_sequence\"], \n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Create model based on configuration\n",
    "    print(f\"Creating {config['model_type']} model...\")\n",
    "    model = MLP(**config[\"model_config\"])\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6961c6-06a4-4721-aa5a-84d196c56d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
