{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amir/Codes/NN-dynamic-scaling already in Python path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import the utility module and Setup the path\n",
    "import notebook_utils\n",
    "notebook_utils.setup_path()\n",
    "\n",
    "from src.models.layers import TransformerBatchNorm\n",
    "from src.utils.monitor import NetworkMonitor\n",
    "\n",
    "from typing import Union, List, Dict, Set\n",
    "\n",
    "# Replace TypeVar with Union for proper type handling\n",
    "NormalizationLayer = Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm,  nn.GroupNorm, TransformerBatchNorm]\n",
    "\n",
    "# Define activation function types properly\n",
    "ActivationFunction = Union[nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, \n",
    "                         nn.LeakyReLU, nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, \n",
    "                         nn.Softplus, nn.Softmin, nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, \n",
    "                         nn.Hardshrink, nn.Softsign, nn.GLU, nn.CELU, nn.Identity]\n",
    "\n",
    "\n",
    "class CloneAwareFlatten(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom flatten module that ensures duplicated features remain adjacent when flattening\n",
    "    convolutional feature maps.\n",
    "    \n",
    "    When cloning channels in convolutional layers, the standard nn.Flatten would arrange features\n",
    "    as [a(0,0), a'(0,0), b(0,0), b'(0,0), ...] where features are grouped by spatial position.\n",
    "    \n",
    "    This module rearranges to keep all spatial positions of the same channel together:\n",
    "    [a(0,0), a'(0,0), a(0,1), a'(0,1), ..., b(0,0), b'(0,0), ...] ensuring duplicated\n",
    "    features remain adjacent.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_dim=1, end_dim=-1):\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Standard flattening for non-starting dimensions or non-4D tensors\n",
    "        if x.dim() != 4 or self.start_dim > 1:\n",
    "            start_dim = self.start_dim if self.start_dim >= 0 else x.dim() + self.start_dim\n",
    "            end_dim = self.end_dim if self.end_dim >= 0 else x.dim() + self.end_dim\n",
    "            \n",
    "            shape = x.shape\n",
    "            new_shape = list(shape[:start_dim]) + [-1]\n",
    "            return x.reshape(*new_shape)\n",
    "        \n",
    "        # Special handling for 4D tensors with channel duplication\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # If channels are not even, use standard flattening\n",
    "        if channels % 2 != 0:\n",
    "            return x.reshape(batch_size, -1)\n",
    "        \n",
    "        half_channels = channels // 2\n",
    "        \n",
    "        # Step 1: Reshape to separate duplicated channels\n",
    "        # [batch, channels, h, w] -> [batch, half_channels, 2, h, w]\n",
    "        x_reshaped = x.view(batch_size, half_channels, 2, height, width)\n",
    "        \n",
    "        # Step 2: Permute to get the desired order\n",
    "        # [batch, half_channels, 2, h, w] -> [batch, half_channels, h, w, 2]\n",
    "        x_permuted = x_reshaped.permute(0, 1, 3, 4, 2)\n",
    "        \n",
    "        # Step 3: Flatten\n",
    "        # [batch, half_channels, h, w, 2] -> [batch, half_channels * h * w * 2]\n",
    "        return x_permuted.reshape(batch_size, -1)\n",
    "\n",
    "def clone_linear(base_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    # Get module dimensions\n",
    "    base_in_features = base_module.in_features\n",
    "    base_out_features = base_module.out_features\n",
    "    cloned_in_features = cloned_module.in_features\n",
    "    cloned_out_features = cloned_module.out_features\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_features % base_in_features != 0 or cloned_out_features % base_out_features != 0:\n",
    "        raise ValueError(f\"Linear module dimensions are not integer multiples: \"\n",
    "                         f\"{base_in_features}→{cloned_in_features}, {base_out_features}→{cloned_out_features}\")\n",
    "        \n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_features // base_in_features\n",
    "    out_expansion = cloned_out_features // base_out_features\n",
    "    \n",
    "    print(f\"Cloning Linear module: {base_in_features}→{cloned_in_features}, {base_out_features}→{cloned_out_features}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion] = base_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if base_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = base_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_conv1d(base_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    # Get module dimensions\n",
    "    base_in_channels = base_module.in_channels\n",
    "    base_out_channels = base_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // base_in_channels\n",
    "    out_expansion = cloned_out_channels // base_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv1d module: {base_in_channels}→{cloned_in_channels}, {base_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % base_in_channels != 0 or cloned_out_channels % base_out_channels != 0:\n",
    "        raise ValueError(f\"Conv1d module dimensions are not integer multiples: \"\n",
    "                         f\"{base_in_channels}→{cloned_in_channels}, {base_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :] = base_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if base_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = base_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "    \n",
    "def clone_conv2d(base_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    # Get module dimensions\n",
    "    base_in_channels = base_module.in_channels\n",
    "    base_out_channels = base_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // base_in_channels\n",
    "    out_expansion = cloned_out_channels // base_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv2d module: {base_in_channels}→{cloned_in_channels}, {base_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % base_in_channels != 0 or cloned_out_channels % base_out_channels != 0:\n",
    "        raise ValueError(f\"Conv2d module dimensions are not integer multiples: \"\n",
    "                         f\"{base_in_channels}→{cloned_in_channels}, {base_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :] = base_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if base_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = base_module.bias.data\n",
    "    return cloned_module\n",
    "    \n",
    "\n",
    "def clone_normalization(\n",
    "    base_module: NormalizationLayer, \n",
    "    cloned_module: NormalizationLayer,\n",
    ") -> NormalizationLayer:\n",
    "    \"\"\"Clone normalization layer parameters with proper handling of different types.\"\"\"\n",
    "    assert isinstance(cloned_module, type(base_module)), \"Cloned module must be of the same type as base module\"\n",
    "    \n",
    "    # Check properties that exist for the specific normalization type\n",
    "    if hasattr(base_module, 'affine') and hasattr(cloned_module, 'affine'):\n",
    "        assert base_module.affine == cloned_module.affine, \"Affine property must match\"\n",
    "    \n",
    "    # Handle BatchNorm-specific properties\n",
    "    if isinstance(base_module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        if hasattr(base_module, 'track_running_stats') and hasattr(cloned_module, 'track_running_stats'):\n",
    "            assert base_module.track_running_stats == cloned_module.track_running_stats, \"Track running stats property must match\"\n",
    "    \n",
    "    # Clone weights and biases\n",
    "    if hasattr(base_module, 'weight') and base_module.weight is not None and cloned_module.weight is not None:\n",
    "        expansion = cloned_module.weight.data.shape[0] // base_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            cloned_module.weight.data[i::expansion] = base_module.weight.data\n",
    "            if hasattr(base_module, 'bias') and base_module.bias is not None and cloned_module.bias is not None:\n",
    "                cloned_module.bias.data[i::expansion] = base_module.bias.data\n",
    "    \n",
    "    # Clone running stats for BatchNorm layers\n",
    "    if hasattr(base_module, 'running_mean') and base_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // base_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                cloned_module.running_mean.data[i::expansion] = base_module.running_mean.data\n",
    "                cloned_module.running_var.data[i::expansion] = base_module.running_var.data\n",
    "    \n",
    "    # Clone num_batches_tracked for BatchNorm layers\n",
    "    if hasattr(base_module, 'num_batches_tracked') and base_module.num_batches_tracked is not None:\n",
    "        if hasattr(cloned_module, 'num_batches_tracked') and cloned_module.num_batches_tracked is not None:\n",
    "            cloned_module.num_batches_tracked.data.copy_(base_module.num_batches_tracked.data)\n",
    "    \n",
    "    return cloned_module\n",
    "    \n",
    "    \n",
    "def clone_embedding(base_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    # Get module dimensions\n",
    "    base_num_embeddings = base_module.num_embeddings\n",
    "    base_embedding_dim = base_module.embedding_dim\n",
    "    cloned_num_embeddings = cloned_module.num_embeddings\n",
    "    cloned_embedding_dim = cloned_module.embedding_dim\n",
    "    \n",
    "    # Calculate expansion factors\n",
    "    num_expansion = cloned_num_embeddings // base_num_embeddings\n",
    "    dim_expansion = cloned_embedding_dim // base_embedding_dim\n",
    "    \n",
    "    print(f\"Cloning Embedding module: {base_num_embeddings}→{cloned_num_embeddings}, {base_embedding_dim}→{cloned_embedding_dim}, num expansion: {num_expansion}, dim expansion: {dim_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_num_embeddings % base_num_embeddings != 0 or cloned_embedding_dim % base_embedding_dim != 0:\n",
    "        raise ValueError(f\"Embedding module dimensions are not integer multiples: \"\n",
    "                         f\"{base_num_embeddings}→{cloned_num_embeddings}, {base_embedding_dim}→{cloned_embedding_dim}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(num_expansion):\n",
    "        for j in range(dim_expansion):\n",
    "            cloned_module.weight.data[j::dim_expansion, i::num_expansion] = base_module.weight.data \n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_activation(base_module: ActivationFunction, cloned_module: ActivationFunction) -> ActivationFunction:\n",
    "    \"\"\"Clone activation function parameters, handling configuration parameters properly.\"\"\"\n",
    "    assert isinstance(cloned_module, type(base_module)), \"Cloned module must be of the same type as base module\"\n",
    "    \n",
    "    # Handle configuration parameters for different activation types\n",
    "    if isinstance(base_module, nn.LeakyReLU):\n",
    "        cloned_module.negative_slope = base_module.negative_slope\n",
    "    \n",
    "    elif isinstance(base_module, (nn.ELU, nn.CELU)):\n",
    "        cloned_module.alpha = base_module.alpha\n",
    "    \n",
    "    elif isinstance(base_module, nn.Threshold):\n",
    "        cloned_module.threshold = base_module.threshold\n",
    "        cloned_module.value = base_module.value\n",
    "    \n",
    "    elif isinstance(base_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        cloned_module.dim = base_module.dim\n",
    "    \n",
    "    elif isinstance(base_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        cloned_module.lambd = base_module.lambd\n",
    "        \n",
    "    elif isinstance(base_module, nn.GLU):\n",
    "        cloned_module.dim = base_module.dim\n",
    "    \n",
    "    # Handle PReLU specifically (has learnable parameters)\n",
    "    elif isinstance(base_module, nn.PReLU):\n",
    "        if base_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # If base is a single parameter, broadcast to all channels\n",
    "            cloned_module.weight.data.fill_(base_module.weight.data[0])\n",
    "        elif base_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            # Channel-wise parameters need proper expansion\n",
    "            expansion = cloned_module.num_parameters // base_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                cloned_module.weight.data[i::expansion] = base_module.weight.data\n",
    "        else:\n",
    "            # Direct copy if dimensions match\n",
    "            cloned_module.weight.data.copy_(base_module.weight.data)\n",
    "    \n",
    "    # Handle other parameterized activation functions if they have weights\n",
    "    # This is a general catch-all for any other activation function with parameters\n",
    "    elif hasattr(base_module, 'weight') and hasattr(cloned_module, 'weight'):\n",
    "        if base_module.weight is not None and cloned_module.weight is not None:\n",
    "            if cloned_module.weight.data.shape == base_module.weight.data.shape:\n",
    "                cloned_module.weight.data.copy_(base_module.weight.data)\n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_dropout(base_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    \"\"\"Clone dropout module parameters.\"\"\"\n",
    "    assert cloned_module.p == base_module.p, \"Dropout probability must match\"\n",
    "    # Print warning if dropout p > 0\n",
    "    if cloned_module.p > 0:\n",
    "        print(f\"Warning: Dropout probability is set to {cloned_module.p}, cloning is not perfect\")\n",
    "    return cloned_module\n",
    "\n",
    "def clone_flatten(base_module: nn.Flatten) -> CloneAwareFlatten:\n",
    "    \"\"\"\n",
    "    Clone parameters from a standard Flatten and return a new CloneAwareFlatten.\n",
    "    \n",
    "    Args:\n",
    "        base_module: base nn.Flatten module\n",
    "        \n",
    "    Returns:\n",
    "        A new CloneAwareFlatten module with the same parameters\n",
    "    \"\"\"\n",
    "    return CloneAwareFlatten(\n",
    "        start_dim=base_module.start_dim,\n",
    "        end_dim=base_module.end_dim\n",
    "    )\n",
    "\n",
    "\n",
    "def is_parameter_free(module: nn.Module) -> bool:\n",
    "    \"\"\"Check if a module has no parameters.\"\"\"\n",
    "    return len(list(module.parameters())) == 0\n",
    "\n",
    "\n",
    "def clone_parameter_free(base_module: nn.Module, cloned_module: nn.Module) -> nn.Module:\n",
    "    \"\"\"Clone a parameter-free module.\"\"\"\n",
    "    assert isinstance(cloned_module, type(base_module)), \"Cloned module must be of the same type as base module\"\n",
    "    assert is_parameter_free(base_module), \"base module must be parameter free\"\n",
    "    assert is_parameter_free(cloned_module), \"Cloned module must be parameter free\"\n",
    "    \n",
    "    # For parameter-free modules, there's no need to copy weights\n",
    "    # Just make sure they're of the same type, which we've already checked\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "# Validation functions\n",
    "\n",
    "def validate_activation_cloning(base_module: ActivationFunction, cloned_module: ActivationFunction):\n",
    "    assert isinstance(cloned_module, type(base_module)), \"Cloned module must be of the same type as base module\"\n",
    "    \n",
    "    # Validate configuration parameters for different activation types\n",
    "    if isinstance(base_module, nn.LeakyReLU):\n",
    "        assert base_module.negative_slope == cloned_module.negative_slope, \"LeakyReLU negative_slope does not match\"\n",
    "    \n",
    "    elif isinstance(base_module, (nn.ELU, nn.CELU)):\n",
    "        assert base_module.alpha == cloned_module.alpha, \"Alpha parameter does not match\"\n",
    "    \n",
    "    elif isinstance(base_module, nn.Threshold):\n",
    "        assert base_module.threshold == cloned_module.threshold, \"Threshold value does not match\"\n",
    "        assert base_module.value == cloned_module.value, \"Replacement value does not match\"\n",
    "    \n",
    "    elif isinstance(base_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        assert base_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    elif isinstance(base_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        assert base_module.lambd == cloned_module.lambd, \"Lambda parameter does not match\"\n",
    "        \n",
    "    elif isinstance(base_module, nn.GLU):\n",
    "        assert base_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    # Validate PReLU parameters\n",
    "    elif isinstance(base_module, nn.PReLU):\n",
    "        if base_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # All elements should be equal to the single parameter\n",
    "            assert torch.all(cloned_module.weight.data == base_module.weight.data[0])\n",
    "        elif base_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            expansion = cloned_module.num_parameters // base_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.weight.data[i::expansion], base_module.weight.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_dropout_cloning(base_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    assert cloned_module.p == base_module.p, \"Dropout probability must match\"\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_embedding_cloning(base_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    num_expansion = cloned_module.num_embeddings // base_module.num_embeddings\n",
    "    dim_expansion = cloned_module.embedding_dim // base_module.embedding_dim\n",
    "    for j in range(num_expansion):\n",
    "        for i in range(dim_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::num_expansion, i::dim_expansion], base_module.weight.data)\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_normalization_cloning(base_module: NormalizationLayer, cloned_module: NormalizationLayer):\n",
    "    assert isinstance(cloned_module, type(base_module)), \"Cloned module must be of the same type as base module\"\n",
    "    \n",
    "    if hasattr(base_module, 'weight') and base_module.weight is not None and hasattr(cloned_module, 'weight'):\n",
    "        expansion = cloned_module.weight.data.shape[0] // base_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[i::expansion], base_module.weight.data)\n",
    "            \n",
    "            if hasattr(base_module, 'bias') and base_module.bias is not None and hasattr(cloned_module, 'bias'):\n",
    "                assert torch.allclose(cloned_module.bias.data[i::expansion], base_module.bias.data)\n",
    "    \n",
    "    # Check running stats for BatchNorm layers\n",
    "    if hasattr(base_module, 'running_mean') and base_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // base_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.running_mean.data[i::expansion], base_module.running_mean.data)\n",
    "                assert torch.allclose(cloned_module.running_var.data[i::expansion], base_module.running_var.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_linear_cloning(base_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    in_expansion = cloned_module.in_features // base_module.in_features\n",
    "    out_expansion = cloned_module.out_features // base_module.out_features\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion], base_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], base_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "    \n",
    "def validate_conv1d_cloning(base_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    in_expansion = cloned_module.in_channels // base_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // base_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :], base_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], base_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_conv2d_cloning(base_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    in_expansion = cloned_module.in_channels // base_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // base_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :], base_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], base_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "\n",
    "\n",
    "\n",
    "def clone_module(\n",
    "    base_module: nn.Module, \n",
    "    cloned_module: nn.Module,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Clone parameters from a base module to a cloned module.\n",
    "    \n",
    "    Args:\n",
    "        base_module: base module with smaller dimensions\n",
    "        cloned_module: Target module with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if cloning was successful, False otherwise\n",
    "    \"\"\"\n",
    "    success = True\n",
    "    \n",
    "    # Define normalization and activation types inline for easier checking\n",
    "    norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)\n",
    "    activation_types = (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, nn.LeakyReLU, \n",
    "                       nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, nn.Softplus, nn.Softmin, \n",
    "                       nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, nn.Hardshrink, nn.Softsign, \n",
    "                       nn.GLU, nn.CELU, nn.Identity)\n",
    "    \n",
    "    if isinstance(base_module, nn.Linear):\n",
    "        clone_linear(base_module, cloned_module)\n",
    "    elif isinstance(base_module, nn.Conv1d):\n",
    "        clone_conv1d(base_module, cloned_module)\n",
    "    elif isinstance(base_module, nn.Conv2d):\n",
    "        clone_conv2d(base_module, cloned_module)\n",
    "    elif isinstance(base_module, norm_types):\n",
    "        clone_normalization(base_module, cloned_module)\n",
    "    elif isinstance(base_module, nn.Embedding):\n",
    "        clone_embedding(base_module, cloned_module)\n",
    "    elif isinstance(base_module, activation_types):\n",
    "        clone_activation(base_module, cloned_module)\n",
    "    elif isinstance(base_module, nn.Dropout):\n",
    "        clone_dropout(base_module, cloned_module)\n",
    "    elif isinstance(base_module, nn.Flatten):\n",
    "        pass # Flatten is handled separately\n",
    "    elif is_parameter_free(base_module) and is_parameter_free(cloned_module):\n",
    "        clone_parameter_free(base_module, cloned_module)\n",
    "    else:\n",
    "        success = False\n",
    "        print(f\"Unsupported module type: {type(base_module)}\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "\n",
    "def model_clone(base_model: nn.Module, cloned_model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Clone parameters from a base model to a cloned model.\n",
    "    \n",
    "    Args:\n",
    "        base_model: base model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    # First, replace all Flatten modules with CloneAwareFlatten\n",
    "    for name, module in list(cloned_model.named_modules()):\n",
    "        if isinstance(module, nn.Flatten):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            module_name = name.split('.')[-1]\n",
    "            \n",
    "            # Find parent module to modify\n",
    "            if parent_name:\n",
    "                parent = cloned_model.get_submodule(parent_name)\n",
    "            else:\n",
    "                parent = cloned_model\n",
    "                \n",
    "            # Create and replace with CloneAwareFlatten\n",
    "            setattr(parent, module_name, CloneAwareFlatten(\n",
    "                start_dim=module.start_dim,\n",
    "                end_dim=module.end_dim\n",
    "            ))\n",
    "            print(f\"Replaced Flatten with CloneAwareFlatten at {name}\")\n",
    "    \n",
    "    # Second, handle direct parameters of the model (not within modules)\n",
    "    for name, param in list(base_model.named_parameters(recurse=False)):\n",
    "        if hasattr(cloned_model, name):\n",
    "            base_param = getattr(base_model, name)\n",
    "            cloned_param = getattr(cloned_model, name)\n",
    "            \n",
    "            # Check if dimensions differ and can be expanded\n",
    "            if base_param.shape != cloned_param.shape:\n",
    "                # For embedding dimensions (typically last dimension in transformers)\n",
    "                if len(base_param.shape) >= 2 and base_param.shape[:-1] == cloned_param.shape[:-1]:\n",
    "                    base_dim = base_param.shape[-1]\n",
    "                    cloned_dim = cloned_param.shape[-1]\n",
    "                    \n",
    "                    if cloned_dim % base_dim == 0:\n",
    "                        expansion = cloned_dim // base_dim\n",
    "                        # Duplicate across embedding dimension\n",
    "                        for i in range(expansion):\n",
    "                            cloned_param.data[..., i::expansion] = base_param.data\n",
    "                        print(f\"Cloned parameter {name} with embedding expansion {expansion}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Parameter {name} dimensions don't match and can't be expanded automatically\")\n",
    "                else:\n",
    "                    print(f\"Warning: Parameter {name} shapes don't match and can't be expanded automatically\")\n",
    "            else:\n",
    "                # Exact shape match, just copy\n",
    "                cloned_param.data.copy_(base_param.data)\n",
    "                print(f\"Cloned parameter {name} with direct copy\")\n",
    "    \n",
    "    # Finally, process module parameters\n",
    "    for name, base_module in base_model.named_modules():\n",
    "        try:\n",
    "            cloned_module = cloned_model.get_submodule(name)\n",
    "            print(f\"Cloning module {name}\")\n",
    "            clone_module(base_module, cloned_module)\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Could not find matching module for {name}\")\n",
    "    \n",
    "    return cloned_model\n",
    "\n",
    "\n",
    "\n",
    "def test_activation_cloning(base_model, cloned_model, input, target, tolerance=1e-3, check_equality=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    base_monitor = NetworkMonitor(base_model,)\n",
    "    cloned_monitor = NetworkMonitor(cloned_model,)\n",
    "    base_monitor.register_hooks()\n",
    "    cloned_monitor.register_hooks()\n",
    "\n",
    "    y1 = base_model(input)\n",
    "    y2 = cloned_model(input)\n",
    "    l1 = criterion(y1, target)\n",
    "    l2 = criterion(y2, target)\n",
    "    l1.backward()\n",
    "    l2.backward()\n",
    "    if check_equality:\n",
    "        assert torch.allclose(y1, y2,atol=tolerance), \"Outputs do not match after cloning\"\n",
    "\n",
    "    un_explained_vars = []\n",
    "    for act_type in ['forward', 'backward']:\n",
    "        if act_type == 'forward':\n",
    "            base_acts = base_monitor.get_latest_activations()\n",
    "            clone_acts = cloned_monitor.get_latest_activations()\n",
    "        elif act_type == 'backward':\n",
    "            base_acts = base_monitor.get_latest_gradients()\n",
    "            clone_acts = cloned_monitor.get_latest_gradients()\n",
    "\n",
    "        for key, a1 in base_acts.items():\n",
    "            a2 = clone_acts[key]\n",
    "            s1, s2 = torch.tensor(a1.shape), torch.tensor(a2.shape)\n",
    "            print(f\"key: {key}, a1: {a1.shape}, a2: {a2.shape}\")\n",
    "            i = (s1 != s2).nonzero()\n",
    "            if len(i)==0:\n",
    "                if check_equality:\n",
    "                    assert torch.allclose(a1, a2, atol=tolerance), f\"Activations for {key} do not match\"\n",
    "            elif len(i)==1:\n",
    "                i = i[0][0]\n",
    "                expansion = a2.shape[i] // a1.shape[i]\n",
    "                # check expansion depending on the dimension \n",
    "                for j in range(expansion):\n",
    "                    print(f\"mismatch dim: {i}, checking slice: {j}, expansion: {expansion}\")\n",
    "                    slices = []\n",
    "                    if i==0:\n",
    "                        slice = a2[j::expansion]\n",
    "                    elif i==1:\n",
    "                        slice = a2[:, j::expansion]\n",
    "                    elif i==2:\n",
    "                        slice = a2[:, :, j::expansion]\n",
    "                    elif i==3:\n",
    "                        slice = a2[:, :, :, j::expansion]\n",
    "                    slices.append(slice)\n",
    "                    if check_equality:\n",
    "                        assert torch.allclose(slice, a1, atol=tolerance), f\"Activations for {key} do not match\"\n",
    "                slices = torch.stack(slices)\n",
    "                if slices.shape[0]>1:\n",
    "                    print(f\"slices for {key} shape = {slices.shape}\")\n",
    "                    std, rms = slices.std(dim=0), ((slices**2).mean(dim=0)**0.5)\n",
    "                    unexplained = (std/rms).mean().item()\n",
    "                    print(f\"unexplained variance for {key} is {unexplained}\")\n",
    "                    un_explained_vars[f'{key}_{act_type}'] = unexplained\n",
    "                    assert unexplained<tolerance, f\"unexplained variance is higher than the threshold {tolerance}\"\n",
    "\n",
    "            elif len(i)>1:\n",
    "                assert False, f\"Activations for {key} more than one dimension mismatch, this is unexpected behavior\"\n",
    "                    \n",
    "            print(f\"All {act_type} activations match after cloning up to tolerance {tolerance}\")\n",
    "            return un_explained_vars\n",
    "    \n",
    "def test_various_models_cloning(normalization='none', drpout_p=0.0, activation='relu', tolerance=1e-3,check_equality=False):\n",
    "    from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "    \n",
    "    # generate random input and targets\n",
    "    x_flat = torch.randn(32, 10) # for MLP \n",
    "    x = torch.randn(32, 3, 32, 32)\n",
    "    y = torch.randint(0, 2, (32,))\n",
    "    \n",
    "    \n",
    "    base_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32,], activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2,], activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = model_clone(base_model, cloned_model)\n",
    "    test_activation_cloning(base_model, cloned_model, x_flat, y, tolerance=tolerance, check_equality=check_equality)\n",
    "    \n",
    "    base_model = CNN(in_channels=3, num_classes=2, conv_channels=[64, 128, 256], activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = CNN(in_channels=3, num_classes=2, conv_channels=[64*2, 128*2, 256*2], activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = model_clone(base_model, cloned_model)\n",
    "    test_activation_cloning(base_model, cloned_model, x, y, tolerance=tolerance, check_equality=check_equality)\n",
    "\n",
    "    base_model = ResNet(in_channels=3, num_classes=2, base_channels=64, activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = ResNet(in_channels=3, num_classes=2, base_channels=64*2, activation=activation, dropout_p=drpout_p, normalization=normalization)\n",
    "    cloned_model = model_clone(base_model, cloned_model)\n",
    "    test_activation_cloning(base_model, cloned_model, x, y, tolerance=tolerance, check_equality=check_equality)\n",
    "\n",
    "    base_model = VisionTransformer(\n",
    "        in_channels=3, \n",
    "        num_classes=2, \n",
    "        embed_dim=64, \n",
    "        depth=2, \n",
    "        dropout_p=drpout_p,\n",
    "        attn_drop_rate=drpout_p,\n",
    "        activation=activation,)\n",
    "\n",
    "    cloned_model = VisionTransformer(\n",
    "        in_channels=3, \n",
    "        num_classes=2, \n",
    "        patch_size=4, \n",
    "        embed_dim=64*2, \n",
    "        depth=2, \n",
    "        dropout_p=drpout_p,\n",
    "        attn_drop_rate=drpout_p,\n",
    "        activation=activation,)\n",
    "\n",
    "    cloned_model = model_clone(base_model, cloned_model)\n",
    "\n",
    "    test_activation_cloning(base_model, cloned_model, x, y, tolerance=tolerance)\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test the cloning functionality with various models\n",
    "#     for activation in ['relu', 'tanh', 'gelu']:\n",
    "#         for normalization in ['none', 'layer', 'batch']:\n",
    "#             test_various_models_cloning(activation=activation, normalization=normalization,drpout_p=0.0, tolerance=1e-8, check_equality=False)    \n",
    "# if __name__ == \"__main__\":\n",
    "#     test_various_models_cloning(activation=activation, normalization=normalization,drpout_p=0.0, tolerance=0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amir/Codes/NN-dynamic-scaling already in Python path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Import the utility module and Setup the path\n",
    "import notebook_utils\n",
    "notebook_utils.setup_path()\n",
    "\n",
    "# Import your cloning functions\n",
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "from src.utils.monitor import NetworkMonitor\n",
    "# Import the cloning function defined in your code\n",
    "\n",
    "# For models that require flattened input (like MLP)\n",
    "class ModelWrapper(nn.Module):\n",
    "    \"\"\"Wrapper to handle input reshaping for MLP models.\"\"\"\n",
    "    def __init__(self, model, flatten=False):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.flatten = flatten\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.flatten:\n",
    "            batch_size = x.size(0)\n",
    "            x = x.view(batch_size, -1)\n",
    "        return self.model(x)\n",
    "\n",
    "def load_cifar10(batch_size=128):\n",
    "    \"\"\"Load and prepare CIFAR-10 dataset with data augmentation.\"\"\"\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='../data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='../data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(trainloader, desc=\"Training\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': running_loss / (progress_bar.n + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(trainloader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, testloader, criterion, device):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return test_loss / len(testloader), 100. * correct / total\n",
    "\n",
    "def create_model(model_type, expanded=False, activation='relu', normalization='batch', dropout_p=0.0):\n",
    "    \"\"\"Create a model based on the specified type and configuration.\"\"\"\n",
    "    if model_type == 'mlp':\n",
    "        if expanded:\n",
    "            return MLP(input_size=3*32*32, output_size=10, hidden_sizes=[512*2, 256*2], \n",
    "                      activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "        else:\n",
    "            return MLP(input_size=3*32*32, output_size=10, hidden_sizes=[512, 256], \n",
    "                      activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "    \n",
    "    elif model_type == 'cnn':\n",
    "        if expanded:\n",
    "            return CNN(in_channels=3, num_classes=10, conv_channels=[64*2, 128*2, 256*2], \n",
    "                      activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "        else:\n",
    "            return CNN(in_channels=3, num_classes=10, conv_channels=[64, 128, 256], \n",
    "                      activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "    \n",
    "    elif model_type == 'resnet':\n",
    "        if expanded:\n",
    "            return ResNet(in_channels=3, num_classes=10, base_channels=64*2, \n",
    "                         activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "        else:\n",
    "            return ResNet(in_channels=3, num_classes=10, base_channels=64, \n",
    "                         activation=activation, dropout_p=dropout_p, normalization=normalization)\n",
    "    \n",
    "    elif model_type == 'vit':\n",
    "        if expanded:\n",
    "            return VisionTransformer(in_channels=3, num_classes=10, embed_dim=64*2, depth=2, \n",
    "                                    dropout_p=dropout_p, attn_drop_rate=dropout_p, activation=activation)\n",
    "        else:\n",
    "            return VisionTransformer(in_channels=3, num_classes=10, embed_dim=64, depth=2, \n",
    "                                    dropout_p=dropout_p, attn_drop_rate=dropout_p, activation=activation)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "def run_cloning_experiment(model_type='cnn', num_epochs=5, activation='relu', normalization='batch', \n",
    "                     dropout_p=0.0, lr=0.01, weight_decay=5e-4, batch_size=128, \n",
    "                     validate_cloning_every=2, tolerance=1e-3):\n",
    "    \"\"\"\n",
    "    Run the complete cloning experiment in a notebook environment.\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of model ('mlp', 'cnn', 'resnet', 'vit')\n",
    "        num_epochs: Number of epochs to train each phase\n",
    "        activation: Activation function to use\n",
    "        normalization: Normalization type\n",
    "        dropout_p: Dropout probability\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for optimizer\n",
    "        batch_size: Batch size for training\n",
    "        validate_cloning_every: Check cloning property every N epochs\n",
    "        tolerance: Tolerance for activation similarity check\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results dictionary containing training metrics and cloning validation results\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load CIFAR-10\n",
    "    trainloader, testloader, classes = load_cifar10(batch_size)\n",
    "    \n",
    "    # Set up criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Results tracking\n",
    "    results = {\n",
    "        'base_model': {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'epoch_times': []},\n",
    "        'cloned_model': {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'epoch_times': []},\n",
    "        'scratch_model': {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'epoch_times': []}\n",
    "    }\n",
    "    \n",
    "    # 1. Train base model\n",
    "    print(f\"\\n{'='*20} Training base {model_type.upper()} model {'='*20}\")\n",
    "    base_model = create_model(model_type, expanded=False, activation=activation, \n",
    "                            normalization=normalization, dropout_p=dropout_p)\n",
    "    \n",
    "    # Wrap MLP model to handle input reshaping\n",
    "    needs_flatten = model_type == 'mlp'\n",
    "    base_model = ModelWrapper(base_model, flatten=needs_flatten).to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(base_model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(base_model, trainloader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(base_model, testloader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        results['base_model']['train_loss'].append(train_loss)\n",
    "        results['base_model']['train_acc'].append(train_acc)\n",
    "        results['base_model']['test_loss'].append(test_loss)\n",
    "        results['base_model']['test_acc'].append(test_acc)\n",
    "        results['base_model']['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        print(f\"Base model - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # 2. Clone and continue training\n",
    "    print(f\"\\n{'='*20} Training cloned {model_type.upper()} model {'='*20}\")\n",
    "    expanded_model = create_model(model_type, expanded=True, activation=activation, \n",
    "                                normalization=normalization, dropout_p=dropout_p)\n",
    "    \n",
    "    # Extract the inner model for cloning if wrapped\n",
    "    inner_base_model = base_model.model if needs_flatten else base_model\n",
    "    \n",
    "    # Clone the model\n",
    "    cloned_model_inner = model_clone(inner_base_model, expanded_model)\n",
    "    cloned_model = ModelWrapper(cloned_model_inner, flatten=needs_flatten).to(device)\n",
    "    \n",
    "    # Create an identical model for reference to verify cloning properties\n",
    "    reference_model = create_model(model_type, expanded=False, activation=activation, \n",
    "                                 normalization=normalization, dropout_p=dropout_p)\n",
    "    reference_model.load_state_dict(inner_base_model.state_dict())\n",
    "    reference_model = reference_model.to(device)\n",
    "    \n",
    "    # Validate initial cloning\n",
    "    print(\"Validating initial cloning properties...\")\n",
    "    # Get a small batch for validation\n",
    "    val_inputs, val_targets = next(iter(testloader))\n",
    "    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "    \n",
    "    # Cloning validation results\n",
    "    cloning_validation_results = {'epochs': [], 'unexplained_variance': []}\n",
    "    \n",
    "    # Initial validation\n",
    "    if needs_flatten:\n",
    "        inner_model = cloned_model.model\n",
    "        val_inputs_flattened = val_inputs.view(val_inputs.size(0), -1)\n",
    "        try:\n",
    "            unexplained_var = test_activation_cloning(\n",
    "                reference_model, inner_model, val_inputs_flattened, val_targets, \n",
    "                tolerance=tolerance, check_equality=False, \n",
    "            )\n",
    "            cloning_validation_results['epochs'].append(0)\n",
    "            cloning_validation_results['unexplained_variance'].append(unexplained_var)\n",
    "            print(f\"Initial cloning validation passed! Unexplained variance: {unexplained_var:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Initial cloning validation failed: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            unexplained_var = test_activation_cloning(\n",
    "                reference_model, cloned_model, val_inputs, val_targets, \n",
    "                tolerance=tolerance, check_equality=False, \n",
    "            )\n",
    "            cloning_validation_results['epochs'].append(0)\n",
    "            cloning_validation_results['unexplained_variance'].append(unexplained_var)\n",
    "            print(f\"Initial cloning validation passed! Unexplained variance: {unexplained_var:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Initial cloning validation failed: {e}\")\n",
    "    \n",
    "    optimizer = optim.SGD(cloned_model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(cloned_model, trainloader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(cloned_model, testloader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        results['cloned_model']['train_loss'].append(train_loss)\n",
    "        results['cloned_model']['train_acc'].append(train_acc)\n",
    "        results['cloned_model']['test_loss'].append(test_loss)\n",
    "        results['cloned_model']['test_acc'].append(test_acc)\n",
    "        results['cloned_model']['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        print(f\"Cloned model - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Periodically validate cloning properties\n",
    "        if (epoch + 1) % validate_cloning_every == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Validating cloning properties after epoch {epoch+1}...\")\n",
    "            # Get a fresh batch for validation\n",
    "            val_inputs, val_targets = next(iter(testloader))\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "            \n",
    "            try:\n",
    "                # Need to handle MLP differently due to input flattening\n",
    "                if needs_flatten:\n",
    "                    inner_model = cloned_model.model\n",
    "                    val_inputs_flattened = val_inputs.view(val_inputs.size(0), -1)\n",
    "                    unexplained_vars = test_activation_cloning(\n",
    "                        reference_model, inner_model, val_inputs_flattened, val_targets, \n",
    "                        tolerance=tolerance, check_equality=False, \n",
    "                    )\n",
    "                else:\n",
    "                    unexplained_vars = test_activation_cloning(\n",
    "                        reference_model, cloned_model, val_inputs, val_targets, \n",
    "                        tolerance=tolerance, check_equality=False, \n",
    "                    )\n",
    "                \n",
    "                cloning_validation_results['epochs'].append(epoch + 1)\n",
    "                cloning_validation_results['unexplained_variance'].append(unexplained_vars)\n",
    "                print(f\"Cloning validation passed! Unexplained variance: {sum(unexplained_vars)/len(unexplained_vars):.6f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Cloning validation failed: {e}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # Save cloned model and cloning validation results\n",
    "    cloned_model_path = os.path.join(save_path, f\"{model_type}_cloned_model.pth\")\n",
    "    torch.save(cloned_model.state_dict(), cloned_model_path)\n",
    "    print(f\"Cloned model saved to {cloned_model_path}\")\n",
    "    \n",
    "    # Save cloning validation results\n",
    "    cloning_results_path = os.path.join(save_path, f\"{model_type}_cloning_validation.pth\")\n",
    "    torch.save(cloning_validation_results, cloning_results_path)\n",
    "    \n",
    "    # Plot cloning validation results\n",
    "    if cloning_validation_results['epochs']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = cloning_validation_results['epochs']\n",
    "        unexplained_var = cloning_validation_results['unexplained_variance']\n",
    "        \n",
    "        # Filter out None values if any validation failed\n",
    "        valid_points = [(e, v) for e, v in zip(epochs, unexplained_var) if v is not None]\n",
    "        if valid_points:\n",
    "            valid_epochs, valid_vars = zip(*valid_points)\n",
    "            plt.plot(valid_epochs, valid_vars, 'b-o', label='Unexplained Variance')\n",
    "            plt.axhline(y=tolerance, color='r', linestyle='--', label=f'Tolerance ({tolerance})')\n",
    "            plt.title(f'Cloning Property Validation - {model_type.upper()}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Unexplained Variance')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_path, f\"{model_type}_cloning_validation.png\"))\n",
    "            plt.close()\n",
    "    \n",
    "    # 3. Train from scratch for 2*num_epochs\n",
    "    print(f\"\\n{'='*20} Training expanded {model_type.upper()} model from scratch {'='*20}\")\n",
    "    scratch_model_inner = create_model(model_type, expanded=True, activation=activation, \n",
    "                                     normalization=normalization, dropout_p=dropout_p)\n",
    "    scratch_model = ModelWrapper(scratch_model_inner, flatten=needs_flatten).to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(scratch_model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2*num_epochs)\n",
    "    \n",
    "    for epoch in range(2*num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{2*num_epochs}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(scratch_model, trainloader, criterion, optimizer, device)\n",
    "        test_loss, test_acc = evaluate(scratch_model, testloader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        results['scratch_model']['train_loss'].append(train_loss)\n",
    "        results['scratch_model']['train_acc'].append(train_acc)\n",
    "        results['scratch_model']['test_loss'].append(test_loss)\n",
    "        results['scratch_model']['test_acc'].append(test_acc)\n",
    "        results['scratch_model']['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        print(f\"Scratch model - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # Save scratch model\n",
    "    scratch_model_path = os.path.join(save_path, f\"{model_type}_scratch_model.pth\")\n",
    "    torch.save(scratch_model.state_dict(), scratch_model_path)\n",
    "    print(f\"Scratch model saved to {scratch_model_path}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(save_path, f\"{model_type}_results.pth\")\n",
    "    torch.save(results, results_path)\n",
    "    \n",
    "    # Plot and save results\n",
    "    plot_results(results, model_type, num_epochs, save_path)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results, model_type, num_epochs):\n",
    "    \"\"\"Plot training and testing curves for all models.\"\"\"\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    \n",
    "    # Set up x-axis for each model\n",
    "    epochs_base = list(range(1, num_epochs + 1))\n",
    "    epochs_cloned = list(range(num_epochs + 1, 2 * num_epochs + 1))\n",
    "    epochs_scratch = list(range(1, 2 * num_epochs + 1))\n",
    "    \n",
    "    # Combined epochs for full timeline view\n",
    "    epochs_combined = list(range(1, 2 * num_epochs + 1))\n",
    "    \n",
    "    # Plot train loss\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(epochs_base, results['base_model']['train_loss'], 'b-', label='Base Model')\n",
    "    plt.plot(epochs_cloned, results['cloned_model']['train_loss'], 'r-', label='Cloned Model')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['train_loss'], 'g-', label='From Scratch')\n",
    "    plt.title(f'Training Loss - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot train accuracy\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs_base, results['base_model']['train_acc'], 'b-', label='Base Model')\n",
    "    plt.plot(epochs_cloned, results['cloned_model']['train_acc'], 'r-', label='Cloned Model')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['train_acc'], 'g-', label='From Scratch')\n",
    "    plt.title(f'Training Accuracy - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot test loss\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(epochs_base, results['base_model']['test_loss'], 'b-', label='Base Model')\n",
    "    plt.plot(epochs_cloned, results['cloned_model']['test_loss'], 'r-', label='Cloned Model')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['test_loss'], 'g-', label='From Scratch')\n",
    "    plt.title(f'Test Loss - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs_base, results['base_model']['test_acc'], 'b-', label='Base Model')\n",
    "    plt.plot(epochs_cloned, results['cloned_model']['test_acc'], 'r-', label='Cloned Model')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['test_acc'], 'g-', label='From Scratch')\n",
    "    plt.title(f'Test Accuracy - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot epoch times\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs_base, results['base_model']['epoch_times'], 'b-', label='Base Model')\n",
    "    plt.plot(epochs_cloned, results['cloned_model']['epoch_times'], 'r-', label='Cloned Model')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['epoch_times'], 'g-', label='From Scratch')\n",
    "    plt.title(f'Epoch Training Time - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Combined test accuracy plot (base → cloned vs. scratch)\n",
    "    plt.subplot(3, 2, 6)\n",
    "    # Combine base and cloned for continuous line\n",
    "    combined_acc = results['base_model']['test_acc'] + results['cloned_model']['test_acc']\n",
    "    plt.plot(epochs_combined, combined_acc, 'b-', label='Base → Cloned')\n",
    "    plt.plot(epochs_scratch, results['scratch_model']['test_acc'], 'g-', label='From Scratch')\n",
    "    plt.axvline(x=num_epochs, color='r', linestyle='--', label='Cloning Point')\n",
    "    plt.title(f'Test Accuracy Comparison - {model_type.upper()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cloning validation results if available\n",
    "    if 'cloning_validation' in results and results['cloning_validation']['epochs']:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = results['cloning_validation']['epochs']\n",
    "        unexplained_var = results['cloning_validation']['unexplained_variance']\n",
    "        \n",
    "        plt.plot(epochs, unexplained_var, 'b-o', label='Unexplained Variance')\n",
    "        plt.title(f'Cloning Property Validation - {model_type.upper()}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Unexplained Variance')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nExperiment Summary:\")\n",
    "    print(f\"{'Model Type':15} {'Final Acc':12} {'Best Acc':12} {'Final Loss':12} {'Avg Time/Epoch':15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    final_results = {\n",
    "        'Base Model': {\n",
    "            'Final Test Acc': results['base_model']['test_acc'][-1],\n",
    "            'Best Test Acc': max(results['base_model']['test_acc']),\n",
    "            'Final Test Loss': results['base_model']['test_loss'][-1],\n",
    "            'Avg Epoch Time': np.mean(results['base_model']['epoch_times'])\n",
    "        },\n",
    "        'Cloned Model': {\n",
    "            'Final Test Acc': results['cloned_model']['test_acc'][-1],\n",
    "            'Best Test Acc': max(results['cloned_model']['test_acc']),\n",
    "            'Final Test Loss': results['cloned_model']['test_loss'][-1],\n",
    "            'Avg Epoch Time': np.mean(results['cloned_model']['epoch_times'])\n",
    "        },\n",
    "        'Scratch Model': {\n",
    "            'Final Test Acc': results['scratch_model']['test_acc'][-1],\n",
    "            'Best Test Acc': max(results['scratch_model']['test_acc']),\n",
    "            'Final Test Loss': results['scratch_model']['test_loss'][-1],\n",
    "            'Avg Epoch Time': np.mean(results['scratch_model']['epoch_times'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for model, metrics in final_results.items():\n",
    "        print(f\"{model:15} {metrics['Final Test Acc']:12.2f} {metrics['Best Test Acc']:12.2f} \"\n",
    "              f\"{metrics['Final Test Loss']:12.4f} {metrics['Avg Epoch Time']:15.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "==================== Training base MLP model ====================\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:07<00:00, 54.38it/s, loss=1.77, acc=36.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model - Train Loss: 1.7612, Train Acc: 36.22%, Test Loss: 1.5666, Test Acc: 43.66%, Time: 9.44s\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:07<00:00, 54.27it/s, loss=1.62, acc=42.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model - Train Loss: 1.5917, Train Acc: 42.46%, Test Loss: 1.4822, Test Acc: 46.78%, Time: 9.50s\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:07<00:00, 54.59it/s, loss=1.54, acc=45.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model - Train Loss: 1.5153, Train Acc: 45.41%, Test Loss: 1.4124, Test Acc: 49.16%, Time: 9.46s\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:07<00:00, 54.07it/s, loss=1.45, acc=48.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model - Train Loss: 1.4479, Train Acc: 48.13%, Test Loss: 1.3661, Test Acc: 50.86%, Time: 9.51s\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|█████████▊| 385/391 [00:06<00:00, 64.82it/s, loss=1.43, acc=49.4]"
     ]
    }
   ],
   "source": [
    "# Example usage in a notebook\n",
    "results = run_cloning_experiment(\n",
    "    model_type='mlp',  # Options: 'mlp', 'cnn', 'resnet', 'vit'\n",
    "    num_epochs=5,\n",
    "    activation='relu',\n",
    "    normalization='batch',\n",
    "    dropout_p=0.0,\n",
    "    validate_cloning_every=1,\n",
    "    tolerance=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
