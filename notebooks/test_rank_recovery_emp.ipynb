{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Configuration: MLP NoNorm ---\n",
      "Using fixed val batch of size 512 for rank eval.\n",
      "Logging ranks for Epoch 0 (Initialization) for MLP NoNorm...\n",
      "Epoch 0 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:30.4 | H1_ReLU:48.5 | H2_Linear:26.3 | H2_ReLU:38.0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:14.4 | H1_ReLU:31.3 | H2_Linear:16.1 | H2_ReLU:28.2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:18.3 | H1_ReLU:38.3 | H2_Linear:18.6 | H2_ReLU:31.4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:21.2 | H1_ReLU:42.8 | H2_Linear:20.2 | H2_ReLU:32.9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:22.6 | H1_ReLU:44.8 | H2_Linear:21.1 | H2_ReLU:34.9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:24.8 | H1_ReLU:48.0 | H2_Linear:20.3 | H2_ReLU:35.1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:25.9 | H1_ReLU:48.7 | H2_Linear:21.0 | H2_ReLU:36.4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:27.1 | H1_ReLU:50.5 | H2_Linear:21.8 | H2_ReLU:37.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:27.6 | H1_ReLU:50.7 | H2_Linear:22.0 | H2_ReLU:37.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:29.6 | H1_ReLU:53.0 | H2_Linear:22.7 | H2_ReLU:38.8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:30.9 | H1_ReLU:54.7 | H2_Linear:22.6 | H2_ReLU:37.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:31.9 | H1_ReLU:55.7 | H2_Linear:23.7 | H2_ReLU:39.4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:31.7 | H1_ReLU:55.7 | H2_Linear:24.1 | H2_ReLU:39.6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:32.6 | H1_ReLU:56.6 | H2_Linear:24.6 | H2_ReLU:40.5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:33.6 | H1_ReLU:57.6 | H2_Linear:25.1 | H2_ReLU:41.0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 [MLP NoNorm] Ranks: Input:34.7 | H1_Linear:33.0 | H1_ReLU:56.6 | H2_Linear:24.1 | H2_ReLU:40.4 ...\n",
      "Finished training for MLP NoNorm\n",
      "\n",
      "--- Running Configuration: MLP BatchNorm ---\n",
      "Using fixed val batch of size 512 for rank eval.\n",
      "Logging ranks for Epoch 0 (Initialization) for MLP BatchNorm...\n",
      "Epoch 0 [MLP BatchNorm] Ranks: Input:34.7 | H1_Linear:29.7 | H1_BatchNorm:29.7 | H1_ReLU:47.7 | H2_Linear:27.8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [MLP BatchNorm] Ranks: Input:34.7 | H1_Linear:11.6 | H1_BatchNorm:11.6 | H1_ReLU:23.9 | H2_Linear:14.0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [MLP BatchNorm]:  81%|████████  | 158/196 [00:01<00:00, 84.69it/s, loss=1.457]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D # For custom legends if needed later\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Matplotlib Styling (Optional) ---\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 13, # Slightly smaller for subplots\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 8, # Smaller for potentially many layer names\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 8,\n",
    "    'figure.titlesize': 15, # Main figure title\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.markersize': 4\n",
    "})\n",
    "\n",
    "# --- 1. Helper Functions ---\n",
    "\n",
    "def compute_effective_rank(activations_batch):\n",
    "    if activations_batch.ndim == 1:\n",
    "        if activations_batch.shape[0] > 1: return 1.0\n",
    "        else: activations_batch = activations_batch.unsqueeze(0)\n",
    "    if activations_batch.shape[1] == 0: return 0.0\n",
    "    if activations_batch.shape[1] == 1: return 1.0\n",
    "    if activations_batch.shape[0] <= 1: return 1.0\n",
    "\n",
    "    std_devs = torch.std(activations_batch, dim=0)\n",
    "    valid_features_mask = std_devs > 1e-5\n",
    "    if valid_features_mask.sum() < 2: return float(valid_features_mask.sum().item())\n",
    "    activations_batch_filtered = activations_batch[:, valid_features_mask]\n",
    "\n",
    "    try:\n",
    "        if activations_batch_filtered.shape[0] <= 1 or activations_batch_filtered.shape[1] < 2:\n",
    "            return float(activations_batch_filtered.shape[1] > 0)\n",
    "        corr_matrix = torch.corrcoef(activations_batch_filtered.T)\n",
    "    except RuntimeError: return 1.0\n",
    "    if torch.isnan(corr_matrix).any(): corr_matrix = torch.nan_to_num(corr_matrix, nan=0.0)\n",
    "\n",
    "    s_unnormalized = torch.linalg.svdvals(corr_matrix)\n",
    "    sum_s = torch.sum(s_unnormalized)\n",
    "    if sum_s < 1e-12: return 0.0\n",
    "    s_norm_for_entropy = s_unnormalized / sum_s\n",
    "    s_norm_for_entropy = s_norm_for_entropy[s_norm_for_entropy > 1e-15]\n",
    "    if len(s_norm_for_entropy) == 0: return 0.0\n",
    "    entropy = -torch.sum(s_norm_for_entropy * torch.log(s_norm_for_entropy))\n",
    "    return torch.exp(entropy).item()\n",
    "\n",
    "def get_activation_fn_and_name(activation_name_str):\n",
    "    name = activation_name_str.lower()\n",
    "    if name == \"relu\": return nn.ReLU(), \"ReLU\"\n",
    "    elif name == \"tanh\": return nn.Tanh(), \"Tanh\"\n",
    "    elif name == \"sigmoid\": return nn.Sigmoid(), \"Sigmoid\"\n",
    "    elif name == \"gelu\": return nn.GELU(), \"GELU\"\n",
    "    elif name == \"identity\": return nn.Identity(), \"Identity\"\n",
    "    else: raise ValueError(f\"Unsupported activation: {activation_name_str}\")\n",
    "\n",
    "# --- 2. Model Definition ---\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, norm_type, activation_fn_instance, dropout_p, is_output_layer_block, block_idx):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.norm = None\n",
    "        self.act_fn_instance = activation_fn_instance # Store for naming\n",
    "        self.norm_type_str = norm_type # Store for naming\n",
    "        self.is_output_layer_block = is_output_layer_block\n",
    "        self.block_idx = block_idx # e.g., 0 for H1, 1 for H2...\n",
    "\n",
    "        layer_prefix = f\"H{block_idx + 1}\" if not is_output_layer_block else \"Output\"\n",
    "        self.layer_names = {\"linear\": f\"{layer_prefix}_Linear\"}\n",
    "\n",
    "\n",
    "        if not is_output_layer_block and norm_type:\n",
    "            if norm_type.lower() == \"batchnorm\":\n",
    "                self.norm = nn.BatchNorm1d(out_dim)\n",
    "                self.layer_names[\"norm\"] = f\"{layer_prefix}_BatchNorm\"\n",
    "            elif norm_type.lower() == \"layernorm\":\n",
    "                self.norm = nn.LayerNorm(out_dim)\n",
    "                self.layer_names[\"norm\"] = f\"{layer_prefix}_LayerNorm\"\n",
    "            elif norm_type.lower() != \"none\":\n",
    "                raise ValueError(f\"Unsupported norm_type: {norm_type}\")\n",
    "        \n",
    "        if not is_output_layer_block:\n",
    "            self.act = activation_fn_instance\n",
    "            act_name = self.act.__class__.__name__ if not isinstance(self.act, nn.modules.activation.Tanh) else \"Tanh\" # Tanh doesn't have a clean name sometimes\n",
    "            if isinstance(self.act, nn.Identity): act_name = \"Identity\" # Handle explicit Identity\n",
    "            self.layer_names[\"act\"] = f\"{layer_prefix}_{act_name}\"\n",
    "        else:\n",
    "            self.act = nn.Identity() # Output layer has no activation for logits\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p) if not is_output_layer_block and dropout_p > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, record_activations_in_block=False):\n",
    "        recorded_stages_in_block = [] # List of (name, tensor)\n",
    "\n",
    "        x_linear = self.linear(x)\n",
    "        if record_activations_in_block:\n",
    "            recorded_stages_in_block.append((self.layer_names[\"linear\"], x_linear.detach().cpu()))\n",
    "        \n",
    "        current_x = x_linear\n",
    "        if not self.is_output_layer_block:\n",
    "            if self.norm:\n",
    "                current_x = self.norm(current_x)\n",
    "                if record_activations_in_block:\n",
    "                    recorded_stages_in_block.append((self.layer_names[\"norm\"], current_x.detach().cpu()))\n",
    "            \n",
    "            current_x = self.act(current_x)\n",
    "            if record_activations_in_block:\n",
    "                recorded_stages_in_block.append((self.layer_names[\"act\"], current_x.detach().cpu()))\n",
    "            \n",
    "            output_for_next_layer = self.dropout(current_x)\n",
    "        else: # Output layer block\n",
    "            output_for_next_layer = current_x # This is the logits\n",
    "\n",
    "        if record_activations_in_block:\n",
    "            return output_for_next_layer, recorded_stages_in_block\n",
    "        else:\n",
    "            return output_for_next_layer, []\n",
    "\n",
    "\n",
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, layer_dims, activation_name=\"relu\", norm_type=None, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.layer_dims = layer_dims\n",
    "        self.input_dim = layer_dims[0]\n",
    "        self.activation_name_str = activation_name # Store for reference\n",
    "        self.norm_type_str = norm_type\n",
    "        \n",
    "        act_fn_instance, _ = get_activation_fn_and_name(activation_name)\n",
    "        self.layer_stage_names = [\"Input\"] # Start with input\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            is_final_block = (i == len(layer_dims) - 2)\n",
    "            block = MLPBlock(\n",
    "                layer_dims[i], layer_dims[i+1],\n",
    "                norm_type, act_fn_instance, dropout_p,\n",
    "                is_output_layer_block=is_final_block,\n",
    "                block_idx=i \n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "            # Collect layer names from the block\n",
    "            self.layer_stage_names.append(block.layer_names[\"linear\"])\n",
    "            if \"norm\" in block.layer_names:\n",
    "                self.layer_stage_names.append(block.layer_names[\"norm\"])\n",
    "            if \"act\" in block.layer_names: # \"act\" only if not output block\n",
    "                 self.layer_stage_names.append(block.layer_names[\"act\"])\n",
    "            \n",
    "    def forward(self, x, record_activations=False):\n",
    "        x_flattened = x.view(x.size(0), -1)\n",
    "        if x_flattened.shape[1] != self.input_dim:\n",
    "            raise ValueError(f\"Input dim mismatch: {x_flattened.shape[1]} vs {self.input_dim}\")\n",
    "\n",
    "        all_recorded_stages = [] # List of (name, tensor)\n",
    "        current_features = x_flattened\n",
    "        \n",
    "        if record_activations:\n",
    "            all_recorded_stages.append((\"Input\", current_features.detach().cpu()))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            current_features, block_recorded_activations = block(current_features, record_activations_in_block=record_activations)\n",
    "            if record_activations:\n",
    "                all_recorded_stages.extend(block_recorded_activations)\n",
    "        \n",
    "        final_model_output = current_features\n",
    "\n",
    "        if record_activations:\n",
    "            return final_model_output, all_recorded_stages\n",
    "        return final_model_output\n",
    "\n",
    "# --- 3. Data Handling ---\n",
    "def get_cifar10_dataloaders(batch_size=128, num_workers=2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    try:\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) # Changed path\n",
    "    except Exception as e: print(f\"Error training set: {e}\"); raise\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    try:\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) # Changed path\n",
    "    except Exception as e: print(f\"Error test set: {e}\"); raise\n",
    "    testloader = DataLoader(testset, batch_size=batch_size*2, shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    return trainloader, testloader\n",
    "\n",
    "# --- 4. Training and Evaluation Loop with Rank Logging ---\n",
    "def _log_ranks_for_epoch(model, fixed_val_images, epoch_num, model_config_name, all_rank_data_list):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, recorded_stages = model(fixed_val_images, record_activations=True)\n",
    "        current_epoch_ranks_info = []\n",
    "        for stage_name, acts_batch in recorded_stages:\n",
    "            eff_rank = np.nan\n",
    "            if acts_batch.size(0) > 1 and acts_batch.ndim > 1 and acts_batch.shape[1] > 0:\n",
    "                eff_rank = compute_effective_rank(acts_batch)\n",
    "            \n",
    "            all_rank_data_list.append({\n",
    "                'model_config': model_config_name, 'epoch': epoch_num,\n",
    "                'layer_name': stage_name, 'eff_rank': eff_rank\n",
    "            })\n",
    "            current_epoch_ranks_info.append(f\"{stage_name}:{eff_rank:.1f}\" if not np.isnan(eff_rank) else f\"{stage_name}:NaN\")\n",
    "    \n",
    "    print(f\"Epoch {epoch_num} [{model_config_name}] Ranks: {' | '.join(current_epoch_ranks_info[:5])} ...\") # Print first few\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_with_rank_logging(\n",
    "    model_config_name, model, train_loader, val_loader, \n",
    "    optimizer, criterion, num_epochs, device, \n",
    "    log_rank_every_n_epochs=1, rank_eval_batch_size=256\n",
    "    ):\n",
    "    \n",
    "    all_rank_data = [] \n",
    "    fixed_val_images = None \n",
    "    try: # Prepare fixed validation batch\n",
    "        fixed_val_images_list = []\n",
    "        num_batches_for_rank = 1\n",
    "        if val_loader.batch_size and val_loader.batch_size > 0 :\n",
    "            num_batches_for_rank = max(1, rank_eval_batch_size // val_loader.batch_size)\n",
    "        elif rank_eval_batch_size > 0 :\n",
    "             print(f\"Warning: val_loader.batch_size is None/0. Will fetch {rank_eval_batch_size} samples.\")\n",
    "        else: raise ValueError(\"val_loader.batch_size is None/0, and rank_eval_batch_size not positive.\")\n",
    "\n",
    "        val_iter_for_rank = iter(val_loader)\n",
    "        current_fetched_samples = 0\n",
    "        while current_fetched_samples < rank_eval_batch_size:\n",
    "            try:\n",
    "                imgs, _ = next(val_iter_for_rank)\n",
    "                fixed_val_images_list.append(imgs)\n",
    "                current_fetched_samples += imgs.size(0)\n",
    "            except StopIteration: print(\"Warning: Val loader exhausted for rank batch.\"); break\n",
    "        if not fixed_val_images_list: raise ValueError(\"Val loader empty/too small for rank batch.\")\n",
    "        fixed_val_images_cat = torch.cat(fixed_val_images_list, dim=0)\n",
    "        if fixed_val_images_cat.size(0) > rank_eval_batch_size:\n",
    "            fixed_val_images_cat = fixed_val_images_cat[:rank_eval_batch_size]\n",
    "        if fixed_val_images_cat.size(0) == 0: raise ValueError(\"No images for fixed_val_images.\")\n",
    "        fixed_val_images = fixed_val_images_cat.to(device)\n",
    "        print(f\"Using fixed val batch of size {fixed_val_images.shape[0]} for rank eval.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not prepare fixed val batch: {e}. Falling back to train batch.\")\n",
    "        try:\n",
    "            fb_imgs, _ = next(iter(train_loader)) \n",
    "            if fb_imgs.size(0) > rank_eval_batch_size: fb_imgs = fb_imgs[:rank_eval_batch_size]\n",
    "            fixed_val_images = fb_imgs.to(device)\n",
    "        except Exception as fallback_e: print(f\"Fallback failed: {fallback_e}. No rank logging.\"); return pd.DataFrame(all_rank_data)\n",
    "\n",
    "    # --- Log Ranks at Epoch 0 (Initialization) ---\n",
    "    if fixed_val_images is not None and fixed_val_images.nelement() > 0:\n",
    "        print(f\"Logging ranks for Epoch 0 (Initialization) for {model_config_name}...\")\n",
    "        _log_ranks_for_epoch(model, fixed_val_images, 0, model_config_name, all_rank_data)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                            desc=f\"Epoch {epoch+1}/{num_epochs} [{model_config_name}]\", leave=False)\n",
    "        for i, data in progress_bar:\n",
    "            inputs, labels = data; inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "            loss.backward(); optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0: progress_bar.set_postfix({'loss': f'{running_loss / 100:.3f}'}); running_loss = 0.0\n",
    "        \n",
    "        if (epoch + 1) % log_rank_every_n_epochs == 0 and fixed_val_images is not None and fixed_val_images.nelement() > 0:\n",
    "            _log_ranks_for_epoch(model, fixed_val_images, epoch + 1, model_config_name, all_rank_data)\n",
    "\n",
    "    print(f\"Finished training for {model_config_name}\")\n",
    "    return pd.DataFrame(all_rank_data)\n",
    "\n",
    "# --- 5. Main Experiment Orchestration ---\n",
    "def run_main_experiment(\n",
    "    num_epochs=10, log_rank_every_n_epochs=1, mlp_hidden_layers=[256, 128], \n",
    "    activation_name=\"relu\", dropout_p=0.1, learning_rate=1e-3,\n",
    "    batch_size=256, rank_eval_batch_size_config=512\n",
    "    ):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "    try: train_loader, val_loader = get_cifar10_dataloaders(batch_size=batch_size)\n",
    "    except Exception as e: print(f\"Failed to load CIFAR-10: {e}. Aborting.\"); return\n",
    "\n",
    "    input_dim = 32 * 32 * 3; num_classes = 10\n",
    "    layer_dims_template = [input_dim] + mlp_hidden_layers + [num_classes]\n",
    "\n",
    "    configurations = {\n",
    "        \"MLP NoNorm\": {\"norm_type\": None},\n",
    "        \"MLP BatchNorm\": {\"norm_type\": \"batchnorm\"},\n",
    "        \"MLP LayerNorm\": {\"norm_type\": \"layernorm\"}\n",
    "    }\n",
    "    all_experimental_rank_data = []\n",
    "    # To get consistent layer stage names for plotting x-axis later\n",
    "    sample_model_for_layer_names = ConfigurableMLP(layer_dims_template, activation_name, None, 0.0)\n",
    "    plot_layer_stage_names = sample_model_for_layer_names.layer_stage_names\n",
    "    del sample_model_for_layer_names\n",
    "\n",
    "\n",
    "    for config_name, params in configurations.items():\n",
    "        print(f\"\\n--- Running Configuration: {config_name} ---\")\n",
    "        model = ConfigurableMLP(layer_dims_template, activation_name, params[\"norm_type\"], dropout_p).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate); criterion = nn.CrossEntropyLoss()\n",
    "        rank_data_df = train_model_with_rank_logging(\n",
    "            config_name, model, train_loader, val_loader, optimizer, criterion, \n",
    "            num_epochs, device, log_rank_every_n_epochs, rank_eval_batch_size_config\n",
    "        )\n",
    "        all_experimental_rank_data.append(rank_data_df)\n",
    "        del model; \n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    if not all_experimental_rank_data or all(df.empty for df in all_experimental_rank_data):\n",
    "        print(\"No rank data. Exiting plot.\"); return\n",
    "    final_rank_df = pd.concat(all_experimental_rank_data, ignore_index=True)\n",
    "\n",
    "    # --- 6. Visualization ---\n",
    "    if final_rank_df.empty: print(\"Final rank df empty. Skipping plot.\"); return\n",
    "        \n",
    "    num_configs = len(configurations)\n",
    "    fig, axs = plt.subplots(num_configs, 1, figsize=(12, 6 * num_configs), sharex=True, sharey=True)\n",
    "    if num_configs == 1: axs = [axs] # Ensure axs is always a list\n",
    "\n",
    "    unique_epochs = sorted(final_rank_df['epoch'].dropna().unique())\n",
    "    if not unique_epochs: print(\"No epochs in data. Skipping plot.\"); return\n",
    "    \n",
    "    epochs_to_plot = unique_epochs\n",
    "    if len(unique_epochs) > 10: # Select a subset if too many epochs for clarity\n",
    "        indices = np.round(np.linspace(0, len(unique_epochs) - 1, min(10, len(unique_epochs)))).astype(int)\n",
    "        epochs_to_plot = [unique_epochs[i] for i in indices]\n",
    "\n",
    "    epoch_cmap = plt.cm.get_cmap('viridis', len(epochs_to_plot))\n",
    "    \n",
    "    # Map layer names to integer indices for consistent plotting on x-axis\n",
    "    # Use the predefined plot_layer_stage_names derived from a sample model\n",
    "    layer_name_to_idx = {name: i for i, name in enumerate(plot_layer_stage_names)}\n",
    "\n",
    "\n",
    "    for i, (config_name, params_config) in enumerate(configurations.items()):\n",
    "        ax = axs[i]\n",
    "        config_data = final_rank_df[final_rank_df['model_config'] == config_name]\n",
    "        if config_data.empty: ax.set_title(f\"{config_name} (No Data)\"); continue\n",
    "\n",
    "        for epoch_idx, epoch in enumerate(epochs_to_plot):\n",
    "            epoch_specific_data = config_data[config_data['epoch'] == epoch].copy()\n",
    "            if not epoch_specific_data.empty:\n",
    "                # Map layer names to their pre-defined indices for x-axis\n",
    "                epoch_specific_data['plot_x_idx'] = epoch_specific_data['layer_name'].map(layer_name_to_idx)\n",
    "                epoch_specific_data.sort_values(by='plot_x_idx', inplace=True)\n",
    "                \n",
    "                # Filter out rows where plot_x_idx might be NaN (if a layer_name was unexpected)\n",
    "                epoch_specific_data_plot = epoch_specific_data.dropna(subset=['plot_x_idx', 'eff_rank'])\n",
    "\n",
    "                if not epoch_specific_data_plot.empty:\n",
    "                    ax.plot(epoch_specific_data_plot['plot_x_idx'], epoch_specific_data_plot['eff_rank'],\n",
    "                            color=epoch_cmap(epoch_idx), \n",
    "                            linestyle='-', marker='o', alpha=0.7, markersize=5,\n",
    "                            label=f\"Epoch {epoch}\" if i == 0 else None # Label epochs only on the first subplot's lines\n",
    "                           )\n",
    "        \n",
    "        ax.set_title(f\"Effective Rank: {config_name} (Activation: {activation_name.upper()})\")\n",
    "        ax.set_ylabel(\"Effective Rank\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        if i == num_configs - 1: # X-axis labels only on the last subplot\n",
    "            ax.set_xticks(list(layer_name_to_idx.values()))\n",
    "            ax.set_xticklabels([name.replace(\"_\", \" \") for name in plot_layer_stage_names], rotation=45, ha='right')\n",
    "            ax.set_xlabel(\"Layer Stage\")\n",
    "        else:\n",
    "            ax.set_xticklabels([]) # No x-labels for upper subplots\n",
    "\n",
    "    # Common colorbar for epochs\n",
    "    norm_epochs = plt.Normalize(vmin=min(epochs_to_plot), vmax=max(epochs_to_plot))\n",
    "    sm = plt.cm.ScalarMappable(cmap=epoch_cmap, norm=norm_epochs)\n",
    "    sm.set_array([])\n",
    "    \n",
    "    # Position colorbar to the right of all subplots\n",
    "    fig.subplots_adjust(right=0.85) # Make space for colorbar\n",
    "    cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7]) # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, orientation='vertical', label='Epoch')\n",
    "    if epochs_to_plot:\n",
    "        cbar.set_ticks(epochs_to_plot)\n",
    "        cbar.ax.set_yticklabels([str(e) for e in epochs_to_plot])\n",
    "    \n",
    "    fig.tight_layout(rect=[0, 0, 0.85, 0.96]) # Adjust rect to prevent overlap with colorbar and title\n",
    "    plt.suptitle(f\"Effective Rank Evolution Across Layers and Epochs\", y=0.99)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_main_experiment(\n",
    "        num_epochs=15,            \n",
    "        log_rank_every_n_epochs=1, \n",
    "        mlp_hidden_layers=[128, 64], # Smaller for quicker test\n",
    "        activation_name=\"relu\",   \n",
    "        dropout_p=0.0, # No dropout for clearer rank signal initially\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "        rank_eval_batch_size_config=512 \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
