{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6b4e25-d707-4026-9cc6-489c207da0c6",
   "metadata": {},
   "source": [
    "## Theory Overview: Stability of Neuronal Coupling via the Restricted Hessian\n",
    "\n",
    "## Manifold Stability and the Symmetric‐Units Example\n",
    "\n",
    "### 1. Manifold Stability (General Concept)\n",
    "\n",
    "In a **gradient flow** system, parameters $\\theta$ evolve over time according to \n",
    "$$\n",
    "\\frac{d\\theta}{dt} \\;=\\; -\\,\\nabla f(\\theta),\n",
    "$$\n",
    "where $f(\\theta)$ is the loss function. A **manifold** $M\\subset \\mathbb{R}^n$ is said to be *invariant* under this flow if, whenever $\\theta(0)$ lies on $M$, the trajectory $\\theta(t)$ **remains** on $M$ for all $t>0$. In that case, the gradient $\\nabla f(\\theta)$ is always *tangent* to $M$ for any $\\theta\\in M$.  \n",
    "\n",
    "An **additional** property we often want is *stability*: if we start *near* the manifold $M$, will the flow bring us *closer* to $M$? If so, we say the manifold is **locally stable** or **attracting**. Formally, if the parameters deviate slightly off $M$, a stable manifold implies the dynamics push them back toward $M$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example: Symmetric Units in an MLP\n",
    "\n",
    "Consider a multi‐layer perceptron (MLP) with many neurons. Each neuron $i$ has:\n",
    "- An *incoming weight vector* $W_i$, i.e. its row in the layer’s weight matrix (plus possibly a bias $b_i$).\n",
    "- An *outgoing weight vector* in the **next** layer’s weight matrix (i.e. a column if you view that matrix from the next layer’s perspective).\n",
    "\n",
    "Now suppose two neurons, $i$ and $j$, have **identical (or nearly identical)** incoming weights ($W_i \\approx W_j$) and identical biases ($b_i \\approx b_j$) as well as *identical outgoing weights*. Then the set of all parameters $\\theta$ that enforce “$W_i = W_j$, $b_i = b_j$, and outgoing weights also equal” is a **manifold** in parameter space.  \n",
    "\n",
    "1. **Invariance:**  \n",
    "   - If your parameter $\\theta$ lies exactly on this “symmetry manifold,” the gradient $\\nabla f(\\theta)$ typically satisfies $\\nabla f(\\theta)\\big|_{W_i} = \\nabla f(\\theta)\\big|_{W_j}$.  \n",
    "   - This implies that under gradient descent, the updates for units $i$ and $j$ remain identical, so the parameters for those two neurons move *together*, keeping $W_i$ and $W_j$ (and $b_i$, $b_j$) equal at all times.  \n",
    "\n",
    "2. **Stable Symmetry:**  \n",
    "   - We further ask if small deviations off that manifold (i.e. $W_i \\neq W_j$ by a small amount) get pulled **back** to it by the gradient flow.  \n",
    "   - If so, we say the manifold of “coupled units” is **locally stable**: that means if at some point in training they are nearly identical, the gradient flow *reinforces* that similarity rather than letting them diverge.\n",
    "\n",
    "Hence, for two symmetric neurons, the **subspace** of parameters satisfying $W_i = W_j$ and $b_i = b_j$ is an **invariant manifold**. Demonstrating **local stability** amounts to showing that, near that point, the gradient flow *reduces* the difference $W_i - W_j$, $b_i - b_j$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Local Stability Criterion (Sketch)\n",
    "\n",
    "A common way to test local stability is:\n",
    "1. Identify the **manifold** $M$: in this case, $M = \\{\\theta : W_i = W_j,\\ b_i = b_j,\\ \\dots\\}$.  \n",
    "2. Check the **gradient** is tangent at $M$: ensures *invariance*.  \n",
    "3. Study the **Hessian** $\\nabla^2 f(\\theta_0)$ at a point $\\theta_0 \\in M$. In particular, look at directions **normal** to $M$—i.e., directions that *break* the symmetry (differences $W_i - W_j$).  \n",
    "   - If the Hessian is *positively curved* in those normal directions (for gradient *descent*), small deviations get pushed back to $M$.  \n",
    "   - Concretely, that means the restricted Hessian on the difference directions is **positive definite**, which implies a restoring force toward the manifold.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Conclusion\n",
    "\n",
    "- **Manifold Stability:** A manifold $M$ of parameters is stable under gradient descent if starting near $M$ keeps you (or pushes you) closer to $M$.  \n",
    "- **Symmetric Neurons:** Having two neurons with identical weights/biases is a special case of such a manifold. If the gradient flow is tangent to that manifold (no immediate push off it) *and* any small deviations are corrected (positive curvature in the difference directions), then *neuron coupling* is stable in the local neighborhood.  \n",
    "\n",
    "This concept underscores why, in practice, if two neurons become nearly identical, they may remain so (or become even more similar) throughout training—reflecting a **stable** symmetry in the parameter space.\n",
    "\n",
    "In a multi-layer neural network (MLP), each “unit” (or neuron) has associated trainable parameters—most notably the incoming weights and the bias for that neuron. When we **couple** two neurons, we deliberately constrain or make their parameters very similar so that they effectively share “behavior.” \n",
    "\n",
    "### 1. Manifold of Coupled Neurons\n",
    "\n",
    "If we denote two neurons in the same layer by indices $i$ and $j$, then coupling them amounts to imposing a relationship such as\n",
    "$$\n",
    "    W_i \\;\\approx\\; W_j, \n",
    "    \\quad\n",
    "    b_i \\;\\approx\\; b_j,\n",
    "$$\n",
    "where $W_i, W_j$ are the *rows* (or *columns*, depending on context) in the weight matrix corresponding to neurons $i$ and $j$, and $b_i, b_j$ are their biases.\n",
    "\n",
    "In a strict sense, **perfect** coupling would set $W_i = W_j$ and $b_i = b_j$.  Geometrically, this means the parameters lie on a *subspace* (or *manifold*) defined by those equality constraints.\n",
    "\n",
    "### 2. Gradient Flow and Stability\n",
    "\n",
    "We typically train neural networks via **gradient descent** on a loss function $f(\\theta)$, where $\\theta$ collects all network parameters.  For local stability analysis, we consider a trajectory $\\dot{\\theta}(t) = -\\,\\nabla f(\\theta(t))$.  \n",
    "\n",
    "- If we start exactly on the “coupled manifold,” meaning $W_i = W_j$ and $b_i = b_j$, and the flow **keeps** us there (i.e., no forces pulling us off), that subspace is *invariant*.  \n",
    "- More importantly, we want to see if *small deviations* off this manifold get pulled back (stable) or pushed away (unstable).\n",
    "\n",
    "### 3. Hessian Restriction and Normal Directions\n",
    "\n",
    "To test local stability near a point $\\theta_0$ on the coupled manifold, we look at the **Hessian** $\\nabla^2 f(\\theta_0)$.  The directions “normal” to the manifold are those that *break* the coupling constraints—for instance, a small difference $\\delta = (W_i - W_j)$.\n",
    "\n",
    "1. **Invariance Condition:**  \n",
    "   If $\\theta_0$ lies on the coupled manifold, for the manifold to be invariant under gradient descent, the gradient $\\nabla f(\\theta_0)$ must *respect* the coupling constraints (i.e., no immediate push off the manifold).\n",
    "\n",
    "2. **Stability Condition (Positive Curvature):**  \n",
    "   In gradient descent, small deviations in a normal direction $\\delta$ are pushed back to the manifold if the Hessian $\\nabla^2 f(\\theta_0)$ is **positive** in that direction.  Formally, we want\n",
    "   $$\n",
    "     \\delta^\\top \\,\\nabla^2 f(\\theta_0)\\,\\delta \\;>\\; 0\n",
    "   $$\n",
    "   for any $\\delta$ that lies in the normal space.  Concretely, if $\\delta$ is the difference of parameters between neurons $i$ and $j$, then the 2×2 (or small block) of the Hessian capturing partial derivatives w.r.t. these neurons’ parameters should be **positive definite**.\n",
    "\n",
    "### 4. The Experiment\n",
    "\n",
    "1. **Coupling Operation:**  \n",
    "   - We select neurons $i$ and $j$ in layer $\\ell$ and make their parameters similar by copying $W_i \\approx W_j$ and $b_i \\approx b_j$.  \n",
    "   - We optionally add a small random perturbation so they are not *exactly* identical.\n",
    "\n",
    "2. **Hessian Computation:**  \n",
    "   - We compute the Hessian of the loss function w.r.t. the relevant parameters (in practice, w.r.t. the entire layer or just the biases).  \n",
    "   - We extract the submatrix of that Hessian that corresponds specifically to $W_i, W_j$ (or $b_i, b_j$), which can be visualized as a 2×2 block if we look at just the pair $\\{i, j\\}$.  \n",
    "\n",
    "3. **Testing Positive Definiteness:**  \n",
    "   - If the 2×2 Hessian block \n",
    "     $$\n",
    "       \\begin{pmatrix}\n",
    "         H_{ii} & H_{ij}\\\\\n",
    "         H_{ij} & H_{jj}\n",
    "       \\end{pmatrix}\n",
    "     $$\n",
    "     is **positive definite**, it means any infinitesimal difference between neurons $i$ and $j$ leads to an *increase* in loss—and thus gradient descent tends to “push” the parameters back together.  Mathematically, that requires $H_{ii}>0$ and $\\det>0$.  \n",
    "   - In simpler terms, the difference direction $\\delta = W_i - W_j$ (or $b_i - b_j$) is a direction of *positive* curvature.  \n",
    "\n",
    "### 5. Conclusion\n",
    "\n",
    "By examining the **restricted Hessian** in these “difference directions,” we can test **whether coupling two neurons is locally stable** under gradient descent. Positive curvature in that sub-block indicates that if they begin coupled (equal parameters) and deviate slightly, the training dynamics will bring them back together. If the curvature is negative or indefinite, then small deviations grow larger, implying the coupling is *unstable*.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Coupled Manifold:** Parameters satisfying $W_i = W_j, b_i = b_j$.  \n",
    "- **Local Stability Check:** Restrict the Hessian to the subspace normal to the manifold (the difference directions).  \n",
    "- **Positive Definiteness:** Ensures stable coupling; negative or indefinite curvature indicates potential instability.\n",
    "\n",
    "This theoretical framework underlies the experiment: **we couple two neurons, measure the Hessian’s curvature in the difference directions, and then draw conclusions about the stability of that coupling under gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa865d-8584-4291-9203-2c7715084f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd.functional import hessian\n",
    "from torch.func import jacrev, hessian\n",
    "\n",
    "# Add wandb import\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(40)\n",
    "np.random.seed(40)\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_sizes, activation_type='relu', eps = 1e-5):\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        \n",
    "        # Set activation function based on input parameter\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_type == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation type: {activation_type}\")\n",
    "\n",
    "        self.eps = eps \n",
    "        \n",
    "        # Create layers list starting with flattening the input\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Build layer architecture\n",
    "        layer_sizes = [input_dim] + hidden_sizes + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def bn(self, x):\n",
    "        # Calculate batch statistics\n",
    "        batch_mean = x.mean(0, keepdim=True)\n",
    "        batch_var = x.var(0, unbiased=False, keepdim=True)\n",
    "        \n",
    "        # Normalize\n",
    "        x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        return x_normalized\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation to all but the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.bn(x) \n",
    "                x = self.activation(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def get_layer_weights(self, layer_idx):\n",
    "        \"\"\"Return the weight matrix of a specific layer\"\"\"\n",
    "        return self.layers[layer_idx].weight\n",
    "        \n",
    "    def couple_units(self, layer_idx, unit_i, unit_j, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Couple units i and j in layer layer_idx by making their incoming \n",
    "        and outgoing weights similar with a small perturbation.\n",
    "        \"\"\"\n",
    "        # Handle incoming weights (weights of the specified layer)\n",
    "        if layer_idx < len(self.layers):\n",
    "            # Get the weights of the specified layer (but don't modify directly)\n",
    "            weights = self.get_layer_weights(layer_idx).clone()\n",
    "            \n",
    "            # Make unit_j similar to unit_i with small perturbation\n",
    "            # We keep unit_i as is and set unit_j to be similar\n",
    "            noise = torch.randn_like(weights[unit_i]) * epsilon * torch.mean(weights[unit_i]**2)**0.5\n",
    "            \n",
    "            # Create new weights tensor with the modified values\n",
    "            new_weights = weights.clone()\n",
    "            new_weights[unit_j] = weights[unit_i] + noise\n",
    "            \n",
    "            # Update the layer weights with the new tensor\n",
    "            self.layers[layer_idx].weight.data = new_weights\n",
    "            \n",
    "            # Debug: verify the coupling\n",
    "            cosine_sim = self.measure_unit_similarity(layer_idx, unit_i, unit_j, 'cosine')\n",
    "            print(f\"DEBUG - After coupling incoming weights: cosine similarity = {cosine_sim:.4f}\")\n",
    "            # Log the coupling metrics to wandb\n",
    "        \n",
    "        # Handle outgoing weights (weights of the next layer)\n",
    "        if layer_idx + 1 < len(self.layers):\n",
    "            # Get the weights of the next layer (but don't modify directly)\n",
    "            next_weights = self.get_layer_weights(layer_idx + 1).clone()\n",
    "            \n",
    "            # For outgoing weights, we need to process the columns\n",
    "            # Extract the columns corresponding to unit_i and unit_j\n",
    "            outgoing_i = next_weights[:, unit_i].clone()\n",
    "            \n",
    "            # Create perturbation for outgoing weights\n",
    "            out_noise = torch.randn_like(outgoing_i) * epsilon * torch.norm(outgoing_i)\n",
    "            \n",
    "            # Set unit_j's outgoing weights based on unit_i with noise\n",
    "            next_weights[:, unit_j] = outgoing_i + out_noise\n",
    "            \n",
    "            # Update the next layer weights\n",
    "            self.layers[layer_idx + 1].weight.data = next_weights\n",
    "            \n",
    "            # Show the outgoing similarity\n",
    "            cos_sim_out = torch.dot(next_weights[:, unit_i], next_weights[:, unit_j]) / (\n",
    "                torch.norm(next_weights[:, unit_i]) * torch.norm(next_weights[:, unit_j]))\n",
    "            print(f\"DEBUG - After coupling outgoing weights: cosine similarity = {cos_sim_out.item():.4f}\")\n",
    "            # Log the coupling metrics to wandb\n",
    "            wandb.log({\"outgoing_weights_cosine_similarity\": cos_sim_out.item()})\n",
    "    \n",
    "    def measure_unit_similarity(self, layer_idx, unit_i, unit_j, metric='cosine'):\n",
    "        \"\"\"\n",
    "        Measure the similarity between units i and j in layer layer_idx.\n",
    "        \"\"\"\n",
    "        weights = self.get_layer_weights(layer_idx)\n",
    "        \n",
    "        if metric == 'cosine':\n",
    "            # Compute cosine similarity between incoming weight vectors\n",
    "            norm_i = torch.norm(weights[unit_i])\n",
    "            norm_j = torch.norm(weights[unit_j])\n",
    "            \n",
    "            if norm_i > 0 and norm_j > 0:\n",
    "                cos_sim = torch.dot(weights[unit_i], weights[unit_j]) / (norm_i * norm_j)\n",
    "                return cos_sim.item()\n",
    "            else:\n",
    "                return 0.0\n",
    "        elif metric == 'euclidean':\n",
    "            # Compute Euclidean distance between incoming weight vectors\n",
    "            distance = torch.norm(weights[unit_i] - weights[unit_j])\n",
    "            return distance.item()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def layer_hessian(model, criterion, layer_index, row_indices=None, col_indices=None):\n",
    "    N, M = model.layers[layer_index].weight.shape \n",
    "    device = model.layers[layer_index].weight.device\n",
    "    if row_indices is None:\n",
    "        row_indices = range(N)\n",
    "    if col_indices is None:\n",
    "        col_indices = range(M)\n",
    "    n = len(row_indices)\n",
    "    m = len(col_indices)\n",
    "    w = torch.zeros((n,m),device=device)\n",
    "    b = torch.zeros((n), device=device)\n",
    "    def f(params, x, y):\n",
    "        x = model.flatten(x)\n",
    "        w, b = params\n",
    "        R = torch.eye(N, device=device)[:,row_indices]\n",
    "        C = torch.eye(M, device=device)[col_indices,:]\n",
    "        \n",
    "        W = R @ w @ C\n",
    "        B = R @ b\n",
    "        h = x\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            \n",
    "            h = h@(layer.weight + (W if i==layer_index else 0)).T  + layer.bias + (B if i==layer_index else 0)\n",
    "            if i < len(model.layers) - 1:\n",
    "                h = model.bn(h)\n",
    "                h = model.activation(h)\n",
    "        loss = criterion(h, y) \n",
    "        return loss \n",
    "\n",
    "    def hess(x,y):\n",
    "        return hessian(f,)((w,b), x, y)\n",
    "    return hess \n",
    "\n",
    "\n",
    "def couping_hessian(x, y, model, criterion, layer_index, unit_i, unit_j, sampling_n = None):\n",
    "    if sampling_n is not None:\n",
    "        sampled_units = range(sampling_n)\n",
    "    else:\n",
    "        sampled_units = None\n",
    "    hess1 = layer_hessian(model, criterion, layer_index, row_indices=[unit_i, unit_j], col_indices=sampled_units)\n",
    "    hess2 = layer_hessian(model, criterion, layer_index+1, row_indices=sampled_units, col_indices=[unit_i, unit_j])\n",
    "    H1 = hess1(x, y)\n",
    "    H2 = hess2(x, y)\n",
    "    h1,b1 = H1[0][0], H1[1][1]\n",
    "    h2, b2 = H2[0][0], H2[1][1]\n",
    "    # compute the hessian restriced to the sub-space orthogonal to the coupled units  \n",
    "    h1 = h1[0,:,0,:] + h1[1,:,1,:] - h1[0,:,1,:] - h1[1,:,0,:]\n",
    "    h2 = h2[:, 0,:,0] + h2[:, 1,:,1] - h2[:,0,:,1] - h2[:, 1,:,0]\n",
    "    incoming_Hessian_eigvals = torch.linalg.eigvalsh(h1)\n",
    "    outgoign_Hessian_eigvals = torch.linalg.eigvalsh(h2)\n",
    "    \n",
    "    return incoming_Hessian_eigvals, outgoign_Hessian_eigvals\n",
    "\n",
    "    \n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    # Add a small epsilon for numerical stability.\n",
    "    eps = 1e-8\n",
    "    return torch.dot(a, b) / (a.norm() * b.norm() + eps)\n",
    "\n",
    "def measure_coupling_metrics(model,  i: int, j: int, layer_index: int, inputs, labels, criterion):\n",
    "    metrics = {}\n",
    "\n",
    "    ### 1. Incoming Weights (from layer layer_index)\n",
    "    # Assume a Linear layer with weight of shape (out_features, in_features)\n",
    "    layer = model.layers[layer_index]\n",
    "    W = layer.weight.data         # shape: (out_features, in_features)\n",
    "    dW = layer.weight.grad.data   # same shape\n",
    "    # For incoming weights, the weight vector for neuron i is the i-th row.\n",
    "    w1 = W[i]  # shape: (in_features,)\n",
    "    w2 = W[j]\n",
    "    dw1 = dW[i]\n",
    "    dw2 = dW[j]\n",
    "\n",
    "    incoming = {}\n",
    "    incoming['w1_norm'] = w1.norm().item()\n",
    "    incoming['w2_norm'] = w2.norm().item()\n",
    "    incoming['dw1_norm'] = dw1.norm().item()\n",
    "    incoming['dw2_norm'] = dw2.norm().item()\n",
    "    incoming['cos_w1_w2'] = cosine_similarity(w1, w2).item()\n",
    "    incoming['cos_dw1_dw2'] = cosine_similarity(dw1, dw2).item()\n",
    "\n",
    "    # Define the difference (or \"coupling noise\") vector.\n",
    "    e = w1 - w2\n",
    "    incoming['w1_w2_norm'] = e.norm().item()\n",
    "    incoming['dw1_dw2_norm'] = (dw1-dw2).norm().item()\n",
    "    incoming['cos_dw1,w1-w2'] = cosine_similarity(dw1, w1-w2).item()\n",
    "    incoming['cos_dw2,w2-w1'] = cosine_similarity(dw2, w2-w1).item()\n",
    "    incoming['cos_dw1_dw2,w1_w2'] = cosine_similarity(dw1 - dw2, w1-w2).item()\n",
    "    incoming['ip_dw1_dw2,w1_w2'] = torch.dot(dw1 - dw2, w1-w2).item()\n",
    "\n",
    "        \n",
    "    ### 2. Outgoing Weights (from the next layer, layer_index+1)\n",
    "    # For outgoing weights, we take the columns corresponding to the same unit indices.\n",
    "    # In a Linear layer the weight shape is (out_features, in_features) so the column i\n",
    "    # corresponds to the incoming weights for neuron i from the previous layer.\n",
    "    next_layer = model.layers[layer_index + 1]\n",
    "    W_next = next_layer.weight.data       # shape: (out_features_next, in_features_next)\n",
    "    dW_next = next_layer.weight.grad.data\n",
    "    # For outgoing coupling, select the columns i and j.\n",
    "    w1 = W_next[:, i]   # shape: (out_features_next,)\n",
    "    w2 = W_next[:, j]\n",
    "    dw1 = dW_next[:, i]\n",
    "    dw2 = dW_next[:, j]\n",
    "\n",
    "    outgoing = {}\n",
    "    outgoing['w1_norm'] = w1.norm().item()\n",
    "    outgoing['w2_norm'] = w2.norm().item()\n",
    "    outgoing['dw1_norm'] = dw1.norm().item()\n",
    "    outgoing['dw2_norm'] = dw2.norm().item()\n",
    "    outgoing['cos_w1_w2'] = cosine_similarity(w1, w2).item()\n",
    "    outgoing['cos_dw1_dw2'] = cosine_similarity(dw1, dw2).item()\n",
    "\n",
    "    # Define the difference (or \"coupling noise\") vector.\n",
    "    e = w1 - w2\n",
    "    outgoing['w1_w2_norm'] = e.norm().item()\n",
    "    outgoing['dw1_dw2_norm'] = (dw1-dw2).norm().item()\n",
    "    outgoing['cos_dw1,w1-w2'] = cosine_similarity(dw1, w1-w2).item()\n",
    "    outgoing['cos_dw2,w2-w1'] = cosine_similarity(dw2, w2-w1).item()\n",
    "    outgoing['cos_dw1_dw2,w1_w2'] = cosine_similarity(dw1 - dw2, w1-w2).item()\n",
    "    outgoing['ip_dw1_dw2,w1_w2'] = torch.dot(dw1 - dw2, w1-w2).item()\n",
    "\n",
    "    in_He, out_He = couping_hessian(inputs, labels, model, criterion, layer_index, i, j,sampling_n=100)\n",
    "    \n",
    "    incoming['He_eigs'] = in_He.tolist() \n",
    "    outgoing['He_eigs'] = out_He.tolist() \n",
    "\n",
    "\n",
    "    for k,v in incoming.items():\n",
    "        metrics[f'incoming_{k}'] = v\n",
    "        \n",
    "    for k,v in outgoing.items():\n",
    "        metrics[f'outgoing_{k}'] = v\n",
    "\n",
    "    metrics['incoming_He_min'] = in_He.min().item()\n",
    "    metrics['incoming_He_max'] = in_He.max().item() \n",
    "    metrics['incoming_He_mean'] = in_He.mean().item()\n",
    "    metrics['incoming_He_std'] = in_He.std().item()\n",
    "    \n",
    "    metrics['outgoing_He_min'] = out_He.min().item()\n",
    "    metrics['outgoing_He_max'] = out_He.max().item()\n",
    "    metrics['outgoing_He_mean'] = out_He.mean().item()\n",
    "    metrics['outgoing_He_std'] = out_He.std().item()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10_continual(first_classes, second_classes, batch_size=128):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 dataset split for continual learning.\n",
    "    first_classes and second_classes should be lists of class indices.\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Download and load the training dataset\n",
    "    full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "    full_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "    \n",
    "    # Create subsets for first task (first set of classes)\n",
    "    first_train_indices = [i for i, (_, label) in enumerate(full_trainset) if label in first_classes]\n",
    "    first_test_indices = [i for i, (_, label) in enumerate(full_testset) if label in first_classes]\n",
    "    \n",
    "    first_trainset = Subset(full_trainset, first_train_indices)\n",
    "    first_testset = Subset(full_testset, first_test_indices)\n",
    "    \n",
    "    # Create subsets for second task (second set of classes)\n",
    "    second_train_indices = [i for i, (_, label) in enumerate(full_trainset) if label in second_classes]\n",
    "    second_test_indices = [i for i, (_, label) in enumerate(full_testset) if label in second_classes]\n",
    "    \n",
    "    second_trainset = Subset(full_trainset, second_train_indices)\n",
    "    second_testset = Subset(full_testset, second_test_indices)\n",
    "    \n",
    "    # Determine number of workers based on device\n",
    "    # When using CUDA, we want to use multiple workers for the DataLoader\n",
    "    # When using CPU, fewer workers might be better\n",
    "    num_workers = 2 if device.type == 'cuda' else 0\n",
    "    \n",
    "    # Pin memory for faster data transfer to GPU\n",
    "    pin_memory = device.type == 'cuda'\n",
    "    \n",
    "    # Create dataloaders\n",
    "    first_trainloader = DataLoader(\n",
    "        first_trainset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    first_testloader = DataLoader(\n",
    "        first_testset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    second_trainloader = DataLoader(\n",
    "        second_trainset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    second_testloader = DataLoader(\n",
    "        second_testset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    print(f\"First task: {len(first_trainset)} training samples, {len(first_testset)} test samples\")\n",
    "    print(f\"Second task: {len(second_trainset)} training samples, {len(second_testset)} test samples\")\n",
    "    \n",
    "    # Log dataset information to wandb\n",
    "    wandb.log({\n",
    "        \"first_task_train_samples\": len(first_trainset),\n",
    "        \"first_task_test_samples\": len(first_testset),\n",
    "        \"second_task_train_samples\": len(second_trainset),\n",
    "        \"second_task_test_samples\": len(second_testset)\n",
    "    })\n",
    "    \n",
    "    return (first_trainloader, first_testloader), (second_trainloader, second_testloader)\n",
    "\n",
    "\n",
    "def train_continual_learning(model, data_loaders, coupling_info, \n",
    "                           epochs_first_task=10, epochs_second_task=10, \n",
    "                           learning_rate=0.001,):\n",
    "    # Unpack data loaders\n",
    "    (first_trainloader, first_testloader), (second_trainloader, second_testloader) = data_loaders\n",
    "    \n",
    "    # Extract coupling parameters\n",
    "    layer_idx = coupling_info['layer']\n",
    "    unit_i = coupling_info['unit_i']\n",
    "    unit_j = coupling_info['unit_j']\n",
    "    epsilon = coupling_info['epsilon']\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    metrics_history = []\n",
    "    for phi, train_loader,epochs in  ((1,first_trainloader,epochs_first_task), (2,second_trainloader,epochs_second_task)):\n",
    "        # applying coupling at end of task 1 \n",
    "        if phi==2:\n",
    "            print(\"\\n===== Applying unit coupling before phase 2  =====\")\n",
    "            model.couple_units(layer_idx, unit_i, unit_j, epsilon)\n",
    "            wandb.log({\"coupling_applied\": True})\n",
    "        print(f\"\\n===== Phase {phi} =====\")\n",
    "        step = 0 \n",
    "        for epoch in range(epochs):\n",
    "            # Training loop\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for i, (inputs, labels) in tqdm.tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "                step += 1 \n",
    "                \n",
    "                # Move input data to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    metrics = measure_coupling_metrics(model, unit_i, unit_j, layer_idx, inputs, labels, criterion)\n",
    "                    metrics_history.append(metrics)\n",
    "                \n",
    "                    # Log metrics to wandb\n",
    "                    wandb.log({\n",
    "                        f\"phase{phi}/step\": step,\n",
    "                        f\"phase{phi}/epoch\": epoch,\n",
    "                        f\"phase{phi}/batch_loss\": loss.item(),\n",
    "                        **{f\"phase{phi}/coupling/{k}\": v for k, v in metrics.items() if k not in ['phase', 'epoch', 'step', 'loss']}\n",
    "                    })\n",
    "                \n",
    "        \n",
    "            # epoch-level metrics\n",
    "            model.eval()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            acc_first = evaluate_model(model, first_testloader)\n",
    "            acc_second = evaluate_model(model, second_testloader)\n",
    "            metrics = {\n",
    "                'phase': phi,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': epoch_loss,\n",
    "                'test_accuracies_first': acc_first,\n",
    "                'test_accuracies_second': acc_second,\n",
    "            }\n",
    "            \n",
    "            # Log epoch metrics to wandb\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"phase\": phi,\n",
    "                \"train_loss\": epoch_loss,\n",
    "                \"test_acc_task1\": acc_first,\n",
    "                \"test_acc_task2\": acc_second,\n",
    "            })\n",
    "            \n",
    "            print(f\"Task 1 Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\")\n",
    "            print(f\"  Acc (Task 1): {acc_first:.2f}%, Acc (Task 2): {acc_second:.2f}%\")\n",
    "\n",
    "            \n",
    "    # print_model(model, layer_idx)\n",
    "    \n",
    "    # Clean up to ensure all GPU memory is released\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics_history\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model accuracy on a dataset\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move data to the appropriate device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Free memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return 100 * correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def run_continual_learning_experiment(\n",
    "    hidden_sizes=[7]*5, \n",
    "    activation_type='relu',\n",
    "    first_classes=[0, 1, 2, 3, 4],  # First 5 CIFAR-10 classes\n",
    "    second_classes=[5, 6, 7, 8, 9],  # Second 5 CIFAR-10 classes\n",
    "    couple_layer=1,\n",
    "    couple_units=(10, 20),\n",
    "    epsilon=0.01,\n",
    "    epochs_first_task=5,\n",
    "    epochs_second_task=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    run_name=None,\n",
    "):\n",
    "    \"\"\"Run the complete continual learning experiment\"\"\"\n",
    "    # Initialize wandb\n",
    "    if run_name is None:\n",
    "        run_name = f\"continual_learning_exp_{activation_type}_{epsilon}_{epochs_first_task}_{epochs_second_task}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"continual-learning-coupling\",\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"hidden_sizes\": hidden_sizes,\n",
    "            \"activation_type\": activation_type,\n",
    "            \"first_classes\": first_classes,\n",
    "            \"second_classes\": second_classes,\n",
    "            \"couple_layer\": couple_layer,\n",
    "            \"couple_units\": couple_units,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"epochs_first_task\": epochs_first_task,\n",
    "            \"epochs_second_task\": epochs_second_task,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"device\": device.type,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get class names for better reporting\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    first_class_names = [class_names[i] for i in first_classes]\n",
    "    second_class_names = [class_names[i] for i in second_classes]\n",
    "    \n",
    "    print(f\"First task classes: {first_class_names}\")\n",
    "    print(f\"Second task classes: {second_class_names}\")\n",
    "    \n",
    "    # Log class information to wandb\n",
    "    wandb.config.update({\n",
    "        \"first_class_names\": first_class_names,\n",
    "        \"second_class_names\": second_class_names\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Load datasets for continual learning\n",
    "        data_loaders = load_cifar10_continual(first_classes, second_classes, batch_size=batch_size)\n",
    "        \n",
    "        # Calculate input dimensions for CIFAR-10 (3 channels, 32x32 images)\n",
    "        input_dim = 3 * 32 * 32\n",
    "        output_dim = 10  # All CIFAR-10 classes\n",
    "        \n",
    "        # Create the model\n",
    "        model = ConfigurableMLP(input_dim, output_dim, hidden_sizes, activation_type)\n",
    "        print(f\"Created MLP with architecture: {input_dim} -> {' -> '.join(map(str, hidden_sizes))} -> {output_dim}\")\n",
    "        print(f\"Activation function: {activation_type}\")\n",
    "        \n",
    "        # Log model architecture to wandb\n",
    "        wandb.config.update({\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "            \"model_architecture\": f\"{input_dim} -> {' -> '.join(map(str, hidden_sizes))} -> {output_dim}\"\n",
    "        })\n",
    "        \n",
    "        # Move model to device\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Watch model with wandb to track gradients and parameters\n",
    "        wandb.watch(model, log=\"all\", log_freq=100)\n",
    "        \n",
    "        # Coupling parameters\n",
    "        coupling_info = {\n",
    "            'layer': couple_layer,\n",
    "            'unit_i': couple_units[0],\n",
    "            'unit_j': couple_units[1],\n",
    "            'epsilon': epsilon\n",
    "        }\n",
    "        \n",
    "        print(f\"Will couple units {couple_units[0]} and {couple_units[1]} in layer {couple_layer}, meaning we couple \")\n",
    "        print(f\"rows  {couple_units[0]} and {couple_units[1]} of weight matrix layer {couple_layer}\")\n",
    "        print(f\"and columns {couple_units[0]} and {couple_units[1]} of weight matrix layer {couple_layer+1}\")\n",
    "        print(f\" using {epsilon:.3f} perturbation\")\n",
    "        \n",
    "        # Train with continual learning approach\n",
    "        metrics = train_continual_learning(\n",
    "            model, \n",
    "            data_loaders,\n",
    "            coupling_info,\n",
    "            epochs_first_task=epochs_first_task,\n",
    "            epochs_second_task=epochs_second_task,\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "\n",
    "        model = model.to('cpu')\n",
    "        \n",
    "        # Finish wandb run\n",
    "        wandb.finish()\n",
    "        \n",
    "        return model, metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log any errors\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        wandb.finish()\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        # Make sure memory is cleaned up\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the continual learning experiment\n",
    "    config = dict(\n",
    "        first_classes=[0, 1,8,9],  # First 5 CIFAR-10 classes (airplane, auto, ship, truck )\n",
    "        second_classes=[2, 5, 6, 7, 3,4],  # Second 5 CIFAR-10 classes (dog, frog, horse, bird, cat, deer)\n",
    "        hidden_sizes=[500,]*6, \n",
    "        activation_type='tanh',\n",
    "        couple_layer=3,  # Second layer (0-indexed)\n",
    "        couple_units=(0, 1),  # Couple units 0 and 1\n",
    "        epsilon=0.01,  # Perturbation factor\n",
    "        epochs_first_task=40,\n",
    "        epochs_second_task=30,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=512,\n",
    "    )\n",
    "    experiments = []\n",
    "    for epochs_first_task in [1,5,10,]:\n",
    "        print(f'\\n\\n================= trainig with {epochs_first_task} epochs =================\\n\\n') \n",
    "        config['epochs_first_task'] = epochs_first_task\n",
    "        for exp_idx in range(5):\n",
    "            print(\"\\n\\n\\nexperimental config = \", config)\n",
    "            model, metrics = run_continual_learning_experiment(**config,)\n",
    "            experiments.append((config,metrics)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0b28c-0bbf-47fc-ab7a-6383ccd6f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436b664-9b85-49c4-8eab-6b4b4258e7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    first_classes=[0, 1,2,3,4],  # First 5 CIFAR-10 classes (airplane, auto, ship, truck )\n",
    "    second_classes=[5,6,7,8,9],  # Second 5 CIFAR-10 classes (dog, frog, horse, bird, cat, deer)\n",
    "    hidden_sizes=[500,]*6, \n",
    "    activation_type='tanh',\n",
    "    couple_layer=3,  # Second layer (0-indexed)\n",
    "    couple_units=(0, 1),  # Couple units 0 and 1\n",
    "    epsilon=0.001,  # Perturbation factor\n",
    "    epochs_first_task=5,\n",
    "    epochs_second_task=30,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=512,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConfigurableMLP(\n",
    "    input_dim = 3 * 32 * 32 , \n",
    "    output_dim = 10, \n",
    "    hidden_sizes=config['hidden_sizes'], \n",
    "    activation_type=config['activation_type']\n",
    ")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "loaders = load_cifar10_continual(config['first_classes'], config['second_classes'], batch_size=config['batch_size'])\n",
    "(first_trainloader, first_testloader), (second_trainloader, second_testloader)\n",
    "\n",
    "train_loader = first_trainloader\n",
    "\n",
    "model.couple_units(config['couple_layer'],unit_i=config['couple_units'][0], unit_j=config['couple_units'][1],epsilon=config['epsilon'])\n",
    "\n",
    "for epoch in tqdm.trange(config['epochs_first_task']):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # couple these units every epoch \n",
    "    # model.couple_units(layer_index,unit_i, unit_j,epsilon=config['epsilon'])\n",
    "\n",
    "    \n",
    "    losses = [] \n",
    "    for it, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move input data to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    sim = model.measure_unit_similarity(config['couple_layer'], config['couple_units'][0], config['couple_units'][1])\n",
    "    loss = np.mean(losses)\n",
    "    print(f\"loss = {loss}, coupling sim = {sim} \")\n",
    "\n",
    "# model.couple_units(layer_index,unit_i, unit_j,epsilon=config['epsilon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f30e78-cc47-4f20-95e4-575cb941e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_loss(model, criterion, layer_index, row_indices=None, col_indices=None, ):\n",
    "    N, M = model.layers[layer_index].weight.shape \n",
    "    device = model.layers[layer_index].weight.device\n",
    "    if row_indices is None:\n",
    "        row_indices = range(N)\n",
    "    if col_indices is None:\n",
    "        col_indices = range(M)\n",
    "    n = len(row_indices)\n",
    "    m = len(col_indices)\n",
    "    w = torch.randn((n,m),device=device)\n",
    "    w[1,:] = -w[0,:]\n",
    "    b = torch.randn((n), device=device)\n",
    "    b[1] = - b[0]\n",
    "    R = torch.eye(N, device=device)[:,row_indices]\n",
    "    C = torch.eye(M, device=device)[col_indices,:]\n",
    "    W = R @ w @ C\n",
    "    B = R @ b\n",
    "    def f(c, loader):\n",
    "        total_loss = 0\n",
    "        step = 0 \n",
    "        for (x, y) in (loader):\n",
    "            step += 1\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = model.flatten(x)\n",
    "            \n",
    "            h = x\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                \n",
    "                h = h@(layer.weight + (c*W if i==layer_index else 0)).T  + layer.bias + (c*B if i==layer_index else 0)\n",
    "                if i < len(model.layers) - 1:\n",
    "                    h = model.bn(h)\n",
    "                    h = model.activation(h)\n",
    "            loss = criterion(h, y) \n",
    "            total_loss += loss \n",
    "        return total_loss / step \n",
    "\n",
    "    return f, w, b \n",
    "\n",
    "\n",
    "# pick range according to average elements of the weight matrix \n",
    "eps = (model.layers[layer_index].weight**2).mean().item()**0.5 \n",
    "\n",
    "for it in tqdm.trange(10):\n",
    "    f, w, b = perturb_loss(model, criterion, layer_index=layer_index, row_indices=[unit_i, unit_j])\n",
    "    \n",
    "    f, w, b = perturb_loss(model, criterion, layer_index=layer_index, row_indices=[unit_i, unit_j])\n",
    "    cs = torch.arange(-eps,eps,eps/5)\n",
    "    losses = []\n",
    "    for c in (cs):\n",
    "        losses.append(f(c, second_testloader).item())\n",
    "    plt.plot(cs, losses,label=f'it {it}')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bb024-992c-4400-aee6-d1b274df4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.layers[3].weight**2).mean()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca560e8d-5cfe-4e56-9a1f-aa72c1e11f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(experiments[0][1][-1]['outgoing_H_eigs'],alpha=.7)\n",
    "\n",
    "plt.hist(experiments[1][1][-1]['outgoing_H_eigs'],alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a3968-dd9f-483a-8c82-569eba91e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "col = 'incoming_ip_dw1_dw2,w1_w2'\n",
    "df = pd.DataFrame(experiments[0][1])\n",
    "df = df.loc[df.phase==1]\n",
    "plt.plot(df[col].values[:100])\n",
    "df = pd.DataFrame(experiments[-1][1])\n",
    "df = df.loc[df.phase==1]\n",
    "plt.plot(df[col].values[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e5c3d-5def-4860-9541-1853778ead45",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'incoming_cos_w1_w2'\n",
    "df = pd.DataFrame(experiments[0][1])\n",
    "df = df.loc[df.phase==1]\n",
    "plt.plot(df[col].values)\n",
    "df = pd.DataFrame(experiments[-1][1])\n",
    "df = df.loc[df.phase==1]\n",
    "plt.plot(df[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3859551-0c9e-4d96-ac51-24d0431cc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb72a7-3170-4cb1-bf6e-9ab86b15dcd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_sizes, activation_type='relu'):\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        \n",
    "        # Set activation function based on input parameter\n",
    "        if activation_type == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation_type == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation_type == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation type: {activation_type}\")\n",
    "        \n",
    "        # Create layers list starting with flattening the input\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Build layer architecture\n",
    "        layer_sizes = [input_dim] + hidden_sizes + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Apply activation to all but the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def get_layer_weights(self, layer_idx):\n",
    "        \"\"\"Return the weight matrix of a specific layer\"\"\"\n",
    "        return self.layers[layer_idx].weight\n",
    "\n",
    "\n",
    "input_dim, output_dim = 2, 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConfigurableMLP(input_dim, output_dim, [3,5],'relu')\n",
    "x, y = torch.randn(10, input_dim), torch.randint(output_dim, (10,))\n",
    "out = model(x)\n",
    "loss = criterion(out, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ede817-25b9-48d8-9587-7345238f1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074c361-9fd1-48de-9d66-f1a185ecf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "input_dim, output_dim = 2, 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConfigurableMLP(input_dim, output_dim, [500]*5,'tanh')\n",
    "\n",
    "layer_index = 1\n",
    "row_indices = [1, 2]\n",
    "col_indices = [1]\n",
    "\n",
    "\n",
    "unit_i, unit_j = 0, 1\n",
    "\n",
    "# hess1 = layer_hessian(model, criterion, layer_index, row_indices=[unit_i, unit_j], col_indices=sampled_units)\n",
    "# hess2 = layer_hessian(model, criterion, layer_index+1, row_indices=sampled_units, col_indices=[unit_i, unit_j])\n",
    "\n",
    "x, y = torch.randn(500, input_dim), torch.randint(output_dim, (500,))\n",
    "couping_hessian(x, y, model, criterion, layer_index, unit_i, unit_j, sampling_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412739a7-684c-4fad-813e-4f2ce7d0e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B1.shape, B2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb6a64-b9b1-4538-9486-3f52fa8c81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_indices = [1, 2]\n",
    "col_indices = [1]\n",
    "n = len(row_indices)\n",
    "m = len(col_indices)\n",
    "N, M = model.layers[layer_index].weight.shape \n",
    "R = torch.eye(N)[:,row_indices]\n",
    "C = torch.eye(M)[col_indices,:]\n",
    "\n",
    "w = torch.zeros((n,m))\n",
    "b = torch.zeros((n))\n",
    "\n",
    "W = R @ w @ C\n",
    "B = R @ b\n",
    "\n",
    "R.shape, C.shape, w.shape, b.shape, W.shape, B.shape, model.layers[layer_index].weight.shape ,model.layers[layer_index].bias.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa550-c7e9-454d-9859-583da80780cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c053a9-868b-4426-83d5-2dc57264c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a reproducible random seed\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# --------------\n",
    "# Hyperparameters\n",
    "# --------------\n",
    "d = 100            # dimension of w_i, g_i\n",
    "num_steps = 200000\n",
    "eta = 0.01     # learning rate\n",
    "\n",
    "# Activation f and derivative f'(z)\n",
    "def f(z):\n",
    "    return torch.tanh(z)\n",
    "\n",
    "def fprime(z):\n",
    "    # Derivative of tanh(z) = 1 - tanh^2(z)\n",
    "    return 1. - torch.tanh(z)**2\n",
    "\n",
    "# def f(z):\n",
    "#     return torch.max(z, 0)[0]\n",
    "# def fprime(z):\n",
    "#     return (z>0).float()\n",
    "\n",
    "# --------------\n",
    "# Initialization\n",
    "# --------------\n",
    "# For simplicity, initialize w1, w2, g1, g2 randomly on the unit sphere\n",
    "def random_unit_vector(dim):\n",
    "    v = torch.randn(dim)\n",
    "    v = v / v.norm()\n",
    "    return v\n",
    "\n",
    "w1 = random_unit_vector(d)\n",
    "w2 = random_unit_vector(d)\n",
    "g1 = random_unit_vector(d)\n",
    "g2 = random_unit_vector(d)\n",
    "\n",
    "w2 = w2/w2.norm()\n",
    "g2 = g2/g2.norm()\n",
    "\n",
    "# --------------\n",
    "# Helper to measure angles\n",
    "# --------------\n",
    "def angle_between(u, v):\n",
    "    # angle in [0, pi], computed via dot product\n",
    "    cos = (u @ v) / (u.norm() * v.norm())\n",
    "    # Clip numerical noise in case cos>1 or cos<-1\n",
    "    cos = torch.clamp(cos, -1.0, 1.0)\n",
    "    # return torch.acos(cos).item()\n",
    "    return cos.item()\n",
    "\n",
    "# --------------\n",
    "# Store angles over time\n",
    "# --------------\n",
    "angles_w = []\n",
    "angles_g = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Sample x ~ N(0, I_d)\n",
    "    x = torch.randn(d)\n",
    "    # Sample b ~ N(0, I_d)\n",
    "    b = torch.randn(d)\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = w1 @ x\n",
    "    z2 = w2 @ x\n",
    "    h1 = f(z1)\n",
    "    h2 = f(z2)\n",
    "    # y1 = g1 * h1, y2 = g2 * h2 (vector outputs)\n",
    "    # But for the gradient update, we only need the dot(b, y_i) terms.\n",
    "\n",
    "    # Compute gradients wrt w1, w2\n",
    "    # grad_w1 = (b . g1) * f'(z1) * x\n",
    "    # note: (b . g1) is a scalar\n",
    "    grad_w1 = (b @ g1) * fprime(z1) * x\n",
    "    grad_w2 = (b @ g2) * fprime(z2) * x\n",
    "\n",
    "    # Compute gradients wrt g1, g2\n",
    "    # grad_g1 = b * f(z1) = b * h1\n",
    "    grad_g1 = b * h1\n",
    "    grad_g2 = b * h2\n",
    "\n",
    "    # Gradient descent updates\n",
    "    w1 = w1 - eta * grad_w1\n",
    "    w2 = w2 - eta * grad_w2\n",
    "    g1 = g1 - eta * grad_g1\n",
    "    g2 = g2 - eta * grad_g2\n",
    "\n",
    "    # Renormalize to have unit norm\n",
    "    w1 = w1 / w1.norm()\n",
    "    w2 = w2 / w2.norm()\n",
    "    g1 = g1 / g1.norm()\n",
    "    g2 = g2 / g2.norm()\n",
    "\n",
    "    # Record angles\n",
    "    angles_w.append(angle_between(w1, w2))\n",
    "    angles_g.append(angle_between(g1, g2))\n",
    "\n",
    "\n",
    "# After the loop, examine final angles\n",
    "final_angle_w = angles_w[-1]\n",
    "final_angle_g = angles_g[-1]\n",
    "print(f\"Final angle between w1, w2 (in degrees): {final_angle_w*180/math.pi:.2f}\")\n",
    "print(f\"Final angle between g1, g2 (in degrees): {final_angle_g*180/math.pi:.2f}\")\n",
    "\n",
    "plt.plot(angles_w)\n",
    "plt.plot(angles_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfc987-7cd2-481b-95bb-0e7330b1b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = 100\n",
    "lambda_lr = 0.001\n",
    "steps = 20000\n",
    "\n",
    "# Initialize unit vectors\n",
    "w1 = torch.randn(d); w1 /= w1.norm()\n",
    "w2 = torch.randn(d); w2 /= w2.norm()\n",
    "g1 = torch.randn(d); g1 /= g1.norm()\n",
    "g2 = torch.randn(d); g2 /= g2.norm()\n",
    "\n",
    "theta_w_history = []\n",
    "theta_g_history = []\n",
    "\n",
    "for i in range(steps):\n",
    "    x = torch.randn(d)\n",
    "    b = torch.randn(d)\n",
    "    \n",
    "    z1 = w1 @ x; z2 = w2 @ x\n",
    "    h1 = torch.tanh(z1); h2 = torch.tanh(z2)  # Non-linear activation\n",
    "    f_prime1 = 1 - h1**2; f_prime2 = 1 - h2**2\n",
    "\n",
    "    if i%4==0:\n",
    "        grad_w1,grad_w2,grad_g1,grad_g2 = 0,0,0,0\n",
    "    grad_w1 += f_prime1 * (g1 @ b) * x\n",
    "    grad_w2 += f_prime2 * (g2 @ b) * x\n",
    "    grad_g1 += h1 * b\n",
    "    grad_g2 += h2 * b\n",
    "    \n",
    "    w1 = w1 - lambda_lr * grad_w1; w1 /= w1.norm()\n",
    "    w2 = w2 - lambda_lr * grad_w2; w2 /= w2.norm()\n",
    "    g1 = g1 - lambda_lr * grad_g1; g1 /= g1.norm()\n",
    "    g2 = g2 - lambda_lr * grad_g2; g2 /= g2.norm()\n",
    "    \n",
    "    theta_w = (w1 @ w2).item()\n",
    "    theta_g = (g1 @ g2).item()\n",
    "    theta_w_history.append(theta_w)\n",
    "    theta_g_history.append(theta_g)\n",
    "\n",
    "plt.plot(theta_w_history, label='θ_w')\n",
    "plt.plot(theta_g_history, label='θ_g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48046749-42aa-49b4-9c78-4ac20f9b141a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
