{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9583be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9221e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/amir/Codes/NN-dynamic-scaling to Python path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import the utility module and Setup the path\n",
    "import notebook_utils\n",
    "notebook_utils.setup_path()\n",
    "\n",
    "from src.models.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f11ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning module layers.linear_0: 10→10, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.linear_1: 64→128, 32→64, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.linear_2: 32→64, 16→32, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.out: 16→32, 2→2, in expansion: 2, out expansion: 1\n",
      "Passed 24 out of 24 tests\n",
      "\n",
      "Model cloning test: PASSED\n",
      "Passed 7 out of 7 tests\n",
      "\n",
      "Activation cloning test: PASSED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clone_model_parameters(src_model, cloned_model):\n",
    "    \"\"\"\n",
    "    Clone parameters from a smaller model to a larger model using a module-based approach.\n",
    "    \n",
    "    For linear layers, weights are scaled by 1/n (where n is the input expansion factor)\n",
    "    to ensure equivalent functionality after cloning.\n",
    "    \n",
    "    Args:\n",
    "        src_model: Source model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    # First verify model structures\n",
    "    src_modules = {name: module for name, module in src_model.named_modules() if isinstance(module, nn.Linear)}\n",
    "    cloned_modules = {name: module for name, module in cloned_model.named_modules() if isinstance(module, nn.Linear)}\n",
    "    \n",
    "    # Check if modules match\n",
    "    if set(src_modules.keys()) != set(cloned_modules.keys()):\n",
    "        raise ValueError(\"Source and cloned models have different module structures\")\n",
    "    \n",
    "    # Process each module individually\n",
    "    for name, src_module in src_modules.items():\n",
    "        cloned_module = cloned_modules[name]\n",
    "        \n",
    "        # Get module dimensions\n",
    "        src_in_features = src_module.in_features\n",
    "        src_out_features = src_module.out_features\n",
    "        cloned_in_features = cloned_module.in_features\n",
    "        cloned_out_features = cloned_module.out_features\n",
    "        \n",
    "        # Calculate expansion factors\n",
    "        in_expansion = cloned_in_features // src_in_features\n",
    "        out_expansion = cloned_out_features // src_out_features\n",
    "        \n",
    "        print(f\"Cloning module {name}: {src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "        \n",
    "        \n",
    "        # Verify expansion factors are valid\n",
    "        if cloned_in_features % src_in_features != 0 or cloned_out_features % src_out_features != 0:\n",
    "            raise ValueError(f\"Module {name} dimensions are not integer multiples: \"\n",
    "                             f\"{src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}\")\n",
    "        \n",
    "        # Clone the weights with proper scaling\n",
    "        for i in range(in_expansion):\n",
    "            for j in range(out_expansion):\n",
    "                cloned_module.weight.data[j::out_expansion, i::in_expansion] = src_module.weight.data / in_expansion\n",
    "    \n",
    "        \n",
    "        # Clone the bias if present (no scaling needed for bias)\n",
    "        if src_module.bias is not None and cloned_module.bias is not None:\n",
    "            # cloned the bias vector \n",
    "            for j in range(out_expansion):\n",
    "                cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    \n",
    "    # For non-linear modules (if any), copy parameters without scaling\n",
    "    for name, param in src_model.named_parameters():\n",
    "        # Skip parameters that belong to linear layers (already handled)\n",
    "        if any(module_name in name for module_name in src_modules.keys()):\n",
    "            continue\n",
    "        \n",
    "        if name in cloned_model.state_dict():\n",
    "            cloned_param = cloned_model.state_dict()[name]\n",
    "            src_shape = torch.tensor(param.shape)\n",
    "            cloned_shape = torch.tensor(cloned_param.shape)\n",
    "            \n",
    "            # If shapes match, directly copy\n",
    "            if tuple(src_shape) == tuple(cloned_shape):\n",
    "                cloned_param.copy_(param)\n",
    "                print(f\"Parameter {name} copied directly (dimensions match)\")\n",
    "            else:\n",
    "                # For other parameters that need expansion but no scaling\n",
    "                non_matching_dims = src_shape != cloned_shape\n",
    "                \n",
    "                # Create indices for blockwise expansion\n",
    "                indices = []\n",
    "                expansion_info = []\n",
    "                \n",
    "                for i, (s1, s2) in enumerate(zip(src_shape, cloned_shape)):\n",
    "                    if s1 == s2:\n",
    "                        indices.append(torch.arange(s2))\n",
    "                    else:\n",
    "                        expansion_factor = s2 // s1\n",
    "                        expansion_info.append(f\"dim {i}: {expansion_factor}x\")\n",
    "                        indices.append(torch.div(torch.arange(s2), expansion_factor, rounding_mode='floor'))\n",
    "                \n",
    "                # Create the grid and copy\n",
    "                grid = torch.meshgrid(*indices, indexing='ij')\n",
    "                cloned_param.copy_(param[grid])\n",
    "                \n",
    "                if expansion_info:\n",
    "                    print(f\"Parameter {name} cloned with blockwise expansion: {', '.join(expansion_info)}\")\n",
    "    \n",
    "    return cloned_model\n",
    "    \n",
    "    \n",
    "def validate_model_cloning(src_model, cloned_model):\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for name, module in src_model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # print(f\"source module {name}: {module.in_features}→{module.out_features},  cloned module {cloned_model.get_submodule(name).in_features}→{cloned_model.get_submodule(name).out_features}\")\n",
    "            module2 = cloned_model.get_submodule(name)\n",
    "            in_expansion = module2.in_features // module.in_features\n",
    "            out_expansion = module2.out_features // module.out_features\n",
    "            # print(f\"Expansion factors (outxin): {out_expansion}x{in_expansion}\")\n",
    "            for j in range(out_expansion):\n",
    "                for i in range(in_expansion):\n",
    "                    passed += torch.allclose(module2.weight.data[j::out_expansion, i::in_expansion], module.weight.data/in_expansion)\n",
    "                    passed += torch.allclose(module2.bias.data[j::out_expansion], module.bias.data)\n",
    "                    total += 2\n",
    "    \n",
    "    print(f\"Passed {passed} out of {total} tests\")\n",
    "    return passed == total  \n",
    "\n",
    "def validate_activation_clonign(src_model, cloned_model):\n",
    "    from src.utils.monitor import NetworkMonitor\n",
    "\n",
    "    src_monitor = NetworkMonitor(src_model, )\n",
    "    cloned_monitor = NetworkMonitor(cloned_model, )\n",
    "    src_monitor.register_hooks()\n",
    "    cloned_monitor.register_hooks()\n",
    "\n",
    "\n",
    "    d = src_model.input_size\n",
    "    x = torch.randn(10, d)\n",
    "    src_model(x)\n",
    "    cloned_model(x)\n",
    "\n",
    "    acts, acts2 = src_monitor.get_latest_activations(), cloned_monitor.get_latest_activations()\n",
    "\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for key in acts.keys():\n",
    "        a1, a2 = acts[key], acts2[key]\n",
    "        if a1.shape[1] != a2.shape[1]:\n",
    "            diffs = (a1 - a2[:,::2])\n",
    "        else: #only for the last layer\n",
    "            diffs = (a1 - a2)\n",
    "        # print(f\"Diff for {key}: {diffs.abs().max().item()}\")\n",
    "        passed += diffs.abs().max().item() < 1e-5\n",
    "        total += 1\n",
    "    print(f\"Passed {passed} out of {total} tests\")\n",
    "    return passed == total\n",
    "\n",
    "# src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32, 16], activation=\"relu\", dropout_p=0.0)\n",
    "# cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2, 16*2], activation=\"relu\", dropout_p=0.0)\n",
    "\n",
    "# cloned_model = clone_model_parameters(src_model, cloned_model)\n",
    "# test1(src_model, cloned_model)\n",
    "# test2(src_model, cloned_model)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create source and target models with random weights\n",
    "    src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32, 16], activation=\"relu\", dropout_p=0.0)\n",
    "    cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2, 16*2], activation=\"relu\", dropout_p=0.0)\n",
    "    \n",
    "    # Clone the parameters\n",
    "    cloned_model = clone_model_parameters(src_model, cloned_model)\n",
    "    \n",
    "    # Test the cloning with a functional test\n",
    "    success = validate_model_cloning(src_model, cloned_model)\n",
    "    print(\"\\nModel cloning test:\", \"PASSED\" if success else \"FAILED\")\n",
    "\n",
    "    \n",
    "    \n",
    "    success = validate_activation_clonign(src_model, cloned_model)\n",
    "    print(\"\\nActivation cloning test:\", \"PASSED\" if success else \"FAILED\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99d6de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([256]) tensor([1., 1., 1., 1., 1.], grad_fn=<SliceBackward0>)\n",
      "bias torch.Size([256]) tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import CNN\n",
    "model = CNN()\n",
    "model.layers.conv_0.weight.data.shape\n",
    "for k,v in model.layers.norm_2.named_parameters():\n",
    "    print(k,v.shape, v[:5])\n",
    "model.layers.norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "07df9b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (layers): ModuleDict(\n",
       "    (patch_embed): PatchEmbedding(\n",
       "      (layers): ModuleDict(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (block_0): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_1): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_2): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_3): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_4): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_5): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_6): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_7): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_8): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_9): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_10): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (block_11): TransformerBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (layers): ModuleDict(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): TransformerMLP(\n",
       "          (layers): ModuleDict(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.1, inplace=False)\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=192, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "\n",
    "model = VisionTransformer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf3c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Union, List, Dict, Set\n",
    "\n",
    "# Replace TypeVar with Union for proper type handling\n",
    "NormalizationLayer = Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm]\n",
    "\n",
    "# Define activation function types properly\n",
    "ActivationFunction = Union[nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, \n",
    "                         nn.LeakyReLU, nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, \n",
    "                         nn.Softplus, nn.Softmin, nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, \n",
    "                         nn.Hardshrink, nn.Softsign, nn.GLU, nn.CELU, nn.Identity]\n",
    "\n",
    "def clone_linear(src_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    # Get module dimensions\n",
    "    src_in_features = src_module.in_features\n",
    "    src_out_features = src_module.out_features\n",
    "    cloned_in_features = cloned_module.in_features\n",
    "    cloned_out_features = cloned_module.out_features\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_features % src_in_features != 0 or cloned_out_features % src_out_features != 0:\n",
    "        raise ValueError(f\"Linear module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}\")\n",
    "        \n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_features // src_in_features\n",
    "    out_expansion = cloned_out_features // src_out_features\n",
    "    \n",
    "    print(f\"Cloning Linear module: {src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_conv1d(src_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    # Get module dimensions\n",
    "    src_in_channels = src_module.in_channels\n",
    "    src_out_channels = src_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // src_in_channels\n",
    "    out_expansion = cloned_out_channels // src_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv1d module: {src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % src_in_channels != 0 or cloned_out_channels % src_out_channels != 0:\n",
    "        raise ValueError(f\"Conv1d module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "    \n",
    "def clone_conv2d(src_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    # Get module dimensions\n",
    "    src_in_channels = src_module.in_channels\n",
    "    src_out_channels = src_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // src_in_channels\n",
    "    out_expansion = cloned_out_channels // src_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv2d module: {src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % src_in_channels != 0 or cloned_out_channels % src_out_channels != 0:\n",
    "        raise ValueError(f\"Conv2d module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "    \n",
    "\n",
    "def clone_normalization(\n",
    "    src_module: NormalizationLayer, \n",
    "    cloned_module: NormalizationLayer,\n",
    ") -> NormalizationLayer:\n",
    "    \"\"\"Clone normalization layer parameters with proper handling of different types.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Check properties that exist for the specific normalization type\n",
    "    if hasattr(src_module, 'affine') and hasattr(cloned_module, 'affine'):\n",
    "        assert src_module.affine == cloned_module.affine, \"Affine property must match\"\n",
    "    \n",
    "    # Handle BatchNorm-specific properties\n",
    "    if isinstance(src_module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        if hasattr(src_module, 'track_running_stats') and hasattr(cloned_module, 'track_running_stats'):\n",
    "            assert src_module.track_running_stats == cloned_module.track_running_stats, \"Track running stats property must match\"\n",
    "    \n",
    "    # Clone weights and biases\n",
    "    if hasattr(src_module, 'weight') and src_module.weight is not None and cloned_module.weight is not None:\n",
    "        expansion = cloned_module.weight.data.shape[0] // src_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            cloned_module.weight.data[i::expansion] = src_module.weight.data\n",
    "            if hasattr(src_module, 'bias') and src_module.bias is not None and cloned_module.bias is not None:\n",
    "                cloned_module.bias.data[i::expansion] = src_module.bias.data\n",
    "    \n",
    "    # Clone running stats for BatchNorm layers\n",
    "    if hasattr(src_module, 'running_mean') and src_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // src_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                cloned_module.running_mean.data[i::expansion] = src_module.running_mean.data\n",
    "                cloned_module.running_var.data[i::expansion] = src_module.running_var.data\n",
    "    \n",
    "    # Clone num_batches_tracked for BatchNorm layers\n",
    "    if hasattr(src_module, 'num_batches_tracked') and src_module.num_batches_tracked is not None:\n",
    "        if hasattr(cloned_module, 'num_batches_tracked') and cloned_module.num_batches_tracked is not None:\n",
    "            cloned_module.num_batches_tracked.data.copy_(src_module.num_batches_tracked.data)\n",
    "    \n",
    "    return cloned_module\n",
    "    \n",
    "    \n",
    "def clone_embedding(src_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    # Get module dimensions\n",
    "    src_num_embeddings = src_module.num_embeddings\n",
    "    src_embedding_dim = src_module.embedding_dim\n",
    "    cloned_num_embeddings = cloned_module.num_embeddings\n",
    "    cloned_embedding_dim = cloned_module.embedding_dim\n",
    "    \n",
    "    # Calculate expansion factors\n",
    "    num_expansion = cloned_num_embeddings // src_num_embeddings\n",
    "    dim_expansion = cloned_embedding_dim // src_embedding_dim\n",
    "    \n",
    "    print(f\"Cloning Embedding module: {src_num_embeddings}→{cloned_num_embeddings}, {src_embedding_dim}→{cloned_embedding_dim}, num expansion: {num_expansion}, dim expansion: {dim_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_num_embeddings % src_num_embeddings != 0 or cloned_embedding_dim % src_embedding_dim != 0:\n",
    "        raise ValueError(f\"Embedding module dimensions are not integer multiples: \"\n",
    "                         f\"{src_num_embeddings}→{cloned_num_embeddings}, {src_embedding_dim}→{cloned_embedding_dim}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(num_expansion):\n",
    "        for j in range(dim_expansion):\n",
    "            cloned_module.weight.data[j::dim_expansion, i::num_expansion] = src_module.weight.data \n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_activation(src_module: ActivationFunction, cloned_module: ActivationFunction) -> ActivationFunction:\n",
    "    \"\"\"Clone activation function parameters, handling configuration parameters properly.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Handle configuration parameters for different activation types\n",
    "    if isinstance(src_module, nn.LeakyReLU):\n",
    "        cloned_module.negative_slope = src_module.negative_slope\n",
    "    \n",
    "    elif isinstance(src_module, (nn.ELU, nn.CELU)):\n",
    "        cloned_module.alpha = src_module.alpha\n",
    "    \n",
    "    elif isinstance(src_module, nn.Threshold):\n",
    "        cloned_module.threshold = src_module.threshold\n",
    "        cloned_module.value = src_module.value\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        cloned_module.dim = src_module.dim\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        cloned_module.lambd = src_module.lambd\n",
    "        \n",
    "    elif isinstance(src_module, nn.GLU):\n",
    "        cloned_module.dim = src_module.dim\n",
    "    \n",
    "    # Handle PReLU specifically (has learnable parameters)\n",
    "    elif isinstance(src_module, nn.PReLU):\n",
    "        if src_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # If source is a single parameter, broadcast to all channels\n",
    "            cloned_module.weight.data.fill_(src_module.weight.data[0])\n",
    "        elif src_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            # Channel-wise parameters need proper expansion\n",
    "            expansion = cloned_module.num_parameters // src_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                cloned_module.weight.data[i::expansion] = src_module.weight.data\n",
    "        else:\n",
    "            # Direct copy if dimensions match\n",
    "            cloned_module.weight.data.copy_(src_module.weight.data)\n",
    "    \n",
    "    # Handle other parameterized activation functions if they have weights\n",
    "    # This is a general catch-all for any other activation function with parameters\n",
    "    elif hasattr(src_module, 'weight') and hasattr(cloned_module, 'weight'):\n",
    "        if src_module.weight is not None and cloned_module.weight is not None:\n",
    "            if cloned_module.weight.data.shape == src_module.weight.data.shape:\n",
    "                cloned_module.weight.data.copy_(src_module.weight.data)\n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_dropout(src_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    \"\"\"Clone dropout module parameters.\"\"\"\n",
    "    assert cloned_module.p == src_module.p, \"Dropout probability must match\"\n",
    "    # Print warning if dropout p > 0\n",
    "    if cloned_module.p > 0:\n",
    "        print(f\"Warning: Dropout probability is set to {cloned_module.p}, cloning is not perfect\")\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def is_parameter_free(module: nn.Module) -> bool:\n",
    "    \"\"\"Check if a module has no parameters.\"\"\"\n",
    "    return len(list(module.parameters())) == 0\n",
    "\n",
    "\n",
    "def clone_parameter_free(src_module: nn.Module, cloned_module: nn.Module) -> nn.Module:\n",
    "    \"\"\"Clone a parameter-free module.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    assert is_parameter_free(src_module), \"Source module must be parameter free\"\n",
    "    assert is_parameter_free(cloned_module), \"Cloned module must be parameter free\"\n",
    "    \n",
    "    # For parameter-free modules, there's no need to copy weights\n",
    "    # Just make sure they're of the same type, which we've already checked\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "# Validation functions\n",
    "\n",
    "def validate_activation_cloning(src_module: ActivationFunction, cloned_module: ActivationFunction):\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Validate configuration parameters for different activation types\n",
    "    if isinstance(src_module, nn.LeakyReLU):\n",
    "        assert src_module.negative_slope == cloned_module.negative_slope, \"LeakyReLU negative_slope does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.ELU, nn.CELU)):\n",
    "        assert src_module.alpha == cloned_module.alpha, \"Alpha parameter does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, nn.Threshold):\n",
    "        assert src_module.threshold == cloned_module.threshold, \"Threshold value does not match\"\n",
    "        assert src_module.value == cloned_module.value, \"Replacement value does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        assert src_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        assert src_module.lambd == cloned_module.lambd, \"Lambda parameter does not match\"\n",
    "        \n",
    "    elif isinstance(src_module, nn.GLU):\n",
    "        assert src_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    # Validate PReLU parameters\n",
    "    elif isinstance(src_module, nn.PReLU):\n",
    "        if src_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # All elements should be equal to the single parameter\n",
    "            assert torch.all(cloned_module.weight.data == src_module.weight.data[0])\n",
    "        elif src_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            expansion = cloned_module.num_parameters // src_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.weight.data[i::expansion], src_module.weight.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_dropout_cloning(src_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    assert cloned_module.p == src_module.p, \"Dropout probability must match\"\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_embedding_cloning(src_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    num_expansion = cloned_module.num_embeddings // src_module.num_embeddings\n",
    "    dim_expansion = cloned_module.embedding_dim // src_module.embedding_dim\n",
    "    for j in range(num_expansion):\n",
    "        for i in range(dim_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::num_expansion, i::dim_expansion], src_module.weight.data)\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_normalization_cloning(src_module: NormalizationLayer, cloned_module: NormalizationLayer):\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    if hasattr(src_module, 'weight') and src_module.weight is not None and hasattr(cloned_module, 'weight'):\n",
    "        expansion = cloned_module.weight.data.shape[0] // src_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[i::expansion], src_module.weight.data)\n",
    "            \n",
    "            if hasattr(src_module, 'bias') and src_module.bias is not None and hasattr(cloned_module, 'bias'):\n",
    "                assert torch.allclose(cloned_module.bias.data[i::expansion], src_module.bias.data)\n",
    "    \n",
    "    # Check running stats for BatchNorm layers\n",
    "    if hasattr(src_module, 'running_mean') and src_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // src_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.running_mean.data[i::expansion], src_module.running_mean.data)\n",
    "                assert torch.allclose(cloned_module.running_var.data[i::expansion], src_module.running_var.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_linear_cloning(src_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    in_expansion = cloned_module.in_features // src_module.in_features\n",
    "    out_expansion = cloned_module.out_features // src_module.out_features\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "    \n",
    "def validate_conv1d_cloning(src_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    in_expansion = cloned_module.in_channels // src_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // src_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_conv2d_cloning(src_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    in_expansion = cloned_module.in_channels // src_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // src_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "\n",
    "\n",
    "def clone_module(\n",
    "    src_module: nn.Module, \n",
    "    cloned_module: nn.Module,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Clone parameters from a source module to a cloned module.\n",
    "    \n",
    "    Args:\n",
    "        src_module: Source module with smaller dimensions\n",
    "        cloned_module: Target module with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if cloning was successful, False otherwise\n",
    "    \"\"\"\n",
    "    success = True\n",
    "    \n",
    "    # Define normalization and activation types inline for easier checking\n",
    "    norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)\n",
    "    activation_types = (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, nn.LeakyReLU, \n",
    "                       nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, nn.Softplus, nn.Softmin, \n",
    "                       nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, nn.Hardshrink, nn.Softsign, \n",
    "                       nn.GLU, nn.CELU, nn.Identity)\n",
    "    \n",
    "    if isinstance(src_module, nn.Linear):\n",
    "        clone_linear(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Conv1d):\n",
    "        clone_conv1d(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Conv2d):\n",
    "        clone_conv2d(src_module, cloned_module)\n",
    "    elif isinstance(src_module, norm_types):\n",
    "        clone_normalization(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Embedding):\n",
    "        clone_embedding(src_module, cloned_module)\n",
    "    elif isinstance(src_module, activation_types):\n",
    "        clone_activation(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Dropout):\n",
    "        clone_dropout(src_module, cloned_module)\n",
    "    elif is_parameter_free(src_module) and is_parameter_free(cloned_module):\n",
    "        clone_parameter_free(src_module, cloned_module)\n",
    "    else:\n",
    "        success = False\n",
    "        print(f\"Unsupported module type: {type(src_module)}\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "def clone_model(src_model: nn.Module, cloned_model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Clone parameters from a source model to a cloned model.\n",
    "    \n",
    "    Args:\n",
    "        src_model: Source model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    # Keep track of processed modules to avoid double-processing\n",
    "    processed_modules = set()\n",
    "    \n",
    "    # Get direct children modules only, to avoid processing nested modules multiple times\n",
    "    src_direct_children = {name: module for name, module in src_model.named_children()}\n",
    "    \n",
    "    for name, src_child in src_direct_children.items():\n",
    "        try:\n",
    "            cloned_child = cloned_model.get_submodule(name)\n",
    "            print(f\"Cloning module {name}\")\n",
    "            \n",
    "            # Clone the current module\n",
    "            success = clone_module(src_child, cloned_child)\n",
    "            if not success:\n",
    "                print(f\"Warning: Failed to clone module {name} directly\")\n",
    "                \n",
    "                # If direct cloning failed, try recursively cloning its children\n",
    "                recursive_success = clone_model(src_child, cloned_child)\n",
    "                \n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Could not find matching module for {name} in the cloned model\")\n",
    "            \n",
    "    return cloned_model\n",
    "\n",
    "\n",
    "def clone_model(src_model: nn.Module, cloned_model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Clone parameters from a source model to a cloned model.\n",
    "    \n",
    "    Args:\n",
    "        src_model: Source model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process each module individually\n",
    "    for name, src_module in src_model.named_modules():\n",
    "        cloned_module = cloned_model.get_submodule(name)\n",
    "        print(f\"Cloning module {name}\")\n",
    "        clone_module(src_module, cloned_module)\n",
    "    \n",
    "    return cloned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b39d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9a65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc0c7437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning module \n",
      "Unsupported module type: <class 'src.models.mlp.MLP'>\n",
      "Cloning module layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.linear_0\n",
      "Cloning Linear module: 10→10, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.act_0\n",
      "Cloning module layers.linear_1\n",
      "Cloning Linear module: 64→128, 32→64, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.act_1\n",
      "Cloning module layers.out\n",
      "Cloning Linear module: 32→64, 2→2, in expansion: 2, out expansion: 1\n",
      "Output from source model: tensor([[-0.3410, -0.0723],\n",
      "        [-0.3435, -0.1194],\n",
      "        [-0.2754, -0.0096],\n",
      "        [-0.2427, -0.0972],\n",
      "        [-0.2637, -0.0710],\n",
      "        [-0.1828, -0.1549],\n",
      "        [-0.3514, -0.0417],\n",
      "        [-0.2227, -0.0717],\n",
      "        [-0.2692, -0.0477],\n",
      "        [-0.1762, -0.0253],\n",
      "        [-0.2311, -0.0928],\n",
      "        [-0.1939, -0.0128],\n",
      "        [-0.2574, -0.1233],\n",
      "        [-0.3244, -0.2309],\n",
      "        [-0.1472, -0.0440],\n",
      "        [-0.2799, -0.0553],\n",
      "        [-0.2711, -0.0886],\n",
      "        [-0.1895, -0.0313],\n",
      "        [-0.2873, -0.0961],\n",
      "        [-0.1829, -0.0029],\n",
      "        [-0.2241, -0.0603],\n",
      "        [-0.1892, -0.1177],\n",
      "        [-0.2425, -0.1266],\n",
      "        [-0.2645, -0.0578],\n",
      "        [-0.2967, -0.1164],\n",
      "        [-0.2342, -0.1671],\n",
      "        [-0.2729, -0.0606],\n",
      "        [-0.2812, -0.0961],\n",
      "        [-0.2380, -0.0950],\n",
      "        [-0.2881, -0.0098],\n",
      "        [-0.2387, -0.1067],\n",
      "        [-0.1287, -0.0181]], grad_fn=<AddmmBackward0>)\n",
      "Output from cloned model: tensor([[-0.3410, -0.0723],\n",
      "        [-0.3435, -0.1194],\n",
      "        [-0.2754, -0.0096],\n",
      "        [-0.2427, -0.0972],\n",
      "        [-0.2637, -0.0710],\n",
      "        [-0.1828, -0.1549],\n",
      "        [-0.3514, -0.0417],\n",
      "        [-0.2227, -0.0717],\n",
      "        [-0.2692, -0.0477],\n",
      "        [-0.1762, -0.0253],\n",
      "        [-0.2311, -0.0928],\n",
      "        [-0.1939, -0.0128],\n",
      "        [-0.2574, -0.1233],\n",
      "        [-0.3244, -0.2309],\n",
      "        [-0.1472, -0.0440],\n",
      "        [-0.2799, -0.0553],\n",
      "        [-0.2711, -0.0886],\n",
      "        [-0.1895, -0.0313],\n",
      "        [-0.2873, -0.0961],\n",
      "        [-0.1829, -0.0029],\n",
      "        [-0.2241, -0.0603],\n",
      "        [-0.1892, -0.1177],\n",
      "        [-0.2425, -0.1266],\n",
      "        [-0.2645, -0.0578],\n",
      "        [-0.2967, -0.1164],\n",
      "        [-0.2342, -0.1671],\n",
      "        [-0.2729, -0.0606],\n",
      "        [-0.2812, -0.0961],\n",
      "        [-0.2380, -0.0950],\n",
      "        [-0.2881, -0.0098],\n",
      "        [-0.2387, -0.1067],\n",
      "        [-0.1287, -0.0181]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "\n",
    "src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32,], activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2,], activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = clone_model(src_model, cloned_model)\n",
    "\n",
    "x = torch.randn(32, 10)\n",
    "y1 = src_model(x)\n",
    "y2 = cloned_model(x)\n",
    "print(\"Output from source model:\", y1)\n",
    "print(\"Output from cloned model:\", y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d02581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = src_model.layers.linear_0.weight.data, cloned_model.layers.linear_0.weight.data\n",
    "b1, b2 = src_model.layers.linear_0.bias.data, cloned_model.layers.linear_0.bias.data\n",
    "w1.shape, w2.shape\n",
    "\n",
    "torch.allclose(w2[::2,:], w1),torch.allclose(b2[::2], b1)\n",
    "\n",
    "w1, w2 = src_model.layers.linear_1.weight.data, cloned_model.layers.linear_1.weight.data\n",
    "b1, b2 = src_model.layers.linear_1.bias.data, cloned_model.layers.linear_1.bias.data\n",
    "\n",
    "torch.allclose(w2[::2,::2], w1/2),torch.allclose(w2[1::2,::2], w1/2),torch.allclose(w2[::2,1::2], w1/2),torch.allclose(w2[1::2,1::2], w1/2),torch.allclose(b2[::2], b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c651a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All activations are close \n"
     ]
    }
   ],
   "source": [
    "h1, h2 = src_model.layers.linear_0(x), cloned_model.layers.linear_0(x)\n",
    "h1, h2 = src_model.layers.act_0(h1), cloned_model.layers.act_0(h2)\n",
    "h1, h2 = src_model.layers.linear_1(h1), cloned_model.layers.linear_1(h2)\n",
    "h1, h2 = src_model.layers.act_1(h1), cloned_model.layers.act_1(h2)\n",
    "h1, h2 = src_model.layers.out(h1), cloned_model.layers.out(h2)\n",
    "h1.shape, h2.shape\n",
    "if h1.shape[1] != h2.shape[1]:\n",
    "    assert h2.shape[1] % h1.shape[1] == 0, \"Output dimensions are not integer multiples\"\n",
    "    expansion_factor = h2.shape[1] // h1.shape[1]\n",
    "    for i in range(expansion_factor):\n",
    "        assert torch.allclose(h2[:,i::expansion_factor], h1, atol=1e-5), f\"Output mismatch at expansion factor {i}\"\n",
    "print(\"All activations are close \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd87d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = src_model.layers.out.weight.data, cloned_model.layers.out.weight.data\n",
    "b1, b2 = src_model.layers.out.bias.data, cloned_model.layers.out.bias.data\n",
    "\n",
    "w1.shape, w2.shape, b1.shape, b2.shape\n",
    "torch.allclose(w2[:,::2],w1/2),torch.allclose(w2[:,1::2],w1/2), torch.allclose(b2, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a581ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.9605e-08,  0.0000e+00,  5.9605e-08,  2.9802e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00],\n",
       "        [ 2.9802e-08,  0.0000e+00,  0.0000e+00,  2.9802e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-5.9605e-08,  0.0000e+00,  1.4901e-08,  0.0000e+00]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e0541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
