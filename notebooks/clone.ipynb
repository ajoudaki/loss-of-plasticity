{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9583be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amir/Codes/NN-dynamic-scaling already in Python path\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/amir/Codes/NN-dynamic-scaling to Python path\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f11ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amir/Codes/NN-dynamic-scaling already in Python path\n",
      "Cloning module layers.linear_0: 10→10, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.linear_1: 64→128, 32→64, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.linear_2: 32→64, 16→32, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.out: 16→32, 2→2, in expansion: 2, out expansion: 1\n",
      "Passed 24 out of 24 tests\n",
      "\n",
      "Model cloning test: PASSED\n",
      "Passed 7 out of 7 tests\n",
      "\n",
      "Activation cloning test: PASSED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import the utility module and Setup the path\n",
    "import notebook_utils\n",
    "notebook_utils.setup_path()\n",
    "\n",
    "from src.models.mlp import MLP\n",
    "\n",
    "def clone_model_parameters(src_model, cloned_model):\n",
    "    \"\"\"\n",
    "    Clone parameters from a smaller model to a larger model using a module-based approach.\n",
    "    \n",
    "    For linear layers, weights are scaled by 1/n (where n is the input expansion factor)\n",
    "    to ensure equivalent functionality after cloning.\n",
    "    \n",
    "    Args:\n",
    "        src_model: Source model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    # First verify model structures\n",
    "    src_modules = {name: module for name, module in src_model.named_modules() if isinstance(module, nn.Linear)}\n",
    "    cloned_modules = {name: module for name, module in cloned_model.named_modules() if isinstance(module, nn.Linear)}\n",
    "    \n",
    "    # Check if modules match\n",
    "    if set(src_modules.keys()) != set(cloned_modules.keys()):\n",
    "        raise ValueError(\"Source and cloned models have different module structures\")\n",
    "    \n",
    "    # Process each module individually\n",
    "    for name, src_module in src_modules.items():\n",
    "        cloned_module = cloned_modules[name]\n",
    "        \n",
    "        # Get module dimensions\n",
    "        src_in_features = src_module.in_features\n",
    "        src_out_features = src_module.out_features\n",
    "        cloned_in_features = cloned_module.in_features\n",
    "        cloned_out_features = cloned_module.out_features\n",
    "        \n",
    "        # Calculate expansion factors\n",
    "        in_expansion = cloned_in_features // src_in_features\n",
    "        out_expansion = cloned_out_features // src_out_features\n",
    "        \n",
    "        print(f\"Cloning module {name}: {src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "        \n",
    "        \n",
    "        # Verify expansion factors are valid\n",
    "        if cloned_in_features % src_in_features != 0 or cloned_out_features % src_out_features != 0:\n",
    "            raise ValueError(f\"Module {name} dimensions are not integer multiples: \"\n",
    "                             f\"{src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}\")\n",
    "        \n",
    "        # Clone the weights with proper scaling\n",
    "        for i in range(in_expansion):\n",
    "            for j in range(out_expansion):\n",
    "                cloned_module.weight.data[j::out_expansion, i::in_expansion] = src_module.weight.data / in_expansion\n",
    "    \n",
    "        \n",
    "        # Clone the bias if present (no scaling needed for bias)\n",
    "        if src_module.bias is not None and cloned_module.bias is not None:\n",
    "            # cloned the bias vector \n",
    "            for j in range(out_expansion):\n",
    "                cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    \n",
    "    # For non-linear modules (if any), copy parameters without scaling\n",
    "    for name, param in src_model.named_parameters():\n",
    "        # Skip parameters that belong to linear layers (already handled)\n",
    "        if any(module_name in name for module_name in src_modules.keys()):\n",
    "            continue\n",
    "        \n",
    "        if name in cloned_model.state_dict():\n",
    "            cloned_param = cloned_model.state_dict()[name]\n",
    "            src_shape = torch.tensor(param.shape)\n",
    "            cloned_shape = torch.tensor(cloned_param.shape)\n",
    "            \n",
    "            # If shapes match, directly copy\n",
    "            if tuple(src_shape) == tuple(cloned_shape):\n",
    "                cloned_param.copy_(param)\n",
    "                print(f\"Parameter {name} copied directly (dimensions match)\")\n",
    "            else:\n",
    "                # For other parameters that need expansion but no scaling\n",
    "                non_matching_dims = src_shape != cloned_shape\n",
    "                \n",
    "                # Create indices for blockwise expansion\n",
    "                indices = []\n",
    "                expansion_info = []\n",
    "                \n",
    "                for i, (s1, s2) in enumerate(zip(src_shape, cloned_shape)):\n",
    "                    if s1 == s2:\n",
    "                        indices.append(torch.arange(s2))\n",
    "                    else:\n",
    "                        expansion_factor = s2 // s1\n",
    "                        expansion_info.append(f\"dim {i}: {expansion_factor}x\")\n",
    "                        indices.append(torch.div(torch.arange(s2), expansion_factor, rounding_mode='floor'))\n",
    "                \n",
    "                # Create the grid and copy\n",
    "                grid = torch.meshgrid(*indices, indexing='ij')\n",
    "                cloned_param.copy_(param[grid])\n",
    "                \n",
    "                if expansion_info:\n",
    "                    print(f\"Parameter {name} cloned with blockwise expansion: {', '.join(expansion_info)}\")\n",
    "    \n",
    "    return cloned_model\n",
    "    \n",
    "    \n",
    "def test_model_cloning(src_model, cloned_model):\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for name, module in src_model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # print(f\"source module {name}: {module.in_features}→{module.out_features},  cloned module {cloned_model.get_submodule(name).in_features}→{cloned_model.get_submodule(name).out_features}\")\n",
    "            module2 = cloned_model.get_submodule(name)\n",
    "            in_expansion = module2.in_features // module.in_features\n",
    "            out_expansion = module2.out_features // module.out_features\n",
    "            # print(f\"Expansion factors (outxin): {out_expansion}x{in_expansion}\")\n",
    "            for j in range(out_expansion):\n",
    "                for i in range(in_expansion):\n",
    "                    passed += torch.allclose(module2.weight.data[j::out_expansion, i::in_expansion], module.weight.data/in_expansion)\n",
    "                    passed += torch.allclose(module2.bias.data[j::out_expansion], module.bias.data)\n",
    "                    total += 2\n",
    "    \n",
    "    print(f\"Passed {passed} out of {total} tests\")\n",
    "    return passed == total  \n",
    "\n",
    "def test_activation_clonign(src_model, cloned_model):\n",
    "    from src.utils.monitor import NetworkMonitor\n",
    "\n",
    "    src_monitor = NetworkMonitor(src_model, )\n",
    "    cloned_monitor = NetworkMonitor(cloned_model, )\n",
    "    src_monitor.register_hooks()\n",
    "    cloned_monitor.register_hooks()\n",
    "\n",
    "\n",
    "    d = src_model.input_size\n",
    "    x = torch.randn(10, d)\n",
    "    src_model(x)\n",
    "    cloned_model(x)\n",
    "\n",
    "    acts, acts2 = src_monitor.get_latest_activations(), cloned_monitor.get_latest_activations()\n",
    "\n",
    "    passed = 0\n",
    "    total = 0\n",
    "    for key in acts.keys():\n",
    "        a1, a2 = acts[key], acts2[key]\n",
    "        if a1.shape[1] != a2.shape[1]:\n",
    "            diffs = (a1 - a2[:,::2])\n",
    "        else: #only for the last layer\n",
    "            diffs = (a1 - a2)\n",
    "        # print(f\"Diff for {key}: {diffs.abs().max().item()}\")\n",
    "        passed += diffs.abs().max().item() < 1e-5\n",
    "        total += 1\n",
    "    print(f\"Passed {passed} out of {total} tests\")\n",
    "    return passed == total\n",
    "\n",
    "# src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32, 16], activation=\"relu\", dropout_p=0.0)\n",
    "# cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2, 16*2], activation=\"relu\", dropout_p=0.0)\n",
    "\n",
    "# cloned_model = clone_model_parameters(src_model, cloned_model)\n",
    "# test1(src_model, cloned_model)\n",
    "# test2(src_model, cloned_model)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create source and target models with random weights\n",
    "    src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32, 16], activation=\"relu\", dropout_p=0.0)\n",
    "    cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2, 16*2], activation=\"relu\", dropout_p=0.0)\n",
    "    \n",
    "    # Clone the parameters\n",
    "    cloned_model = clone_model_parameters(src_model, cloned_model)\n",
    "    \n",
    "    # Test the cloning with a functional test\n",
    "    success = test_model_cloning(src_model, cloned_model)\n",
    "    print(\"\\nModel cloning test:\", \"PASSED\" if success else \"FAILED\")\n",
    "\n",
    "    \n",
    "    \n",
    "    success = test_activation_clonign(src_model, cloned_model)\n",
    "    print(\"\\nActivation cloning test:\", \"PASSED\" if success else \"FAILED\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99d6de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([256]) tensor([1., 1., 1., 1., 1.], grad_fn=<SliceBackward0>)\n",
      "bias torch.Size([256]) tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import CNN\n",
    "model = CNN()\n",
    "model.layers.conv_0.weight.data.shape\n",
    "for k,v in model.layers.norm_2.named_parameters():\n",
    "    print(k,v.shape, v[:5])\n",
    "model.layers.norm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07df9b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (layers): ModuleDict(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU()\n",
       "    (layer1_block0): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer1_block1): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2_block0): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2_block1): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3_block0): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3_block1): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4_block0): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4_block1): BasicBlock(\n",
       "      (layers): ModuleDict(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "\n",
    "model = ResNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "abf3c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([10]) tensor([1., 1., 1., 1., 1.], grad_fn=<SliceBackward0>)\n",
      "bias torch.Size([10]) tensor([0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l = nn.BatchNorm1d(10)\n",
    "for k,v in l.named_parameters():\n",
    "    print(k,v.shape, v[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b39d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning module layers.linear_0: 10→10, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.linear_1: 64→128, 32→64, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.linear_2: 32→64, 16→32, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.out: 16→32, 2→2, in expansion: 2, out expansion: 1\n",
      "Diff for layers.linear_0: 0.0\n",
      "Diff for layers.act_0: 0.0\n",
      "Diff for layers.linear_1: 1.7881393432617188e-07\n",
      "Diff for layers.act_1: 1.7881393432617188e-07\n",
      "Diff for layers.linear_2: 5.960464477539063e-08\n",
      "Diff for layers.act_2: 5.960464477539063e-08\n",
      "Diff for layers.out: 0.08214077353477478\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53f9a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0097), tensor(-0.2224))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model.layers.linear_0.bias.data[0], cloned_model.layers.linear_0.bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc0c7437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 2: Diff max for layers.linear_0: 2.2091572284698486\n",
      "case 2: Diff max for layers.act_0: 1.2357994318008423\n",
      "case 2: Diff max for layers.linear_1: 1.0886716842651367\n",
      "case 2: Diff max for layers.act_1: 0.828102171421051\n",
      "case 2: Diff max for layers.linear_2: 0.52223801612854\n",
      "case 2: Diff max for layers.act_2: 0.35224783420562744\n",
      "case 3: Diff max for layers.out: 0.4210308790206909\n"
     ]
    }
   ],
   "source": [
    "for key in acts.keys():\n",
    "    if acts[key].shape[0] != acts2[key].shape[0]:\n",
    "        print(f\"case 1: Diff max for {key}: {(acts[key] - acts2[key][::2]).abs().max().item()}\")\n",
    "    elif acts[key].shape[1] != acts2[key].shape[1] and acts[key].shape[0] == acts2[key].shape[0]:\n",
    "        print(f\"case 2: Diff max for {key}: {(acts[key] - acts2[key][:,::2]).abs().max().item()}\")\n",
    "    elif acts[key].shape[1] != acts2[key].shape[1] and acts[key].shape[0] != acts2[key].shape[0]:\n",
    "        print(f\"case 2: Diff max for {key}: {(acts[key] - acts2[key][::2,::2]).abs().max().item()}\")\n",
    "    else:\n",
    "        diffs = acts[key]- acts2[key]\n",
    "        print (f\"case 3: Diff max for {key}: {diffs.abs().max().item()}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d02581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0235, -0.0760,  0.0396,  ...,  0.0529,  0.0249, -0.0878],\n",
       "        [-0.0603,  0.0927,  0.0358,  ..., -0.0336,  0.0566, -0.0045],\n",
       "        [-0.0821, -0.0516, -0.0679,  ...,  0.0044,  0.0684,  0.0292],\n",
       "        ...,\n",
       "        [-0.0473,  0.0655,  0.0621,  ..., -0.0619, -0.0801,  0.0128],\n",
       "        [-0.0593,  0.0851, -0.0510,  ...,  0.0862, -0.0646, -0.0052],\n",
       "        [-0.0688,  0.0260,  0.0799,  ...,  0.0811,  0.0772,  0.0347]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model.layers.linear_1.weight.data- cloned_model.layers.linear_1.weight.data[::2,::2]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651a1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-5.9605e-08,  2.9802e-08,  0.0000e+00, -2.9802e-08],\n",
       "        [ 1.4901e-08,  2.9802e-08,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(2, 4)\n",
    "lin2 = nn.Linear(4, 12)\n",
    "m, n = torch.tensor(lin2.weight.data.shape)//torch.tensor(lin.weight.data.shape)\n",
    "m, n = m.item(), n.item()\n",
    "for j in range(m):\n",
    "    lin2.bias.data[j::m] = lin.bias.data[:]\n",
    "    for i in range(n):\n",
    "        lin2.weight.data[j::m, i::n] = lin.weight.data / n\n",
    "\n",
    "x = torch.randn(5, 2)\n",
    "x2 = torch.randn(5, 2*n)\n",
    "for j in range(n):\n",
    "    x2[:,j::n] = x\n",
    "x2[:,::2] == x, x2[:,1::2] == x # this holdds \n",
    "\n",
    "y = lin(x)\n",
    "y2 = lin2(x2)\n",
    "\n",
    "y2[:,::m] - y\n",
    "\n",
    "# lin2.weight.data[::2,::2] = lin.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd87d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]),\n",
       " torch.Size([5, 2]),\n",
       " torch.Size([5, 12]),\n",
       " torch.Size([5, 4]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, x.shape, y2.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a581ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.9605e-08,  0.0000e+00,  5.9605e-08,  2.9802e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00],\n",
       "        [ 2.9802e-08,  0.0000e+00,  0.0000e+00,  2.9802e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-5.9605e-08,  0.0000e+00,  1.4901e-08,  0.0000e+00]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2[:,::m] - y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e0541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
