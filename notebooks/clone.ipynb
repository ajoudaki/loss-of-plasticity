{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9221e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amir/Codes/NN-dynamic-scaling already in Python path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import the utility module and Setup the path\n",
    "import notebook_utils\n",
    "notebook_utils.setup_path()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Union, List, Dict, Set\n",
    "\n",
    "# Replace TypeVar with Union for proper type handling\n",
    "NormalizationLayer = Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm]\n",
    "\n",
    "# Define activation function types properly\n",
    "ActivationFunction = Union[nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, \n",
    "                         nn.LeakyReLU, nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, \n",
    "                         nn.Softplus, nn.Softmin, nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, \n",
    "                         nn.Hardshrink, nn.Softsign, nn.GLU, nn.CELU, nn.Identity]\n",
    "\n",
    "\n",
    "class CloneAwareFlatten(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom flatten module that ensures duplicated features remain adjacent when flattening\n",
    "    convolutional feature maps.\n",
    "    \n",
    "    When cloning channels in convolutional layers, the standard nn.Flatten would arrange features\n",
    "    as [a(0,0), a'(0,0), b(0,0), b'(0,0), ...] where features are grouped by spatial position.\n",
    "    \n",
    "    This module rearranges to keep all spatial positions of the same channel together:\n",
    "    [a(0,0), a'(0,0), a(0,1), a'(0,1), ..., b(0,0), b'(0,0), ...] ensuring duplicated\n",
    "    features remain adjacent.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_dim=1, end_dim=-1):\n",
    "        super().__init__()\n",
    "        self.start_dim = start_dim\n",
    "        self.end_dim = end_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Standard flattening for non-starting dimensions or non-4D tensors\n",
    "        if x.dim() != 4 or self.start_dim > 1:\n",
    "            start_dim = self.start_dim if self.start_dim >= 0 else x.dim() + self.start_dim\n",
    "            end_dim = self.end_dim if self.end_dim >= 0 else x.dim() + self.end_dim\n",
    "            \n",
    "            shape = x.shape\n",
    "            new_shape = list(shape[:start_dim]) + [-1]\n",
    "            return x.reshape(*new_shape)\n",
    "        \n",
    "        # Special handling for 4D tensors with channel duplication\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # If channels are not even, use standard flattening\n",
    "        if channels % 2 != 0:\n",
    "            return x.reshape(batch_size, -1)\n",
    "        \n",
    "        half_channels = channels // 2\n",
    "        \n",
    "        # Step 1: Reshape to separate duplicated channels\n",
    "        # [batch, channels, h, w] -> [batch, half_channels, 2, h, w]\n",
    "        x_reshaped = x.view(batch_size, half_channels, 2, height, width)\n",
    "        \n",
    "        # Step 2: Permute to get the desired order\n",
    "        # [batch, half_channels, 2, h, w] -> [batch, half_channels, h, w, 2]\n",
    "        x_permuted = x_reshaped.permute(0, 1, 3, 4, 2)\n",
    "        \n",
    "        # Step 3: Flatten\n",
    "        # [batch, half_channels, h, w, 2] -> [batch, half_channels * h * w * 2]\n",
    "        return x_permuted.reshape(batch_size, -1)\n",
    "\n",
    "def clone_linear(src_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    # Get module dimensions\n",
    "    src_in_features = src_module.in_features\n",
    "    src_out_features = src_module.out_features\n",
    "    cloned_in_features = cloned_module.in_features\n",
    "    cloned_out_features = cloned_module.out_features\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_features % src_in_features != 0 or cloned_out_features % src_out_features != 0:\n",
    "        raise ValueError(f\"Linear module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}\")\n",
    "        \n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_features // src_in_features\n",
    "    out_expansion = cloned_out_features // src_out_features\n",
    "    \n",
    "    print(f\"Cloning Linear module: {src_in_features}→{cloned_in_features}, {src_out_features}→{cloned_out_features}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_conv1d(src_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    # Get module dimensions\n",
    "    src_in_channels = src_module.in_channels\n",
    "    src_out_channels = src_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // src_in_channels\n",
    "    out_expansion = cloned_out_channels // src_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv1d module: {src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % src_in_channels != 0 or cloned_out_channels % src_out_channels != 0:\n",
    "        raise ValueError(f\"Conv1d module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "\n",
    "    \n",
    "def clone_conv2d(src_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    # Get module dimensions\n",
    "    src_in_channels = src_module.in_channels\n",
    "    src_out_channels = src_module.out_channels\n",
    "    cloned_in_channels = cloned_module.in_channels\n",
    "    cloned_out_channels = cloned_module.out_channels\n",
    "    # Calculate expansion factors\n",
    "    in_expansion = cloned_in_channels // src_in_channels\n",
    "    out_expansion = cloned_out_channels // src_out_channels\n",
    "    \n",
    "    print(f\"Cloning Conv2d module: {src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}, in expansion: {in_expansion}, out expansion: {out_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_in_channels % src_in_channels != 0 or cloned_out_channels % src_out_channels != 0:\n",
    "        raise ValueError(f\"Conv2d module dimensions are not integer multiples: \"\n",
    "                         f\"{src_in_channels}→{cloned_in_channels}, {src_out_channels}→{cloned_out_channels}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(in_expansion):\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :] = src_module.weight.data / in_expansion\n",
    "    \n",
    "    # Clone the bias if present (no scaling needed for bias)\n",
    "    if src_module.bias is not None and cloned_module.bias is not None:\n",
    "        for j in range(out_expansion):\n",
    "            cloned_module.bias.data[j::out_expansion] = src_module.bias.data\n",
    "    return cloned_module\n",
    "    \n",
    "\n",
    "def clone_normalization(\n",
    "    src_module: NormalizationLayer, \n",
    "    cloned_module: NormalizationLayer,\n",
    ") -> NormalizationLayer:\n",
    "    \"\"\"Clone normalization layer parameters with proper handling of different types.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Check properties that exist for the specific normalization type\n",
    "    if hasattr(src_module, 'affine') and hasattr(cloned_module, 'affine'):\n",
    "        assert src_module.affine == cloned_module.affine, \"Affine property must match\"\n",
    "    \n",
    "    # Handle BatchNorm-specific properties\n",
    "    if isinstance(src_module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        if hasattr(src_module, 'track_running_stats') and hasattr(cloned_module, 'track_running_stats'):\n",
    "            assert src_module.track_running_stats == cloned_module.track_running_stats, \"Track running stats property must match\"\n",
    "    \n",
    "    # Clone weights and biases\n",
    "    if hasattr(src_module, 'weight') and src_module.weight is not None and cloned_module.weight is not None:\n",
    "        expansion = cloned_module.weight.data.shape[0] // src_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            cloned_module.weight.data[i::expansion] = src_module.weight.data\n",
    "            if hasattr(src_module, 'bias') and src_module.bias is not None and cloned_module.bias is not None:\n",
    "                cloned_module.bias.data[i::expansion] = src_module.bias.data\n",
    "    \n",
    "    # Clone running stats for BatchNorm layers\n",
    "    if hasattr(src_module, 'running_mean') and src_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // src_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                cloned_module.running_mean.data[i::expansion] = src_module.running_mean.data\n",
    "                cloned_module.running_var.data[i::expansion] = src_module.running_var.data\n",
    "    \n",
    "    # Clone num_batches_tracked for BatchNorm layers\n",
    "    if hasattr(src_module, 'num_batches_tracked') and src_module.num_batches_tracked is not None:\n",
    "        if hasattr(cloned_module, 'num_batches_tracked') and cloned_module.num_batches_tracked is not None:\n",
    "            cloned_module.num_batches_tracked.data.copy_(src_module.num_batches_tracked.data)\n",
    "    \n",
    "    return cloned_module\n",
    "    \n",
    "    \n",
    "def clone_embedding(src_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    # Get module dimensions\n",
    "    src_num_embeddings = src_module.num_embeddings\n",
    "    src_embedding_dim = src_module.embedding_dim\n",
    "    cloned_num_embeddings = cloned_module.num_embeddings\n",
    "    cloned_embedding_dim = cloned_module.embedding_dim\n",
    "    \n",
    "    # Calculate expansion factors\n",
    "    num_expansion = cloned_num_embeddings // src_num_embeddings\n",
    "    dim_expansion = cloned_embedding_dim // src_embedding_dim\n",
    "    \n",
    "    print(f\"Cloning Embedding module: {src_num_embeddings}→{cloned_num_embeddings}, {src_embedding_dim}→{cloned_embedding_dim}, num expansion: {num_expansion}, dim expansion: {dim_expansion}\")\n",
    "    \n",
    "    # Verify expansion factors are valid\n",
    "    if cloned_num_embeddings % src_num_embeddings != 0 or cloned_embedding_dim % src_embedding_dim != 0:\n",
    "        raise ValueError(f\"Embedding module dimensions are not integer multiples: \"\n",
    "                         f\"{src_num_embeddings}→{cloned_num_embeddings}, {src_embedding_dim}→{cloned_embedding_dim}\")\n",
    "    \n",
    "    # Clone the weights with proper scaling\n",
    "    for i in range(num_expansion):\n",
    "        for j in range(dim_expansion):\n",
    "            cloned_module.weight.data[j::dim_expansion, i::num_expansion] = src_module.weight.data \n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_activation(src_module: ActivationFunction, cloned_module: ActivationFunction) -> ActivationFunction:\n",
    "    \"\"\"Clone activation function parameters, handling configuration parameters properly.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Handle configuration parameters for different activation types\n",
    "    if isinstance(src_module, nn.LeakyReLU):\n",
    "        cloned_module.negative_slope = src_module.negative_slope\n",
    "    \n",
    "    elif isinstance(src_module, (nn.ELU, nn.CELU)):\n",
    "        cloned_module.alpha = src_module.alpha\n",
    "    \n",
    "    elif isinstance(src_module, nn.Threshold):\n",
    "        cloned_module.threshold = src_module.threshold\n",
    "        cloned_module.value = src_module.value\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        cloned_module.dim = src_module.dim\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        cloned_module.lambd = src_module.lambd\n",
    "        \n",
    "    elif isinstance(src_module, nn.GLU):\n",
    "        cloned_module.dim = src_module.dim\n",
    "    \n",
    "    # Handle PReLU specifically (has learnable parameters)\n",
    "    elif isinstance(src_module, nn.PReLU):\n",
    "        if src_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # If source is a single parameter, broadcast to all channels\n",
    "            cloned_module.weight.data.fill_(src_module.weight.data[0])\n",
    "        elif src_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            # Channel-wise parameters need proper expansion\n",
    "            expansion = cloned_module.num_parameters // src_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                cloned_module.weight.data[i::expansion] = src_module.weight.data\n",
    "        else:\n",
    "            # Direct copy if dimensions match\n",
    "            cloned_module.weight.data.copy_(src_module.weight.data)\n",
    "    \n",
    "    # Handle other parameterized activation functions if they have weights\n",
    "    # This is a general catch-all for any other activation function with parameters\n",
    "    elif hasattr(src_module, 'weight') and hasattr(cloned_module, 'weight'):\n",
    "        if src_module.weight is not None and cloned_module.weight is not None:\n",
    "            if cloned_module.weight.data.shape == src_module.weight.data.shape:\n",
    "                cloned_module.weight.data.copy_(src_module.weight.data)\n",
    "    \n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "def clone_dropout(src_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    \"\"\"Clone dropout module parameters.\"\"\"\n",
    "    assert cloned_module.p == src_module.p, \"Dropout probability must match\"\n",
    "    # Print warning if dropout p > 0\n",
    "    if cloned_module.p > 0:\n",
    "        print(f\"Warning: Dropout probability is set to {cloned_module.p}, cloning is not perfect\")\n",
    "    return cloned_module\n",
    "\n",
    "def clone_flatten(src_module: nn.Flatten) -> CloneAwareFlatten:\n",
    "    \"\"\"\n",
    "    Clone parameters from a standard Flatten and return a new CloneAwareFlatten.\n",
    "    \n",
    "    Args:\n",
    "        src_module: Source nn.Flatten module\n",
    "        \n",
    "    Returns:\n",
    "        A new CloneAwareFlatten module with the same parameters\n",
    "    \"\"\"\n",
    "    return CloneAwareFlatten(\n",
    "        start_dim=src_module.start_dim,\n",
    "        end_dim=src_module.end_dim\n",
    "    )\n",
    "\n",
    "\n",
    "def is_parameter_free(module: nn.Module) -> bool:\n",
    "    \"\"\"Check if a module has no parameters.\"\"\"\n",
    "    return len(list(module.parameters())) == 0\n",
    "\n",
    "\n",
    "def clone_parameter_free(src_module: nn.Module, cloned_module: nn.Module) -> nn.Module:\n",
    "    \"\"\"Clone a parameter-free module.\"\"\"\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    assert is_parameter_free(src_module), \"Source module must be parameter free\"\n",
    "    assert is_parameter_free(cloned_module), \"Cloned module must be parameter free\"\n",
    "    \n",
    "    # For parameter-free modules, there's no need to copy weights\n",
    "    # Just make sure they're of the same type, which we've already checked\n",
    "    return cloned_module\n",
    "\n",
    "\n",
    "# Validation functions\n",
    "\n",
    "def validate_activation_cloning(src_module: ActivationFunction, cloned_module: ActivationFunction):\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    # Validate configuration parameters for different activation types\n",
    "    if isinstance(src_module, nn.LeakyReLU):\n",
    "        assert src_module.negative_slope == cloned_module.negative_slope, \"LeakyReLU negative_slope does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.ELU, nn.CELU)):\n",
    "        assert src_module.alpha == cloned_module.alpha, \"Alpha parameter does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, nn.Threshold):\n",
    "        assert src_module.threshold == cloned_module.threshold, \"Threshold value does not match\"\n",
    "        assert src_module.value == cloned_module.value, \"Replacement value does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Softmax, nn.LogSoftmax)):\n",
    "        assert src_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    elif isinstance(src_module, (nn.Hardshrink, nn.Softshrink)):\n",
    "        assert src_module.lambd == cloned_module.lambd, \"Lambda parameter does not match\"\n",
    "        \n",
    "    elif isinstance(src_module, nn.GLU):\n",
    "        assert src_module.dim == cloned_module.dim, \"Dimension parameter does not match\"\n",
    "    \n",
    "    # Validate PReLU parameters\n",
    "    elif isinstance(src_module, nn.PReLU):\n",
    "        if src_module.num_parameters == 1 and cloned_module.num_parameters > 1:\n",
    "            # All elements should be equal to the single parameter\n",
    "            assert torch.all(cloned_module.weight.data == src_module.weight.data[0])\n",
    "        elif src_module.num_parameters > 1 and cloned_module.num_parameters > 1:\n",
    "            expansion = cloned_module.num_parameters // src_module.num_parameters\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.weight.data[i::expansion], src_module.weight.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_dropout_cloning(src_module: nn.Dropout, cloned_module: nn.Dropout):\n",
    "    assert cloned_module.p == src_module.p, \"Dropout probability must match\"\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_embedding_cloning(src_module: nn.Embedding, cloned_module: nn.Embedding):\n",
    "    num_expansion = cloned_module.num_embeddings // src_module.num_embeddings\n",
    "    dim_expansion = cloned_module.embedding_dim // src_module.embedding_dim\n",
    "    for j in range(num_expansion):\n",
    "        for i in range(dim_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::num_expansion, i::dim_expansion], src_module.weight.data)\n",
    "    print(\"Passed all tests\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_normalization_cloning(src_module: NormalizationLayer, cloned_module: NormalizationLayer):\n",
    "    assert isinstance(cloned_module, type(src_module)), \"Cloned module must be of the same type as source module\"\n",
    "    \n",
    "    if hasattr(src_module, 'weight') and src_module.weight is not None and hasattr(cloned_module, 'weight'):\n",
    "        expansion = cloned_module.weight.data.shape[0] // src_module.weight.data.shape[0] \n",
    "        for i in range(expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[i::expansion], src_module.weight.data)\n",
    "            \n",
    "            if hasattr(src_module, 'bias') and src_module.bias is not None and hasattr(cloned_module, 'bias'):\n",
    "                assert torch.allclose(cloned_module.bias.data[i::expansion], src_module.bias.data)\n",
    "    \n",
    "    # Check running stats for BatchNorm layers\n",
    "    if hasattr(src_module, 'running_mean') and src_module.running_mean is not None:\n",
    "        if hasattr(cloned_module, 'running_mean') and cloned_module.running_mean is not None:\n",
    "            expansion = cloned_module.running_mean.data.shape[0] // src_module.running_mean.data.shape[0]\n",
    "            for i in range(expansion):\n",
    "                assert torch.allclose(cloned_module.running_mean.data[i::expansion], src_module.running_mean.data)\n",
    "                assert torch.allclose(cloned_module.running_var.data[i::expansion], src_module.running_var.data)\n",
    "    \n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_linear_cloning(src_module: nn.Linear, cloned_module: nn.Linear):\n",
    "    in_expansion = cloned_module.in_features // src_module.in_features\n",
    "    out_expansion = cloned_module.out_features // src_module.out_features\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "    \n",
    "def validate_conv1d_cloning(src_module: nn.Conv1d, cloned_module: nn.Conv1d):\n",
    "    in_expansion = cloned_module.in_channels // src_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // src_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "    \n",
    "\n",
    "def validate_conv2d_cloning(src_module: nn.Conv2d, cloned_module: nn.Conv2d):\n",
    "    in_expansion = cloned_module.in_channels // src_module.in_channels\n",
    "    out_expansion = cloned_module.out_channels // src_module.out_channels\n",
    "    for j in range(out_expansion):\n",
    "        for i in range(in_expansion):\n",
    "            assert torch.allclose(cloned_module.weight.data[j::out_expansion, i::in_expansion, :, :], src_module.weight.data/in_expansion)\n",
    "            assert torch.allclose(cloned_module.bias.data[j::out_expansion], src_module.bias.data)\n",
    "    print(\"Passed all tests\")\n",
    "\n",
    "\n",
    "\n",
    "def clone_module(\n",
    "    src_module: nn.Module, \n",
    "    cloned_module: nn.Module,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Clone parameters from a source module to a cloned module.\n",
    "    \n",
    "    Args:\n",
    "        src_module: Source module with smaller dimensions\n",
    "        cloned_module: Target module with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if cloning was successful, False otherwise\n",
    "    \"\"\"\n",
    "    success = True\n",
    "    \n",
    "    # Define normalization and activation types inline for easier checking\n",
    "    norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)\n",
    "    activation_types = (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.SELU, nn.GELU, nn.SiLU, nn.ELU, nn.LeakyReLU, \n",
    "                       nn.PReLU, nn.Threshold, nn.Softmax, nn.LogSoftmax, nn.Softplus, nn.Softmin, \n",
    "                       nn.Hardsigmoid, nn.Hardswish, nn.Softshrink, nn.Hardshrink, nn.Softsign, \n",
    "                       nn.GLU, nn.CELU, nn.Identity)\n",
    "    \n",
    "    if isinstance(src_module, nn.Linear):\n",
    "        clone_linear(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Conv1d):\n",
    "        clone_conv1d(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Conv2d):\n",
    "        clone_conv2d(src_module, cloned_module)\n",
    "    elif isinstance(src_module, norm_types):\n",
    "        clone_normalization(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Embedding):\n",
    "        clone_embedding(src_module, cloned_module)\n",
    "    elif isinstance(src_module, activation_types):\n",
    "        clone_activation(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Dropout):\n",
    "        clone_dropout(src_module, cloned_module)\n",
    "    elif isinstance(src_module, nn.Flatten):\n",
    "        pass # Flatten is handled separately\n",
    "    elif is_parameter_free(src_module) and is_parameter_free(cloned_module):\n",
    "        clone_parameter_free(src_module, cloned_module)\n",
    "    else:\n",
    "        success = False\n",
    "        print(f\"Unsupported module type: {type(src_module)}\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "\n",
    "def clone_model(src_model: nn.Module, cloned_model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Clone parameters from a source model to a cloned model.\n",
    "    \n",
    "    Args:\n",
    "        src_model: Source model with smaller dimensions\n",
    "        cloned_model: Target model with larger dimensions\n",
    "        \n",
    "    Returns:\n",
    "        cloned_model: The target model with cloned parameters\n",
    "    \"\"\"\n",
    "    for name, module in list(cloned_model.named_modules()):\n",
    "        if isinstance(module, nn.Flatten):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            module_name = name.split('.')[-1]\n",
    "            \n",
    "            # Find parent module to modify\n",
    "            if parent_name:\n",
    "                parent = cloned_model.get_submodule(parent_name)\n",
    "            else:\n",
    "                parent = cloned_model\n",
    "                \n",
    "            # Create and replace with CloneAwareFlatten\n",
    "            setattr(parent, module_name, CloneAwareFlatten(\n",
    "                start_dim=module.start_dim,\n",
    "                end_dim=module.end_dim\n",
    "            ))\n",
    "            print(f\"Replaced Flatten with CloneAwareFlatten at {name}\")\n",
    "    # Process each module individually\n",
    "    for name, src_module in src_model.named_modules():\n",
    "        cloned_module = cloned_model.get_submodule(name)\n",
    "        print(f\"Cloning module {name}\")\n",
    "        clone_module(src_module, cloned_module)\n",
    "    \n",
    "    return cloned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0c7437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning module \n",
      "Unsupported module type: <class 'src.models.mlp.MLP'>\n",
      "Cloning module layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.linear_0\n",
      "Cloning Linear module: 10→10, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.act_0\n",
      "Cloning module layers.linear_1\n",
      "Cloning Linear module: 64→128, 32→64, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.act_1\n",
      "Cloning module layers.out\n",
      "Cloning Linear module: 32→64, 2→2, in expansion: 2, out expansion: 1\n",
      "Output from source model: tensor([[-0.1834,  0.2274],\n",
      "        [-0.2082,  0.2723],\n",
      "        [-0.1581,  0.1333],\n",
      "        [-0.2022,  0.2726],\n",
      "        [-0.2721,  0.2446],\n",
      "        [-0.3425,  0.3018],\n",
      "        [-0.1492,  0.2978],\n",
      "        [-0.2177,  0.3049],\n",
      "        [-0.2437,  0.1399],\n",
      "        [-0.1879,  0.1989],\n",
      "        [-0.2049,  0.2483],\n",
      "        [-0.1018,  0.1314],\n",
      "        [-0.0530,  0.1803],\n",
      "        [-0.1930,  0.3193],\n",
      "        [-0.0641,  0.1352],\n",
      "        [-0.1829,  0.2519],\n",
      "        [-0.1513,  0.2725],\n",
      "        [-0.2082,  0.2067],\n",
      "        [-0.2071,  0.1487],\n",
      "        [-0.2810,  0.2518],\n",
      "        [-0.2781,  0.1914],\n",
      "        [-0.1721,  0.0569],\n",
      "        [-0.1047,  0.2185],\n",
      "        [-0.1339,  0.1246],\n",
      "        [-0.1891,  0.1128],\n",
      "        [-0.1867,  0.1282],\n",
      "        [-0.0443,  0.1397],\n",
      "        [-0.2399,  0.1036],\n",
      "        [-0.1359,  0.2024],\n",
      "        [-0.1446,  0.2379],\n",
      "        [-0.2257,  0.2438],\n",
      "        [-0.2091,  0.2201]], grad_fn=<AddmmBackward0>)\n",
      "Output from cloned model: tensor([[-0.1834,  0.2274],\n",
      "        [-0.2082,  0.2723],\n",
      "        [-0.1581,  0.1333],\n",
      "        [-0.2022,  0.2726],\n",
      "        [-0.2721,  0.2446],\n",
      "        [-0.3425,  0.3018],\n",
      "        [-0.1492,  0.2978],\n",
      "        [-0.2177,  0.3049],\n",
      "        [-0.2437,  0.1399],\n",
      "        [-0.1879,  0.1989],\n",
      "        [-0.2049,  0.2483],\n",
      "        [-0.1018,  0.1314],\n",
      "        [-0.0530,  0.1803],\n",
      "        [-0.1930,  0.3193],\n",
      "        [-0.0641,  0.1352],\n",
      "        [-0.1829,  0.2519],\n",
      "        [-0.1513,  0.2725],\n",
      "        [-0.2082,  0.2067],\n",
      "        [-0.2071,  0.1487],\n",
      "        [-0.2810,  0.2518],\n",
      "        [-0.2781,  0.1914],\n",
      "        [-0.1721,  0.0569],\n",
      "        [-0.1047,  0.2185],\n",
      "        [-0.1339,  0.1246],\n",
      "        [-0.1891,  0.1128],\n",
      "        [-0.1867,  0.1282],\n",
      "        [-0.0443,  0.1397],\n",
      "        [-0.2399,  0.1036],\n",
      "        [-0.1359,  0.2024],\n",
      "        [-0.1446,  0.2379],\n",
      "        [-0.2257,  0.2438],\n",
      "        [-0.2091,  0.2201]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4901e-08, -5.9605e-08],\n",
       "        [ 1.4901e-08, -2.9802e-08],\n",
       "        [ 1.4901e-08, -1.4901e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00, -2.9802e-08],\n",
       "        [ 0.0000e+00, -5.9605e-08],\n",
       "        [-1.4901e-08,  5.9605e-08],\n",
       "        [ 2.9802e-08,  0.0000e+00],\n",
       "        [ 5.9605e-08, -1.4901e-08],\n",
       "        [ 2.9802e-08,  0.0000e+00],\n",
       "        [-1.4901e-08,  0.0000e+00],\n",
       "        [ 7.4506e-09, -2.9802e-08],\n",
       "        [ 2.2352e-08,  0.0000e+00],\n",
       "        [ 4.4703e-08,  0.0000e+00],\n",
       "        [-1.4901e-08,  4.4703e-08],\n",
       "        [ 0.0000e+00,  2.9802e-08],\n",
       "        [ 0.0000e+00, -2.9802e-08],\n",
       "        [-2.9802e-08,  5.9605e-08],\n",
       "        [-2.9802e-08, -2.9802e-08],\n",
       "        [ 2.9802e-08, -5.9605e-08],\n",
       "        [-2.9802e-08, -5.9605e-08],\n",
       "        [ 2.9802e-08, -7.4506e-09],\n",
       "        [ 0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  7.4506e-09],\n",
       "        [ 0.0000e+00,  2.2352e-08],\n",
       "        [-4.4703e-08, -2.9802e-08],\n",
       "        [-1.4901e-08, -2.9802e-08],\n",
       "        [ 0.0000e+00, -1.4901e-08],\n",
       "        [-2.9802e-08,  0.0000e+00],\n",
       "        [-1.4901e-08, -2.9802e-08],\n",
       "        [-2.9802e-08,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "\n",
    "src_model = MLP(input_size=10, output_size=2, hidden_sizes=[64, 32,], activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = MLP(input_size=10, output_size=2, hidden_sizes=[64*2, 32*2,], activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = clone_model(src_model, cloned_model)\n",
    "\n",
    "x = torch.randn(32, 10)\n",
    "y1 = src_model(x)\n",
    "y2 = cloned_model(x)\n",
    "print(\"Output from source model:\", y1)\n",
    "print(\"Output from cloned model:\", y2)\n",
    "\n",
    "y1-y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d02581b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = src_model.layers.linear_0.weight.data, cloned_model.layers.linear_0.weight.data\n",
    "b1, b2 = src_model.layers.linear_0.bias.data, cloned_model.layers.linear_0.bias.data\n",
    "w1.shape, w2.shape\n",
    "\n",
    "torch.allclose(w2[::2,:], w1),torch.allclose(b2[::2], b1)\n",
    "\n",
    "w1, w2 = src_model.layers.linear_1.weight.data, cloned_model.layers.linear_1.weight.data\n",
    "b1, b2 = src_model.layers.linear_1.bias.data, cloned_model.layers.linear_1.bias.data\n",
    "\n",
    "torch.allclose(w2[::2,::2], w1/2),torch.allclose(w2[1::2,::2], w1/2),torch.allclose(w2[::2,1::2], w1/2),torch.allclose(w2[1::2,1::2], w1/2),torch.allclose(b2[::2], b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c651a1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All activations are close \n"
     ]
    }
   ],
   "source": [
    "h1, h2 = src_model.layers.linear_0(x), cloned_model.layers.linear_0(x)\n",
    "h1, h2 = src_model.layers.act_0(h1), cloned_model.layers.act_0(h2)\n",
    "h1, h2 = src_model.layers.linear_1(h1), cloned_model.layers.linear_1(h2)\n",
    "h1, h2 = src_model.layers.act_1(h1), cloned_model.layers.act_1(h2)\n",
    "h1, h2 = src_model.layers.out(h1), cloned_model.layers.out(h2)\n",
    "h1.shape, h2.shape\n",
    "if h1.shape[1] != h2.shape[1]:\n",
    "    assert h2.shape[1] % h1.shape[1] == 0, \"Output dimensions are not integer multiples\"\n",
    "    expansion_factor = h2.shape[1] // h1.shape[1]\n",
    "    for i in range(expansion_factor):\n",
    "        assert torch.allclose(h2[:,i::expansion_factor], h1, atol=1e-5), f\"Output mismatch at expansion factor {i}\"\n",
    "print(\"All activations are close \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd87d3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = src_model.layers.out.weight.data, cloned_model.layers.out.weight.data\n",
    "b1, b2 = src_model.layers.out.bias.data, cloned_model.layers.out.bias.data\n",
    "\n",
    "w1.shape, w2.shape, b1.shape, b2.shape\n",
    "torch.allclose(w2[:,::2],w1/2),torch.allclose(w2[:,1::2],w1/2), torch.allclose(b2, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70a581ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced Flatten with CloneAwareFlatten at layers.flatten\n",
      "Cloning module \n",
      "Unsupported module type: <class 'src.models.cnn.CNN'>\n",
      "Cloning module layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.conv_0\n",
      "Cloning Conv2d module: 3→3, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.norm_0\n",
      "Cloning module layers.act_0\n",
      "Cloning module layers.pool_0\n",
      "Cloning module layers.conv_1\n",
      "Cloning Conv2d module: 64→128, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.norm_1\n",
      "Cloning module layers.act_1\n",
      "Cloning module layers.pool_1\n",
      "Cloning module layers.conv_2\n",
      "Cloning Conv2d module: 128→256, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.norm_2\n",
      "Cloning module layers.act_2\n",
      "Cloning module layers.pool_2\n",
      "Cloning module layers.flatten\n",
      "Cloning module layers.fc_0\n",
      "Cloning Linear module: 4096→8192, 512→512, in expansion: 2, out expansion: 1\n",
      "Cloning module layers.fc_act_0\n",
      "Cloning module layers.fc_out\n",
      "Cloning Linear module: 512→512, 2→2, in expansion: 1, out expansion: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([32, 2]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models import MLP, CNN, ResNet, VisionTransformer\n",
    "\n",
    "src_model = CNN(in_channels=3, num_classes=2, conv_channels=[64, 128, 256], activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = CNN(in_channels=3, num_classes=2, conv_channels=[64*2, 128*2, 256*2], activation=\"relu\", dropout_p=0.0)\n",
    "\n",
    "cloned_model = clone_model(src_model, cloned_model)\n",
    "\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "y1 = src_model(x)\n",
    "y2 = cloned_model(x)\n",
    "\n",
    "torch.allclose(y1, y2,atol=1e-3), y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "425e0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same shape: \n",
      "torch.Size([32, 2]) torch.Size([32, 2]) True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w1 = src_model.layers.conv_0.weight.data\n",
    "w2 = cloned_model.layers.conv_0.weight.data\n",
    "b1 = src_model.layers.conv_0.bias.data\n",
    "b2 = cloned_model.layers.conv_0.bias.data\n",
    "\n",
    "w1.shape, w2.shape, b1.shape, b2.shape\n",
    "# w2[::2,:,:,:]==w1\n",
    "torch.allclose(w2[::2,:,:,:], w1),torch.allclose(w2[1::2,:,:,:], w1),torch.allclose(b2[::2], b1)\n",
    "\n",
    "h1, h2 = src_model.layers.conv_0(x), cloned_model.layers.conv_0(x)\n",
    "h1, h2 = src_model.layers.norm_0(h1), cloned_model.layers.norm_0(h2)\n",
    "h1, h2 = src_model.layers.act_0(h1), cloned_model.layers.act_0(h2)\n",
    "h1, h2 = src_model.layers.pool_0(h1), cloned_model.layers.pool_0(h2)\n",
    "\n",
    "\n",
    "h1, h2 = src_model.layers.conv_1(h1), cloned_model.layers.conv_1(h2)\n",
    "h1, h2 = src_model.layers.norm_1(h1), cloned_model.layers.norm_1(h2)\n",
    "h1, h2 = src_model.layers.act_1(h1), cloned_model.layers.act_1(h2)\n",
    "h1, h2 = src_model.layers.pool_1(h1), cloned_model.layers.pool_1(h2)\n",
    "\n",
    "# go to layer 2 \n",
    "h1, h2 = src_model.layers.conv_2(h1), cloned_model.layers.conv_2(h2)\n",
    "h1, h2 = src_model.layers.norm_2(h1), cloned_model.layers.norm_2(h2)\n",
    "h1, h2 = src_model.layers.act_2(h1), cloned_model.layers.act_2(h2)\n",
    "h1, h2 = src_model.layers.pool_2(h1), cloned_model.layers.pool_2(h2)\n",
    "\n",
    "\n",
    "h1, h2 = src_model.layers.flatten(h1), cloned_model.layers.flatten(h2)\n",
    "\n",
    "h1, h2 = src_model.layers.fc_0(h1), cloned_model.layers.fc_0(h2)\n",
    "h1, h2 = src_model.layers.fc_act_0(h1), cloned_model.layers.fc_act_0(h2) \n",
    "h1, h2 = src_model.layers.fc_out(h1), cloned_model.layers.fc_out(h2)\n",
    "\n",
    "#, torch.allclose(h2[:,1::2],h1, atol=1e-3)\n",
    "if h1.shape[1] != h2.shape[1]:\n",
    "    print(\"Different shape: \")\n",
    "    print(h1.shape, h2.shape, torch.allclose(h2[:,::2],h1, atol=1e-3), torch.allclose(h2[:,1::2],h1, atol=1e-3))\n",
    "else:\n",
    "    print(\"Same shape: \")\n",
    "    print(h1.shape, h2.shape, torch.allclose(h2,h1, atol=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6373e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced Flatten with CloneAwareFlatten at layers.flatten\n",
      "Cloning module \n",
      "Unsupported module type: <class 'src.models.resnet.ResNet'>\n",
      "Cloning module layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.conv1\n",
      "Cloning Conv2d module: 3→3, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.bn1\n",
      "Cloning module layers.activation\n",
      "Cloning module layers.layer1_block0\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer1_block0.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer1_block0.layers.conv1\n",
      "Cloning Conv2d module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer1_block0.layers.bn1\n",
      "Cloning module layers.layer1_block0.layers.activation\n",
      "Cloning module layers.layer1_block0.layers.conv2\n",
      "Cloning Conv2d module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer1_block0.layers.bn2\n",
      "Cloning module layers.layer1_block1\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer1_block1.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer1_block1.layers.conv1\n",
      "Cloning Conv2d module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer1_block1.layers.bn1\n",
      "Cloning module layers.layer1_block1.layers.activation\n",
      "Cloning module layers.layer1_block1.layers.conv2\n",
      "Cloning Conv2d module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer1_block1.layers.bn2\n",
      "Cloning module layers.layer2_block0\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer2_block0.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer2_block0.layers.conv1\n",
      "Cloning Conv2d module: 64→128, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer2_block0.layers.bn1\n",
      "Cloning module layers.layer2_block0.layers.activation\n",
      "Cloning module layers.layer2_block0.layers.conv2\n",
      "Cloning Conv2d module: 128→256, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer2_block0.layers.bn2\n",
      "Cloning module layers.layer2_block0.layers.downsample\n",
      "Unsupported module type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Cloning module layers.layer2_block0.layers.downsample.0\n",
      "Cloning Conv2d module: 64→128, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer2_block0.layers.downsample.1\n",
      "Cloning module layers.layer2_block1\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer2_block1.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer2_block1.layers.conv1\n",
      "Cloning Conv2d module: 128→256, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer2_block1.layers.bn1\n",
      "Cloning module layers.layer2_block1.layers.activation\n",
      "Cloning module layers.layer2_block1.layers.conv2\n",
      "Cloning Conv2d module: 128→256, 128→256, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer2_block1.layers.bn2\n",
      "Cloning module layers.layer3_block0\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer3_block0.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer3_block0.layers.conv1\n",
      "Cloning Conv2d module: 128→256, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer3_block0.layers.bn1\n",
      "Cloning module layers.layer3_block0.layers.activation\n",
      "Cloning module layers.layer3_block0.layers.conv2\n",
      "Cloning Conv2d module: 256→512, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer3_block0.layers.bn2\n",
      "Cloning module layers.layer3_block0.layers.downsample\n",
      "Unsupported module type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Cloning module layers.layer3_block0.layers.downsample.0\n",
      "Cloning Conv2d module: 128→256, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer3_block0.layers.downsample.1\n",
      "Cloning module layers.layer3_block1\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer3_block1.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer3_block1.layers.conv1\n",
      "Cloning Conv2d module: 256→512, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer3_block1.layers.bn1\n",
      "Cloning module layers.layer3_block1.layers.activation\n",
      "Cloning module layers.layer3_block1.layers.conv2\n",
      "Cloning Conv2d module: 256→512, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer3_block1.layers.bn2\n",
      "Cloning module layers.layer4_block0\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer4_block0.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer4_block0.layers.conv1\n",
      "Cloning Conv2d module: 256→512, 512→1024, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer4_block0.layers.bn1\n",
      "Cloning module layers.layer4_block0.layers.activation\n",
      "Cloning module layers.layer4_block0.layers.conv2\n",
      "Cloning Conv2d module: 512→1024, 512→1024, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer4_block0.layers.bn2\n",
      "Cloning module layers.layer4_block0.layers.downsample\n",
      "Unsupported module type: <class 'torch.nn.modules.container.Sequential'>\n",
      "Cloning module layers.layer4_block0.layers.downsample.0\n",
      "Cloning Conv2d module: 256→512, 512→1024, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer4_block0.layers.downsample.1\n",
      "Cloning module layers.layer4_block1\n",
      "Unsupported module type: <class 'src.models.resnet.BasicBlock'>\n",
      "Cloning module layers.layer4_block1.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.layer4_block1.layers.conv1\n",
      "Cloning Conv2d module: 512→1024, 512→1024, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer4_block1.layers.bn1\n",
      "Cloning module layers.layer4_block1.layers.activation\n",
      "Cloning module layers.layer4_block1.layers.conv2\n",
      "Cloning Conv2d module: 512→1024, 512→1024, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.layer4_block1.layers.bn2\n",
      "Cloning module layers.avgpool\n",
      "Cloning module layers.flatten\n",
      "Cloning module layers.fc\n",
      "Cloning Linear module: 512→1024, 2→2, in expansion: 2, out expansion: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([32, 2]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model = ResNet(in_channels=3, num_classes=2, base_channels=64, activation=\"relu\", dropout_p=0.0)\n",
    "cloned_model = ResNet(in_channels=3, num_classes=2, base_channels=64*2, activation=\"relu\", dropout_p=0.0)\n",
    "\n",
    "cloned_model = clone_model(src_model, cloned_model)\n",
    "\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "y1 = src_model(x)\n",
    "y2 = cloned_model(x)\n",
    "\n",
    "torch.allclose(y1, y2,atol=1e-3), y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a435df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning module \n",
      "Unsupported module type: <class 'src.models.vit.VisionTransformer'>\n",
      "Cloning module layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.patch_embed\n",
      "Unsupported module type: <class 'src.models.vit.PatchEmbedding'>\n",
      "Cloning module layers.patch_embed.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.patch_embed.layers.proj\n",
      "Cloning Conv2d module: 3→3, 64→128, in expansion: 1, out expansion: 2\n",
      "Cloning module layers.pos_drop\n",
      "Cloning module layers.block_0\n",
      "Unsupported module type: <class 'src.models.vit.TransformerBlock'>\n",
      "Cloning module layers.block_0.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_0.layers.norm1\n",
      "Cloning module layers.block_0.layers.attn\n",
      "Unsupported module type: <class 'src.models.vit.Attention'>\n",
      "Cloning module layers.block_0.layers.attn.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_0.layers.attn.layers.qkv\n",
      "Cloning Linear module: 64→128, 192→384, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_0.layers.attn.layers.attn_drop\n",
      "Cloning module layers.block_0.layers.attn.layers.proj\n",
      "Cloning Linear module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_0.layers.attn.layers.proj_drop\n",
      "Cloning module layers.block_0.layers.norm2\n",
      "Cloning module layers.block_0.layers.mlp\n",
      "Unsupported module type: <class 'src.models.vit.TransformerMLP'>\n",
      "Cloning module layers.block_0.layers.mlp.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_0.layers.mlp.layers.fc1\n",
      "Cloning Linear module: 64→128, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_0.layers.mlp.layers.act\n",
      "Cloning module layers.block_0.layers.mlp.layers.drop1\n",
      "Cloning module layers.block_0.layers.mlp.layers.fc2\n",
      "Cloning Linear module: 256→512, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_0.layers.mlp.layers.drop2\n",
      "Cloning module layers.block_1\n",
      "Unsupported module type: <class 'src.models.vit.TransformerBlock'>\n",
      "Cloning module layers.block_1.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_1.layers.norm1\n",
      "Cloning module layers.block_1.layers.attn\n",
      "Unsupported module type: <class 'src.models.vit.Attention'>\n",
      "Cloning module layers.block_1.layers.attn.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_1.layers.attn.layers.qkv\n",
      "Cloning Linear module: 64→128, 192→384, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_1.layers.attn.layers.attn_drop\n",
      "Cloning module layers.block_1.layers.attn.layers.proj\n",
      "Cloning Linear module: 64→128, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_1.layers.attn.layers.proj_drop\n",
      "Cloning module layers.block_1.layers.norm2\n",
      "Cloning module layers.block_1.layers.mlp\n",
      "Unsupported module type: <class 'src.models.vit.TransformerMLP'>\n",
      "Cloning module layers.block_1.layers.mlp.layers\n",
      "Unsupported module type: <class 'torch.nn.modules.container.ModuleDict'>\n",
      "Cloning module layers.block_1.layers.mlp.layers.fc1\n",
      "Cloning Linear module: 64→128, 256→512, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_1.layers.mlp.layers.act\n",
      "Cloning module layers.block_1.layers.mlp.layers.drop1\n",
      "Cloning module layers.block_1.layers.mlp.layers.fc2\n",
      "Cloning Linear module: 256→512, 64→128, in expansion: 2, out expansion: 2\n",
      "Cloning module layers.block_1.layers.mlp.layers.drop2\n",
      "Cloning module layers.norm\n",
      "Cloning module layers.head\n",
      "Cloning Linear module: 64→128, 2→2, in expansion: 2, out expansion: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, torch.Size([32, 2]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model = VisionTransformer(\n",
    "    in_channels=3, \n",
    "    num_classes=2, \n",
    "    embed_dim=64, \n",
    "    depth=2, \n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    activation=\"relu\",)\n",
    "\n",
    "cloned_model = VisionTransformer(\n",
    "    in_channels=3, \n",
    "    num_classes=2, \n",
    "    patch_size=4, \n",
    "    embed_dim=64*2, \n",
    "    depth=2, \n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    activation=\"relu\",)\n",
    "\n",
    "cloned_model = clone_model(src_model, cloned_model)\n",
    "\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "y1 = src_model(x)\n",
    "y2 = cloned_model(x)\n",
    "torch.allclose(y1, y2,atol=1e-3), y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e7daf34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patch_embed', 'pos_drop', 'block_0', 'block_1', 'norm', 'head'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model.layers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f1cc0b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2]), torch.Size([32, 2]), True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the ViT forward function: \n",
    "    # def forward(self, x):\n",
    "    #     x = self.layers['patch_embed'](x)\n",
    "        \n",
    "    #     B = x.shape[0]\n",
    "    #     cls_token = self.cls_token.expand(B, -1, -1)\n",
    "    #     x = torch.cat((cls_token, x), dim=1)\n",
    "        \n",
    "    #     x = x + self.pos_embed\n",
    "    #     x = self.layers['pos_drop'](x)\n",
    "\n",
    "    #     for i in range(self.depth):\n",
    "    #         x = self.layers[f'block_{i}'](x)\n",
    "        \n",
    "    #     x = self.layers['norm'](x)\n",
    "    #     x = x[:, 0]  # Use CLS token for classification\n",
    "    #     x = self.layers['head'](x)\n",
    "            \n",
    "    #     return x\n",
    "cloned_model.cls_token.data[:,:,::2] = src_model.cls_token.data\n",
    "cloned_model.cls_token.data[:,:,1::2] = src_model.cls_token.data\n",
    "cloned_model.pos_embed.data[:,:,::2] = src_model.pos_embed.data\n",
    "cloned_model.pos_embed.data[:,:,1::2] = src_model.pos_embed.data\n",
    "x = torch.randn(32, 3, 32, 32)\n",
    "h1, h2 = src_model.layers.patch_embed(x), cloned_model.layers.patch_embed(x)\n",
    "cls_token1, cls_token2 = src_model.cls_token.expand(h1.shape[0], -1, -1), cloned_model.cls_token.expand(h2.shape[0], -1, -1)\n",
    "h1, h2 = torch.cat((cls_token1, h1), dim=1), torch.cat((cls_token2, h2), dim=1)\n",
    "h1, h2 = h1 + src_model.pos_embed, h2 + cloned_model.pos_embed\n",
    "h1, h2 = src_model.layers.pos_drop(h1), cloned_model.layers.pos_drop(h2)\n",
    "h1, h2 = src_model.layers.block_0(h1), cloned_model.layers.block_0(h2)\n",
    "h1, h2 = src_model.layers.block_1(h1), cloned_model.layers.block_1(h2)\n",
    "h1, h2 = src_model.layers.norm(h1), cloned_model.layers.norm(h2)\n",
    "\n",
    "h1.shape, h2.shape, torch.allclose(h2[:,:,::2], h1, atol=1e-2), torch.allclose(h2[:,:,1::2], h1, atol=1e-2)\n",
    "\n",
    "h1, h2 = h1[:, 0], h2[:, 0]\n",
    "\n",
    "h1.shape, h2.shape, torch.allclose(h2[:,::2], h1, atol=1e-2), torch.allclose(h2[:,1::2], h1, atol=1e-2)\n",
    "\n",
    "h1, h2 = src_model.layers.head(h1), cloned_model.layers.head(h2)\n",
    "\n",
    "h1.shape, h2.shape, torch.allclose(h1, h2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fb6064a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.7024e-04,  1.7179e-04],\n",
       "        [-5.5595e-04,  9.0450e-05],\n",
       "        [-3.0534e-04, -3.2416e-04],\n",
       "        [-3.9641e-04, -3.0972e-05],\n",
       "        [-5.5676e-04, -2.8667e-04],\n",
       "        [-2.2934e-04, -2.0367e-04],\n",
       "        [-2.9618e-04,  2.3894e-05],\n",
       "        [-4.8099e-04,  3.1775e-04],\n",
       "        [-3.3120e-04, -4.3871e-04],\n",
       "        [-4.3447e-04, -3.1921e-04],\n",
       "        [-4.9408e-04,  6.2302e-05],\n",
       "        [-3.1758e-04,  2.8926e-04],\n",
       "        [-6.9366e-04, -2.2227e-04],\n",
       "        [-3.5624e-04, -1.4901e-04],\n",
       "        [-3.2962e-04, -4.1284e-05],\n",
       "        [-2.6935e-04, -8.6188e-05],\n",
       "        [-3.1736e-04, -1.9766e-05],\n",
       "        [-3.6038e-04,  6.0596e-05],\n",
       "        [-4.3157e-04, -1.6320e-04],\n",
       "        [-5.0400e-04, -1.3572e-04],\n",
       "        [-7.1657e-04, -3.5712e-04],\n",
       "        [-2.0284e-04, -9.4675e-05],\n",
       "        [-6.5736e-04,  8.7515e-05],\n",
       "        [-3.7079e-04, -5.9351e-05],\n",
       "        [-4.4346e-04, -9.6247e-05],\n",
       "        [-2.1453e-04,  1.4697e-04],\n",
       "        [-4.2505e-04, -1.2357e-04],\n",
       "        [-5.6989e-04,  3.0449e-04],\n",
       "        [-4.7022e-04,  6.3360e-05],\n",
       "        [-3.9900e-04, -1.8962e-04],\n",
       "        [-6.3613e-04, -8.1211e-06],\n",
       "        [-4.4063e-04, -2.0823e-04]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 - h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c153e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
