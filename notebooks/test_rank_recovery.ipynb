{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting normalization experiments...\n",
      "Generating dataset...\n",
      "\n",
      "==================================================\n",
      "Testing No Normalization\n",
      "==================================================\n",
      "Epoch 1/3, Train Loss: 2.3030, Val Loss: 2.3031\n",
      "Epoch 2/3, Train Loss: 2.3029, Val Loss: 2.3027\n",
      "Epoch 3/3, Train Loss: 2.3027, Val Loss: 2.3030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 79/79 [00:00<00:00, 1435.88it/s]\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2027: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2027: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BN without affine\n",
      "==================================================\n",
      "Epoch 1/3, Train Loss: 2.3189, Val Loss: 2.3133\n",
      "Epoch 2/3, Train Loss: 2.3074, Val Loss: 2.3104\n",
      "Epoch 3/3, Train Loss: 2.3048, Val Loss: 2.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 79/79 [00:00<00:00, 1224.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BN with affine\n",
      "==================================================\n",
      "Epoch 1/3, Train Loss: 2.3158, Val Loss: 2.3099\n",
      "Epoch 2/3, Train Loss: 2.3060, Val Loss: 2.3088\n",
      "Epoch 3/3, Train Loss: 2.3046, Val Loss: 2.3101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 79/79 [00:00<00:00, 1206.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing LN without affine\n",
      "==================================================\n",
      "Epoch 1/3, Train Loss: 2.3098, Val Loss: 2.3059\n",
      "Epoch 2/3, Train Loss: 2.3051, Val Loss: 2.3055\n",
      "Epoch 3/3, Train Loss: 2.3046, Val Loss: 2.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 79/79 [00:00<00:00, 1313.37it/s]\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2027: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing LN with affine\n",
      "==================================================\n",
      "Epoch 1/3, Train Loss: 2.3090, Val Loss: 2.3101\n",
      "Epoch 2/3, Train Loss: 2.3053, Val Loss: 2.3086\n",
      "Epoch 3/3, Train Loss: 2.3040, Val Loss: 2.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 79/79 [00:00<00:00, 1309.97it/s]\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_distn_infrastructure.py:2027: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/local/home/ajoudaki/miniconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:573: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average results across all layers:\n",
      "         config_name  pre_gaussianity  post_gaussianity  pre_rank  post_rank  \\\n",
      "0     BN with affine         0.192188          0.000000  2.834760   5.782184   \n",
      "1  BN without affine         0.203125          0.000000  3.464566   7.291030   \n",
      "2     LN with affine         0.190625          0.051562  8.209245  14.001234   \n",
      "3  LN without affine         0.193750          0.042188  7.983810  13.389853   \n",
      "4   No Normalization         0.198437          0.262500  2.777870   3.343492   \n",
      "\n",
      "   rank_improvement  \n",
      "0          2.947424  \n",
      "1          3.826464  \n",
      "2          5.791989  \n",
      "3          5.406044  \n",
      "4          0.565621  \n",
      "Experiment completed! Results and visualizations are saved in the f'{save_path}' directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron with configurable normalization layers\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=3072,  # 32x32x3 for CIFAR-10 flattened images\n",
    "                 hidden_dims=[256, 128, 64], \n",
    "                 output_dim=10,\n",
    "                 activation=nn.ReLU,\n",
    "                 norm_type=None,  # 'bn', 'ln', None\n",
    "                 affine=True,     # Whether to use learnable affine parameters in normalization\n",
    "                 track_pre_activations=False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.norm_type = norm_type\n",
    "        self.affine = affine\n",
    "        self.track_pre_activations = track_pre_activations\n",
    "        \n",
    "        # Create network\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            \n",
    "            # Normalization layer\n",
    "            if norm_type == 'bn':\n",
    "                layers.append(nn.BatchNorm1d(dim, affine=affine))\n",
    "            elif norm_type == 'ln':\n",
    "                layers.append(nn.LayerNorm(dim, elementwise_affine=affine))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(activation())\n",
    "            \n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.pre_activations = []  # For storing pre-activation values\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.pre_activations = []  # Reset for each forward pass\n",
    "        \n",
    "        # Store each layer's output for rank analysis\n",
    "        activations = []\n",
    "        h = x\n",
    "        \n",
    "        layer_index = 0\n",
    "        while layer_index < len(self.layers):\n",
    "            # Apply linear layer\n",
    "            linear_layer = self.layers[layer_index]\n",
    "            h = linear_layer(h)\n",
    "            \n",
    "            # Store pre-activation (after linear, before norm and activation)\n",
    "            if self.track_pre_activations:\n",
    "                self.pre_activations.append(h.detach().clone())\n",
    "            \n",
    "            # Move to next layer (could be norm or activation)\n",
    "            layer_index += 1\n",
    "            \n",
    "            # Apply normalization if present\n",
    "            if layer_index < len(self.layers) and (\n",
    "                isinstance(self.layers[layer_index], nn.BatchNorm1d) or \n",
    "                isinstance(self.layers[layer_index], nn.LayerNorm)):\n",
    "                norm_layer = self.layers[layer_index]\n",
    "                h = norm_layer(h)\n",
    "                layer_index += 1\n",
    "            \n",
    "            # Apply activation if present\n",
    "            if layer_index < len(self.layers) and (\n",
    "                isinstance(self.layers[layer_index], nn.ReLU) or\n",
    "                isinstance(self.layers[layer_index], nn.Tanh) or\n",
    "                isinstance(self.layers[layer_index], nn.Sigmoid)):\n",
    "                activation_layer = self.layers[layer_index]\n",
    "                h = activation_layer(h)\n",
    "                # Store post-activation\n",
    "                activations.append(h.detach().clone())\n",
    "                layer_index += 1\n",
    "        \n",
    "        return h, activations\n",
    "\n",
    "def load_cifar10_subset(n_train=50000, n_val=10000):\n",
    "    \"\"\"\n",
    "    Load a subset of the CIFAR-10 dataset with standardization\n",
    "    as recommended for CIFAR-10\n",
    "    \"\"\"\n",
    "    # Mean and std values for CIFAR-10 standardization\n",
    "    cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "    cifar_std = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar_mean, cifar_std)\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR-10 dataset\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False,\n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Take subsets if specified\n",
    "    if n_train < len(train_dataset):\n",
    "        train_indices = np.random.choice(len(train_dataset), n_train, replace=False)\n",
    "        train_dataset = Subset(train_dataset, train_indices)\n",
    "    \n",
    "    if n_val < len(test_dataset):\n",
    "        val_indices = np.random.choice(len(test_dataset), n_val, replace=False)\n",
    "        test_dataset = Subset(test_dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def compute_effective_rank(matrix):\n",
    "    \"\"\"\n",
    "    Compute the effective rank of a matrix using singular values\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix = matrix.cpu().numpy()\n",
    "    \n",
    "    # Compute SVD\n",
    "    s = np.linalg.svd(matrix, compute_uv=False)\n",
    "    \n",
    "    # Normalize singular values\n",
    "    s_norm = s / np.sum(s)\n",
    "    \n",
    "    # Remove zeros to avoid log(0)\n",
    "    s_norm = s_norm[s_norm > 1e-10]\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = -np.sum(s_norm * np.log(s_norm))\n",
    "    \n",
    "    # Effective rank\n",
    "    return np.exp(entropy)\n",
    "\n",
    "def test_gaussianity(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test if each feature follows a Gaussian distribution\n",
    "    Returns the percentage of features that pass the test\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    gaussian_count = 0\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature = data[:, i]\n",
    "        # Use Shapiro-Wilk test for normality\n",
    "        # Taking a random sample of 5000 points max (Shapiro-Wilk works best with smaller samples)\n",
    "        if len(feature) > 5000:\n",
    "            feature = np.random.choice(feature, size=5000, replace=False)\n",
    "        _, p_value = stats.shapiro(feature)\n",
    "        \n",
    "        if p_value > alpha:\n",
    "            gaussian_count += 1\n",
    "    \n",
    "    return gaussian_count / n_features\n",
    "\n",
    "def test_distribution_properties(data, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test distribution properties of each feature\n",
    "    \n",
    "    Returns:\n",
    "    - gaussianity: percentage of features following a Gaussian distribution\n",
    "    - standard_gaussianity: percentage of features following a standard Gaussian (mean≈0, std≈1)\n",
    "    - mean_deviation: average absolute deviation from zero mean\n",
    "    - std_deviation: average absolute deviation from unit std\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    gaussian_count = 0\n",
    "    standard_gaussian_count = 0\n",
    "    mean_devs = []\n",
    "    std_devs = []\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature = data[:, i]\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        mean = np.mean(feature)\n",
    "        std = np.std(feature)\n",
    "        mean_devs.append(abs(mean))\n",
    "        std_devs.append(abs(std - 1.0))\n",
    "        \n",
    "        # Shapiro-Wilk test for normality\n",
    "        if len(feature) > 5000:\n",
    "            sampled_feature = np.random.choice(feature, size=5000, replace=False)\n",
    "            _, p_value = stats.shapiro(sampled_feature)\n",
    "        else:\n",
    "            _, p_value = stats.shapiro(feature)\n",
    "        \n",
    "        # Check if Gaussian\n",
    "        if p_value > alpha:\n",
    "            gaussian_count += 1\n",
    "            \n",
    "            # Check if standard Gaussian (mean close to 0, std close to 1)\n",
    "            if abs(mean) < 0.1 and abs(std - 1.0) < 0.1:\n",
    "                standard_gaussian_count += 1\n",
    "    \n",
    "    results = {\n",
    "        'gaussianity': gaussian_count / n_features,\n",
    "        'standard_gaussianity': standard_gaussian_count / n_features,\n",
    "        'mean_deviation': np.mean(mean_devs),\n",
    "        'std_deviation': np.mean(std_devs)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_feature_distributions(data, layer_name, norm_type, n_features=5):\n",
    "    \"\"\"\n",
    "    Plot histograms of a few random features to visually inspect their distributions\n",
    "    \"\"\"\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "    \n",
    "    # Select random features\n",
    "    feature_indices = np.random.choice(data.shape[1], size=min(n_features, data.shape[1]), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(feature_indices), figsize=(15, 3))\n",
    "    if len(feature_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(feature_indices):\n",
    "        feature = data[:, idx]\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot histogram\n",
    "        ax.hist(feature, bins=30, alpha=0.7, density=True)\n",
    "        \n",
    "        # Plot normal distribution for comparison\n",
    "        mu, std = np.mean(feature), np.std(feature)\n",
    "        x = np.linspace(mu - 3*std, mu + 3*std, 100)\n",
    "        ax.plot(x, stats.norm.pdf(x, mu, std), 'r-', alpha=0.7)\n",
    "        \n",
    "        # Shapiro-Wilk test\n",
    "        if len(feature) > 5000:\n",
    "            sampled_feature = np.random.choice(feature, size=5000, replace=False)\n",
    "            _, p_value = stats.shapiro(sampled_feature)\n",
    "        else:\n",
    "            _, p_value = stats.shapiro(feature)\n",
    "        \n",
    "        ax.set_title(f\"Feature {idx}\\np-value: {p_value:.3f}\")\n",
    "        \n",
    "    plt.suptitle(f\"{layer_name} - {norm_type}\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    \"\"\"\n",
    "    Train the model and return training metrics\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Flatten the inputs for MLP\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Flatten the inputs for MLP\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                \n",
    "                outputs, _ = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return {'train_losses': train_losses, 'val_losses': val_losses}\n",
    "\n",
    "def analyze_layer_activations(model, data_loader, norm_type, affine, save_path):\n",
    "    \"\"\"\n",
    "    Analyze pre and post activations of each layer in the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.track_pre_activations = True\n",
    "    \n",
    "    # Collect pre and post activations\n",
    "    all_pre_activations = []\n",
    "    all_post_activations = []\n",
    "    \n",
    "    # First, do a single forward pass to determine the structure\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # Flatten the inputs for MLP\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            _, activations = model(inputs)\n",
    "            break\n",
    "    \n",
    "    # Initialize lists with the correct number of layers\n",
    "    all_pre_activations = [[] for _ in range(len(model.pre_activations))]\n",
    "    all_post_activations = [[] for _ in range(len(activations))]\n",
    "    \n",
    "    # Now collect all activations\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(data_loader, desc=\"Collecting activations\"):\n",
    "            inputs = inputs.to(device)\n",
    "            # Flatten the inputs for MLP\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            _, activations = model(inputs)\n",
    "            \n",
    "            # Store pre activations\n",
    "            for i, pre_act in enumerate(model.pre_activations):\n",
    "                all_pre_activations[i].append(pre_act)\n",
    "            \n",
    "            # Store post activations\n",
    "            for i, post_act in enumerate(activations):\n",
    "                all_post_activations[i].append(post_act)\n",
    "    \n",
    "    # Concatenate batches\n",
    "    all_pre_activations = [torch.cat(pre_acts, dim=0) for pre_acts in all_pre_activations]\n",
    "    all_post_activations = [torch.cat(post_acts, dim=0) for post_acts in all_post_activations]\n",
    "    \n",
    "    # Analyze each layer\n",
    "    results = []\n",
    "    \n",
    "    # Ensure we only analyze layers where we have both pre and post activations\n",
    "    n_layers = min(len(all_pre_activations), len(all_post_activations))\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        pre_act = all_pre_activations[layer_idx]\n",
    "        post_act = all_post_activations[layer_idx]\n",
    "        \n",
    "        # Compute covariance matrices\n",
    "        pre_cov = np.cov(pre_act.cpu().numpy(), rowvar=False)\n",
    "        post_cov = np.cov(post_act.cpu().numpy(), rowvar=False)\n",
    "        \n",
    "        # Compute effective ranks\n",
    "        pre_rank = compute_effective_rank(pre_cov)\n",
    "        post_rank = compute_effective_rank(post_cov)\n",
    "        rank_improvement = post_rank - pre_rank\n",
    "        \n",
    "        # Test Gaussianity\n",
    "        pre_gaussianity = test_gaussianity(pre_act)\n",
    "        post_gaussianity = test_gaussianity(post_act)\n",
    "        \n",
    "        # Layer info\n",
    "        layer_info = {\n",
    "            'layer_idx': layer_idx,\n",
    "            'pre_rank': pre_rank,\n",
    "            'post_rank': post_rank,\n",
    "            'rank_improvement': rank_improvement,\n",
    "            'pre_gaussianity': pre_gaussianity,\n",
    "            'post_gaussianity': post_gaussianity,\n",
    "            'norm_type': norm_type,\n",
    "            'affine': affine\n",
    "        }\n",
    "        \n",
    "        results.append(layer_info)\n",
    "        \n",
    "        # Create and save visualization plots\n",
    "        plot_dir = f\"{save_path}/norm_{norm_type}_affine_{affine}\"\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        \n",
    "        # Plot feature distributions\n",
    "        pre_fig = plot_feature_distributions(\n",
    "            pre_act, f\"Layer {layer_idx} Pre-Activation\", \n",
    "            f\"{norm_type}, affine={affine}\"\n",
    "        )\n",
    "        pre_fig.savefig(f\"{plot_dir}/layer_{layer_idx}_pre_dist.png\")\n",
    "        plt.close(pre_fig)\n",
    "        \n",
    "        post_fig = plot_feature_distributions(\n",
    "            post_act, f\"Layer {layer_idx} Post-Activation\", \n",
    "            f\"{norm_type}, affine={affine}\"\n",
    "        )\n",
    "        post_fig.savefig(f\"{plot_dir}/layer_{layer_idx}_post_dist.png\")\n",
    "        plt.close(post_fig)\n",
    "        \n",
    "        # Plot covariance matrices\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "        \n",
    "        sns.heatmap(pre_cov, ax=axes[0], cmap='coolwarm')\n",
    "        axes[0].set_title(f\"Pre-Activation Covariance\\nEff Rank: {pre_rank:.2f}\")\n",
    "        \n",
    "        sns.heatmap(post_cov, ax=axes[1], cmap='coolwarm')\n",
    "        axes[1].set_title(f\"Post-Activation Covariance\\nEff Rank: {post_rank:.2f}\")\n",
    "        \n",
    "        plt.suptitle(f\"Layer {layer_idx}, {norm_type}, affine={affine}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{plot_dir}/layer_{layer_idx}_covariance.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_experiment(input_dim=3072, hidden_dims=[128, 64, 32], output_dim=10, \n",
    "                  activation=nn.ReLU, batch_size=128, epochs=5, save_path=\"../outputs/rank_preservation_cifar10\"):\n",
    "    \"\"\"\n",
    "    Run the experiment for different normalization configurations using CIFAR-10\n",
    "    \"\"\"\n",
    "    # Load CIFAR-10 data\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    train_dataset, val_dataset = load_cifar10_subset(n_train=50000, n_val=10000)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Normalization configurations to test\n",
    "    configs = [\n",
    "        {'norm_type': None, 'affine': False, 'name': 'No Normalization'},\n",
    "        {'norm_type': 'bn', 'affine': False, 'name': 'BN without affine'},\n",
    "        {'norm_type': 'bn', 'affine': True, 'name': 'BN with affine'},\n",
    "        {'norm_type': 'ln', 'affine': False, 'name': 'LN without affine'},\n",
    "        {'norm_type': 'ln', 'affine': True, 'name': 'LN with affine'}\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {config['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = MLP(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            output_dim=output_dim,\n",
    "            activation=activation,\n",
    "            norm_type=config['norm_type'],\n",
    "            affine=config['affine']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train model\n",
    "        train_metrics = train_model(model, train_loader, val_loader, epochs=epochs)\n",
    "        \n",
    "        # Analyze activations\n",
    "        layer_results = analyze_layer_activations(\n",
    "            model, val_loader, config['norm_type'], config['affine'], save_path\n",
    "        )\n",
    "        \n",
    "        # Add to all results\n",
    "        for res in layer_results:\n",
    "            res['config_name'] = config['name']\n",
    "            all_results.append(res)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def plot_combined_results(results, save_path=\"../outputs/rank_preservation_cifar10\"):\n",
    "    \"\"\"\n",
    "    Create summary plots comparing different normalization strategies\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create plots directory\n",
    "    os.makedirs(f\"{save_path}/summary\", exist_ok=True)\n",
    "    \n",
    "    # 1. Rank improvement by layer and normalization type\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='layer_idx', y='rank_improvement', hue='config_name', data=df)\n",
    "    plt.title('Rank Improvement (Post - Pre) by Layer and Normalization')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Rank Improvement')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/summary/rank_improvement.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Pre-activation Gaussianity by layer and normalization type\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='layer_idx', y='pre_gaussianity', hue='config_name', data=df)\n",
    "    plt.title('Pre-Activation Gaussianity by Layer and Normalization')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Fraction of Features with Gaussian Distribution')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/summary/pre_gaussianity.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Correlation: Gaussianity vs Rank Improvement\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='pre_gaussianity', y='rank_improvement', \n",
    "                   hue='config_name', style='layer_idx', s=100, data=df)\n",
    "    plt.title('Correlation: Pre-Activation Gaussianity vs Rank Improvement')\n",
    "    plt.xlabel('Pre-Activation Gaussianity')\n",
    "    plt.ylabel('Rank Improvement')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/summary/gaussianity_vs_improvement.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Pre and Post effective rank by layer and normalization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    sns.barplot(x='layer_idx', y='pre_rank', hue='config_name', ax=axes[0], data=df)\n",
    "    axes[0].set_title('Pre-Activation Effective Rank')\n",
    "    axes[0].set_xlabel('Layer Index')\n",
    "    axes[0].set_ylabel('Effective Rank')\n",
    "    axes[0].grid(True, axis='y')\n",
    "    \n",
    "    sns.barplot(x='layer_idx', y='post_rank', hue='config_name', ax=axes[1], data=df)\n",
    "    axes[1].set_title('Post-Activation Effective Rank')\n",
    "    axes[1].set_xlabel('Layer Index')\n",
    "    axes[1].set_ylabel('Effective Rank')\n",
    "    axes[1].grid(True, axis='y')\n",
    "    \n",
    "    plt.suptitle('Effective Rank by Layer and Normalization', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_path}/summary/effective_ranks.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Average results table\n",
    "    avg_results = df.groupby('config_name').agg({\n",
    "        'pre_gaussianity': 'mean',\n",
    "        'post_gaussianity': 'mean',\n",
    "        'pre_rank': 'mean',\n",
    "        'post_rank': 'mean',\n",
    "        'rank_improvement': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(\"\\nAverage results across all layers:\")\n",
    "    print(avg_results)\n",
    "    \n",
    "    # Save as CSV\n",
    "    avg_results.to_csv(f'{save_path}/summary/average_results.csv', index=False)\n",
    "    \n",
    "    return avg_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create output directories\n",
    "    save_path = \"../outputs/rank_preservation_cifar10\"\n",
    "    os.makedirs(f\"{save_path}\", exist_ok=True)\n",
    "    \n",
    "    # Run experiments\n",
    "    print(\"Starting normalization experiments with CIFAR-10...\")\n",
    "    all_results = run_experiment(\n",
    "        input_dim=3072,  # 32x32x3 for CIFAR-10 flattened images\n",
    "        hidden_dims=[128,]*5,\n",
    "        output_dim=10,\n",
    "        activation=nn.ReLU,\n",
    "        batch_size=128,\n",
    "        epochs=3,  # Fewer epochs for faster experiment\n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # Plot summary results\n",
    "    avg_results = plot_combined_results(all_results, save_path=save_path)\n",
    "    \n",
    "    print(f\"Experiment completed! Results and visualizations are saved in the '{save_path}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
