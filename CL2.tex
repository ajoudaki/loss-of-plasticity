\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{margin=1in}


\newcommand{\Loss}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\He}{\mathrm{He}}

\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{remark}{Remark}[section]

\title{On a Mathematical Understanding of Loss of Plasticity}
\date{}

\begin{document}

\maketitle

\section{What is loss of plasticity}
We can generally define loss of plasticity as a situation where a model parameters $\theta$ exploration of the entire space $\Theta$ will be limited to a sub-space  that is substantially smaller. More formally:

\begin{definition}[Loss of plasticity]
    Given a model with parameters $\theta$ and parameter space $\Theta$ with a manifold $\mathcal{M} \subseteq \Theta$ where the gradient flow is tangent to the manifold:
    \begin{align}
        \forall \theta \in \mathcal{M} \implies \nabla_\theta\mathcal{L}(\theta) \in T_\theta\mathcal{M}
    \end{align}
    
    We classify the stability of this manifold based on the Hessian's properties in the normal directions:
    \begin{align}
        &\text{Stable manifold:}\ \forall \theta \in \mathcal{M}, \forall v \in N_\theta\mathcal{M} \setminus \{0\} \implies v^T\nabla_\theta^2 \mathcal{L}(\theta)v > 0 \\
        &\text{Unstable manifold:}\ \forall \theta \in \mathcal{M}, \forall v \in N_\theta\mathcal{M} \setminus \{0\} \implies v^T\nabla_\theta^2 \mathcal{L}(\theta)v < 0 \\
        &\text{Saddle manifold:}\ \forall \theta \in \mathcal{M}, \exists v_1, v_2 \in N_\theta\mathcal{M} \setminus \{0\} \text{ s.t. } v_1^T\nabla_\theta^2 \mathcal{L}(\theta)v_1 > 0 \text{ and } v_2^T\nabla_\theta^2 \mathcal{L}(\theta)v_2 < 0
    \end{align}


    where $T_\theta\mathcal{M}$ is the tangent space and $N_\theta\mathcal{M}$ is the normal space to $\mathcal{M}$ at point $\theta$.
\end{definition}

\begin{remark}[Local vs. global loss of plasticity. ]
 If the conditions hold for a specific parameter $\theta,$ but not necessarily over the entire manifold, we say that the manifold is stable/unstable/saddle \emph{locally}, while if the conditions hold over the entire manifold, we say that the conditions hold \emph{globally}. 
\end{remark}

We can make the following remark:
\begin{remark}
    If a model has lost its plasticity, we can conclude that, once the model reaches any point of the manifold $\mathcal{M},$ it will never escape it under gradient flow. More formally: 
    % \begin{align*}
    %     \theta(t_0) \in \mathcal{M} \implies \theta(t) \in \mathcal{M} \quad \forall t \geq t_0
    % \end{align*}
    % where $\theta(t)$ follows the gradient flow equation:
    \begin{align*}
        \frac{d\theta(t)}{dt} = -\nabla_\theta \mathcal{L}(\theta(t)) \implies \theta(t) = \theta(t_0) + \int_{t_0}^t -\nabla_\theta \mathcal{L}(\theta(t))  \in \mathcal{M}   \quad \forall t \geq t_0
    \end{align*}
\end{remark}
As a consequence, if we follow the gradient flow, we will always move on the lower-dimensional manifold.

Another version of this remark reveals the different types of stability of loss of plasticity, and their implication:

\begin{remark}[Perturbation dynamics orthogonal to the manifold]
    Let $\theta_0 \in \mathcal{M}$ and consider a small perturbation in the normal direction $\epsilon v$, where $v \in N_{\theta_0}\mathcal{M}$ with $\|v\| = 1$ and $\epsilon > 0$ small. 
    
    Under gradient flow dynamics, the instantaneous rate of change of the squared distance to the manifold can be expressed as:
    \begin{align}
        \left.\frac{d}{dt}\right|_{t=0} \text{dist}^2(\theta_0 + \epsilon v, \mathcal{M}) = -2\epsilon \cdot v^T\nabla_{\theta}^2\mathcal{L}(\theta_0)v + O(\epsilon^2)
    \end{align}
    
    Therefore,  we can conclude that, if the manifold curvature is positive/convex in the perturbation direction,  
    \begin{itemize}
        \item If $v^T\nabla_{\theta}^2\mathcal{L}(\theta_0)v > 0$ (stable direction), the distance decreases
        \item If $v^T\nabla_{\theta}^2\mathcal{L}(\theta_0)v < 0$ (unstable direction), the distance increases
        \item If $v^T\nabla_{\theta}^2\mathcal{L}(\theta_0)v = 0$ (neutral direction), higher-order terms determine the behavior
    \end{itemize}
    Another conclusion is that if a manifold curvature is positive (convex) in all directions orthogonal to the manifold, all perturbations will be reverted back, if is negative (concave) in all direction, it will escape in all directions, and if it is a saddle point, it may escape or revert back to the manifold depending on the direction of the perturbation. 
\end{remark}

\begin{remark}[Structural loss of plasticity. ]
If this properties hold for \emph{any} data and label distribution, we say this property holds 
\end{remark}



There is arguably a stronger notion of loss of plasticity: that even if we perturb the model slightly, the model will revert back to the manifold:



\section{Can we prove existence of such a manifold?}


Before we give our theorem on existence of such manifold, let us first formally define a network: 


\begin{definition}[Feed-forward Neural Network]
    A feed-forward neural network is defined by a directed acyclic graph $G = (V, E)$ where:
    \begin{align}
        V_{\text{in}} &= \{v \in V : \text{in}(v) = \emptyset\} & \text{(input nodes)} \\
        V_{\text{out}} &= \{v \in V : \text{out}(v) = \emptyset\} & \text{(output nodes)} \\
        h(v) &= 
        \begin{cases}
            x_v, & \text{if } v \in V_{\text{in}} \\
            f_v\Big(\sum_{u \in \text{in}(v)} w(u, v) \cdot h(u)\Big), & \text{otherwise}
        \end{cases}
    \end{align}
    where $x \in \mathbb{R}^{|V_{\text{in}}| \times d_0}$ are the inputs, $w(u, v) \in \mathbb{R}$ are the weights, $f_v$ the activation function or a mix of activation and normalization layers, and the network output is given by $h(V_{\text{out}})$.
\end{definition}

\begin{definition}[Backpropagation]
    Given a feed-forward neural network defined by a DAG $G = (V, E)$, input-label pairs $(x, y)$, and a loss function $\mathcal{L}(h(V_{\text{out}}), y)$, the gradient computation proceeds as follows:
    
    First, we define the error signal $\delta(v)$ for each node $v \in V$:
    \begin{align}
        \delta(v) &= 
        \begin{cases}
            \frac{\partial \mathcal{L}}{\partial h(v)}, & \text{if } v \in V_{\text{out}} \\
            \sum_{u \in \text{out}(v)} \delta(u) \cdot w(v, u) \cdot f'_u(z(u)), & \text{otherwise}
        \end{cases}
    \end{align}
    
    Then, the gradient of the loss with respect to each weight is:
    \begin{align}
        \frac{\partial \mathcal{L}}{\partial w(u, v)} = \delta(v) \cdot f'_v(z(v)) \cdot h(u)
    \end{align}
    
    where $z(v) = \sum_{u \in \text{in}(v)} w(u, v) \cdot h(u)$ is the pre-activation value at node $v$ and $f'_v$ is the derivative of the activation function.
\end{definition}

\subsection{Frozen units and frozen upstream parameters}
In addition to the duplication-based mechanisms discussed above, another common route to loss of plasticity arises when certain units become saturated (i.e., they lie in a regime where the activation's derivative is zero or effectively zero). In such cases, the gradient with respect to the parameters upstream of those units is also zero, causing those parameters to remain frozen under gradient-based updates. Below we formalize this phenomenon and then illustrate how it can emerge in common activation and softmax layers.

% \subsubsection{Formal Freezing Proposition}
Let $\mathcal{M}$ be a module with parameters $\Theta$, whose output includes at least one activation $h_i(\cdot)$ that depends on some subset of parameters $\Theta_{\text{upstream}} \subset \Theta$. Suppose that for all training inputs $x\in \mathcal{D}$, we have
\begin{align*}
\frac{\partial \,h_i(x;\Theta)}{\partial \,z} \;=\; 0
\end{align*}
for the relevant pre-activation $z$, meaning $h_i(\cdot)$ is in a saturated regime throughout the dataset $\mathcal{D}$. In standard backpropagation, the gradient of the loss $\mathcal{L}$ with respect to any $\theta \in \Theta_{\text{upstream}}$ must pass through $\frac{\partial h_i}{\partial z}$. Hence
\begin{align*}
\frac{\partial \,\mathcal{L}}{\partial \,\theta}
\;=\;
\frac{\partial \,\mathcal{L}}{\partial \,h_i}
\;\cdot\;
\underbrace{
\frac{\partial \,h_i}{\partial \,z}
}_{0}
\;\cdot\;
\frac{\partial \,z}{\partial \,\theta}
\;=\; 0.
\end{align*}
Thus, all updates to $\theta$ are zero.

\begin{proposition}[Saturated Unit $\implies$ Frozen Upstream Parameters]
\label{prop:SaturatedUnitsFrozenUpstream}
If $\mathcal{M}$ has an activation $h_i(\cdot)$ that is fully saturated on the training set $\mathcal{D}$ (i.e., its derivative is zero everywhere on $\mathcal{D}$), then for every parameter $\theta\in \Theta_{\text{upstream}}$ that influences $h_i(\cdot)$ solely via $z$, we have
\begin{align*}
\Delta \theta \;=\; 0
\end{align*}
under standard gradient-based training. Consequently, these parameters remain frozen in subsequent updates, eliminating any possibility that they can learn new features downstream.
\end{proposition}

Implication for Loss of Learning Capacity: This immediately implies a loss of learning capacity, in the sense introduced earlier: the subspace of parameters $\Theta_{\text{upstream}}$ is no longer free to change, effectively reducing the dimensionality of possible new representations. Even if the rest of the network continues training, these upstream parameters $\Theta_{\text{upstream}}$ are locked, preventing the saturated unit from ever contributing novel features in the future.

\subsubsection{How Saturation Arises in Common Modules}
Below are several typical scenarios in which the derivative of a neuron or sub-block becomes zero, thus freezing a portion of upstream weights:
\begin{enumerate}
    \item Sigmoid / Tanh Neurons: For large positive (or negative) inputs $z$, the derivative of $\sigma(z)$ (resp.\ $\tanh(z)$) becomes extremely small. If the network parameters evolve such that certain neurons consistently receive large-magnitude inputs for all samples in $\mathcal{D}$, then $\partial \sigma/\partial z \approx 0$. By Proposition~\ref{prop:SaturatedUnitsFrozenUpstream}, the weights feeding into those saturated neurons receive no gradient update, permanently freezing them.
    \item ReLU Neurons (``Dead ReLUs''): A $\mathrm{ReLU}(z)=\max(0,z)$ has zero derivative for $z<0$. If a ReLU neuron's input is always negative across $\mathcal{D}$, its activation is zero and its derivative is zero. The upstream weights for that neuron stop updating. Unless there is a mechanism like a slight negative slope (Leaky ReLU) or random reinitialization, such neurons remain inactive indefinitely.
    \item Softmax with Dominant Logit: In a classification layer $\mathbf{p}=\mathrm{softmax}(\mathbf{z})$, if one logit $z_k$ is consistently very large compared to others, then $\mathrm{softmax}(\mathbf{z})\approx(0,\dots,1,\dots,0)$. For class $j\neq k$, the partial derivative $\partial \ell/\partial z_j$ becomes negligible or zero, thus freezing the upstream weights of the other classes. Over time, those ``suppressed logits'' never recover.
    \item Attention Mechanisms: In multi-head attention, a $\mathrm{softmax}$ is used over the key-query scores. If certain queries/keys produce consistently negligible attention weights across all data, the gradient to those heads can vanish. Hence, those attention heads remain ``dead,'' contributing little to the representation---another form of locked capacity.
    \item Amplified by Normalization: Although batch/layer normalization often helps prevent saturation, it can also interact with large weights to push some neurons into near-constant outputs. If, for instance, a scaling factor $\gamma$ becomes huge, the post-norm activation could saturate, further cementing zero derivatives and locking upstream parameters.
\end{enumerate}
These cases all illustrate how saturating a neuron (or logit) effectively removes it from future learning, thereby reducing the network's plasticity. As with the duplication arguments, the parametric condition of zero derivative is stable once reached---barring external noise, re-initialization, or other forms of intervention that might unsaturate the unit. Hence, saturated units provide another concrete, mathematically precise route to loss of learning capacity in overparameterized networks.


\subsection{Loss of plasticity due to cloned units }
Next, we can state the main proposition on loss of plasticity as a result of cloned units:  

\begin{proposition}
    Let $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{W})$ be a large feed-forward neural network with $|\mathcal{V}| = n$ nodes and weight function $\mathcal{W}: \mathcal{E} \rightarrow \mathbb{R}$. Let $G = (V, E, W)$ be a small feed-forward neural network with $|V|$ nodes and weight function $W: E \rightarrow \R$.
    
    Suppose there exists a partition $\{S_v\}_{v \in V}$ of $\mathcal{V}$ such that:
    \begin{align*}
        &\bigcup_{v \in V} S_v = \mathcal{V} && S_u \cap S_v = \emptyset \quad \forall u \neq v
    \end{align*}
    with the input and output nodes having their own partition $\mathcal{V}_{in} = V_{in},\mathcal{V}_{out} = V_{out} . $
    
    Furthermore, suppose the weight functions satisfy the following compression relationship:
    \begin{align}
        &W(S_x,S_y ) = \sum_{z\in S_x}\mathcal{W}(z,y) && \text{incoming weight condition}  \\
        &W(S_x,S_y ) = \sum_{z\in S_y}\mathcal{W}(x,z) && \text{outgoing weight condition}
    \end{align}
    
    Then, if $\mathcal{W}_0$ represents the starting point of the parameters, Under this conditions and with any \emph{gradient-descent based optimization} and with \emph{any data and label distribution}, will be constrained to the following manifold with dimensionality $|E| < |\mathcal{E}|$. More specifically, there is some parameter trajectory $ W_t: E \to \R $ corresponding to the smaller model, such that the larger model parameters will be given by 
    \begin{align*}
        \mathcal{W}_t(u,v) =  \mathcal{W}_0 +  W_t(S_u,S_v) .
    \end{align*}
\end{proposition}

\begin{remark}
    Note that the loss of plasticity established above is \emph{structural}, because the proposition states when the conditions for the loss are met, any gradient descent-based approach will remain restricted to the manifold. 
\end{remark}

\begin{remark}
    The conditions for the graph cloning can be prescribed by an affine manifold with dimension $|E|$ in an otherwise $|\mathcal{E}|$ dimensional space. Thus, this proves that being anywhere on this manifold that will lead to loss of plasticity. 
\end{remark}

\begin{corollary}[Architectural/Training extensions]
    While the proposition is stated for a feed-forward network with linear and activation units only, we can extend its architectural components with the following ways: 
    \begin{itemize}
        \item Bias: we can add bias to the linear units by augmenting the input units with an always $1$ unit, and then adding an edge between this input unit and any unit that needs a bias. Because bias is just like a weight times $1$ unit. 
        \item CNN: while the proposition resembles the definition of an MLP unit, the channels in a CNN module can be effectively viewed as a single unit: in fact, we can view each channel as application of a MLP unit on various patches of the input channels. Thus, in the CNN context, we can form and define the equivalences between channels as opposed to neurons in a MLP. 
        \item Softmax/RMSNorm/LayerNorm layers: in the proposition, the definition of each unit only allows for linear and element-wise activation units. This does not include units that rely non-linearly on a number of features is not be directly covered under the current proposition. For this type of modules, we can create an ad-hoc low-dimensional alternative that is aware of the multiplicity of the duplicated units. More specifically, if a unit takes $n$-dimensional input, and these input features are divided into $m$ partitions: $S_1\dot\cup \dots \dot S_m = \{1,\dots, n\}, $ then we can define the low dimensional counter-part, for example for LayerNorm  we have  
        \begin{align*}
            \text{low-dim-LN}(x)_i = \frac{(x_i-\mu(x))}{\sqrt{var(x)}} \quad 
            \mu(x) = \sum_{i\le m} |S_i| h_m, \quad var(x) =\sum_{i\le m} |S_i| (h_i-\mu_i )^2
        \end{align*}
        where $h_i$ is the value of the input partition $S_i$
        and for softmax we have
        \begin{align*}
            \text{low-dim-Softmax}(x) = \frac{e^{x_i}}{\sum_{j\le m} |S_j| e^{h_j}}
        \end{align*}
        where $h_j$ is the value of the input partition $S_j$.
    \end{itemize}
    We can also extend the proposotion in terms of optimization algorithm:
    \begin{itemize}
        \item SGD with mini-batch gradients: while the definition of the proposition is stated for a single sample, adding multiple sample gradients will be simply the sum of multiple gradients that has the cloning structure. And because addition will preserve the weight cloning structure, the gradients of a SGD optimization will not alter the results. 
        \item SGD with momentum/ Adam: While the statement was given for a vanilla gradient, any subsequent gradient statistics such as momentum or Adam stats will also follow a similar cloning structure, and thus, using momentum or Adam will not alter the results.  
    \end{itemize}
\end{corollary}


\begin{proof}[Proof of proposition]

    \textbf{Forward cloning.} First we prove that for any input $x \in \R^{|V_{in}|},$ any node in each partition will have similar values:
    \begin{align*}
        \forall S \in \{S_1,\dots,S_{|V|}\} \implies \forall u,v \in S, h(u) = h(v). 
    \end{align*}
    We do so by induction over the directed distance from inputs:
    Suppose that for all nodes where $dist(V_{in},v) \le i$ in the larger graph, we have that $h(v)$ is constant across each partition: 
    \begin{align*}
        &T_i:= \{v\in \mathcal{V}: dist(V_{in},v) \le i\} && \text{nodes $i$-steps from input}\\
        &\forall S \in \{S_1,\dots,S_{|V|}\}, \; \forall u,v \in (S\cap T_i): h(v) = h(u)  && \text{induction hypothesis}
    \end{align*}
    First, note that the induction is trivially valid for $i=0,$ because inputs in the larger and smaller model coincide. 
    For some partition $S$, consider all nodes $v \in S$ whose distance from the input is $dist(\mathcal{V}_{in},v) \le i+1.$ Then, by the induction hypothesis know that all their incoming units in the same partition will have a similar value. Furthermore, because of the incoming weight condition, we know that the sum of incoming weights from each partition is similar for all these units. Thus, their weights are essentially a re-distribution between units with similar values and thus make no difference. As a result, their values will also be identical, which proves the induction step. Thus, we have proven the forward cloning of the units. 
    
    \textbf{Backward cloning. } We now prove a very similar result but for the backward $\delta(v)$ of all units:
        \begin{align*}
        \forall S \in \{S_1,\dots,S_{|V|}\} \implies \forall u,v \in S, \delta(u) = \delta(v). 
    \end{align*}
    We prove this similar to forward, but the induction is defined as steps from the output. In other words, we prove it first for the output nodes, and then prove it for nodes that are 1,2,... steps away from it: 
        \begin{align*}
        &T_i:= \{v\in \mathcal{V}: dist(v,V_{in}) \le i\} && \text{nodes $i$-steps from input}\\
        &\forall S \in \{S_1,\dots,S_{|V|}\}, \; \forall u,v \in (S\cap T_i): \delta(v) = \delta(u)  && \text{induction hypothesis}
    \end{align*}
    Again, this is trivially true for $i=0,$ because output units are coinciding between the two networks, and we have already established that the forward pass values are similar for these units in the forward cloning step. Thus, the backward errors of the output units will also be cloned. Now, suppose that we have the induction hypothesis for $i,$ and want to prove it for $i+1.$ Consider units $u,v$ in some partition $S,$ that are at most $i+1$ steps away from the output units. By induction hypothesis, all their outgoing units will be cloned, i.e., have similar values across each partition. Now, given the outgoing weight condition, the sum of the weights to each outgoing partition is equal for $u,v.$ Thus, we can view their outgoing weights as redistributing weights between units in same partition. Because by induction hypothesis these units have similar backward values, this redistribution of weights will not change the value of the unit, and thus, we will always have $\delta(v) = \delta(u),$ which proves the induction step.

    \textbf{Weight gradient cloning.} Now that we have proven forward and backward cloning, we can easily prove that given two partitions $S, S',$ any two units from these partitions will have similar weight gradients:
    \begin{align*}
        S,S' \in \{S_1,\dots, S_{|V|}\} \text{ then } \forall u,u' \in S, v,v'\in S': \frac{\partial \Loss}{\partial W(u,v)}=\frac{\partial \Loss}{\partial W(u',v')}.
    \end{align*}
    In other words, the gradient of weights between any units in two partitions will be constant across partitions. More importantly, applying this gradient step will not violate the forward and backward symmetry conditions, and thus the next steps will also have similar gradients. Because these gradients have block-wise structure determined by the partition, and change from initialization can be described a by a weight matrix $W(S_u,S_v)$ whose dimensions is only the number of edges between the partitions $|E|$ rather than the original model $|\mathcal{E}|$. Furthermore, this lower-dimensional manifold can be described as an affine sub-space int he space of all possible parametres. 
\end{proof}



\section{What happens with prolonged training?}

Consider the MLP with the following setting: 
\begin{align*}
    &h^\ell := f(\tilde{z}^\ell ), \\
    &\tilde{z}^\ell := \text{normalize}(z^\ell)  && \text{RMS/BN/LN normalization} \\
    & z_\ell:= W^\ell h^{\ell-1}, \\
    & W^\ell_{i,j}\sim N(0,1/d)
\end{align*}
where $h^0:= x \in R^{d\times n}$ represents a batch of $n$ inputs of dimension $d.$ We further assume that $z_L$ represents the final output of dimension $1\times n,$ which we use to define the loss, thus we have:
\begin{align*}
    b^\ell &:= \partial z^L/\partial z^\ell && \text{backward errors}\\
    % &b_L := z_L, && \text{last layer errors}\\
    &b^\ell := f'(z^\ell) \circ ((W^{\ell+1})^\top b^{\ell+1}) && \text{recursive formula}
\end{align*}
where $\circ$ denotes element-wise Hadamard product. 


\paragraph{Flat loss. } After prolonged training, we have a flat landscape. This means that if $L(\theta)$ represents the loss around the parameters $\theta,$ then moving in many directions will not change the loss value: $\langle v, \partial L/\partial \theta\rangle \approx 0 ,$ for many directions $v. $  In some settings, we also expect ``neural collapse'' to occur, which means that the features will progressively collapse to a few class-related directions in the feature directions? 
\begin{align*}
    &G_h^\ell := \frac1d [h^\ell (h^\ell)^\top] \to^{\text{(as $\ell$ grows)}} \text{low-rank} && \text{forward Gram, neural collapse}\\
    &G_b^\ell := \frac1d [b^\ell (b^\ell)^\top ]\to^{\text{(as $\ell$ grows)}}\text{low-rank} && \text{backward Gram, flat loss }
\end{align*}

Now, the question is , how can we implement these forward and backward low-dimensional Gram matrices? The following proposition attempts to shed some light on this, by proposing that non-Gaussianity, or some very specific structures, are needed to implement this. 

\begin{proposition}
    Suppose that each feature, accross the feature dimension can be viewed as a Gaussian random variable. Namely, pre-activations $z^\ell_{\cdot i}$ as drawn from a standard Gaussian distribution. Now, let us suppose the following is true:
\begin{itemize}
    \item The inputs are normalized to have RMS norm $1,$ and are non-duplicate. 
    \item The non-linearity $f$ has stable forward, if it receives Gaussian pre-activations, it remains bounded $\E_{x\sim N(0,1)} f(x)^2 < \infty, $
    and it is not a bounded degree polynomial (e.g. it is not a quadratic or cubic function). 
    \item The weight matrices can be viewed Gaussian matrices, with norm of each row and column equal to $1,$ but there is no duplicate incoming or outgoing units, but otherwise with any arbitrary row and column covariance structure. In otherwise, the row and column covariance of the weights has unit diagonals, and off-diagonals strictly less than one. 
    \item the right singular vectors of $W^\ell$ span the entire column space of $h^{\ell-1}$ for all $\ell.$
\end{itemize}
Under the conditions above, we have that the Gram matrix of forward and backward matrices remain full-rank. 
\end{proposition}

\begin{proof}
\textit{forward Gram.} Suppose that we start with a matrix $H$ and multiply it with Gaussian matrix $W,$ and pass it through an activation layer $f( W H).$ Thus, the gram matrix would be $\frac1d f(WH)^\top f(WH).$ Because $W$ is Gaussian, then $W H$ is also Gaussian. And because no columns of $H$ are duplicate elements, and that the right singular vectors of $W$ span their column space, the projection $W H $ will not create duplicate elements. More precisely, in order to have $Wh_i = W h_j,$ we either need that the two columns $h_i= h_j,$ or that their difference vector is orthogonal to the weights: $W (h_i - h_j ) = 0,$ which implies that rows of $W$ do not span the columns of $H.$ Therefore, we can conclude that columns of $WH $ are non-duplicate. Now, because of RMS normalization, we have $\text{LN}(W H)$ that is approximately Gaussian, and each column RMS norm is $1,$ and there are no duplicate columns. Under these conditions, $LN(WH)$ will be distributed as Gaussian with Gram matrix $\frac1d LN(WH)^\top LN(WH) = \frac1d W^\top W,$ which we assumed to have unit diagonals and off-diagonals less than one. Thus, we can now invoke the following theorem to argue that $f(WH)$ will be full rank. As a consequence, we can say that, with the condition that no duplicate columns exist in our last year activation, and that the row covariance of the weight matrix has unit diagonals and no duplicate rows, the next layer activations will be full rank and its covariance will also be full rank. 

\begin{lemma}
\label{prop:RankPreservation}
Let $ f: \mathbb{R} \to \mathbb{R}$ be square-integrable with respect to a Gaussian kernel, that is, $\mathbb{E}_{X\sim \mathcal{N}(0,1)}[f(X)^2] < \infty$, and suppose that $f$ is not a bounded degree polynomial. Let $C\in \mathbb{R}^{n\times n}$ be positive semidefinite with $C_{ii}=1$ and $|C_{ij}|<1$ for $i\neq j$. Define jointly Gaussian variables $x_1,\dots,x_n$ with covariance $\mathbb{E}[x_ix_j]=C_{ij}$. Then setting $y_i = f(x_i)$, the covariance matrix $M$ with the entries $M_{ij}=\mathbb{E}[f(x_i)f(x_j)]$ is of full rank, even if $C$ is degenerate.
\end{lemma}

\begin{proof}[Proof of lemma]
Because $f$ is square-integrable, it has an infinite Hermite expansion $f(z)=\sum_{k=0}^{\infty} b_k\, \He_k(z)$. Mehler's formula implies that $\mathbb{E}[\He_k(x_i)\He_\ell(x_j)]$ vanishes unless $k=\ell$, in which case it scales like $C_{ij}^k$. Summing across $k$, we get 
\begin{align*}
\mathbb{E}[f(x_i)f(x_j)] 
= 
\sum_{k=0}^{\infty} b_k^2\,C_{ij}^k.
\end{align*}
Since $b_k\neq 0$ for infinitely many $k$, and $|C_{ij}|<1$, eventually the Hadamard powers $C^{\odot k}$ are strictly positive semidefinite (by Gershgorin arguments), forcing the resulting sum to be positive definite. Thus, $M$ is full-rank.
\end{proof}

\end{proof}


\textit{Backward gram.} Now, we want to argue that the backward Gram matrices are also full rank. First of all, we can argue similar to the forward Gram that the following activation derivative grams will also be full-rank:
\begin{align*}
    \frac1d f'(z^\ell)^\top f'(z^\ell). 
\end{align*}
This is because $f'$ will also have a Hermite polynomial expansion and its series expansion will also be infinitely long (not a bounded degree polynomial). 

Now, if we assume that the random-ness from higher layers is independent from the lower layers, we can conclude that the covariance of $f'(z^\ell)\circ ((W^{\ell+1})^\top b^{\ell+1})$ will be a Hadamard product of covariances of $f'(z^\ell)$ and $(W^{\ell+1})^\top b^{\ell+1}$: 
\begin{align*}
    \E [\frac1d (b^{\ell})^\top b^\ell] = (\E \frac1d f'(z^\ell)^\top f'(z^\ell)) \circ [(\E\frac1d (W^{\ell+1})^\top b^\ell)^\top (\E\frac1d (W^{\ell+1})^\top b^\ell)]
\end{align*}
Here, we can use Oppenheim's inequality to argue that as long as no columns of $W^\ell$ are zero, the resulting covariance will be full-rank. 

\textit{Induction.} So far, we only argued that if the conditions hold up to a certain layer in the forward Gram, the next layer will be full rank. But this is only shown in expectation. To make the result hold deterministically, we can assume the mean field (infinite width regime), where we can assume that  Gram will converge to its expected value. 




\section{Introduction and Background}



In the influential perspective of Richard Sutton, two major obstacles hinder truly continual deep learning:
(1) Catastrophic forgetting: old knowledge is overwritten.
(2) Loss of plasticity: neural networks, despite large overparameterization, may not adapt to new tasks after extended training. In loss of plasticity, a large, deep,, and complicated model may exhibit learning capacity of a network that is far shallower and has fewer active parameters. Despite much research on practical ways to understand loss of plasticity, prevent it, and recover from it, there is currently no mathematical understanding of these notions that will make these intuitions about loss of plasticity more rigorous. Our goal is to provide a mathematical foundation to clarify how loss of plasticity manifests itself, why it persists, and how it might be mitigated and recovered.

\subsection{Objectives}
We aim to unify these lines of thought into a coherent theory of loss of plasticity. Our overarching questions:
\begin{itemize}
    \item Sufficient conditions: what are the conditions a large network's parameters cause it to lose its learning capacity for future tasks?
    \item Persistence: why are these conditions persistent and not changing with gradient-based updates?
    \item Emergence: Why and how do these conditions emerge as a result of prolonged training?
    \item Prevention and recovery: What architectural and training configurations will help prevent loss of plasticity, and what can we do to recover from it?
\end{itemize}

\section{Background}

Through a series of works, Richard Sutton and collaborators studied how standard backpropagation in nonstationary or incremental environments (class-incremental tasks, slowly changing regression, RL with shifting friction) causes the network's ability to learn new tasks to degrade over time: Weights grow in magnitude, a fraction of neurons become effectively ``dead'' or ``saturated'', and internal representations lose their diversity (e.g., a drop in effective rank).
In experiments, they found that even large architectures eventually learn no better than shallow or linear models, highlighting an irreversible collapse of plasticity. Sutton proposed to periodically ``refresh'' part of the network (via random re-initialization or ``continual backpropagation'') to restore plasticity, mirroring how biological systems may sustain continued adaptability.

Before outlining our main theoretical results, we summarize how they relate to the points raised in Richard Sutton's extensive empirical and conceptual work:
\begin{itemize}
    \item Saturated or Dead Neurons: Our formal conditions do not require saturation or ``dying'' units. We show that duplication of features (via blockwise weight sums) can also enforce a collapsed representation.
    \item Growing Weights: Large magnitudes can cause saturations in certain activations, effectively replicating the same output for multiple neurons. This is one route to plasticity loss, but not the only one.
    \item Rank Deficits: Empirically, Sutton reports a progressive decline in the rank of hidden representations. Our theoretical viewpoint interprets rank deficit as a special case of ``duplication'' or ``blocked'' structure in the parameterization: once weights sum up in certain ways, the network's effective dimension of representation remains small.
    \item Continual Randomness: Adding noise (or reinitializing a fraction of neurons) can break the parametric constraints that lock the big network to a smaller capacity. Sutton's ``continual backpropagation'' is a prime example of systematically injecting randomness to avoid or reverse these constraints.
\end{itemize}
Thus, the condition-based approach we propose can be seen as a more general, unified perspective on the ways that neural networks can lose plasticity, going beyond the narrower phenomena of saturations or dead neurons.

\section{Theoretical result: sufficient conditions for loss of plasticity}

In the sequel, we propose a formal definition of loss of plasticity, first with a global parametric perspective on the entire model, and later with a modular perspective for typical building blocks (fully connected layers, LayerNorm, RMSNorm, softmax, etc.), showing how each can lock itself to a smaller version. This modular approach allows us to study modules in isolation and later extend our theory to a model that is built upon these modules. We conclude by drawing parallels to Sutton's empirical observations and discussing open questions about how these conditions might emerge spontaneously during training.

\subsection{Preliminary}

We first articulate a precise condition that captures both the collapse of representation and the locking of learning dynamics. This is essential for bridging the observed phenomenon notion with a rigorous parametric statement.

\begin{definition}[Loss of Plasticity]
\label{def:loss_of_plasticity}
Let $\mathcal{N}$ be a large neural network with parameter set $\Theta$. We say $\mathcal{N}$ has lost plasticity if there exists a parametric condition $\mathcal{C} \subseteq \Theta$ such that:
\begin{enumerate}
    \item Persistence: Once $\Theta$ enters $\mathcal{C}$ at some training time, standard gradient-based updates (e.g., SGD, Adam) keep $\Theta \in \mathcal{C}$ indefinitely, unless explicitly perturbed (e.g., reinit).
    \item Loss of Representation: The hidden-unit activations in $\mathcal{N}$ can be perfectly reconstructed from a smaller network $\mathcal{N}_{\text{small}}$, i.e., the bigger model's intermediate features add no new representational capacity.
    \item Loss of Learning: The parameter updates $\Delta \Theta$ in $\mathcal{N}$ coincide with those of $\mathcal{N}_{\text{small}}$, up to a known blockwise scaling in the learning rates. Equivalently, the bigger network's training trajectory is a ``duplicate'' of the smaller network's, offering no additional degrees of freedom.
\end{enumerate}
\end{definition}

Under this definition, once condition $\mathcal{C}$ is met, the large network cannot exploit its nominal extra parameters to learn new tasks more effectively. This state precisely matches the notion that the network becomes ``frozen'' to a smaller capacity: the forward pass is the same, and the gradient pass yields no new directions beyond those used by a smaller net.

The persistence clause ensures that such a state is stable---loss of plasticity is not just a transient phenomenon.

\subsection{Forward-Backward Symmetry (FBS) Conditions}
We now present the core concept: a set of FBS conditions on a large network's parameters that imply (1) it replicates a smaller network's forward pass, and (2) it replicates that smaller network's backward pass (gradients), up to blockwise scaling factors. Once in such an FBS condition, the network's training trajectory is effectively identical to that of the smaller network. This yields a direct realization of the definition of loss of plasticity in Definition~\ref{def:loss_of_plasticity}.

\begin{proposition}[Forward-Backward Symmetry Conditions]
\label{prop:FBS_conditions}
Consider a feed-forward neural network expressed as a directed acyclic graph (DAG) of parametric or non-parametric layers. Suppose there is a smaller DAG such that:
\begin{enumerate}
    \item Each node (or set of neurons) in the large DAG is partitioned so multiple big-layer neurons are duplicates of a single smaller-layer neuron.
    \item The incoming edges from each duplicate block in the large DAG sum up to the smaller DAG's single edge value for all duplications, and the same for outgoing edges.
    \item Node-local parameters (biases, scale shifts, etc.) are likewise copied or shared within each block.
\end{enumerate}
Then:
\begin{itemize}
    \item The large DAG's forward output blockwise equals the smaller DAG's output.
    \item The backward gradients also replicate, so under blockwise scaling of learning rates, the large DAG updates track the smaller DAG.
    \item This parametric constraint remains invariant under gradient updates with the appropriate scaling, so the large network remains locked to the smaller one.
\end{itemize}
Hence, the large network satisfies both loss of representation and loss of learning, fulfilling Definition~\ref{def:loss_of_plasticity}.
\end{proposition}

\begin{proof}[Proof Sketch]
One performs a topological sort over the DAG from inputs to outputs. For each block-partition, the sum-of-rows/columns ensures the big node sees exactly the same net input as the smaller node would. The activation thus matches up to repeated coordinates, implying forward duplication. A reverse topological argument using the standard chain rule reveals that partial derivatives also replicate blockwise. Scaling the learning rate by the product of input-block and output-block multiplicities keeps the parametric sums intact. Therefore, once the network enters such an FBS condition, it remains so under gradient updates.
\end{proof}

\begin{remark}
In older terminology, these constraints were referred to as a ``covering.'' We prefer the term FBS conditions here, to emphasize the forward and backward duplication at the parametric level.
\end{remark}

\subsection{Modular Approach: Instantiation to Standard Layers}
While describing FBS conditions at a global scale is correct, it can be tedious to verify every edge's row/column sum constraints. Modern networks are typically composed of standard modules: fully connected layers, normalizations, residuals, softmax, etc. Each module can be shown to satisfy FBS conditions if we impose appropriate parametric forms (e.g., row-sum duplication for a matrix, identical LN scale factors across blocks). By stacking or composing these modules, we obtain the entire network in an FBS condition.

We present technical derivations for ``mixing modules'' (fully connected, LN, RMSNorm, softmax) that demonstrate how each can lock itself to a smaller version.

\subsubsection{Fully Connected (FC) Layers}
\begin{align*}
  \mathcal{M}(\mathbf{x};W,b)
  =
  W\,\mathbf{x} + b, 
  \quad
  W\in\mathbb{R}^{D_{\text{out}}\times D_{\text{in}}},\;
  b\in \mathbb{R}^{D_{\text{out}}}.
\end{align*}
If we want to replicate a smaller shape $(n \to m)$, we duplicate the input dimension $i$ $\alpha_i$-fold and the output dimension $j$ $\beta_j$-fold.

\begin{proposition}[FC Layer under FBS]
\label{prop:FC_FBS}
Let $\mathbf{x}\in \mathbb{R}^{D_{\text{in}}}$ partition into blocks $\{S_1,\dots,S_n\}$ with $|S_i|=\alpha_i$, and let the output partition be $\{T_1,\dots,T_m\}$ with $|T_j|=\beta_j$. Suppose:
\begin{enumerate}
    \item $\sum_{u\in S_i}W_{v,u} = G_{j,i}$ for all $v\in T_j$.
    \item $\sum_{v\in T_j}W_{v,u} = G_{j,i}$ for all $u\in S_i$.
    \item $b_v = c_j$ for $v\in T_j$.
\end{enumerate}
Then if the input is blockwise duplicated, the output is also blockwise duplicated, matching the smaller FC's forward pass. Under blockwise learning rate $\eta_{(i,j)}=\eta_{\text{small}}/(\alpha_i\beta_j)$, these constraints remain invariant, so the larger FC layer is locked to the smaller one and cannot exceed its representational or learning capacity.
\end{proposition}

\begin{proof}[Proof Sketch]
Grouping the input coordinates in blocks $S_i$, the row-sum ensures the net input from block $S_i$ to row $v$ in block $T_j$ is $G_{j,i}$ times the block's partial activation. The same logic applies in reverse for the gradient. Summing partial derivatives in each block reproduces the smaller FC's gradient. The row/column sums are preserved if each parameter update is scaled by $\frac{1}{\alpha_i\beta_j}$.
\end{proof}

\subsubsection{LayerNorm}
A layer norm for a vector $\mathbf{x}\in\mathbb{R}^d$ is
\begin{align*}
  \mathrm{LN}(\mathbf{x};\gamma,\beta)
  \;=\;
  \gamma\odot \frac{\mathbf{x}-\mu(\mathbf{x})}{\sigma(\mathbf{x})} + \beta,
\end{align*}
where $\mu,\sigma$ are the mean and standard deviation across coordinates.

\begin{proposition}[LayerNorm under FBS]
\label{prop:LN_FBS}
Let $\mathrm{LN}_{\text{small}}\colon \mathbb{R}^{m}\to\mathbb{R}^{m}$ with scale $\gamma'$ and shift $\beta'$. Suppose we build a ``big'' LN of dimension $d=\sum_{j=1}^m |T_j|$ by:
\begin{enumerate}
    \item For each coordinate $k\in T_j$, set $\gamma_k=\gamma'_j$ and $\beta_k=\beta'_j$.
    \item Replace the LN's mean and standard deviation by weighted versions that sum over $|T_j|$ duplicates.
\end{enumerate}
Then for block-duplicated inputs, the LN output is also blockwise constant, matching $\mathrm{LN}_{\text{small}}$. The partial derivatives with respect to scale/shift replicate the smaller LN's, and under blockwise learning rate $\frac{1}{|T_j|}$, these constraints persist. Hence the LN is locked to the smaller LN dimension.
\end{proposition}

\begin{proof}[Proof Sketch]
When each block $\mathbf{x}_{T_j}$ is identical, the weighted mean and variance mirror the smaller LN's single dimension. The scale $\gamma_k=\gamma'_j$ and shift $\beta_k=\beta'_j$ ensure the same output. The gradient merges across $|T_j|$ coordinates. Under scaling $\eta_{j} = \eta_{\text{small}} / |T_j|$, the duplication remains stable.
\end{proof}

\subsubsection{RMSNorm}
RMSNorm is similar but normalizes by the root-mean-square:
\begin{align*}
  \mathrm{RMSNorm}(\mathbf{x};\gamma)
  \;=\;
  \gamma \odot \frac{\mathbf{x}}{\sqrt{\tfrac{1}{d}\sum_{k=1}^d x_k^2 + \epsilon}}.
\end{align*}

\begin{proposition}[RMSNorm under FBS]
\label{prop:RMS_FBS}
If we partition the dimension $d$ into blocks $\{\beta_1,\dots,\beta_m\}$, replicate $\gamma_k=\gamma'_j$ across each block, and compute a weighted RMS across the blocks, then for blockwise duplicated $\mathbf{x}$, the RMSNorm output is a direct replicate of the smaller RMSNorm dimension. The gradients also match blockwise, so a suitable learning rate scaling keeps the RMSNorm locked to the smaller dimension.
\end{proposition}

\begin{proof}[Proof Outline]
The argument is identical to that of LayerNorm except for the omission of mean-centering. The partial derivatives replicate identically across each block, and the RMS is computed with the same weighted structure.
\end{proof}

\subsubsection{Softmax}
Finally, consider a softmax $\mathrm{softmax}(\mathbf{z})\in\mathbb{R}^d$. We can force each block to have the same logit value, effectively replicating a smaller softmax dimension:
\begin{align*}
  p_i 
  =
  \frac{e^{z_i}}{\sum_{k=1}^d e^{z_k}}.
\end{align*}
When block $T_j$ shares the same $z_k$, the exponentials are duplicated, summing in a weighted fashion to replicate the smaller denominator. The gradients $\partial p/\partial z$ also replicate. Coupled with a preceding FC duplication, this yields a classification layer locked to that of a smaller classifier.


\section{Emergence, Prevention, and Recovery from Loss of Plasticity}

We now discuss the why and how these low-capacity structures appear in prolonged training, then outline strategies to prevent or recover from them.

\subsection{Emergence (Why?)}
Empirical and theoretical studies (see, e.g., neural collapse \cite{Papyan2020neural}, low-rank expansions\cite{Arora2019finegrain}, flat-minima arguments \cite{Hochreiter1997flat, Keskar2017large}, and related work) indicate that standard SGD training on large networks tends to converge to ``flat'' regions of the loss landscape. In these regions, the network ``shaves off'' any input variability that is uncorrelated with the training objective, focusing only on features necessary for the classification or regression task at hand.

This progressive loss of information not strictly needed for the current task often manifests as a drop in the rank or diversity of layer representations: once certain features become irrelevant for the task, the layer outputs can collapse onto a lower-dimensional subspace. Over successive tasks (or prolonged training on nonstationary data), the network can lose more and more representational capacity.

For example, in a classification problem with classes $\{0,1,\dots,9\}$, once the network has specialized on distinguishing 0 vs. 1, it may discard internal features relevant to other digits if they are not present in the training distribution at that time. The net result is effectively a rank-deficient representation---even though the raw architecture remains large.

Such rank deficiency (or duplication of features, or saturations) is a hallmark that the network is discarding ``unnecessary'' signals to reduce loss, inadvertently harming its ability to learn future tasks. The direct cause: The SGD preference for wide, flat minima, which arises from the geometry of high-dimensional optimization \cite{Hochreiter1997flat, Chaudhari2019entropy} and from certain regularization or batch effects. Once the model is in a low-rank region, reversing that collapse can be difficult without explicit noise or re-initialization.

\subsection{Emergence (How?): A Rank-Preserving Proposition and Its Implications}
A key theoretical clue is that, if layer preactivations remain roughly Gaussian, a single non-polynomial nonlinearity should preserve or even expand rank, absent additional constraints. The following proposition encapsulates a rank-preservation principle:

\begin{proposition}
\label{prop:RankPreservation}
Let $ f: \mathbb{R} \to \mathbb{R}$ be square-integrable with respect to a Gaussian kernel, that is, $\mathbb{E}_{X\sim \mathcal{N}(0,1)}[f(X)^2] < \infty$, and suppose that $f$ is not a bounded degree polynomial. Let $C\in \mathbb{R}^{n\times n}$ be positive semidefinite with $C_{ii}=1$ and $|C_{ij}|<1$ for $i\neq j$. Define jointly Gaussian variables $x_1,\dots,x_n$ with covariance $\mathbb{E}[x_ix_j]=C_{ij}$. Then setting $y_i = f(x_i)$, the covariance matrix $M$ with the entries $M_{ij}=\mathbb{E}[f(x_i)f(x_j)]$ is of full rank, even if $C$ is degenerate.
\end{proposition}

\begin{proof}[Proof Sketch]
Because $f$ is square-integrable, it has an infinite Hermite expansion $f(z)=\sum_{k=0}^{\infty} b_k\, \He_k(z)$. Mehler's formula implies that $\mathbb{E}[\He_k(x_i)\He_\ell(x_j)]$ vanishes unless $k=\ell$, in which case it scales like $C_{ij}^k$. Summing across $k$, we get 
\begin{align*}
\mathbb{E}[f(x_i)f(x_j)] 
= 
\sum_{k=0}^{\infty} b_k^2\,C_{ij}^k.
\end{align*}
Since $b_k\neq 0$ for infinitely many $k$, and $|C_{ij}|<1$, eventually the Hadamard powers $C^{\odot k}$ are strictly positive semidefinite (by Gershgorin arguments), forcing the resulting sum to be positive definite. Thus, $M$ is full-rank.
\end{proof}

This result implies that purely applying a non-polynomial activation to Gaussian-like data does not yield true rank collapse. Therefore, in a realistic deep net to get lower rank, we must either:
\begin{enumerate}
    \item Create Duplicate Units: The FBS phenomenon, or blockwise summation.
    \item Departure from Gaussian: e.g., heavy-tailed distributions, saturations, or forced linearization near zero.
\end{enumerate}
Prolonged SGD commonly nudges the network into such duplication or saturation modes, especially when certain features become unnecessary for the current objective. Hence actual rank deficiency arises not from the nonlinearity alone, but from the interplay of flat-minima seeking in high-dimensional spaces and either neuron duplications or saturations.

In summary, typical deep nets, left unregularized, can drift to low-capacity solutions via:
\begin{itemize}
    \item Linear/Vanishing: effectively linearizing an activation by forcing small input signals,
    \item Saturated: large inputs for certain neurons, or
    \item Duplicate Units: blockwise FBS constraints.
\end{itemize}
We cannot guarantee these exhaust all forms of plasticity loss, but they represent the major classes consistent with known rank-based phenomena and empirical observations.

\subsection{How to Prevent Loss of Plasticity}
Because we can interpret each mechanism in a mechanical manner, we have avenues to mitigate or prevent them:
\begin{itemize}
    \item Breaking Duplicate Units:
    \begin{itemize}
        \item Dropout: By randomly zeroing activations, dropout breaks forward/backward symmetries among neurons, reducing the chance that two units remain identical.
        \item Noise Injection: More generally, injecting small random perturbations in the updates can prevent perfect duplication from forming.
    \end{itemize}
    \item Avoiding Saturation:
    \begin{itemize}
        \item Batch Normalization: BN or similar normalizations reduce the chance that a neuron consistently sees large or small inputs, thus preventing saturation or purely linear regimes.
        \item Limiting Affine Transform: If BN has a learnable scale/shift, these could grow too large and reintroduce saturations. One might disable or constrain these affine parameters to preserve anti-saturation benefits.
        \item Using activations that don't have saturation points in one or both directions (such as \texttt{LeakyReLU} instead of \texttt{ReLU}).
        \item Using smaller learning rates will likely lead to smaller updates and less chance of strong saturation.
        \item Using weight decay may prevent weights from getting too large. 
        \item Using weight normalization: weight normalization has a similar effect to BN with affine parameters on the pre-activations, but operates on the weights rather than the features. 
    \end{itemize}
    \item Maintaining Non-Gaussian:  
    The rank-preservation Proposition~\ref{prop:RankPreservation} indicates that if pre-activations remain ``Gaussian-like,'' a single non-polynomial layer preserves rank. However, real networks deviate from ideal assumptions.
    \begin{itemize}
        \item Using BN on pre-activations pushes them closer to a Gaussian distribution.
        \item Stronger normalizations that match higher moments (beyond mean, variance) might reduce duplication or saturations, though these are less common in practice.
    \end{itemize}
\end{itemize}
Interestingly, dropout remains one of the few standard modules that explicitly breaks symmetries across duplicated neurons. Additional modules (e.g., layer-level random resets) could be invented to perform a similar role.

\subsection{How to Recover from Loss of Plasticity}
When a network is already ``locked'' into low plasticity, we need to break the symmetrical or saturated parametric constraints explicitly:
\begin{itemize}
    \item Dropout: Even if duplication formed, applying dropout can slowly disrupt identical pathways. But complete duplication may require more aggressive interventions.
    \item Reinitializing/Resetting: Sutton's ``continual backprop'' proposes reinitializing a fraction of neurons. This forcibly breaks block-sum constraints or saturations.
    \item Noise in the Backward Process: Manually injecting noise into gradient updates can ``kick'' the parameters out of stable duplications.
\end{itemize}
While BN or LN might help reduce saturations, they cannot fix an already perfectly duplicated block. In general, any method that disrupts the stable sum-of-weights or sum-of-gradients condition can help the network ``escape'' from a severely collapsed state.

\section{Discussion}

\subsection{Parallels to Sutton's Empirical Observations}

\begin{remark}[Rank Collapse vs. Duplication]
Sutton's experiments often measure a decline in the rank of hidden representations. In our theory, the exact block-partition duplication is a strict form of rank collapse: if certain neurons are literal ``copies'' of each other, the hidden representation's rank can never exceed that of the smaller sub-network. Dead or saturated neurons are a more extreme or different subset of rank collapse; duplications can occur even without saturations.
\end{remark}

\begin{remark}[Recovery of Lost Capacity]
As Sutton suggests, re-initializing a small fraction of neurons (selective reinit) or injecting noise breaks the FBS conditions. Once a row/column sum constraint is broken, the large model can deviate from the smaller sub-network's gradient path. This resonates with the ``continual injection of diversity'' argument that random reinit is crucial to sustaining plasticity.
\end{remark}

\begin{remark}[Are Dead Neurons Necessary?]
Our approach clarifies that dead neurons are not necessary for losing plasticity. A big module can replicate a smaller module purely via duplication constraints, with all neurons active---yet it still fails to exhibit more capacity than the smaller. This shows that the phenomena Sutton observed (dead neurons, big weights) are sufficient but not strictly required conditions for plasticity loss.
\end{remark}

\subsection{Open Questions}

Weight decay, learning rate, new 

\paragraph{Emergence of These Conditions.} A core theoretical gap is how an FBS condition might spontaneously emerge under standard training. While we show that once duplication is perfect, it persists, the question remains: do realistic training processes converge to or approximate such duplication? In practice, partial duplication or partial saturations may suffice to degrade plasticity. The architecture, optimization hyperparameters, and regularization techniques all likely influence whether these conditions form.

\paragraph{Universal vs. Persistent Conditions} We interpret the FBS constraints as a family of canonical conditions for loss of plasticity: they precisely produce a smaller sub-network's forward and backward pass, and remain stable under uniform or blockwise learning rates. Some additional conditions, e.g., saturations or large weights, also produce collapse but do not necessarily map as neatly to an FBS partition. A unifying quest is to identify the minimal set of parametric constraints that are both universal (covering all ways to lose plasticity) and persistent (once formed, they do not vanish).

% Beyond Basic Modules: Our analysis extends readily to CNN filters, multi-head attention blocks, or more advanced normalizations, so long as each can be expressed as a function that mixes inputs in a definable pattern. In principle, the same blockwise sum logic can be applied. A thorough exposition for Transformers or large-scale architectures would be valuable future work.

\section*{References}
\begin{itemize}
    \item R. Sutton's works on continual learning and ``Continual Backpropagation.''
    \item Hyper cloning papers analyzing noiseless vs. noisy expansions of trained nets.
    \item Studies on normalization layers (e.g., LN, RMSNorm) and their role in optimization and rank preservation.
    \item Neural Collapse phenomena in classification \cite{Papyan2020neural}.
    \item Flat minima arguments \cite{Hochreiter1997flat, Keskar2017large}.
    \item Low-rank expansions \cite{Arora2019finegrain,Chaudhari2019entropy}.
\end{itemize}

\end{document}
