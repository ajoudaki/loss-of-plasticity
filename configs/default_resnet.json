{
    "layers": [2, 2, 2, 2],
    "base_channels": 64,
    "activation": "relu",
    "dropout_p": 0.1,
    "use_batchnorm": true,
    "norm_after_activation": false,
    "normalization_affine": true
}