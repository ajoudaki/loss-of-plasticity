{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1270f6db",
   "metadata": {},
   "source": [
    "## facts \n",
    "The Jacobian of a normalied vector \n",
    "\\begin{align*}\n",
    "\\nabla_x (x/\\|x\\|) = 1/\\|x\\| (I - \\tilde x \\tilde x^\\top ), && \\tilde x := x/\\|x\\|\n",
    "\\end{align*}\n",
    "The Jacobian of a RMS-normalied vector \n",
    "\\begin{align*}\n",
    "\\nabla_x (x/\\|x\\|_{rms}) = 1/\\|x\\|_{rms} (I - \\tilde {x} \\tilde {x}^\\top )  && \\tilde x := x/\\|x\\|\n",
    "\\end{align*}\n",
    "A simple formula is to keep in mind that \n",
    "$$\n",
    "x/\\|x\\|_{rms} =\\frac{1}{\\sqrt{n}} \\frac{x}{\\|x\\|}\n",
    "$$\n",
    "\n",
    "# simple model \n",
    "we consider one layer of a neural network where $x_1,\\dots, x_n\\sim N(0,I_d)$ as rows of nxd matrix $X$, and the output backwards are $\\delta_1,\\dots,\\delta_n \\sim N(0,I_d)$ where $n$ is the batch size.  We also have $d\\times d$ matrix $W$ with elements drawn from $W_{ij}\\sim N(0,1/d)$. And we want to understand gradient dynamics of a single linear layer, possibly with a normalization: \n",
    "- Linear only \n",
    "$$\n",
    "y := z:= W X  \n",
    "$$\n",
    "- Linar with batch norm: \n",
    "$$\n",
    "y = a\\cdot z, \\qquad h:=BN(h), h:= W X , \\quad BN(x) = x/ \\|x\\|_{rms} \\quad \\text{ per feature (row) $x$ of input batch $X$}\n",
    "$$\n",
    "- Linear with weight norm:\n",
    "$$\n",
    "y = a\\cdot z, \\qquad z:=\\widetilde{W} x, \\widetilde{w}_r = w_r/\\|w_r\\|\\quad  \\text{per feature (row) of weight matrix}\n",
    "$$\n",
    "- Linear with LayerNorm (LN):\n",
    "$$\n",
    "y = a \\cdot z \\qquad z:= LN(h), h:=W x, LN(x) = x / \\|x\\|_{rms} \\qquad \\text{ per each sample (column) of $X$}\n",
    "$$\n",
    "in both BN and WN,  $a=(1)^d$ are the scale parameters that are learnable. \n",
    "the output backwards for y are assumed to be $\\delta$ (or $\\delta_i$ for sample $x_i$), which are used to compute gradients for other parameters. \n",
    "\n",
    "## Question: \n",
    "For linear, BN, and WN, find $\\partial z_k / \\partial w_k$ given $X_k$ whichthe k-th column of $X$. For LN, given $x_i$ which is the i-th row of $X$ (it corresponds to all features of sample $i$ ), find $\\partial z_k / \\partial W$, this is because in LN, the parameters depend on all the weight matrix not just one row. \n",
    "After you find these formula, find the matrix or tensor representatio for the overall gradients $\\partial z / \\partial W$ given the entire data batch this time. Try find a simple clean and concise formula for each. \n",
    "\n",
    "\n",
    "Summarize the formula you found for each case in a table in the end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc5cae",
   "metadata": {},
   "source": [
    "\n",
    "##  Linear only \n",
    "We have $y = W x$. Thus, the gradients for a single sample  and backward $x,\\delta$ are \n",
    "\\begin{align*}\n",
    "&dW_i = \\delta_i x_i^\\top && \\text{single sample}\\\\\n",
    "&dW = \\sum_{i} \\delta_i x_i^\\top && \\text{whole batch}\n",
    "\\end{align*}\n",
    "\n",
    "## Linear with BN \n",
    "Let's consider one output first $BN(w \\cdot x )$ with $w$ representing one row of $W$, and $\\delta$ is the backward corresponding to this output. We have \n",
    "\\begin{align*}\n",
    "dw_k &= a_k \\delta ^\\top (\\partial BN(X w_k)/\\partial w_k)  &&\\text{w k is a row of x but represented as a column} \\\\\n",
    "&= a_k \\delta ^\\top(\\frac{\\partial BN(X w_k)}{\\partial (X w_k)} \\frac{\\partial X w_k }{\\partial w_k}) \\\\\n",
    "&= a_k \\delta ^\\top\\left(\\frac{1}{\\|X w_k\\|_{rms}}\\left(I - (X w_k)(X w_k)^\\top / \\|X w_k\\|^2\\right)  X\\right)   \\\\\n",
    "&= a_k \\delta ^\\top\\left(\\frac{1}{\\|X w_k\\|_{rms}} (X - X w_k w_k^\\top X^\\top X / \\|X w_k\\|^2  ) \\right)\\\\\n",
    "&= a_k \\delta^\\top \\frac{1}{\\|h_k\\|_{rms}} \\left(I - h_k h_k^\\top /\\|h_k\\|^2\\right) X && h_k := X w_k \n",
    "\\end{align*}\n",
    "ANd for scale \n",
    "\\begin{align*}\n",
    "da_k &= \\delta^\\top BN(X w_k) = \\delta^\\top h_k / \\|h_k\\|_{rms}\n",
    "\\end{align*}\n",
    "We can now write the full gradients by varying $k$ in $1,\\dots, d$. \n",
    "\n",
    "## Linear with WN\n",
    "Let us first WN-based gradient this for one particular sample $x_i$ which is $i$-th row of $X$, and $k$-th feature output featoure weights, which is $w_k$ as defined before. The output here is definedas\n",
    "\\begin{align*}\n",
    "dw_k &= a_k \\delta_k \\frac{\\partial\\tilde w_{k,i} x_{i,k}}{\\partial w_k} \\\\\n",
    "&= a_k \\delta_k x_{i,k} \\frac{\\partial\\tilde{w}_{k,i} }{\\partial w_k} \n",
    "\\end{align*}\n",
    "\n",
    "## Linear with LN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a6205",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
